## Comparison Update

> ##比较更新

In the intervening years, the weaknesses of the Core i7 and Tesla GTX 280 have been addressed by their successors. Intel’s ACV2 added gather instructions, and AVX/512 added scatter instructions, both of which are found in the Intel Skylake series. Nvidia Pascal has double-precision floating-point performance that is onehalf instead of one-eighth the speed of single precision, fast atomic operations, and caches.

> 在随后的几年中，其继任者已经解决了核心 i7 和特斯拉 GTX 280 的弱点。英特尔的 ACV2 添加了收集说明，AVX/512 添加了散点说明，这两者都在英特尔 Skylake 系列中找到。NVIDIA PASCAL 具有双精度的浮点性能，它是单一的，而不是单个精度，快速原子操作和缓存速度的八分之一。

[Figure 4.31](#_bookmark205) lists the characteristics of these two successors, [Figure 4.32](#_bookmark206) compares performance using 3 of the 14 benchmarks in the original paper (those were the ones for which we could find source code), and [Figure 4.33](#_bookmark207) shows the two new roofline models. The new GPU chip is 15 to 50 times faster and the new CPU chips is 50 times faster than their predecessors, and the new GPU is 2–5 times faster than the new CPU.

> [图 4.31](#_ bookmark205)列出了这两个继任者的特征，[图 4.32](#_ bookmark206)使用原始论文中的 14 个基准中的 3 个比较性能(这些是我们可以找到源代码的)，和[图 4.33](#_ Bookmark207)显示了两个新的车顶线模型。新的 GPU 芯片的速度快 15 到 50 倍，而新的 CPU 芯片比其前任快 50 倍，而新的 GPU 的速度比新 CPU 快 2-5 倍。

<span id="_bookmark204" class="anchor"></span> Fallacies and Pitfalls

> <span ID =“ _ bookmark204” class =“锚”> </span>谬论和陷阱

While data-level parallelism is the easiest form of parallelism after ILP from the programmer’s perspective, and plausibly the simplest from the architect’s perspective, it still has many fallacies and pitfalls.

> 从程序员的角度来看，数据级并行性是 ILP 之后的最简单的并行形式，并且从工程师的角度来看，最简单的是最简单的，但它仍然具有许多谬论和陷阱。

Fallacy _GPUs suffer from being coprocessors_.

> 谬误_gpus 遭受了协助程序的痛苦。

Although the split between main memory and GPU memory has disadvantages, there are advantages to being at a distance from the CPU.

> 尽管主内存和 GPU 内存之间的分裂存在缺点，但距离 CPU 的距离有优势。

Figure 4.32 Raw and relative performance measured for modern versions of the two platforms as compared to the relative performance of the original platforms. Like [Figure 4.30](#_bookmark203), SAXPY and DAXPY are used only as a measure of memory bandwidth, so the proper unit is GB/s and not GFLOP/s.

> 图 4.32 与原始平台的相对性能相比，针对两个平台的现代版本测量的原始和相对性能。像[图 4.30](#_ bookmark203)一样，Saxpy 和 Daxpy 仅作为记忆带宽的量度，因此适当的单元为 GB/S 而不是 GFLOP/S。

For example, PTX exists in part because of the I/O device nature of GPUs. This level of indirection between the compiler and the hardware gives GPU architects much more flexibility than system processor architects. It’s often hard to know in advance whether an architecture innovation will be well supported by compilers and libraries and be important to applications. Sometimes a new mechanism will even prove useful for one or two generations and then fade in importance as the IT world changes. PTX allows GPU architects to try innovations speculatively and drop them in subsequent generations if they disappoint or fade in importance, which encourages experimentation. The justification for inclusion is understandably considerably higher for system processors—and thus much less experimentation can occur—as distributing binary machine code normally implies that new features must be supported by all future generations of that architecture. A demonstration of the value of PTX is that the different generation architecture radically changed the hardware instruction set—from being memory-oriented like x86 to being register-oriented like RISC-V _as well as_ doubling the address size to 64 bits—without disrupting the NVIDIA software stack.

> 例如，PTX 之所以存在，部分原因是 GPU 的 I/O 设备性质。编译器和硬件之间的间接水平使 GPU 架构师比系统处理器架构师更灵活。通常很难提前知道建筑创新是否会得到编译器和库的支持，并且对应用程序很重要。有时，一种新的机制甚至将被证明对一两个世代有用，然后随着 IT 世界的变化而逐渐消失。PTX 允许 GPU 架构师尝试创新，如果他们令人失望或淡出重要性，则将其放置在后代，这鼓励了实验。可以理解，系统处理器的理由要高得多，因此可以进行较少的实验，因为分发二进制机器代码通常意味着必须由该架构的所有子孙后代支持新功能。PTX 值的演示是，不同的一代体系结构从根本上更改了硬件指令集，从内存等于 x86 到寄存器，像登记册一样，像 risc-v _是_的，将地址大小加倍至 64 位，而不会扰乱该地址。NVIDIA 软件堆栈。

Figure 4.33 Roofline models of older and newer CPUs versus older and newer GPUs. The higher roofline for each computer is single-precision floating-point performance, and the lower one is double-precision performance.

> 图 4.33 较新的和较新的 CPU 与较新的 GPU 的车顶线模型。每台计算机的较高车顶线是单精度的浮点性能，较低的是双重精确性的性能。

Pitfall _Concentrating on peak performance in vector architectures and ignoring start-up overhead_.

> 陷阱_对矢量体系结构中的峰值性能并忽略启动开销。

Early memory-memory vector processors such as the TI ASC and the CDC STAR100 had long start-up times. For some vector problems, vectors had to be longer than 100 for the vector code to be faster than the scalar code! On the CYBER 205— derived from the STAR-100—the start-up overhead for DAXPY is 158 clock cycles, which substantially increases the break-even point. If the clock rates of the Cray-1 and the CYBER 205 were identical, the Cray-1 would be faster until the vector length was greater than 64. Because the Cray-1 clock rate was also higher (even though the 205 was newer), the crossover point was a vector length over 100.

> 早期的内存内存矢量处理器(例如 Ti ASC 和 CDC Star100)的启动时间很长。对于某些向量问题，使矢量代码比标量代码更快！在网络 205(源自 STAR-100)上，Daxpy 的启动开销是 158 个时钟周期，这大大增加了分离点。如果 cray-1 和网络 205 的时钟速率相同，则 cray-1 将更快，直到向量长度大于 64。，跨界点是 100 范围以上的矢量长度。

Pitfall _Increasing vector performance, without comparable increases in scalar performance_.

> 陷阱_ increas vector 性能，无类似的标量性能_。

This imbalance was a problem on many early vector processors, and a place where Seymour Cray (the architect of the Cray computers) rewrote the rules. Many of the early vector processors had comparatively slow scalar units (as well as large startup overheads). Even today, a processor with lower vector performance but better scalar performance can outperform a processor with higher peak vector performance. Good scalar performance keeps down overhead costs (strip mining, for example) and reduces the impact of Amdahl’s law.

> 在许多早期矢量处理器上，这种不平衡是一个问题，也是西摩·克雷(Cray Computers 的工程师)重写规则的地方。许多早期的矢量处理器具有相对较慢的标量单元(以及大型启动开销)。即使在今天，具有较低矢量性能但标量更高的性能的处理器也可以胜过具有较高峰值向量性能的处理器。良好的标量性能会降低高架成本(例如，采矿)，并降低了 Amdahl 定律的影响。

An excellent example of this comes from comparing a fast scalar processor and a vector processor with lower scalar performance. The Livermore Fortran kernels are a collection of 24 scientific kernels with varying degrees of vectorization. [Figure 4.34](#_bookmark208) shows the performance of two different processors on this benchmark. Despite the vector processor’s higher peak performance, its low scalar performance makes it slower than a fast scalar processor as measured by the harmonic mean.

> 一个很好的例子是比较快速标量处理器和具有较低标量性能的矢量处理器。Livermore Fortran 内核是 24 个科学核的集合，具有不同程度的矢量化。[图 4.34](#_ bookmark208)显示了此基准测试中两个不同处理器的性能。尽管矢量处理器的峰值性能较高，但其低标量性能使其比谐波平均值衡量的快速标量处理器慢。

The flip of this danger today is increasing vector performance—say, by increasing the number of lanes—without increasing scalar performance. Such myopia is another path to an unbalanced computer.

> 如今，这种危险的翻转是在不增加标量性能的情况下提高向量性能(例如，通过增加车道的数量)。这样的近视是通往不平衡计算机的另一条途径。

The next fallacy is closely related.

> 下一个谬论密切相关。

Fallacy _You can get good vector performance without providing memory bandwidth_.

> 谬论您可以在不提供内存带宽的情况下获得良好的矢量性能。

As we saw with the DAXPY loop and the Roofline model, memory bandwidth is quite important to all SIMD architectures. DAXPY requires 1.5 memory references per floating-point operation, and this ratio is typical of many scientific codes. Even if the floating-point operations took no time, a Cray-1 could not increase the performance of the vector sequence used, because it is memory-limited. The Cray-1 performance on Linpack jumped when the compiler used blocking to change the computation so that values could be kept in the vector registers. This approach lowered the number of memory references per FLOP and improved the performance by nearly a factor of two! Thus the memory bandwidth on the Cray-1 became sufficient for a loop that formerly required more bandwidth.

> 正如我们在 Daxpy Loop 和车顶线模型中看到的那样，内存带宽对所有 SIMD 架构都很重要。Daxpy 需要每次浮点操作 1.5 内存参考，并且该比率是许多科学代码的典型特征。即使浮点操作没有时间，Cray-1 也无法提高所使用的向量序列的性能，因为它是内存限制的。当编译器使用阻止来更改计算时，Linpack 上的 Cray-1 性能跳跃，以便可以将值保存在向量寄存器中。这种方法降低了每个翻牌的内存参考数量，并提高了两倍的性能！因此，Cray-1 上的记忆带宽已足以使以前需要更多带宽的循环。

Figure 4.34 Performance measurements for the Livermore Fortran kernels on two different processors. Both the MIPS M/120-5 and the Stardent-1500 (formerly the Ardent Titan-1) use a 16.7 MHz MIPS R2000 chip for the main CPU. The Stardent1500 uses its vector unit for scalar FP and has about half the scalar performance (as measured by the minimum rate) of the MIPS M/120-5, which uses the MIPS R2010 FP chip. The vector processor is more than a factor of 2.5 faster for a highly vectorizable loop (maximum rate). However, the lower scalar performance of the Stardent-1500 negates the higher vector performance when total performance is measured by the harmonic mean on all 24 loops.

> 图 4.34 在两个不同处理器上的 Livermore Fortran 内核的性能测量。MIPS M/120-5 和 Stardent-1500(以前是 Ardent Titan-1)对主 CPU 使用 16.7 MHz MIPS R2000 芯片。Stardent1500 将其矢量单元用于标量 FP，并使用 MIPS M/120-5 的标量性能(按最低速率衡量)，该性能使用 MIPS R2010 FP 芯片。对于高度矢量循环(最大速率)，矢量处理器的速度快于 2.5 倍。但是，当通过所有 24 个回路上的谐波平均值衡量总表现时，Stardent-1500 的标量较低的性能否定了较高的矢量性能。

Fallacy _On GPUs, just add more threads if you don_’_t have enough memory performance_.

> 谬误_ON GPU，如果您没有足够的内存性能，只需添加更多线程即可。

GPUs use many CUDA Threads to hide the latency to main memory. If memory accesses are scattered or not correlated among CUDA Threads, the memory system will get progressively slower in responding to each individual request. Eventually, even many threads will not cover the latency. For the “more CUDA Threads” strategy to work, not only do you need lots of CUDA Threads, but the CUDA Threads themselves also must be well behaved in terms of locality of memory accesses.

> GPU 使用许多 CUDA 线程隐藏对主内存的延迟。如果内存访问在 CUDA 线程之间分散或不相关，则内存系统在响应每个单独的请求时会逐渐变慢。最终，即使许多线程也不会覆盖延迟。为了使“更多的 CUDA 线程”策略起作用，您不仅需要很多 CUDA 线程，而且 CUDA 线程本身也必须在内存访问的局部性方面表现得很好。
