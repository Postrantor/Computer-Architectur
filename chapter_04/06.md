## Cross-Cutting Issues

> ##交叉切割问题

### Energy and DLP: Slow and Wide Versus Fast and Narrow

> ### Energy and DLP：缓慢而宽，快速而狭窄

A fundamental power advantage of data-level parallel architectures comes from the energy equation in [Chapter 1](#_bookmark2). Assuming ample data-level parallelism, the performance is the same if we halve the clock rate and double the execution resources: twice the num- ber of lanes for a vector computer, wider registers and ALUs for multimedia SIMD, and more SIMD Lanes for GPUs. If we can lower the voltage while dropping the clock rate, we can actually reduce energy aswellasthe power for the computation while main- taining the same peak performance. Thus GPUs tend to have lower clock rates than system processors, which rely on high clock rates for performance (see [Section 4.7](#_bookmark198)). Compared to out-of-order processors, DLP processors can have simpler control logic to launch a large number of operations per clock cycle; for example, the con- trol is identical for all lanes in vector processors, and there is no logic to decide on multiple instruction issues or speculative execution logic. They also fetch and decode far fewer instructions. Vector architectures can also make it easier to turn off unused portions of the chip. Each vector instruction explicitly describes all the resources it needs for a number of cycles when the instruction issues.

> 数据级并行体系结构的基本功率优势来自[第 1 章]（#_ bookmark2）中的能量方程。假设有足够的数据级并行性，如果我们将时钟速率减半并翻倍，则性能是相同的：两倍的矢量计算机，较宽的寄存器和多媒体 SIMD 的 Alus，以及 GPU 的更多 SIMD LANES。如果我们可以在降低时钟速率的同时降低电压，那么我们实际上可以降低能量，同时进行计算的功率，同时进行相同的峰值性能。因此，GPU 的时钟速率往往比系统处理器低，该系统处理器依赖于高时钟速率的性能（请参阅[4.7]（#_ bookmark198））。与排序外处理器相比，DLP 处理器可以具有更简单的控制逻辑，可以在每个时钟周期启动大量操作。例如，对向量处理器中的所有车道的控制都是相同的，并且没有逻辑可以决定多个指令问题或投机执行逻辑。他们还获取并解码了更少的说明。向量体系结构还可以使关闭芯片的未使用部分更容易。每个向量指令都会明确描述当指令发出的许多周期所需的所有资源。

### Banked Memory and Graphics Memory

> ###银行内存和图形内存

[Section 4.2](#vector-architecture) noted the importance of substantial memory bandwidth for vector architectures to support unit stride, nonunit stride, and gather-scatter accesses.

> [第 4.2 节]（＃矢量架构）注意到，对向量体系结构的实质内存带宽对支持单位步伐，非单元步幅和聚会划分访问的重要性。

To achieve the highest memory performance, stacked DRAMs are used in the top-end GPUs from AMD and NVIDIA. Intel also uses stacked DRAM in its Xeon Phi product. Also known as _high bandwidth memory_ (_HBM_, _HBM2_), the memory chips are stacked and placed in the same package as the processing chip. The exten- sive width (typically 1024–4096 data wires) provides high bandwidth, while plac- ing the memory chips in the same package as the processor chip reduces latency and power consumption. The capacity of stacked DRAM is typically 8–32 GB.

> 为了达到最高的记忆性能，在 AMD 和 NVIDIA 的高端 GPU 中使用了堆叠的 DRAM。英特尔还在其 Xeon Phi 产品中使用堆叠的 DRAM。也称为* -high 带宽内存*（_hbm_，_hbm2_），内存芯片被堆叠并放在与处理芯片的相同包装中。扩展的宽度（通常为 1024–4096 数据线）提供高带宽，同时将内存芯片放在与处理器芯片相同的包装中，可减少延迟和功耗。堆叠 DRAM 的容量通常为 8-32 GB。

Given all the potential demands on the memory from both the computation tasks and the graphics acceleration tasks, the memory system could see a large number of uncorrelated requests. Unfortunately, this diversity hurts memory per- formance. To cope, the GPU’s memory controller maintains separate queues of traffic bound for different banks, waiting until there is enough traffic to justify opening a row and transferring all requested data at once. This delay improves bandwidth but stretches latency, and the controller must ensure that no processing units starve while waiting for data, for otherwise neighboring processors could become idle. [Section 4.7](#_bookmark198) shows that gather-scatter techniques and memory- bank-aware access techniques can deliver substantial increases in performance versus conventional cache-based architectures.

> 鉴于计算任务和图形加速任务对内存的所有潜在需求，内存系统可以看到大量不相关的请求。不幸的是，这种多样性损害了记忆力。为了应付，GPU 的内存控制器保持了为不同银行绑定的单独的流量队列，等到有足够的流量来证明打开一行并立即传输所有请求的数据。这种延迟改善了带宽，但会延长延迟，并且控制器必须确保在等待数据时不会挨饿，否则相邻处理器可能会闲置。[第 4.7 节]（#\_ bookmark198）表明，聚集筛分技术和内存 - 银行感知访问技术可以提供大量的性能和基于常规缓存的体系结构的大量提高。

### Strided Accesses and TLB Misses

> ###行驶的访问和 TLB 失误

One problem with strided accesses is how they interact with the translation looka- side buffer (TLB) for virtual memory in vector architectures or GPUs. (GPUs also use TLBs for memory mapping.) Depending on how the TLB is organized and the size of the array being accessed in memory, it is even possible to get one TLB miss for every access to an element in the array! The same type of collision can happen with caches, but the performance impact is probably less.

> 障碍的一个问题是，它们如何与矢量体系结构或 GPU 中虚拟内存的翻译外观侧缓冲区（TLB）进行交互。（GPU 还使用 TLB 进行内存映射。）取决于 TLB 的组织方式和内存中的数组的大小，甚至有可能让每个 TLB 失去一个 TLB 遗漏，以便对数组中的每个元素进行访问！缓存可能会发生相同类型的碰撞，但性能影响可能较小。

Given the popularity of graphics applications, GPUs are now found in both mobile clients and traditional servers and heavy-duty desktop computers. [Figure 4.26](#_bookmark199) lists the key characteristics of the NVIDIA Tegra Parker system on a chip for embedded clients, which is popular in automobiles, and the Pascal GPU for servers. GPU server engineers hope to be able to do live animation within five years after a movie is released. GPU-embedded engineers in turn want to do what a server or game console does today on their hardware within five more years.

> 鉴于图形应用程序的普及，现在在移动客户端和传统服务器和重型台式计算机中都可以找到 GPU。[图 4.26]（#\_ bookmark199）在嵌入式客户的芯片上列出了 Nvidia tegra Parker 系统的关键特征，该客户在汽车中很受欢迎，以及用于服务器的 Pascal GPU。GPU 服务器工程师希望能够在电影发行后的五年内进行实时动画。反过来，GPU 包裹的工程师希望在五年内完成服务器或游戏机今天在其硬件上做的事情。

The NVIDIA Tegra P1 has six ARMv8 cores and a smaller Pascal GPU (capa- ble of 750 GFLOPS) and 50 GB/s of memory bandwidth. It is the key component of the NVIDIA DRIVE PX2 computing platform that is used in cars for autono- mous driving. The NVIDIA Tegra X1 is the previous generation and is used in several high-end tablets, such as the Google Pixel C and the NVIDIA Shield TV. It has a Maxwell-class GPU capable of 512 GFLOPS.

> NVIDIA TEGRA P1 具有六个 ARMV8 核心和一个较小的 Pascal GPU（750 Gflops 的 Capable）和 50 GB/s 的内存带宽。它是用于自动驾驶汽车的 NVIDIA DRIVE PX2 计算平台的关键组件。NVIDIA TEGRA X1 是上一代，用于多个高端平板电脑，例如 Google Pixel C 和 Nvidia Shield TV。它具有麦克斯韦级 GPU，能够提供 512 GFLOPS。

Figure 4.26 Key features of the GPUs for embedded clients and servers.

> 图 4.26 嵌入式客户端和服务器的 GPU 的关键功能。

The NVIDIA Tesla P100 is the Pascal GPU discussed extensively in this chap- ter. (Tesla is Nvidia’s name for products targeting general-purpose computing.) The clock rate is 1.4 GHz, and it includes 56 SIMD Processors. The path to HBM2 memory is 4096-bits wide, and it transfers data on both the rising and falling edge of a 0.715 GHz clock, which means a peak memory bandwidth of 732 GB/s. It connects to the host system processor and memory via a PCI Express 16 Gen 3 link, which has a peak bidirectional rate of 32 GB/s.

> NVIDIA TESLA P100 是该章节中广泛讨论的 Pascal GPU。（特斯拉是针对通用计算的产品的 Nvidia 名称。）时钟速率为 1.4 GHz，其中包括 56 个 SIMD 处理器。HBM2 存储器的路径为 4096 位宽，它在 0.715 GHz 时钟的上升和下降边缘上都传输数据，这意味着峰值存储器带宽为 732 GB/s。它通过 PCI Express 16 Gen 3 链路连接到主机系统处理器和内存，该链接的峰值双向速率为 32 GB/s。

All physical characteristics of the P100 die are impressively large: it contains

> P100 模具的所有物理特征都令人印象深刻：它包含

15.3 billion transistors, the die size is 645 mm<sup>2</sup> in a 16-nm TSMC process, and the typical power is 300 W.

> 153 亿晶体管，在 16 nm TSMC 过程中，模具尺寸为 645 mm <sup> 2 </sup>，典型的功率为 300W。
