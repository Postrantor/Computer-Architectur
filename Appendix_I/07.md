## Implementing Cache Coherence

> ##实施缓存连贯

In this section, we explore the challenge of implementing cache coherence, starting first by dealing with the challenges in a snooping coherence protocol, which we simply alluded to in [Chapter 5](#_bookmark213). Implementing a directory protocol adds some additional complexity to a snooping protocol, primarily arising from the absence of broadcast, which forces the use of a different mechanism to resolve races. Furthermore, the larger processor count of a directory-based multiprocessor means that we cannot retain assumptions of unlimited buffering and must find new ways to avoid deadlock, Let’s start with the snooping protocols.

> 在本节中，我们探讨了实现缓存相干性的挑战，首先是应对窥探连贯协议中的挑战，我们只是在[第 5 章]中提到的（#_ bookmark213）。实施目录协议为窥探协议增加了一些额外的复杂性，主要是由于没有广播而引起的，这迫使使用不同的机制解决种族。此外，基于目录的多处理器的较大处理器计数意味着我们不能保留无限缓冲的假设，并且必须找到避免僵局的新方法，让我们从侦听协议开始。

As we mentioned in [Chapter 5](#_bookmark213), the challenge of implementing misses in a snooping coherence protocol without a bus lies in finding a way to make the multi- step miss process appear atomic. Both an upgrade miss and a write miss require the same basic processing and generate the same implementation challenges; for sim- plicity, we focus on upgrade misses. Here are the steps in handling an upgrade miss:

> 正如我们在[第 5 章]（#_ bookmark213）中提到的那样，在没有巴士的窥探连贯协议中实施失误的挑战在于找到一种使多步骤失误过程显现的方法。升级错过和写作错过都需要相同的基本处理，并产生相同的实施挑战。对于模拟，我们专注于升级失误。这是处理升级小姐的步骤：

1. Detect the miss and compose an invalidate message for transmission to other caches.

> 1.检测错过并构成无效的消息，以传输到其他缓存。

2. When access to the broadcast communication link is available, transmit the message.

> 2.当可访问广播通信链接时，请传输消息。

3. When the invalidates have been processed, the processor updates the state of the cache block and then proceeds with the write that caused the upgrade miss.

> 3.当处理无效后，处理器会更新缓存块的状态，然后进行引起升级失误的写入。

There are two related difficulties that can arise. First, how will two processors, P1 and P2, that attempt to upgrade the same cache block at the same time resolve the race? Second, when at step 3, how does a processor know when all invalidates have been processed so that it can complete the step?

> 可能会出现两个相关的困难。首先，两个处理器 P1 和 P2 将如何尝试同时升级同一缓存块？其次，在步骤 3 时，处理器如何知道何时处理所有无效的人以完成步骤？

The solution to finding a winner in the race lies in the ordering imposed by the broadcast communication medium. The communication medium must broadcast any cache miss to all the nodes. If P1 and P2 attempt to broadcast at the same time, we must ensure that either P1’s message will reach P2 first or P2’s will reach P1 first. This property will be true if there is a single channel through which all ingoing and outgoing requests from a node must pass through and if the communication network does not accept a message unless it can guarantee delivery (i.e., it is effectively circuit switched, see [Appendix F](#_bookmark595)). If both P1 and P2 initiate their attempts to broadcast an invalidate simultaneously, then the network can accept only one of these operations and delay the other. This ordering ensures that either P1 or P2 will complete its com- munication in step 2 first. The network can explicitly signal when it accepts a mes- sage and can guarantee it will be the next transmission; alternatively, a processor can simply watch the network for its own request, knowing that once the request is seen, it will be fully transmitted to all processors before any subsequent messages.

> 在比赛中找到获胜者的解决方案在于广播传播媒介施加的订单。通信介质必须向所有节点广播任何缓存。如果 P1 和 P2 尝试同时广播，我们必须确保 P1 的消息将首先达到 P2 或 P2 的消息将首先达到 P1。如果有一个单个通道，则该属性将是正确的，如果该通道都必须通过该节点的所有信息和传出请求必须通过，并且如果通信网络不接受消息，则除非可以保证交货（即，它是有效的电路切换，请参见[附录 F]（#_ bookmark595））。如果 P1 和 P2 都启动了同时广播无效的尝试，那么网络只能接受其中一个操作并延迟另一个操作。该订购可确保 P1 或 P2 首先在步骤 2 中完成通讯。当网络接受 Mes-sage 时，网络可以明确发出信号，并可以保证它将是下一个传输。另外，处理器可以简单地观看网络以获取自己的请求，因为它知道一旦看到请求，它将在任何后续消息之前将其完全传输给所有处理器。

Now, suppose P1 wins the race to transmit its invalidate; once it knows it has won the race, it can continue with step 3 and complete the miss handling. There is a potential problem, however, for P2. When P2 undertook step 1, it believed that the block was in the shared state, but for P1 to advance at step 3, it must know that P2 has processed the invalidate, which must change the state of the block at P2 to inva- lid! One simple solution is for P2 to notice that it has lost the race, by observing that P1’s invalidate is broadcast before its own invalidate. P2 can then invalidate the block and generate a write miss to get the data. P1 will see its invalidate before P2’s, so it will change the block to modified and update the data, which guarantees forward progress and avoids deadlock. When P1 sees the subsequent invalidate to a block in the Modified state (a possibility that cannot arise in our basic protocol discussed in [Chapter 5](#_bookmark213)), it knows that it was the winner of a race. It can simply ignore the invalidate, knowing that it will be followed by a write miss, or it can write the block back to memory and make its state invalid.

> 现在，假设 P1 赢得了传播其无效的比赛；一旦知道它赢得了比赛，它就可以继续使用第 3 步并完成小姐的处理。但是，对于 P2 存在潜在的问题。当 P2 下达步骤 1 时，它认为该块处于共享状态，但是要使 P1 在步骤 3 中前进，它必须知道 P2 已经处理了无效的，这必须将 P2 处的块状态更改为 Inv-LID 呢一个简单的解决方案是，P2 可以注意到它已经失去了比赛，观察到 P1 的无效是在其自身无效之前广播的。然后，P2 可以使块无效并生成写入误以获取数据。P1 将在 P2 之前看到其无效，因此它将更改块以修改和更新数据，从而确保进步并避免僵局。当 P1 看到随后的变化状态下的块无效（在[第 5 章]（#_ bookmark213）中讨论的基本协议中无法引起的可能性时，它知道它是种族的赢家。它可以简单地忽略无效的情况，因为知道它会遵循写入错过，或者可以将块写回内存并使状态无效。

Another solution is to give precedence to incoming requests over outgoing requests, so that before P2 can transmit its invalidate it must handle any pending inval- idates or write misses. If any of those misses are for blocks with the same address as a pending outgoing message, the processor must be prepared to restart the write oper- ation, since the incoming request may cause the state of the block to change. Notice that P1 knows that the invalidates will be processed once it has successfully completed the broadcast, since precedence is given to invalidate messages over outgoing requests. (Becauseitdoesnot employbroadcast, a processor using a directory protocol cannot know when an invalidate is received; instead, explicit acknowledgments are required, as we discuss in the next section. Indeed, as we will see, it cannot even know it has won the race to become the owner until its request is acknowledged.)

> 另一个解决方案是优先考虑传出请求的要求，以便在 P2 传输其无效之前，必须处理任何未决的无效或写入错过。如果这些遗漏中的任何一个是针对与未登出的消息相同地址的块，则必须准备好重新启动写入操作，因为传入请求可能会导致块的状态更改。请注意，P1 知道无效后，将一旦成功完成广播，因此将处理该 p1，因为优先考虑了超过消息而不是传出请求的消息。（因为使用目录协议的处理器无法知道何时收到无效的处理器；而不是我们在下一节中讨论。的确，正如我们所看到的，它甚至不知道它已经赢得了比赛的竞赛成为所有者，直到确认其要求为止。）

Reads will also require a multiple-step process, since we need to get the data back from memory or a remote cache (in a write-back cache system), but reads do not introduce fundamentally new problems beyond what exists for writes.

> 读取还需要一个多步骤过程，因为我们需要从内存或远程缓存（在写下缓存系统中）中获取数据，但是读取并未引入从根本上引入新的问题，而不是存在写作内容。

There are, however, a few additional tricky edge cases that must be handled cor- rectly. For example, in a write-back cache, a processor can generate a read miss that requires a write-back, which it could delay, while giving the read miss priority. If a snoop request appears for the cache block that is to be written back, the processor must discover this and send the data back. Failure to do so can create a deadlock situation. A similar tricky situation exists when a processor generates a write miss, which will make a block exclusive, but, before the processor receives the data and can update the block, other processors generate read misses for that block. The read misses cannot be processed until the writing processor receives the data and updates the block.

> 但是，还有一些其他棘手的边缘案例必须进行操作。例如，在写下缓存中，处理器可以生成需要写下的读取误差，这可能会延迟，同时给予阅读误差优先级。如果出现了要回复的缓存块的侦探请求，则处理器必须发现并将数据发送回。不这样做会造成僵局。当处理器生成写入错过时，也存在类似的棘手情况，这将成为一个块独家，但是，在处理器接收到数据并可以更新块之前，其他处理器会生成该块的读取错误。在写作处理器接收数据并更新块之前，无法处理读取误差。

One of the more difficult problems occurs in a write-back cache where the data for a read or write miss can come either from memory or from one of the processor caches, but the requesting processor will not know _a priori_ where the data will come from. In most bus-based systems, a single global signal is used to indicate whether any processor has the exclusive (and hence the most up-to-date) copy; oth- erwise, the memory responds. These schemes can work with a pipelined intercon- nection by requiring that processors signal whether they have the exclusive copy within a fixed number of cycles after the miss is broadcast.

> 更困难的问题之一发生在写下缓存中，其中读取或写入遗漏的数据可以来自内存或处理器库之一，但是请求处理器将不知道_a priori_数据将来自其中数据。。在大多数基于总线的系统中，使用单个全局信号来指示任何处理器是否具有独家（因此是最新的）副本。otherwise，内存会做出回应。这些方案可以通过要求处理器发出信号来与管道互连的互连一起使用，它们是否在广播后是否具有固定数量的循环中的独家副本。

In a modern multiprocessor, however, it is essentially impossible to bound the amount of time required for a snoop request to be processed. Instead, a mechanism is required to determine whether the memory has an up-to-date copy. One solution is to add coherence bits to the memory, indicating whether the data are exclusive in a remote cache. This mechanism begins to move toward the directory approach, whose implementation challenges we consider next.

> 但是，在现代多处理器中，基本上不可能绑定侦探请求所需的时间。相反，需要一种机制来确定内存是否具有最新副本。一种解决方案是在内存中添加连贯位，表明数据是否在远程缓存中是独有的。这种机制开始朝着目录方法迈进，我们接下来会考虑其实施挑战。

### Implementing Cache Coherence in a DSM Multiprocessor

> ###在 DSM 多处理器中实现缓存连贯

Implementing a directory-based cache coherence protocol requires overcoming all the problems related to nonatomic actions for a snooping protocol without the use of broadcast (see [Chapter 5](#_bookmark213)), which forced a serialization on competing writes and also ensured the serialization required for the memory consistency model. Avoiding the need to broadcast is a central goal for a directory-based system, so another method for ensuring serialization is necessary.

> 实施基于目录的缓存相干协议需要克服与非原子操作相关的所有问题，而无需使用广播（请参阅[第 5 章]（#_ bookmark213）），该协议迫使竞争写作序列化并确保了串行化的序列化。内存一致性模型所需。避免进行广播是基于目录的系统的中心目标，因此需要另一种确保序列化的方法。

The serialization of requests for exclusive access to a memory block is eas- ily enforced since those requests will be serialized when they reach the unique directory for the specified block. If the directory controller simply ensures that one request is completely serviced before the next is begun, writes will be seri- alized. Because the requesters cannot know ahead of time who will win the race and because the communication is not a broadcast, the directory must sig- nal to the winner when it completes the processing of the winner’s request. This is done by a message that supplies the data on a write miss or by an explicit acknowledgment message that grants ownership in response to an invalidation request.

> 序列化对内存块的独家访问请求的序列化将得到强制执行，因为当这些请求到达指定块的唯一目录时，这些请求将被序列化。如果目录控制器只是简单地确保在下一个请求开始之前完全维修一个请求，则写信将进行序列。由于请求者无法提前知道谁将赢得比赛，并且由于沟通不是广播，因此目录必须在获胜者完成获胜者请求时向获奖者介入。这是通过一条消息来完成的，该消息提供了写入失误或明确确认消息，该消息响应无效请求而授予所有权。

What about the loser in this race? The simplest solution is for the system to send a _negative acknowledge_, or _NAK_, which requires that the requesting node regen- erate its request. (This is the equivalent of a collision in the broadcast network in a snooping scheme, which requires that one of the transmitting nodes retry its com- munication.) We will see in the next section why the NAK approach, as opposed to buffering the request, is attractive.

> 这场比赛中的失败者呢？最简单的解决方案是使系统发送_negative engressege_或_nak_，该_nak_要求请求节点重新恢复其请求。（这相当于偷窥方案中广播网络中的碰撞，该方案要求其中一个传输节点重试其连接。）我们将在下一部分中看到为什么 NAK 方法与请求相反，而不是缓冲请求。，很有吸引力。

Although the acknowledgment that a requesting node has ownership is com- pleted when the write miss or ownership acknowledgment message is transmit- ted, we still do not know that the invalidates have been received and processed by the nodes that were in the sharing set. All memory consistency models eventually require (either before the next cache miss or at a synchronization point, for exam- ple) that a processor knows that all the invalidates for a write have been pro- cessed. In a snooping scheme, the nature of the broadcast network provides this assurance.

> 尽管当传输写入错过或所有权确认消息时，请确认请求节点拥有所有权，但我们仍然不知道该节点已被共享集中的节点收到和处理。所有内存一致性模型最终都需要（在下一个缓存失误之前或在同步点之前，要进行检查），即处理器知道所有写作的无效物都已被验证。在窥探计划中，广播网络的性质提供了这种保证。

How can we know when the invalidates are complete in a directory scheme? The only way to know that the invalidates have been completed is to have the des- tination nodes of the invalidate messages (the members of the sharing set) explic- itly acknowledge the invalidation messages sent from the directory. Who should they be acknowledged to? There are two possibilities. In the first the acknowledg- ments can be sent to the directory, which can count them, and when all acknowl- edgments have been received, confirm this with a single message to the original requester. Alternatively, when granting ownership, the directory can tell the reg- ister how many acknowledgments to expect. The destinations of the invalidate messages can then send an acknowledgment directly to the requester, whose iden- tity is provided by the directory. Most existing implementations use the latter scheme, since it reduces the possibility of creating a bottleneck at a directory. Although the requirement for acknowledgments is an additional complexity in directory protocols, this requirement arises from the avoidance of a serialization mechanism, such as the snooping broadcast operation, which in itself is the limit to scalability.

> 我们怎么知道目录方案中无效的何时完成？知道无效的方法已经完成的唯一方法是使无效的消息（共享集合的成员）明确确认从目录发送的无效消息。他们应该承认谁？有两种可能性。首先，确认可以发送到目录，该目录可以计算它们，并且当收到所有确认收入时，请向原始请求者提供一条消息。另外，在授予所有权时，目录可以告诉我有多少承认要期望。然后，无效消息的目的地可以直接向请求者发送确认，该请求者的标志性由目录提供。大多数现有的实现都使用后一个方案，因为它减少了在目录中创建瓶颈的可能性。尽管确认的要求是目录协议中的另一个复杂性，但此要求源于避免序列化机制，例如窥探广播操作，这本身就是对可伸缩性的极限。

### Avoiding Deadlock from Limited Buffering

> ###避免受到有限缓冲的僵局

A new complication in the implementation is introduced by the potential scale of a directory-based multiprocessor. In [Chapter 5](#_bookmark213), we assumed that the network could always accept a coherence message and that the request would be acted upon at some point. In a much larger multiprocessor, this assumption of unlimited buffer- ing may be unreasonable. What happens when the network does not have unlimited buffering? The major implication of this limit is that a cache or directory controller may be unable to complete a message send. This could lead to deadlock.

> 基于目录的多处理器的潜在尺度引入了实现的新并发症。在[第 5 章]（#_ bookmark213）中，我们假设网络可以始终接受连贯的消息，并且该请求将在某个时候进行。在更大的多处理器中，这种无限缓冲的假设可能是不合理的。当网络没有无限缓冲时会发生什么？此限制的主要含义是缓存或目录控制器可能无法完成发送消息。这可能导致僵局。

The potential deadlock arises from three properties, which characterize many deadlock situations:

> 潜在的僵局来自三个属性，这些属性是许多僵局情况的特征：

1. More than one resource is needed to complete a transaction: Message buffers are needed to generate requests, create replies and acknowledgments, and accept replies.

> 1.完成交易需要多个资源：需要消息缓冲区来生成请求，创建答复和确认并接受答复。

2. Resources are held until a nonatomic transaction completes: The buffer used to create the reply cannot be freed until the reply is accepted, for reasons we will see shortly.

> 2.持有资源，直到完成非原子交易完成为止：由于我们很快会看到的原因，无法释放用于创建答复的缓冲区。

3. There is no global partial order on the acquisition of resources: Nodes can gen- erate requests and replies at will.

> 3.没有关于资源获取的全球部分秩序：节点可以随意提出请求和答复。

These characteristics lead to deadlock, and avoiding deadlock requires breaking one of these properties. Freeing up resources without completing a transaction is difficult, since the transaction must be completely backed out and cannot be left half-finished. Hence, our approach will be to try to resolve the need for multiple resources. We cannot simply eliminate this need, but we can try to ensure that the resources will always be available.

> 这些特征导致僵局，避免僵局需要破坏这些特性之一。在不完成交易的情况下释放资源是困难的，因为必须完全备份交易，并且不能半完成。因此，我们的方法是尝试解决对多个资源的需求。我们不能简单地消除这种需求，但是我们可以尝试确保资源始终可用。

One way to ensure that a transaction can always complete is to guarantee that there are always buffers to accept messages. Although this is possible for a small multiprocessor with processors that block on a cache miss or have a small number of outstanding misses, it may not be very practical in a directory protocol, since a single write could generate many invalidate messages. In addition, features such as prefetch and multiple outstanding misses increase the amount of buffering required. There is an alternative strategy, which most systems use and which ensures that a transaction will not actually be initiated until we can guarantee that it has the resources to complete. The strategy has four parts:

> 确保交易始终完成的一种方法是确保总有缓冲区接受消息。尽管这对于一个小型多处理器带有阻塞缓存失误或少数未偿还的处理器的处理器可能是可能的，但在目录协议中可能并不是很实际，因为单个写入可能会产生许多无效的消息。此外，预摘要和多个未偿还的功能增加了所需的缓冲量。有一个替代策略，大多数系统都使用，并确保实际上不会启动交易，直到我们可以保证它具有资源可以完成。该策略有四个部分：

1. A separate network (physical or virtual) is used for requests and replies, where a reply is any message that a controller waits for in transitioning between states. This ensures that new requests cannot block replies that will free up buffers.

> 1.单独的网络（物理或虚拟）用于请求和答复，其中控制器在状态之间过渡时等待的任何答复。这样可以确保新请求无法阻止可以释放缓冲区的答复。

2. Every request that expects a reply allocates space to accept the reply when the request is generated. If no space is available, the request waits. This ensures that a node can always accept a reply message, which will allow the replying node to free its buffer.

> 2.每个期望答复的请求分配空间在生成请求时接受答复。如果没有可用空间，请求等待。这样可以确保节点始终可以接受回复消息，这将允许答复节点释放其缓冲区。

3. Any controller can reject with a NAK any request, but it can never NAK a reply. This prevents a transaction from starting if the controller cannot guarantee that it has buffer space for the reply.

> 3.任何控制器都可以用 NAK 拒绝任何请求，但是它永远无法回复。如果控制器无法保证其具有回复的缓冲空间，则可以防止交易开始。

4. Any request that receives a NAK in response is simply retried.

> 4.任何收到 NAK 响应的请求都是简单地重述的。

To see that there are no deadlocks with the four properties above, we must ensure that all replies can be accepted and that every request is eventually ser- viced. Since a cache controller or directory controller always allocates a buffer to handle the reply before issuing a request, it can always accept the reply when it returns. To see that every request is eventually serviced, we need only show that any request could be completed. Since every request starts with a read or write miss at a cache, it is sufficient to show that any read or write miss is eventually serviced. Since the write miss case includes the actions for a read miss as a sub- set, we focus on showing the write misses are serviced. The simplest situation is when the block is uncached; since that case is subsumed by the case when the block is shared, we focus on the shared and exclusive cases. Let’s consider the case where the block is shared:

> 要看到上面的四个属性没有僵局，我们必须确保所有答复都可以接受，并且最终都可以接受每个请求。由于缓存控制器或目录控制器始终在发出请求之前分配缓冲区以处理答复，因此返回时始终可以接受答复。要查看最终服务的每个请求，我们只需要证明任何请求即可完成。由于每个请求都以缓存的读写或写入遗漏开头，因此足以证明任何读或写入错过最终都可以维修。由于写入 Miss 案件包括读取 Miss 作为子集的动作，因此我们专注于表明写入错过。最简单的情况是块未切除；由于该案例被共享块的情况所包含，因此我们将重点放在共享和独家案例上。让我们考虑分享块的情况：

- The CPU attempts to do a write and generates a write miss that is sent to the directory. For simplicity, we can assume that the processor is stalled. Although it may issue further requests, it should not issue a request for the same cache block until the first one is completed. Requests for independent blocks can be handled separately.

> - CPU 试图进行写作并生成发送给目录的写入错过。为简单起见，我们可以假设处理器停滞不前。尽管它可能会发出进一步的请求，但不应在第一个缓存块上发布同一缓存块的请求。可以单独处理独立块的请求。

- The write miss is sent to the directory controller for this memory block. Note that although one cache controller handles all the requests for a given cache block, regardless of its memory contents, the directory controller handles requests for different blocks as independent events (assuming sufficient buff- ering, which is allocated before the directory issues any further messages on behalf of the request). The only conflict at the directory controller is when two requests arrive for the same block. The controller must wait for the first operation to be completed. It can simply NAK the second request or buffer it, but it should not service the second request for a given memory block until the first is completed.

> - 写入误差将发送到此内存块的目录控制器。请注意，尽管一个缓存控制器处理给定缓存块的所有请求，无论其内存内容如何代表请求）。目录控制器的唯一冲突是当两个请求到达同一块时。控制器必须等待第一个操作完成。它可以简单地提出第二个请求或缓冲它，但不应为给定内存块的第二个请求服务，直到第一个请求完成为止。

- Now consider what happens at the directory controller: Suppose the write miss is the next thing to arrive at the directory controller. The controller sends out the invalidates, which can always be accepted after a limited delay by the cache controller. Note that one possibility is that the cache controller has an outstanding miss for the same block. This is the dual case to the snooping scheme, and we must once again break the tie by forcing the cache controller to accept and act on the directory request. Depending on the exact timing, this cache controller will either get the cache line later from the directory or will receive a NAK and have to restart the process.

> - 现在考虑目录控制器上发生的情况：假设写入是到达目录控制器的下一步。控制器发出无效的人，在缓存控制器有限延迟后，始终可以接受。请注意，一种可能性是缓存控制器对同一块有出色的错过。这是侦听方案的双重情况，我们必须再次通过强迫缓存控制器接受并按照目录请求行事来再次打破领带。根据确切的时间安排，此缓存控制器将稍后从目录中获取缓存线，或者将接收 NAK 并必须重新启动该过程。

The case where the block is exclusive is somewhat trickier. Our analysis begins when the write miss arrives at the directory controller for processing. There are two cases to consider:

> 块是独家的情况有些棘手。当写入错过到达目录控制器以进行处理时，我们的分析开始。有两种情况要考虑：

- The directory controller sends a fetch/invalidate message to the processor where it arrives to find the block in the exclusive state. The cache controller sends a data write-back to the home directory and makes its state invalid. This reply arrives at the home directory controller, which can always accept the reply, since it preallocated the buffer. The directory controller sends back the data to the requesting processor, which can always accept the reply; after the cache is updated, the requesting cache controller notifies the processor.

> - 目录控制器将获取/无效的消息发送给处理器，该消息到达在该处理器中以在独家状态中找到块。缓存控制器向主目录发送数据写入，并使其状态无效。此答复到达 Home Directory Controller，该控制器可以始终接受答复，因为它预先列出了缓冲区。目录控制器将数据发送到请求处理器，该处理器可以始终接受答复；更新缓存后，请求的缓存控制器通知处理器。

- The directory controller sends a fetch/invalidate message to the node indicated as owner. When the message arrives at the owner node, it finds that this cache controller has taken a read or write miss that caused the block to be replaced. In this case, the cache controller has already sent the block to the home directory with a data write-back and made the data unavailable. Since this is exactly the effect of the fetch/invalidate message, the protocol operates correctly in this case as well.

> - 目录控制器将获取/无效消息发送给表示为所有者的节点。当消息到达所有者节点时，它发现该缓存控制器已完成读取或写入误差，导致块被替换。在这种情况下，缓存控制器已经以数据写入后将块发送到了主目录，并使数据不可用。由于这正是获取/无效消息的效果，因此协议在这种情况下也正确运行。

We have shown that our coherence mechanism operates correctly when the cache and directory controller can accept requests for operation on cache blocks for which they have no outstanding operations in progress, when replies are always accepted, and when requests can be NAKed and forced to retry. Like the case of the snooping protocol, the cache controller must be able to break ties, and it always does so by favoring the instructions from the directory. The ability to NAK requests is what allows an implementation with finite buffering to avoid deadlock.

> 我们已经证明，当缓存和目录控制器可以接受在没有出色的操作的高速缓存块上，当始终接受答复时，并且当请求裸露并被迫重试时，我们的连贯机制可以正确运行。像侦探协议的情况一样，缓存控制器必须能够打破领带，并且它总是通过偏爱目录中的指令来做到这一点。NAK 请求的能力允许具有有限缓冲的实施以避免僵局。

### Implementing the Directory Controller

> ###实施目录控制器

To implement a cache coherence scheme, the cache controller must have the same abilities it needed in the snooping case, namely, the capability of handling requests for independent blocks while awaiting a response to a request from the local pro- cessor. The incoming requests are still processed in order, and each one is com- pleted before beginning the next. Should a cache controller receive too many requests in a short period of time, it can NAK them, knowing that the directory will subsequently regenerate the request.

> 为了实现缓存连贯方案，缓存控制器必须具有与侦听案例中所需的相同能力，即处理独立块请求的能力，同时等待本地专业人士的响应。传入的请求仍按顺序处理，并且每个请求在开始下一个请求之前都要完成。如果缓存控制器在短时间内收到太多请求，则可以知道目录随后会再生该请求。

The directory must also be multithreaded and able to handle requests for mul- tiple blocks independently. This situation is somewhat different than having the cache controller handle incoming requests for independent blocks, since the direc- tory controller will need to begin processing one request while an earlier one is still underway. The directory controller cannot wait for one to complete before servic- ing the next request, since this could lead to deadlock. Instead, the directory con- troller must be _reentrant_; that is, it must be capable of suspending its execution while waiting for a reply and accepting another transaction. The only place this must occur is in response to read or write misses, while waiting for a response from the owner. This leads to three important observations:

> 目录还必须进行多线程，并能够独立处理 Multiple 块的请求。这种情况与让 CACHE 控制器处理独立块的传入请求有所不同，因为在较早的一个请求仍在进行时，指导控制器将需要开始处理一个请求。目录控制器无法等待一个人在维修下一个请求之前完成，因为这可能会导致死锁。相反，目录控制器必须为_reentrant_;也就是说，它必须在等待答复并接受另一笔交易的同时暂停执行。在等待所有者的回应时，唯一必须发生的地方是响应读写或写信。这导致了三个重要的观察：

1. The state of the controller need only be saved and restored while either a fetch operation from a remote location or a fetch/invalidate is outstanding.

> 1.在从远程位置的提取操作或获取/无效的情况下，控制器的状态只需保存和恢复。

2. The implementation can bound the number of outstanding transactions being handled in the directory by simply NAKing read or write miss requests that could cause the number of outstanding requests to be exceeded.

> 2.该实施可以通过简单地读取或写出可能导致未偿还请求数量的未读取请求来限制目录中正在处理的未偿交易的数量。

3. If instead of returning the data through the directory, the owner node forwards the data directly to the requester (as well as returning it to the directory), we can eliminate the need for the directory to handle more than one outstanding request. This motivation, in addition to the reduction of latency, is the reason for using the forwarding style of protocol. There are other complexities from forwarding protocols that arise when requests arrive closely spaced in time.

> 3.如果不是通过目录返回数据，则所有者节点将数据直接转发给请求者（以及将其返回到目录），我们可以消除目录的需求，以处理多个未偿还的请求。除了减少延迟外，这种动机是使用协议转发样式的原因。当请求及时地距离紧密到达时，转发协议还会出现其他复杂性。

The major remaining implementation difficulty is to handle NAKs. One alter- native is for each processor to keep track of its outstanding transactions so it knows, when the NAK is received, what the requested transaction was. The alter- native is to bundle the original request into the NAK, so that the controller receiv- ing the NAK can determine what the original request was. Because every request allocates a slot to receive a reply and a NAK is a reply, NAKs can always be received. In fact, the buffer holding the return slot for the request can also hold information about the request, allowing the processor to reissue the request if it is NAKed.

> 剩下的主要实施困难是处理 NAKS。每个处理器的一个变化可以跟踪其出色的交易，因此它知道，何时收到 NAK，请求的交易是什么。Alter-Native 是将原始请求捆绑到 NAK 中，以便收到 NAK 的控制器可以确定原始请求是什么。因为每个请求都分配一个插槽来接收答复，而 NAK 是一个答复，因此可以始终收到 NAKS。实际上，持有请求的返回插槽的缓冲区还可以保留有关请求的信息，从而使处理器在裸露的情况下重新发行了请求。

In practice, great care is required to implement these protocols correctly and to avoid deadlock. The key ideas we have seen in this section—dealing with nona- tomicity and finite buffering—are critical to ensuring a correct implementation. Designers have found that both formal and informal verification techniques are helpful for ensuring that implementations are correct.

> 实际上，需要非常谨慎才能正确实施这些协议并避免僵局。我们在本节中看到的关键想法（以非义务和有限的缓冲方式实现）对于确保正确实施至关重要。设计师发现，正式和非正式验证技术都有助于确保实施正确。
