## Guidelines for DSAs

> ## DSA 指南

Here are five principles that generally guided the designs of the four DSAs we’ll see in [Sections 7.4–7.7](#googles-tensor-processing-unit-an-inference-data-center-accelerator). Not only do these five guidelines lead to increased area and energy efficiency, they also provide two valuable bonus effects. First, they lead to simpler designs, which reduce the cost of NRE of DSAs (see the fallacy in [Section 7.10](#_bookmark385)). Second, for user-facing applications that are commonplace with DSAs, accelerators that follow these principles are a better match to the 99th- percentile response-time deadlines than the time-varying performance optimiza- tions of traditional processors, as we will see in [Section 7.9](#putting-it-all-together-cpus-versus-gpus-versus-dnn-accelerators). [Figure 7.3](#_bookmark328) shows how the four DSAs followed these guidelines.

> 以下是我们将在[7.4-7.7]节中看到的四个 DSA 设计的五个原则（＃googles-tensor-processing-unit-anter-anter-nit-nit-nit-in-inter-in-inter-in-inter-center-accelerator）。这五个准则不仅导致面积和能源效率提高，而且还提供了两种有价值的奖励效果。首先，它们导致更简单的设计，从而降低了 DSA 的 NRE 成本（请参阅[7.10]（#_ bookmark385）中的谬论）。其次，对于与 DSA 司空见惯的面向用户的应用程序，遵循这些原则的加速器相比，与传统处理器的时变性能优化相比，与 99％的响应时间截止日期更好，正如我们将在[[第 7.9 节]（＃putting-it-it-all-together-cpus-vers-vpus-vers-versus-dnn-accelerators）。[图 7.3]（#_ bookmark328）显示了四个 DSA 如何遵循这些准则。

1. _Use dedicated memories to minimize the distance over which data is moved_. The many levels of caches in general-purpose microprocessors use a great deal of area and energy trying to move data optimally for a program. For example, a two-way set associative cache uses 2.5 times as much energy as an equivalent software-controlled scratchpad memory. By definition, the compiler writers and programmers of DSAs understand their domain, so there is no need for the hardware to try to move data for them. Instead, data movement is reduced with software-controlled memories that are dedicated to and tailored for specific functions within the domain.

> 1. \_使用专用记忆，以最大程度地减少移动数据的距离。通用微处理器中的许多级别的缓存使用了大量的区域和能量，试图最佳地移动程序。例如，一个双向集合的关联缓存使用的能量是等效软件控制的 SCRATCHPAD 内存的 2.5 倍。根据定义，DSAS 的编译器作家和程序员了解其域，因此无需硬件尝试为其移动数据。取而代之的是，使用软件控制的记忆降低数据移动，这些记忆专门针对域内的特定功能进行定制。

2. _Invest the resources saved from dropping advanced microarchitectural optimi- zations into more arithmetic units or bigger memories_.

> 2. \_投资从将高级微构造优化的优势删除到更多算术单元或更大的记忆中。

As [Section 7.1](#introduction-5) describes, architects turned the bounty from Moore’s Law into the resource-intensive optimizations for CPUs and GPUs (out-of-order execu- tion, multithreading, multiprocessing, prefetching, address coalescing, etc.).

> 正如[7.1]（＃resoped-5）所描述的那样，建筑师将赏金从摩尔定律转变为用于 CPU 和 GPU 的资源密集型优化（额外执行，多线程，多线程，多处处理，预及，预购，地址合并等等）。）。

Figure 7.3 The four DSAs in this chapter and how closely they followed the five guidelines. Pixel Visual Core typ- ically has 2–16 cores. The first implementation of Pixel Visual Core does not support 8-bit arithmetic.

> 图 7.3 本章中的四个 DSA 及其遵循五个准则的紧密关系。像素视觉核心典型上有 2-16 个核心。像素视觉核心的首次实现不支持 8 位算术。

Given the superior understanding of the execution of programs in these nar- rower domains, these resources are much better spent on more processing units or larger on-chip memory.

> 鉴于对这些 Nar-Rower 域中程序执行的高度了解，这些资源在更多的处理单元或更大的芯片内存储器上花费了得多。

1. _Use the easiest form of parallelism that matches the domain_. Target domains for DSAs almost always have inherent parallelism. The key deci- sions for a DSA are how to take advantage of that parallelism and how to expose it to the software. Design the DSA around the natural granularity of the parallelism of the domain and expose that parallelism simply in the programming model. For example, with respect to data-level parallelism, if SIMD works in the domain, it’s certainly easier for the programmer and the compiler writer than MIMD. Simi- larly, if VLIW can express the instruction-level parallelism for the domain, the design can be smaller and more energy-efficient than out-of-order execution.

> 1. *使用与域*匹配的最简单的并行形式。DSA 的目标域几乎总是具有固有的并行性。DSA 的关键决定是如何利用该并行性以及如何将其暴露于软件。设计围绕域平行性的自然粒度的 DSA，并简单地在编程模型中揭示并行性。例如，关于数据级并行性，如果 SIMD 在域中起作用，那么程序员和编译器作者当然比 MIMD 更容易。相似地，如果 VLIW 可以表达域的指令级并行性，则设计可以比阶外执行更小，更节能。

2. _Reduce data size and type to the simplest needed for the domain_. As we will see, applications in many domains are typically memory-bound, so you can increase the effective memory bandwidth and on-chip memory utiliza- tion by using narrower data types. Narrower and simpler data also let’s you pack more arithmetic units into the same chip area.

> 2. \_Reduce 数据大小和键入域所需的最简单。正如我们将看到的那样，许多域中的应用程序通常是内存的，因此您可以通过使用较窄的数据类型来增加有效的内存带宽和芯片内存内存利用。较窄，更简单的数据还可以使您将更多的算术单元包装到同一芯片区域。

3. _Use a domain-specific programming language to port code to the DSA_. As [Section 7.1](#introduction-5) mentions, a classic challenge for DSAs is getting applications to run on your novel architecture. A long-standing fallacy is assuming that your new computer is so attractive that programmers will rewrite their code just for your hardware. Fortunately, domain-specific programming languages were becoming popular even before architects were forced to switch their attention to DSAs. Examples are Halide for vision processing and TensorFlow for DNNs ([Ragan-Kelley et al., 2013; Abadi et al., 2016](#_bookmark994)). Such languages make porting applications to your DSA much more feasible. As previously mentioned, only a small, compute-intensive portion of the application needs to run on the DSA in some domains, which also simplifies porting.

> 3. _将特定于域的编程语言用于将代码转换为 DSA_。正如[第 7.1 节]（＃resoped-5）提到的那样，DSA 的经典挑战是使应用程序运行您的新颖体系结构。长期存在的谬论假设您的新计算机是如此吸引人，以至于程序员将仅为您的硬件重写其代码。幸运的是，甚至在建筑师被迫将注意力转移到 DSA 之前，特定领域的编程语言也变得流行。示例是 DNNS 视觉处理和张力流的卤化物（[[Ragan-Kelley 等，2013; Abadi et al。，2016]（#\_ bookmark994））。这样的语言使您的 DSA 的移植应用程序更为可行。如前所述，仅在某些域中的 DSA 上运行该应用程序的一小部分，该应用程序需要运行，这也简化了移植。

DSAs introduce many new terms, mostly from the new domains but also from novel architecture mechanisms not seen in conventional processors. As we did in [Chapter 4](#_bookmark165), [Figure 7.4](#_bookmark330) lists the new acronyms, terms, and short explanations to aid the reader.

> DSA 介绍了许多新术语，主要来自新领域，也来自传统处理器中未见的新型体系结构机制。如我们在[第 4 章]（#_ bookmark165）中所做的那样，[图 7.4]（#_ bookmark330）列出了新的首字母缩写词，术语和简短的解释，以帮助读者。
