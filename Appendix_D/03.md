## Definition and Examples of Real Faults and Failures

> ##定义和实际故障和失败的示例

Although people may be willing to live with a computer that occasionally crashes and forces all programs to be restarted, they insist that their information is never lost. The prime directive for storage is then to remember information, no matter what happens.

> 尽管人们可能愿意与偶尔崩溃并迫使所有程序重新启动的计算机生活，但他们坚持认为他们的信息永远不会丢失。然后，无论发生什么情况，存储的主要指令都是要记住信息。

[Chapter 1](#_bookmark2) covered the basics of dependability, and this section expands that information to give the standard definitions and examples of failures.

> [第 1 章](#_ bookmark2)涵盖了可靠性的基础知识，本节扩展了该信息以给出标准定义和失败的示例。

The first step is to clarify confusion over terms. The terms _fault_, _error_, and _fail- ure_ are often used interchangeably, but they have different meanings in the depend- ability literature. For example, is a programming mistake a fault, error, or failure? Does it matter whether we are talking about when it was designed or when the pro- gram is run? If the running program doesn’t exercise the mistake, is it still a fault/ error/failure? Try another one. Suppose an alpha particle hits a DRAM memory cell. Is it a fault/error/failure if it doesn’t change the value? Is it a fault/error/failure if the memory doesn’t access the changed bit? Did a fault/error/failure still occur if the memory had error correction and delivered the corrected value to the CPU? You get the drift of the difficulties. Clearly, we need precise definitions to discuss such events intelligently.

> 第一步是阐明对术语的混乱。术语_fault_，_error_和_fail-ure_通常可以互换使用，但是它们在依赖能力文献中具有不同的含义。例如，编程错误是故障，错误还是故障？无论是在设计何时设计还是何时运行何时运行？如果运行程序没有行使错误，是否仍然是故障/错误/故障？尝试另一个。假设 α 粒子击中 DRAM 记忆单元。如果不更改值，是否是故障/错误/故障？如果内存无法访问更改的位，是否是故障/错误/故障？如果内存校正并将校正值传递给 CPU，是否仍然发生故障/错误/故障？您会得到困难的漂移。显然，我们需要精确的定义来聪明地讨论此类事件。

To avoid such imprecision, this subsection is based on the terminology used by Laprie \[1985\] and Gray and Siewiorek \[1991\], endorsed by IFIP Working Group

> 为了避免这种不精确，该小节基于 Laprie \ [1985 \]和 Grey and Siewiorek \ [1991 \]所使用的术语，并得到 IFIP 工作组的认可。

10.4 and the IEEE Computer Society Technical Committee on Fault Tolerance. We talk about a system as a single module, but the terminology applies to submo- dules recursively. Let’s start with a definition of _dependability_:

> 10.4 和 IEEE 计算机协会的容错技术委员会。我们谈论一个系统作为单个模块，但术语适用于递归的屈服。让我们从_ depentability_的定义开始：

_Computer system_ dependability _is the quality of delivered service such that reli- ance can justifiably be placed on this service. The_ service _delivered by a system is its observed_ actual behavior _as perceived by other system(s) interacting with this system’s users. Each module also has an ideal_ specified behavior*, where a* service specification _is an agreed description of the expected behavior. A system_ failure _occurs when the actual behavior deviates from the specified behavior. The failure occurred because of an_ error*, a defect in that module. The cause of an error is a* fault*.*

> _Computer System_可靠性_是交付服务的质量，因此可以合理地将其放置在此服务上。系统_驱动系统是其观察到的_实际行为_与该系统用户相互作用的其他系统所感知的。每个模块还具有理想的指定行为*，其中*服务规范_是对预期行为的商定描述。当实际行为偏离指定行为时，系统_失败_occurs。故障是由于 AN_错误*而发生的，该模块中的缺陷。错误的原因是*故障*。*

_When a fault occurs, it creates a_ latent error*, which becomes* effective _when it is activated; when the error actually affects the delivered service, a failure occurs. The_

> _发生故障时，它会产生 A_潜在错误*，它将变得*有效_激活时；当错误实际影响交付的服务时，会发生故障。这_

_time between the occurrence of an error and the resulting failure is the_ error latency*. Thus, an error is the manifestation* in the system _of a fault, and a failure is the manifestation_ on the service _of an error. \[p. 3\]_

> _误差的发生与结果故障之间的时间是_错误延迟*。因此，错误是系统_的表现*，而故障是服务_ OF 错误的表现_。\ [p。3 \] _

Let’s go back to our motivating examples above. A programming mistake is a _fault_. The consequence is an _error_ (or _latent error_) in the software. Upon activation, the error becomes _effective_. When this effective error produces erroneous data that affect the delivered service, a _failure_ occurs.

> 让我们回到上面的激励例子。编程错误是_fault_。结果是软件中的_error_(或_latent 错误_)。激活后，错误变为_EFFECTACTY_。当此有效错误会产生影响交付服务的错误数据时，会发生_failure_。

An alpha particle hitting a DRAM can be considered a fault. If it changes the memory, it creates an error. The error will remain latent until the affected memory word is read. If the effective word error affects the delivered service, a failure occurs. If ECC corrected the error, a failure would not occur.

> 击中 DRAM 的 α 粒子可以认为是故障。如果它更改内存，则会产生错误。该错误将保持潜在，直到读取受影响的内存单词为止。如果有效的单词错误影响已交付的服务，则会发生故障。如果 ECC 纠正了错误，则不会发生故障。

A mistake by a human operator is a fault. The resulting altered data is an error.

> 人类操作员的错误是错误。结果更改的数据是错误。

It is latent until activated, and so on as before.

> 它是潜在的，直到被激活为止，依此类推。

To clarify, the relationship among faults, errors, and failures is as follows:

> 要澄清，故障，错误和失败之间的关系如下：

- A fault creates one or more latent errors.

> - 故障会产生一个或多个潜在错误。

- The properties of errors are (1) a latent error becomes effective once activated; \(2\) an error may cycle between its latent and effective states; and (3) an effective error often propagates from one component to another, thereby creating new errors. Thus, either an effective error is a formerly latent error in that component or it has propagated from another error in that component or from elsewhere.

> - 错误的属性是(1)一旦激活，潜在错误就会有效；\(2 \)错误可能在其潜在状态和有效状态之间循环；(3)一个有效的错误通常会从一个组件传播到另一个组件，从而创建新的错误。因此，有效误差是该组件中以前的潜在误差，或者它是从该组件中的另一个错误或其他地方传播的。

- A component failure occurs when the error affects the delivered service.

> - 当错误影响已交付的服务时，会发生组件故障。

- These properties are recursive and apply to any component in the system.

> - 这些属性是递归的，并适用于系统中的任何组件。

Gray and Siewiorek classified faults into four categories according to their cause:

> 灰色和 Siewiorek 根据其原因将错误分为四类：

1. _Hardware faults_—Devices that fail, such as perhaps due to an alpha particle hitting a memory cell

> 1. _ hardware 故障_-失败的设备，例如可能是由于 alpha 粒子击中存储器单元

2. _Design faults_—Faults in software (usually) and hardware design (occasionally)

> 2. _design 故障_-软件(通常)和硬件设计中的故障(偶尔)

3. _Operation faults_—Mistakes by operations and maintenance personnel

> 3. _operation 故障_-行动和维护人员的派系

4. _Environmental faults_—Fire, flood, earthquake, power failure, and sabotage

> 4. _environmental 断层_-火，洪水，地震，电力故障和破坏

Faults are also classified by their duration into transient, intermittent, and perma- nent \[Nelson 1990\]. _Transient faults_ exist for a limited time and are not recurring. _Intermittent faults_ cause a system to oscillate between faulty and fault-free oper- ation. _Permanent faults_ do not correct themselves with the passing of time.

> 断层还通过其持续时间分为瞬态，间歇性和永久性\ [Nelson 1990 \]。_ transient 故障_存在有限的时间，并且不经常出现。_间接故障_导致系统在故障和无故障操作之间振荡。_ permanent 故障_不要在时间的传递中纠正自己。

Now that we have defined the difference between faults, errors, and failures, we are ready to see some real-world examples. Publications of real error rates are rare for two reasons. First, academics rarely have access to significant hardware resources to measure. Second, industrial researchers are rarely allowed to publish failure information for fear that it would be used against their companies in the marketplace. A few exceptions follow.

> 现在，我们已经定义了故障，错误和失败之间的差异，我们准备看到一些现实世界的示例。实际错误率的出版物很少有两个原因。首先，学者很少能访问大量的硬件资源来衡量。其次，很少允许工业研究人员发布失败信息，因为担心它会在市场上与其公司使用。如下一些例外。

### Berkeley’s Tertiary Disk

> ###伯克利的三级磁盘

The Tertiary Disk project at the University of California created an art image server for the Fine Arts Museums of San Francisco in 2000. This database consisted of high-quality images of over 70,000 artworks \[Talagala et al., 2000\]. The database was stored on a cluster, which consisted of 20 PCs connected by a switched Ethernet and containing 368 disks. It occupied seven 7-foot-high racks.

> 加利福尼亚大学的三级磁盘项目于 2000 年为旧金山的美术博物馆创建了一家艺术图像服务器。该数据库由 70,000 多种艺术品的高质量图像组成，[Talagala 等，2000 \]。该数据库存储在一个集群上，该集群由 20 个由开关以太网连接并包含 368 个磁盘连接的 PC 组成。它占据了 7 英尺高的架子。

Figure D.6 shows the failure rates of the various components of Tertiary Disk. In advance of building the system, the designers assumed that SCSI data disks would be the least reliable part of the system, as they are both mechanical and plen- tiful. Next would be the IDE disks since there were fewer of them, then the power supplies, followed by integrated circuits. They assumed that passive devices such as cables would scarcely ever fail.

> 图 D.6 显示了三级磁盘各个组件的故障率。在构建系统之前，设计师认为 SCSI 数据磁盘将是系统中最不可分割的部分，因为它们既机械又富裕。接下来是 IDE 磁盘，因为它们的磁盘较少，然后是电源，其次是集成电路。他们认为，电缆等被动设备几乎不会失败。

Figure D.6 shatters some of those assumptions. Since the designers followed the manufacturer’s advice of making sure the disk enclosures had reduced vibra- tion and good cooling, the data disks were very reliable. In contrast, the PC chassis containing the IDE/ATA disks did not afford the same environmental controls. (The IDE/ATA disks did not store data but helped the application and operating system to boot the PCs.) Figure D.6 shows that the SCSI backplane, cables, and Ethernet cables were no more reliable than the data disks themselves!

> 图 D.6 破坏了其中一些假设。由于设计师遵循制造商的建议，即确保磁盘围栏减少了氛围和良好的冷却，因此数据磁盘非常可靠。相反，包含 IDE/ATA 磁盘的 PC 底盘不提供相同的环境控制。(IDE/ATA 磁盘没有存储数据，而是帮助应用程序和操作系统启动 PC。)图 D.6 显示 SCSI 背板，电缆和以太网电缆不比数据磁盘本身更可靠！

Figure D.6 Failures of components in Tertiary Disk over 18 months of operation. For each type of component, the table shows the total number in the system, the number that failed, and the percentage failure rate. Disk enclosures have two entries in the table because they had two types of problems: backplane integrity failures and power supply failures. Since each enclosure had two power supplies, a power supply failure did not affect availability. This cluster of 20 PCs, contained in seven 7-foot-high, 19-inch-wide racks, hosted 368 8.4 GB, 7200 RPM, 3.5-inch IBM disks. The PCs were P6-200 MHz with 96 MB of DRAM each. They ran FreeBSD 3.0, and the hosts were connected via switched 100 Mbit/sec Ethernet. All SCSI disks were connected to two PCs via double-ended SCSI chains to support RAID 1. The primary application was called the Zoom Project, which in 1998 was the world’s largest art image database, with 72,000 images. See Talagala et al. \[2000b\].

> 图 D.6 在运行 18 个月内，第三纪磁盘中的组件故障。对于每种类型的组件，表显示了系统中的总数，失败的数字以及百分比故障率。磁盘外壳在表中有两个条目，因为它们有两种类型的问题：背板完整性故障和电源故障。由于每个外壳都有两个电源，因此电源故障不会影响可用性。该群集由 20 个 PC 组成，其中包含在 7 英尺高，19 英寸宽的机架中，托管 368 8.4 GB，7200 rpm，3.5 英寸 IBM 磁盘。PC 是 P6-200 MHz，每个 PC 均为 96 MB DRAM。他们运行 FreeBSD 3.0，主机通过切换的 100 mbit/sec 以太网连接。所有 SCSI 磁盘都通过双端 SCSI 链连接到两台 PC，以支持 RAID1。主要应用程序称为 Zoom Project，该项目于 1998 年是世界上最大的艺术图像数据库，拥有 72,000 张图像。参见 Talagala 等。\ [2000b \]。

As Tertiary Disk was a large system with many redundant components, it could survive this wide range of failures. Components were connected and mirrored images were placed so that no single failure could make any image unavailable. This strategy, which initially appeared to be overkill, proved to be vital.

> 由于三级磁盘是一个具有许多冗余组件的大型系统，因此它可以在这种广泛的故障中生存。连接组件并放置了镜像图像，因此没有任何单个故障可以使任何图像都无法使用。这种策略最初似乎过于杀伤，这被证明是至关重要的。

This experience also demonstrated the difference between transient faults and hard faults. Virtually all the failures in Figure D.6 appeared first as transient faults. It was up to the operator to decide if the behavior was so poor that they needed to be replaced or if they could continue. In fact, the word “failure” was not used; instead, the group borrowed terms normally used for dealing with problem employees, with the operator deciding whether a problem component should or should not be “fired.”

> 这种经验还证明了瞬态故障和硬故障之间的差异。实际上，图 D.6 中的所有故障首先显示为瞬态故障。由操作员决定行为是否如此糟糕，以至于需要更换他们或是否可以继续。实际上，没有使用“失败”一词。取而代之的是，该小组借用通常用于处理问题员工的术语，而操作员决定是否应“解雇问题”。

### Tandem

> ###串联

The next example comes from industry. Gray \[1990\] collected data on faults for Tandem Computers, which was one of the pioneering companies in fault-tolerant computing and used primarily for databases. [Figure D.7](#_bookmark547) graphs the faults that caused system failures between 1985 and 1989 in absolute faults per system and in percentage of faults encountered. The data show a clear improvement in the reliability of hardware and maintenance. Disks in 1985 required yearly service by Tandem, but they were replaced by disks that required no scheduled mainte- nance. Shrinking numbers of chips and connectors per system plus software’s abil- ity to tolerate hardware faults reduced hardware’s contribution to only 7% of failures by 1989. Moreover, when hardware was at fault, software embedded in the hardware device (firmware) was often the culprit. The data indicate that soft- ware in 1989 was the major source of reported outages (62%), followed by system operations (15%).

> 下一个示例来自行业。Gray \ [1990 \]收集了有关串联计算机故障的数据，该数据是易于故障计算的开创性公司之一，主要用于数据库。[图 D.7](#_ bookmark547)绘制了导致 1985 年至 1989 年之间系统故障的故障，每个系统的绝对故障以及遇到的故障百分比。数据显示了硬件和维护的可靠性明显提高。1985 年的磁盘需要串联每年一次的服务，但它们被无需安排维护的磁盘所取代。每个系统加上软件的芯片和连接器数量缩小，可以容忍硬件故障将硬件的贡献减少到 1989 年的 7％。此外，当硬件处于故障时，软件嵌入了硬件设备(固件)(固件)通常是罪魁祸首。。数据表明，1989 年的软件是报告中断的主要来源(62％)，其次是系统操作(15％)。

The problem with any such statistics is that the data only refer to what is reported; for example, environmental failures due to power outages were not reported to Tandem because they were seen as a local problem. Data on operation faults are very difficult to collect because operators must report personal mistakes, which may affect the opinion of their managers, which in turn can affect job secu- rity and pay raises. Gray suggested that both environmental faults and operator faults are underreported. His study concluded that achieving higher availability requires improvement in software quality and software fault tolerance, simpler operations, and tolerance of operational faults.

> 任何此类统计数据的问题在于数据仅指报告的内容；例如，由于停电而导致的环境故障未报告为串联，因为它们被视为当地问题。关于操作错误的数据很难收集，因为操作员必须报告个人错误，这可能会影响其经理的意见，这反过来可能会影响工作确定性并加薪。格雷建议，环境故障和操作员缺陷都被低估了。他的研究得出的结论是，实现较高的可用性需要改善软件质量和软件容错性，更简单的操作以及操作故障的耐受性。

### Other Studies of the Role of Operators in Dependability

> ###操作员在可靠性方面作用的其他研究

While Tertiary Disk and Tandem are storage-oriented dependability studies, we need to look outside storage to find better measurements on the role of humans in failures. Murphy and Gent \[1995\] tried to improve the accuracy of data on operator faults by having the system automatically prompt the operator on each boot for the reason for that reboot. They classified consecutive crashes to the same fault as operator fault and included operator actions that directly resulted in crashes, such as giving parameters bad values, bad configurations, and bad appli- cation installation. Although they believed that operator error is under-reported, they did get more accurate information than did Gray, who relied on a form that the operator filled out and then sent up the management chain. The hardware/operating system went from causing 70% of the failures in VAX systems in 1985 to 28% in 1993, and failures due to operators rose from 15% to 52% in that same period. Mur- phy and Gent expected managing systems to be the primary dependability chal- lenge in the future.

> 虽然三级磁盘和串联是面向存储的可靠性研究，但我们需要查看外部存储，以找到有关人类在失败中的作用的更好测量。Murphy and Gent \ [1995 \]试图通过让系统自动提示操作员在每个引导上提示操作员的准确性，以便重新启动操作员。他们将连续崩溃分类为与操作员故障相同的故障，并包括直接导致崩溃的操作员操作，例如给出参数不良值，不良配置和不良应用安装。尽管他们认为操作员错误的报道不足，但他们确实获得了比格雷(Gray)更准确的信息，而格雷(Grey)依赖于操作员填写的表格，然后发送了管理链。硬件/操作系统是从 1985 年在 VAX 系统中造成 70％的故障，至 1993 年的 28％，并且由于运营商而导致的失败从同一时期上升到 52％。Mur-Phy 和 Gent 期望管理系统将来成为主要的可靠性。

Figure D.7 Faults in Tandem between 1985 and 1989. Gray \[1990\] collected these data for fault-tolerant Tandem Computers based on reports of component failures by customers.

> 图 D.7 1985 年至 1989 年之间的串联故障。Gray\ [1990 \]根据客户的组件故障报告，收集了这些数据，用于容忍故障的串联计算机。

The final set of data comes from the government. The Federal Communications Commission (FCC) requires that all telephone companies submit explanations when they experience an outage that affects at least 30,000 people or lasts 30 minutes. These detailed disruption reports do not suffer from the self-reporting problem of earlier figures, as investigators determine the cause of the outage rather than operators of the equipment. Kuhn \[1997\] studied the causes of outages between 1992 and 1994, and Enriquez \[2001\] did a follow-up study for the first half of 2001. Although there was a significant improvement in failures due to over- loading of the network over the years, failures due to humans increased, from about one-third to two-thirds of the customer-outage minutes.

> 最后一组数据来自政府。联邦通信委员会(FCC)要求所有电话公司在遇到至少 30,000 人或持续 30 分钟的停电时提交解释。这些详细的破坏报告不会遭受早期数字的自我报告问题的困扰，因为调查人员确定了停电的原因而不是设备的操作员的原因。Kuhn \ [1997 \]研究了 1992 年至 1994 年之间停电的原因，Enriquez \ [2001 \]在 2001 年上半年进行了后续研究。多年来，由于人类而导致的失败，该网络从大约三分之一增加到三分之二的客户淘汰时间。

These four examples and others suggest that the primary cause of failures in large systems today is faults by human operators. Hardware faults have declined due to a decreasing number of chips in systems and fewer connectors. Hardware dependability has improved through fault tolerance techniques such as memory ECC and RAID. At least some operating systems are considering reliability impli- cations before adding new features, so in 2011 the failures largely occurred elsewhere.

> 这四个例子，另一些例子表明，当今大型系统失败的主要原因是人类操作员的故障。由于系统中芯片数量减少，连接器的芯片数量减少，因此硬件故障已下降。硬件可靠性通过可容忍技术(例如内存 ECC 和 RAID)提高了。至少某些操作系统在添加新功能之前正在考虑可靠性暗示，因此在 2011 年，故障大部分发生在其他地方。

Although failures may be initiated due to faults by operators, it is a poor reflec- tion on the state of the art of systems that the processes of maintenance and upgrad- ing are so error prone. Most storage vendors claim today that customers spend much more on managing storage over its lifetime than they do on purchasing the storage. Thus, the challenge for dependable storage systems of the future is either to tolerate faults by operators or to avoid faults by simplifying the tasks of system administration. Note that RAID 6 allows the storage system to survive even if the operator mistakenly replaces a good disk.

> 尽管由于操作员的故障可能引发故障，但对系统技术状态的反映很差，因此维护和升级过程容易出错。大多数存储供应商今天声称，与购买存储空间相比，客户在管理存储期间花费的花费要多得多。因此，对未来可靠的存储系统的挑战要么要容忍操作员的故障，要么通过简化系统管理任务来避免故障。请注意，RAID 6 允许存储系统生存，即使操作员错误地替换了良好的磁盘。

We have now covered the bedrock issue of dependability, giving definitions, case studies, and techniques to improve it. The next step in the storage tour is performance.

> 现在，我们已经涵盖了可靠性的基岩问题，提供了定义，案例研究和改进的技术。存储之旅的下一步是性能。
