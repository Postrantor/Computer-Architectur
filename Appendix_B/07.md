## Historical Perspective and References

In Section M.3 (available online) we examine the history of caches, virtual mem- ory, and virtual machines. (The historical section covers both this appendix and [Chapter 3](#_bookmark93).) IBM plays a prominent role in the history of all three. References for further reading are included.

Additional reference: Gupta, S. Xiang, P., Yang, Y., Zhou, H., Locality prin- ciple revisited: a probability-based quantitative approach. J. Parallel Distrib. Com- put. 73 (7), 1011–1027.

## Exercises by Amr Zaky

1. [10/10/10/15] <B.1> You are trying to appreciate how important the principle of locality is in justifying the use of a cache memory, so you experiment with a com- puter having an L1 data cache and a main memory (you exclusively focus on data accesses). The latencies (in CPU cycles) of the different kinds of accesses are as follows: cache hit, 1 cycle; cache miss, 110 cycles; main memory access with cache disabled, 105 cycles.
2. [10] <B.1> When you run a program with an overall miss rate of 3%, what will the average memory access time (in CPU cycles) be?
3. [10] <B.1> Next, you run a program specifically designed to produce completely random data addresses with no locality. Toward that end, you use an array of size 1 GB (all of which fits in the main memory). Accesses to random elements of this array are continuously made (using a uniform random number generator to generate the elements indices). If your data cache size is 64 KB, what will the average memory access time be?
4. [10] <B.1> If you compare the result obtained in part (b) with the main memory access time when the cache is disabled, what can you conclude about the role of the principle of locality in justifying the use of cache memory?
5. [15] <B.1> You observed that a cache hit produces a gain of 104 cycles (1 cycle vs. 105), but it produces a loss of 5 cycles in the case of a miss (110 cycles vs. 105). In the general case, we can express these two quantities as G (gain) and L (loss). Using these two quantities (G and L), identify the highest miss rate after which the cache use would be disadvanta- geous.

<!--  -->

1. [15/15] <B.1> For the purpose of this exercise, we assume that we have a 512- byte cache with 64-byte blocks. We will also assume that the main memory is 2 KB large. We can regard the memory as an array of 64-byte blocks: M0, M1, …, M31. [Figure B.30](#_bookmark475) sketches the memory blocks that can reside in different cache blocks if the cache was direct-mapped.
2. [15] <B.1> Show the contents of the table if the cache is organized as a fully- associative cache.
3. [15] <B.1> Repeat part (a) with the cache organized as a four-way set associative cache.

<!-- -->

1. [10/10/10/10/15/10/15/20] <B.1> Cache organization is often influenced by the desire to reduce the cache's power consumption. For that purpose we assume that the cache is physically distributed into a data array (holding the data), a tag array (holding the tags), and replacement array (holding information needed by replace- ment policy). Furthermore, every one of these arrays is physically distributed into multiple subarrays (one per way) that can be individually accessed; for example, a four-way set associative least recently used (LRU) cache would have four data sub- arrays, four tag subarrays, and four replacement subarrays. We assume that the replacement subarrays are accessed once per access when the LRU replacement policy is used, and once per miss if the first-in, first-out (FIFO) replacement policy is used. It is not needed when a random replacement policy is used. For a specific cache, it was determined that the accesses to the different arrays have the following power consumption weights ([Figure B.31](#_bookmark476)):

Figure B.30 Memory blocks distributed to direct-mapped cache.
Figure B.31 Power consumption costs of different operations.

1. [10] <B.1> A cache read hit. All arrays are read simultaneously.
2. [10] <B.1> Repeat part (a) for a cache read miss.
3. [10] <B.1> Repeat part (a) assuming that the cache access is split across two cycles. In the first cycle, all the tag subarrays are accessed. In the second cycle, only the subarray whose tag matched will be accessed.
4. [10] <B.1> Repeat part (c) for a cache read miss (no data array accesses in the second cycle).
5. [15] <B.1> Repeat part (c) assuming that logic is added to predict the cache way to be accessed. Only the tag subarray for the predicted way is accessed in
   cycle one. A way hit (address match in predicted way) implies a cache hit. A way miss dictates examining all the tag subarrays in the second cycle. In case of a way hit, only one data subarray (the one whose tag matched) is accessed in cycle two. Assume the way predictor hits.
6. [10] <B.1> Repeat part (e) assuming that the way predictor misses (the way it choses is wrong). When it fails, the way predictor adds an extra cycle in which it
   accesses all the tag subarrays. Assume the way predictor miss is followed by a cache read hit.
7. [15] <B.1> Repeat part (f) assuming a cache read miss.
8. [20] <B.1> Use parts (e), (f), and (g) for the general case where the workload has the following statistics: way predictor miss rate = 5% and cache miss rate= 3%. (Consider different replacement policies.)

Estimate the memory system (cache + memory) power usage (in power units) for the following configurations. We assume the cache is four-way set associative. Provide answers for the LRU, FIFO, and random replacement policies.

1. [10/10/15/15/15/20] <B.1> We compare the write bandwidth requirements of write-through versus write-back caches using a concrete example. Let us assume that we have a 64 KB cache with a line size of 32 bytes. The cache will allocate a line on a write miss. If configured as a write-back cache, it will write back all of the dirty line if it needs to be replaced. We will also assume that the cache is connected to the lower level in the hierarchy through a 64-bit-wide (8-byte-wide) bus. The number of CPU cycles for a B-bytes write access on this bus is

<!--  -->

1. [10] <B.1> For a write-through cache, how many CPU cycles are spent on write transfers to the memory for all the combined iterations of the j loop?
2. [10] <B.1> If the cache is configured as a write-back cache, how many CPU cycles are spent on writing back a cache line?
3. [15] <B.1> Change PORTION to 8 and repeat part (a).
4. [15] <B.1> What is the minimum number of array updates to the same cache line (before replacing it) that would render the write-back cache superior?
5. [15] <B.1> Think of a scenario where all the words of the cache line will be written (not necessarily using the above code) and a write-through cache will require fewer total CPU cycles than the write-back cache.

<!-- -->

1. [10/10/10/10/] <B.2> You are building a system around a processor with in-order execution that runs at 1.1 GHz and has a CPI of 1.35 excluding memory accesses.

The only instructions that read or write data from memory are loads (20% of all instructions) and stores (10% of all instructions). The memory system for this com- puter is composed of a split L1 cache that imposes no penalty on hits. Both the I- cache and D-cache are direct-mapped and hold 32 KB each. The I-cache has a 2% miss rate and 32-byte blocks, and the D-cache is write-through with a 5% miss rate and 16-byte blocks. There is a write buffer on the D-cache that eliminates stalls for 95% of all writes. The 512 KB write-back, unified L2 cache has 64-byte blocks and an access time of 15 ns. It is connected to the L1 cache by a 128-bit data bus that runs at 266 MHz and can transfer one 128-bit word per bus cycle. Of all memory references sent to the L2 cache in this system, 80% are satisfied without going to main memory. Also, 50% of all blocks replaced are dirty. The 128-bit-wide main memory has an access latency of 60 ns, after which any number of bus words may be transferred at the rate of one per cycle on the 128-bit-wide 133 MHz main mem- ory bus. 2. [10] <B.2> What is the average memory access time for instruction accesses? 3. [10] <B.2> What is the average memory access time for data reads? 4. [10] <B.2> What is the average memory access time for data writes? 5. [10] <B.2> What is the overall CPI, including memory accesses?

<!-- -->

1. [10/15/15] <B.2> Converting miss rate (misses per reference) into misses per instruction relies upon two factors: references per instruction fetched and the frac- tion of fetched instructions that actually commits.
2. [10] <B.2> The formula for misses per instruction on page B-5 is written first in terms of three factors: miss rate, memory accesses, and instruction count. Each of these factors represents actual events. What is different about writing misses per instruction as _miss rate_ times the factor _memory accesses per instruction_?
3. [15] <B.2> Speculative processors will fetch instructions that do not commit. The formula for misses per instruction on page B-5 refers to misses per instruc- tion on the execution path; that is, only the instructions that must actually be executed to carry out the program. Convert the formula for misses per instruc- tion on page B-5 into one that uses only miss rate, references per instruction fetched, and fraction of fetched instructions that commit. Why rely upon these factors rather than those in the formula on page B-5?
4. [15] <B.2> The conversion in part (b) could yield an incorrect value to the extent that the value of the factor references per instruction fetched is not equal to the number of references for any particular instruction. Rewrite the formula of part (b) to correct this deficiency.
5. [20] <B.1, B.3> In systems with a write-through L1 cache backed by a write-back L2 cache instead of main memory, a merging write buffer can be simplified. Explain how this can be done. Are there situations where having a full write buffer (instead of the simple version you have just proposed) could be helpful?
6. [5/5/5] <B.3> We want to observe the following calculation _d<sub>i</sub>_ = _a<sub>i</sub>_ + _b<sub>i</sub>_ ∗ _c<sub>i</sub>_ , _i_ : (0 : 511)

Arrays _a_, _b_, _c_, and _d_ memory layout is displayed below (each has 512 4-byte-wide integer elements).

The above calculation employs a for loop that runs through 512 iterations. Assume a 32 Kbyte 4-way set associative cache with a single cycle access time.

The miss penalty is 100 CPU cycles/access, and so is the cost of a write-back. The cache is a write-back on hits write-allocate on misses cache ([Figure B.32](#_bookmark477)).

1. [5] <B3>How many cycles will an iteration take if all three loads and single store miss in the data cache?
2. [5] <B3>If the cache line size is 16 bytes, what is the average number of cycles an average iteration will take? (Hint: Spatial locality!)
3. [5] <B3>If the cache line size is 64 bytes, what is the average number of cycles an average iteration will take?
4. If the cache is direct-mapped and its size is reduced to 2048 bytes, what is the average number of cycles an average iteration will take?

<!-- -->

1. [20] <B.3> Increasing a cache's associativity (with all other parameters kept con- stant) statistically reduces the miss rate. However, there can be pathological cases where increasing a cache's associativity would increase the miss rate for a partic- ular workload.

Consider the case of direct-mapped compared to a two-way set associative cache of equal size. Assume that the set associative cache uses the LRU replace- ment policy. To simplify, assume that the block size is one word. Now, construct a trace of word accesses that would produce more misses in the two-way associative cache.

(Hint: Focus on constructing a trace of accesses that are exclusively directed to a single set of the two-way set associative cache, such that the same trace would exclusively access two blocks in the direct-mapped cache.)

1. [10/10/15] <B.3> Consider a two-level memory hierarchy made of L1 and L2 data caches. Assume that both caches use write-back policy on write hit and both have the same block size. List the actions taken in response to the following events:
2. [10] <B.3> An L1 cache miss when the caches are organized in an inclusive hierarchy.

Figure B.32 Arrays layout in memory.

1. [10] <B.3> An L1 cache miss when the caches are organized in an exclusive hierarchy.
2. [15] <B.3> In both parts (a) and (b), consider the possibility that the evicted line might be clean or dirty.

<!-- -->

1. [15/20] <B.2, B.3> excluding some instructions from entering the cache can reduce conflict misses.

   1. [15] <B.3> Sketch a program hierarchy where parts of the program would be better excluded from entering the instruction cache. (Hint: Consider a program with code blocks that are placed in deeper loop nests than other blocks.)
   2. [20] <B.2, B.3> Suggest software or hardware techniques to enforce exclu- sion of certain blocks from the instruction cache.
2. [5/15] <B.3>Whereas larger caches have lower miss rates, they also tend to have longer hit times. Assume a direct-mapped 8 KB cache has 0.22 ns hit time and miss rate m1; also assume a 4-way associative 64 KB cache has 0.52 ns hit time and a miss rate m2.
3. [5] <B.3>If the miss penalty is 100 ns, when would it be advantageous to use the smaller cache to reduce the overall memory access time?
4. [15] <B.3>Repeat part (a) for miss penalties of 10 and 1000 cycles. Conclude when it might be advantageous to use a smaller cache.

<!-- -->

1. [15] <B.4> A program is running on a computer with a four-entry fully associa- tive (micro) translation lookaside buffer (TLB) ([Figure B.33](#_bookmark478)): The following is a trace of virtual page numbers accessed by a program. For each access indicate whether it produces a TLB hit/miss and, if it accesses the page table, whether it produces a page hit or fault. Put an X under the page table column if it is not accessed ([Figures B.34](#_bookmark479) and [B.35](#_bookmark480)).
2. [15/15/15/15/] <B.4> Some memory systems handle TLB misses in software (as an exception), while others use hardware for TLB misses.

   1. [15] <B.4> What are the trade-offs between these two methods for handling TLB misses?
   2. [15] <B.4> Will TLB miss handling in software always be slower than TLB miss handling in hardware? Explain.

Figure B.35 Page access trace.

1. [15] <B.4> Are there page table structures that would be difficult to handle in hardware but possible in software? Are there any such structures that would be difficult for software to handle but easy for hardware to manage?
2. [15] <B.4> Why are TLB miss rates for floating-point programs generally higher than those for integer programs?

<!-- -->

1. [20/20] <B.5> It is possible to provide more flexible protection than that in the Intel Pentium architecture by using a protection scheme similar to that used in the Hewlett-Packard Precision Architecture (HP/PA). In such a scheme, each page table entry contains a "protection ID" (key) along with access rights for the page. On each reference, the CPU compares the protection ID in the page table entry with those stored in each of four protection ID registers (access to these registers requires that the CPU be in supervisor mode). If there is no match for the protection ID in the page table entry or if the access is not a permitted access (writing to a read- only page, for example), an exception is generated.
2. [20] <B.5> Explain how this model could be used to facilitate the construction of operating systems from relatively small pieces of code that cannot overwrite each other (microkernels). What advantages might such an operating system have over a monolithic operating system in which any code in the OS can write to any memory location?
3. [20] <B.5> A simple design change to this system would allow two protection IDs for each page table entry, one for read access and the other for either write or execute access (the field is unused if neither the writable nor executable bit is set). What advantages might there be from having different protection IDs for read and write capabilities? (_Hint_: Could this make it easier to share data and code between processes?)
