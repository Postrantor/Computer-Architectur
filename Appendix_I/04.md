## Synchronization: Scaling Up

In this section, we focus first on synchronization performance problems in larger multiprocessors and then on solutions for those problems.

> 在本节中，我们首先关注较大的多处理器中的同步性能问题，然后关注解决这些问题的解决方案。

### Synchronization Performance Challenges

To understand why the simple spin lock scheme presented in [Chapter 5](#_bookmark213) does not scale well, imagine a large multiprocessor with all processors contending for the same lock. The directory or bus acts as a point of serialization for all the processors, leading to lots of contention, as well as traffic. The following example shows how bad things can be.

> 要了解为什么[第 5 章]中介绍的简单旋转锁定方案(#\_bookmark213)不能很好地扩展，请想象一个大型多处理器，所有处理器都争夺同一锁。目录或公共汽车是所有处理器的序列化点，导致了很多争夺以及流量。下面的示例显示了有多糟糕的情况。

Example Suppose there are 10 processors on a bus and each tries to lock a variable simul- taneously. Assume that each bus transaction (read miss or write miss) is 100 clock cycles long. You can ignore the time of the actual read or write of a lock held in the cache, as well as the time the lock is held (they won’t matter much!). Determine the number of bus transactions required for all 10 processors to acquire the lock, assuming they are all spinning when the lock is released at time 0\. About how long will it take to process the 10 requests? Assume that the bus is totally fair so that every pending request is serviced before a new request and that the processors are equally fast.

> 示例 假设总线上有 10 个处理器，每个处理器都试图同时锁定一个变量。假设每个总线事务(读取未命中或写入未命中)的长度为 100 个时钟周期。您可以忽略实际读取或写入缓存中持有的锁的时间，以及持有锁的时间(它们无关紧要！)。确定所有 10 个处理器获取锁所需的总线事务数，假设它们在时间 0\ 释放锁时都在自旋。处理这 10 个请求大约需要多长时间？ 假设总线是完全公平的，因此每个待处理的请求都在新请求之前得到服务，并且处理器同样快。

_Answer_ When `i` processes are contending for the lock, they perform the following sequence of actions, each of which generates a bus transaction:

> `answer` 当 *i* 进程争夺锁定时，他们执行以下操作序列，每个操作都会生成总线交易：

Thus, for `i` processes, there are a total of 2*i* + 1 bus transactions. Note that this assumes that the critical section time is negligible, so that the lock is released before any other processors whose store conditional failed attempt another load linked.

> 因此，对于 *i* 进程，总共有 2* I* + 1 总线交易。请注意，这假设关键部分时间可以忽略不计，因此锁定在其他任何其他处理器上链接其他负载链接的其他处理器之前都会释放锁定。

Thus, for `n` processes, the total number of bus operations is

> 因此，对于 *n* 流程，总线运营总数为

For 10 processes there are 120 bus transactions requiring 12,000 clock cycles or 120 clock cycles per lock acquisition!

> 对于 10 个流程，有 120 辆公交交易需要 12,000 个时钟周期或每次锁定的 120 个时钟周期！

The difficulty in this example arises from contention for the lock and serialization of lock access, as well as the latency of the bus access. (The fairness property of the bus actually makes things worse, since it delays the processor that claims the lock from releasing it; unfortunately, for any bus arbitration scheme some worst-case scenario does exist.) The key advantages of spin locks—that they have low over- head in terms of bus or network cycles and offer good performance when locks are reused by the same processor—are both lost in this example. We will consider alternative implementations in the next section, but before we do that, let’s con- sider the use of spin locks to implement another common high-level synchroniza- tion primitive.

> 此示例的困难是由于锁定和序列化锁定访问以及总线访问的延迟而引起的。(公共汽车的公平属性实际上会使情况变得更糟，因为它延迟了声称锁定锁的处理器；不幸的是，对于任何最坏情况下的公交仲裁方案，确实存在一些最坏的情况。)旋转锁的关键优势 - 在公共汽车或网络周期方面，较低的头部较低，当同一处理器重复使用锁时，可以提供良好的性能 - 在此示例中两者都丢失了。我们将在下一部分中考虑替代实现，但是在这样做之前，让我们确定使用旋转锁实现另一种常见的高级同步原始原始原始原始。

##### `Barrier Synchronization`

> ##### `屏障同步`

One additional common synchronization operation in programs with parallel loops is a `barrier`. A barrier forces all processes to wait until all the processes reach the barrier and then releases all of the processes. A typical implementation of a barrier can be done with two spin locks: one to protect a counter that tallies the processes arriving at the barrier and one to hold the processes until the last process arrives at the barrier. To implement a barrier, we usually use the ability to spin on a variable until it satisfies a test; we use the notation spin(condi- tion) to indicate this. [Figure I.2](#_bookmark741) is a typical implementation, assuming that lock and unlock provide basic spin locks and total is the number of pro- cesses that must reach the barrier.

> 在使用并行循环的程序中，另外一个常见的同步操作是 *barrier*。屏障迫使所有过程等到所有过程到达屏障，然后释放所有过程。可以使用两个旋转锁来完成障碍物的典型实现：一个用于保护柜台的计数器，该计数器将到达障碍物的过程占据，并保存该过程，直到最后一个过程到达屏障。要实现障碍，我们通常会使用能够在变量满足测试之前旋转的能力。我们使用符号自旋(条件)来表示这一点。[图 I.2](#_bookmark741) 是一个典型的实现，假设锁定和解锁提供了基本的旋转锁，而总计是必须达到障碍的程序的数量。

Figure I.2 Code for a simple barrier. The lock counterlock protects the counter so that it can be atomically incremented. The variable count keeps the tally of how many processes have reached the barrier. The variable release is used to hold the processes until the last one reaches the barrier. The operation spin (release==1) causes a process to wait until all processes reach the barrier.

> 图 I.2 简单障碍的代码。锁反锁保护计数器，使其可以原子上递增。变量计数使数量达到了障碍的数量。变量释放用于保存过程，直到最后一个释放到达屏障。操作旋转(释放== 1)会导致一个过程等到所有过程到达屏障。

In practice, another complication makes barrier implementation slightly more complex. Frequently a barrier is used within a loop, so that processes released from the barrier would do some work and then reach the barrier again. Assume that one of the processes never actually leaves the barrier (it stays at the spin operation), which could happen if the OS scheduled another process, for example. Now it is possible that one process races ahead and gets to the barrier again before the last process has left. The "fast" process then traps the remaining "slow" process in the barrier by resetting the flag release. Now all the processes will wait infinitely at the next instance of this barrier because one process is trapped at the last instance, and the number of processes can never reach the value of total.

> 实际上，另一个并发症使障碍的实施稍微更加复杂。经常在循环中使用屏障，因此从障碍物释放的过程将进行一些工作，然后再次到达障碍物。假设其中一个过程从未真正离开屏障(它停留在旋转操作上)，例如，如果操作系统安排了另一个过程，则可能发生。现在，一个过程可能会向前竞争，并在最后一个过程离开之前再次进入障碍。然后，"快速" 过程通过重置标志释放来将剩余的 "慢" 过程捕获在屏障中。现在，所有过程都将在此障碍的下一个实例中无限等待，因为一个过程被困在最后一个实例，并且过程数量永远无法达到总数。

The important observation in this example is that the programmer did nothing wrong. Instead, the implementer of the barrier made some assump- tions about forward progress that cannot be assumed. One obvious solution to this is to count the processes as they exit the barrier (just as we did on entry) and not to allow any process to reenter and reinitialize the barrier until all processes have left the prior instance of this barrier. This extra step would significantly increase the latency of the barrier and the contention, which as we will see shortly are already large. An alternative solution is a `sense- reversing barrier`, which makes use of a private per-process variable, local_sense, which is initialized to 1 for each process. [Figure I.3](#_bookmark742) shows the code for the sense-reversing barrier. This version of a barrier is safely usable; as the next example shows, however, its performance can still be quite poor.

> 在此示例中，重要的观察结果是程序员没有做错任何事情。取而代之的是，障碍的实施者对无法假设的前进进度提出了一些影响。一个明显的解决方案是计算这些过程在退出屏障(就像我们进入时一样)时，不允许任何过程重新进入并重新定位屏障，直到所有过程都离开了此屏障的先前实例为止。这一额外的步骤将大大增加障碍和争论的延迟，正如我们不久之后所看到的那样。另一种解决方案是 *sense-逆向屏障*，它利用私有每个程序变量 local_sense，该变量为每个过程初始化为 1。[图 i.3](#_bookmark742) 显示了逆向障碍的代码。此版本的屏障可安全地使用；但是，正如下一个示例所示，其性能仍然很差。

Figure I.3 Code for a sense-reversing barrier. The key to making the barrier reusable is the use of an alternating pattern of values for the flag release, which controls the exit from the barrier. If a process races ahead to the next instance of this barrier while some other processes are still in the barrier, the fast process cannot trap the other processes, since it does not reset the value of release as it did in [Figure I.2](#_bookmark741).

> 图 I.3 逆向障碍的代码。使屏障重复使用的关键是使用交替的值模式用于标志释放，该图案控制着从屏障中的出口。如果在其他一些过程仍在障碍物中的情况下，一个过程到达此障碍的下一个实例，那么快速过程将无法捕获其他过程，因为它不会像[图 I.2]中那样重置发布值(#\_Bookmark741)。

Example Suppose there are 10 processors on a bus and each tries to execute a barrier simul- taneously. Assume that each bus transaction is 100 clock cycles, as before. You can ignore the time of the actual read or write of a lock held in the cache as the time to execute other nonsynchronization operations in the barrier implementation. Deter- mine the number of bus transactions required for all 10 processors to reach the bar- rier, be released from the barrier, and exit the barrier. Assume that the bus is totally fair, so that every pending request is serviced before a new request and that the processors are equally fast. Don’t worry about counting the processors out of the barrier. How long will the entire process take?

> 示例假设总线上有 10 个处理器，每个处理器都试图执行一个屏障。假设每个总线交易是 100 个时钟周期，就像以前一样。您可以忽略缓存中锁定的锁定的实际读或写入时间，因为在屏障实现中执行其他非同步操作的时间。确定所有 10 个处理器到达酒吧，从障碍物中释放并退出障碍所需的公交交易数量。假设总线是完全公平的，因此在新请求之前为每个待处理请求提供服务，并且处理器同样快。不必担心将处理器从障碍物中计算出来。整个过程需要多长时间？

_Answer_ We assume that load linked and store conditional are used to implement lock and unlock. Figure I.4 shows the sequence of bus events for a processor to traverse the barrier, assuming that the first process to grab the bus does not have the lock. There is a slight difference for the last process to reach the barrier, as described in the caption.

> `answer` 我们假设链接和存储条件用于实现锁定和解锁。图 I.4 显示了处理器穿越屏障的总线事件的序列，假设第一个抓住总线的过程没有锁。如标题中所述，最后一个过程要达到屏障有很小的差异。

For the *i*th process, the number of bus transactions is 3*i* + 4. The last process to reach the barrier requires one less. Thus, for `n` processes, the number of bus trans- actions is

> 对于 _i_ 的过程，总线交易的数量为 3*i* + 4.到达障碍物的最后一个过程需要更少。因此，对于 *n* 流程，公交运算的数量为

For 10 processes, this is 204 bus cycles or 20,400 clock cycles! Our barrier operation takes almost twice as long as the 10-processor lock-unlock sequence.

> 对于 10 个过程，这是 204 个公交周期或 20,400 个时钟周期！我们的屏障操作的时间几乎是 10 处理器锁定序列的两倍。

Figure I.4 Here are the actions, which require a bus transaction, taken when the *i*th process reaches the barrier. The last process to reach the barrier requires one less bus transaction, since its read of release for the spin will hit in the cache!

> 图 I.4 这是当 *i *th 过程到达屏障时采取的行动，需要公交交易。到达障碍物的最后一个过程需要更少的巴士交易，因为它读取了旋转的释放将在缓存中击中！

As we can see from these examples, synchronization performance can be a real bottleneck when there is substantial contention among multiple processes. When there is little contention and synchronization operations are infrequent, we are primarily concerned about the latency of a synchronization primitive— that is, how long it takes an individual process to complete a synchronization operation. Our basic spin lock operation can do this in two bus cycles: one to initially read the lock and one to write it. We could improve this to a single bus cycle by a variety of methods. For example, we could simply spin on the swap operation. If the lock were almost always free, this could be better, but if the lock were not free, it would lead to lots of bus traffic, since each attempt to lock the variable would lead to a bus cycle. In practice, the latency of our spin lock is not quite as bad as we have seen in this example, since the write miss for a data item present in the cache is treated as an upgrade and will be cheaper than a true read miss.

> 从这些示例中我们可以看出，当多个进程之间存在大量争用时，同步性能可能成为真正的瓶颈。当竞争很少且同步操作不频繁时，我们主要关心同步原语的延迟——即单个进程完成同步操作需要多长时间。我们的基本自旋锁操作可以在两个总线周期内完成：一个用于初始读取锁，一个用于写入锁。我们可以通过多种方法将其改进为单个总线周期。例如，我们可以简单地旋转交换操作。如果锁几乎总是空闲的，这可能会更好，但如果锁不是空闲的，就会导致大量的总线流量，因为每次尝试锁定变量都会导致一个总线周期。实际上，自旋锁的延迟并不像我们在这个例子中看到的那么糟糕，因为缓存中存在的数据项的写入未命中被视为升级，并且比真正的读取未命中更便宜。

The more serious problem in these examples is the serialization of each pro- cess’s attempt to complete the synchronization. This serialization is a problem when there is contention because it greatly increases the time to complete the synchronization operation. For example, if the time to complete all 10 lock and unlock operations depended only on the latency in the uncontended case, then it would take 1000 rather than 15,000 cycles to complete the synchroniza- tion operations. The barrier situation is as bad, and in some ways worse, since it is highly likely to incur contention. The use of a bus interconnect exacerbates these problems, but serialization could be just as serious in a directory-based multiprocessor, where the latency would be large. The next subsection presents some solutions that are useful when either the contention is high or the processor count is large.

> 在这些示例中，更严重的问题是每个程序完成同步的尝试的序列化。当存在争议时，这种序列化是一个问题，因为它大大增加了完成同步操作的时间。例如，如果该完成所有 10 个锁定并解锁操作仅取决于无害情况下的延迟，那么完成同步操作将需要 1000 个而不是 15,000 个周期。障碍局势也很糟糕，在某些方面更糟，因为这很可能引起争论。总线互连的使用加剧了这些问题，但是在基于目录的多处理器中，序列化可能同样是严重的，在此延迟很大。下一个小节介绍了一些解决方案，这些解决方案是当竞争者高或处理器计数较大时很有用。

### Synchronization Mechanisms for Larger-Scale Multiprocessors

What we would like are synchronization mechanisms that have low latency in uncontended cases and that minimize serialization in the case where contention is significant. We begin by showing how software implementations can improve the performance of locks and barriers when contention is high; we then explore two basic hardware primitives that reduce serialization while keeping latency low.

> 我们想要的是同步机制在无害的情况下的潜伏期较低，并且在争论很大的情况下最小化序列化。首先，我们展示软件实施如何在竞争较高时改善锁和障碍的性能；然后，我们探索两个基本的硬件基础，以减少序列化，同时保持延迟较低。

##### `Software Implementations`

The major difficulty with our spin lock implementation is the delay due to conten- tion when many processes are spinning on the lock. One solution is to artificially delay processes when they fail to acquire the lock. The best performance is obtained by increasing the delay exponentially whenever the attempt to acquire the lock fails. [Figure I.5](#_bookmark743) shows how a spin lock with `exponential back-off` is imple- mented. Exponential back-off is a common technique for reducing contention in shared resources, including access to shared networks and buses (see [Sections](#_bookmark595) [F.4](#_bookmark595) to [F.8](#_bookmark595)). This implementation still attempts to preserve low latency when con- tention is small by not delaying the initial spin loop. The result is that if many pro- cesses are waiting, the back-off does not affect the processes on their first attempt to acquire the lock. We could also delay that process, but the result would be poorer performance when the lock was in use by only two processes and the first one hap- pened to find it locked.

> 我们的自旋锁实现的主要困难是当许多进程在锁上自旋时由于争用而导致的延迟。一种解决方案是在无法获取锁时人为地延迟进程。每当尝试获取锁失败时，通过以指数方式增加延迟来获得最佳性能。[图 I.5](#_bookmark743) 显示了如何实现具有 "指数退避" 的自旋锁。指数退避是一种减少共享资源争用的常用技术，包括对共享网络和总线的访问(参见[章节](#_bookmark595) [F.4](#_bookmark595) 至 [F.8](#_bookmark595) ). 当竞争较小时，此实现仍会尝试通过不延迟初始自旋循环来保持低延迟。结果是，如果有许多进程在等待，退避不会影响进程在它们第一次尝试获取锁时。我们也可以延迟该进程，但是当只有两个进程在使用锁并且第一个恰好发现它被锁定时，结果会导致性能变差。

Figure I.5 A spin lock with exponential back-off. When the store conditional fails, the process delays itself by the value in R3. The delay can be implemented by decrementing a copy of the value in R3 until it reaches 0. The exact timing of the delay is multiproces- sor dependent, although it should start with a value that is approximately the time to perform the critical section and release the lock. The statement pause R3 should cause a delay of R3 of these time units. The value in R3 is increased by a factor of 2 every time the store conditional fails, which causes the process to wait twice as long before trying to acquire the lock again. The small variations in the rate at which competing processors execute instructions are usually sufficient to ensure that processes will not continually collide. If the natural perturbation in execution time was insufficient, R3 could be initial- ized with a small random value, increasing the variance in the successive delays and reducing the probability of successive collisions.

> 图 I.5 具有指数退避的自旋锁。当条件存储失败时，进程将自身延迟 R3 中的值。延迟可以通过递减 R3 中值的副本直到它达到 0 来实现。延迟的确切时间取决于多处理器，尽管它应该以大约执行临界区和释放时间的值开始 锁。语句 pause R3 应该会导致这些时间单位延迟 R3。每次存储条件失败时，R3 中的值都会增加 2 倍，这会导致进程在尝试再次获取锁之前等待两倍的时间。竞争处理器执行指令的速率的微小变化通常足以确保进程不会持续冲突。如果执行时间的自然扰动不足，可以用一个小的随机值初始化 R3，增加连续延迟的方差并降低连续碰撞的概率。

Another technique for implementing locks is to use queuing locks. Queuing locks work by constructing a queue of waiting processors; whenever a processor frees up the lock, it causes the next processor in the queue to attempt access. This eliminates contention for a lock when it is freed. We show how queuing locks oper- ate in the next section using a hardware implementation, but software implementa- tions using arrays can achieve most of the same benefits. Before we look at hardware primitives, let’s look at a better mechanism for barriers.

> 另一种实现锁的技术是使用队列锁。队列锁通过构建等待处理器队列来工作；每当处理器释放锁时，它都会导致队列中的下一个处理器尝试访问。这消除了锁被释放时的争用。我们将在下一节中使用硬件实现来展示队列锁是如何运行的，但是使用数组的软件实现可以实现大部分相同的好处。在我们研究硬件原语之前，让我们先看看更好的屏障机制。

Our barrier implementation suffers from contention both during the `gather` stage, when we must atomically update the count, and at the `release` stage, when all the processes must read the release flag. The former is more serious because it requires exclusive access to the synchronization variable and thus creates much more serialization; in comparison, the latter generates only read contention. We can reduce the contention by using a `combining tree`, a structure where multiple requests are locally combined in tree fashion. The same combining tree can be used to implement the release process, reducing the contention there.

> 我们的障碍实现都在 *gather* 阶段，当我们必须原子更新计数以及在所有过程必须读取版本标志时，都遭受争夺。前者更为严重，因为它需要独家访问同步变量，从而创造了更多的序列化。相比之下，后者仅产生读取争论。我们可以使用 *combining Tree* 降低争论，该结构是以树时代本地组合的多个请求。相同的组合树可用于实现释放过程，从而减少那里的争论。

Our combining tree barrier uses a predetermined `n`-ary tree structure. We use the variable `k` to stand for the fan-in; in practice, `k` 4 seems to work well. When the *k*th process arrives at a node in the tree, we signal the next level in the tree. When a process arrives at the root, we release all waiting processes. As in our ear- lier example, we use a sense-reversing technique. A tree-based barrier, as shown in [Figure I.6](#_bookmark744), uses a tree to combine the processes and a single signal to release the barrier. Some MPPs (e.g., the T3D and CM-5) have also included hardware sup- port for barriers, but more recent machines have relied on software libraries for this support.

> 我们的组合树屏障使用预定的 *n*-ary 树结构。我们使用变量 *K* 代表粉丝。实际上，_k_ 4 似乎运行良好。当 *k *th 过程到达树中的一个节点时，我们向树中的下一个级别发出信号。当一个过程到达根部时，我们会发布所有等待过程。与我们的示例一样，我们使用一种逆向感知的技术。如[图 I.6](#_bookmark744) 所示，基于树的障碍物使用树结合过程和单个信号来释放屏障。一些 MPP(例如 T3D 和 CM-5)还包括用于障碍的硬件 Sup-Port，但最新的机器依靠软件库来获得此支持。

##### `Hardware Primitives`

In this subsection, we look at two hardware synchronization primitives. The first primitive deals with locks, while the second is useful for barriers and a number of other user-level operations that require counting or supplying distinct indices. In both cases, we can create a hardware primitive where latency is essentially iden- tical to our earlier version, but with much less serialization, leading to better scaling when there is contention.

> 在本小节中，我们查看两个硬件同步原语。第一个原始的交易涉及锁，而第二个则对障碍和许多其他需要计数或提供不同索引的用户级操作很有用。在这两种情况下，我们都可以创建一个硬件原始性，其中延迟本质上是我们较早版本的标志性的，但是序列化的效果要少得多，从而在有争议时会更好地缩放。

The major problem with our original lock implementation is that it introduces a large amount of unneeded contention. For example, when the lock is released all processors generate both a read and a write miss, although at most one processor can successfully get the lock in the unlocked state. This sequence happens on each of the 10 lock/unlock sequences, as we saw in the example on page I-12.

> 我们原始锁定实施的主要问题是它引入了大量不需要的争论。例如，当释放锁定时，所有处理器都会同时生成读取和写入错过，尽管最多可以成功地将锁定在未锁定状态下。正如我们在 I-12 页上的示例中所看到的那样，该序列发生在 10 个锁定/解锁序列中的每个序列上。

We can improve this situation by explicitly handing the lock from one waiting processor to the next. Rather than simply allowing all processors to compete every time the lock is released, we keep a list of the waiting processors and hand the lock to one explicitly, when its turn comes. This sort of mechanism has been called a `queuing lock`. Queuing locks can be implemented either in hardware, which we value of tree\[root].parent should be set to —1 when the tree is initially built.

> 我们可以通过将锁从一个等待处理器转移到另一个锁定，可以改善这种情况。与其简单地允许所有处理器每次释放锁时竞争，我们将保留等待处理器的列表，并在转弯时明确地将锁定交给一个。这种机制称为 *queuing lock*。可以在硬件中实现排队锁，我们对树的价值\ [root ]。当最初构建树时，应将 parent 设置为-1。

Figure I.6 An implementation of a tree-based barrier reduces contention consider- ably. The tree is assumed to be prebuilt statically using the nodes in the array tree. Each node in the tree combines `k` processes and provides a separate counter and lock, so that at most `k` processes contend at each node. When the *k*th process reaches a node in the tree, it goes up to the parent, incrementing the count at the parent. When the count in the parent node reaches `k`, the release flag is set. The count in each node is reset by the last process to arrive. Sense-reversing is used to avoid races as in the simple barrier. The describe here, or in software using an array to keep track of the waiting processes. The basic concepts are the same in either case. Our hardware implementation assumes a directory-based multiprocessor where the individual processor caches are addressable. In a bus-based multiprocessor, a software implementation would be more appropriate and would have each processor using a different address for the lock, permitting the explicit transfer of the lock from one process to another. How does a queuing lock work? On the first miss to the lock variable, the miss is sent to a synchronization controller, which may be integrated with the memory controller (in a bus-based system) or with the directory controller. If the lock is free, it is simply returned to the processor. If the lock is unavailable, the controller creates a record of the node’s request (such as a bit in a vector) and sends the processor back a locked value for the variable, which the processor then spins on. When the lock is freed, the controller selects a processor to go ahead from the list of waiting processors. It can then either update the lock variable in the selected processor’s cache or invalidate the copy, causing the processor to miss and fetch an available copy of the lock.

> 图 I.6 基于树的屏障的实现大大减少了争用。假设树是使用数组树中的节点静态预构建的。树中的每个节点都结合了 "k" 个进程并提供了一个单独的计数器和锁，因此最多 "k" 个进程在每个节点上竞争。当 *k*th 进程到达树中的一个节点时，它会向上移动到父节点，增加父节点的计数。当父节点中的计数达到 "k" 时，释放标志被设置。每个节点中的计数由最后一个到达的进程重置。意义反转用于避免像简单障碍中的竞争。此处描述，或在软件中使用数组来跟踪等待进程。在这两种情况下，基本概念都是相同的。我们的硬件实现假定一个基于目录的多处理器，其中各个处理器缓存是可寻址的。在基于总线的多处理器中，软件实现会更合适，并且让每个处理器使用不同的锁地址，从而允许将锁从一个进程显式传输到另一个进程。排队锁是如何工作的？ 在锁定变量的第一次未命中时，未命中被发送到同步控制器，该同步控制器可以与内存控制器(在基于总线的系统中)或目录控制器集成在一起。如果锁是空闲的，它就简单地返回给处理器。如果锁不可用，控制器会创建节点请求的记录(例如向量中的位)，并向处理器发回变量的锁定值，然后处理器会旋转该变量。当锁被释放时，控制器从等待处理器列表中选择一个处理器继续执行。然后它可以更新所选处理器缓存中的锁变量或使副本无效，从而导致处理器错过并获取锁的可用副本。

Example How many bus transactions and how long does it take to have 10 processors lock and unlock the variable using a queuing lock that updates the lock on a miss? Make the other assumptions about the system the same as those in the earlier example on page I-12.

> 示例，使用 10 个处理器锁定并使用排队锁来更新失误上的锁？使有关系统的其他假设与第 I-12 页的早期示例中的其他假设相同。

_Answer_ For `n` processors, each will initially attempt a lock access, generating a bus trans- action; one will succeed and free up the lock, for a total of `n` + 1 transactions for the first processor. Each subsequent processor requires two bus transactions, one to receive the lock and one to free it up. Thus, the total number of bus transactions is (_n_ + 1)+ 2(_n_ 1) 3*n* 1. Note that the number of bus transactions is now linear in the number of processors contending for the lock, rather than quadratic, as it was with the spin lock we examined earlier. For 10 processors, this requires 29 bus cycles or 2900 clock cycles.

> `answer` 对于 *n* 处理器，每个处理器最初将尝试锁定访问权限，生成总线 trans-consion；一个人将成功并释放锁，对于第一个处理器的总共 *n* + 1 事务。每个后续处理器都需要两项总线交易，一个要接收锁，另一个以释放锁。因此，总线交易的总数为(_n_ + 1) + 2(_n_ 1)3* n* 1.就像我们之前检查的旋转锁一样。对于 10 个处理器，这需要 29 个总线周期或 2900 个时钟周期。

There are a couple of key insights in implementing such a queuing lock capability. First, we need to be able to distinguish the initial access to the lock, so we can per- form the queuing operation, and also the lock release, so we can provide the lock to another processor. The queue of waiting processes can be implemented by a variety of mechanisms. In a directory-based multiprocessor, this queue is akin to the shar- ing set, and similar hardware can be used to implement the directory and queuing lock operations. One complication is that the hardware must be prepared to reclaim such locks, since the process that requested the lock may have been context- switched and may not even be scheduled again on the same processor.

> 实施这种排队的锁定能力有一些关键见解。首先，我们需要能够区分对锁定的初始访问，以便我们可以形成排队操作以及锁定版本，以便我们可以向另一个处理器提供锁。等待过程的队列可以通过多种机制实施。在基于目录的多处理器中，此队列类似于共享集，并且可以使用类似的硬件来实现目录和排队锁定操作。一个复杂的是，必须准备好硬件来回收此类锁，因为请求锁定的过程可能已被上下文切换，甚至可能不会再次安排在同一处理器上。

Queuing locks can be used to improve the performance of our barrier operation. Alternatively, we can introduce a primitive that reduces the amount of time needed to increment the barrier count, thus reducing the serialization at this bottleneck, which should yield comparable performance to using queuing locks. One primitive that has been introduced for this and for building other synchronization operations is `fetch-and-increment`, which atomically fetches a variable and increments its value. The returned value can be either the incremented value or the fetched value. Using fetch-and-increment we can dramatically improve our barrier implementa- tion, compared to the simple code-sensing barrier.

> 排队锁可用于改善我们的屏障操作的性能。另外，我们可以引入一个原始性，该原始性减少了增加屏障计数所需的时间，从而减少了该瓶颈处的序列化，该序列化应产生可比的性能与使用排队锁。为此引入并构建其他同步操作的一种原始性是 *fetch and-increment*，它从原子上获取变量并增加其值。返回的值可以是增量值或获取的值。与简单的密码屏障相比，我们可以使用提取和灌溉来显着改善屏障实现。

Example Write the code for the barrier using fetch-and-increment. Making the same assump- tions as in our earlier example and also assuming that a fetch-and-increment oper- ation, which returns the incremented value, takes 100 clock cycles, determine the time for 10 processors to traverse the barrier. How many bus cycles are required?

> 示例使用提取和提示编写屏障代码。进行与我们之前的示例相同的调解，并假设返回增量值的提取和插入操作需要 100 个时钟周期，请确定 10 个处理器遍历屏障的时间。需要多少个公交周期？

Figure I.7 Code for a sense-reversing barrier using fetch-and-increment to do the counting.

_Answer_ [Figure I.7](#_bookmark745) shows the code for the barrier. For `n` processors, this implementation requires `n` fetch-and-increment operations, `n` cache misses to access the count, and `n` cache misses for the release operation, for a total of 3*n* bus transactions. For 10 processors, this is 30 bus transactions or 3000 clock cycles. Like the queue- ing lock, the time is linear in the number of processors. Of course, fetch-and- increment can also be used in implementing the combining tree barrier, reducing the serialization at each node in the tree.

> `answer` [图 I.7](#_Bookmark745) 显示了屏障的代码。对于 *n* 处理器，此实现需要 *n* 提取和插入操作，*n* 缓存未能访问计数，而 *n* cache 却错过了释放操作，总共进行了 3* n* BUS TRASSACTIONS。对于 10 个处理器，这是 30 个总线交易或 3000 个时钟周期。像队列锁一样，处理器数量是线性的。当然，提取和增量也可以用于实现组合树屏障，从而减少树中每个节点的序列化。

As we have seen, synchronization problems can become quite acute in largerscale multiprocessors. When the challenges posed by synchronization are combined with the challenges posed by long memory latency and potential load imbalance in computations, we can see why getting efficient usage of large-scale parallel pro- cessors is very challenging.

> 如我们所见，在大规模的多处理器中，同步问题可能变得非常急切。当同步提出的挑战与长期记忆潜伏期和计算中潜在负载不平衡所面临的挑战相结合时，我们可以看到为什么要有效地使用大规模并行专业人员，这是非常具有挑战性的。
