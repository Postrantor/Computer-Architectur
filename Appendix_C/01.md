## Introduction

> ＃＃ 介绍

Many readers of this text will have covered the basics of pipelining in another text (such as our more basic text _Computer Organization and Design_) or in another course. Because [Chapter 3](#_bookmark93) builds heavily on this material, readers should ensure that they are familiar with the concepts discussed in this appendix before proceed- ing. As you read [Chapter 3](#_bookmark93), you may find it helpful to turn to this material for a quick review.

> 本文的许多读者将涵盖另一个文本中管道的基础知识（例如我们更基本的文本_Computer 组织和 Design_）或另一个课程。因为[第 3 章]（#_ bookmark93）在此材料上建立了很大的建立，所以读者应确保他们熟悉此附录中讨论的概念。当您阅读[第 3 章]（#_ bookmark93）时，您可能会发现转向此材料以进行快速审查很有帮助。

We begin the appendix with the basics of pipelining, including discussing the data path implications, introducing hazards, and examining the performance of pipelines. This section describes the basic five-stage RISC pipeline that is the basis for the rest of the appendix. [Section C.2](#the-major-hurdle-of-pipeliningpipeline-hazards) describes the issue of hazards, why they cause performance problems, and how they can be dealt with. [Section C.3](#how-is-pipelining-implemented) dis- cusses how the simple five-stage pipeline is actually implemented, focusing on control and how hazards are dealt with.

> 我们从管道上的基础知识开始附录，包括讨论数据路径的含义，引入危害以及检查管道的性能。本节介绍了基本的五阶段 RISC 管道，该管道是附录其余部分的基础。[C.2]（＃梅霍尔 - 赫尔德赫尔德林 - 亨斯 - 危险）描述了危害的问题，为什么会导致绩效问题以及如何处理它们。[c.3 节]（＃如何实施介绍）讨论了如何实际实施简单的五阶段管道，重点关注控制以及如何处理危险。

[Section C.4](#what-makes-pipelining-hard-to-implement) discusses the interaction between pipelining and various aspects of instruction set design, including discussing the important topic of exceptions and their interaction with pipelining. Readers unfamiliar with the concepts of precise and imprecise interrupts and resumption after exceptions will find this material useful, because they are key to understanding the more advanced approaches in [Chapter 3](#_bookmark93).

> [c.4]（＃＃makes pipielining-hard-to-implement）讨论了管道式设计和指令集设计的各个方面之间的相互作用，包括讨论异常的重要主题及其与管道上的相互作用。读者不熟悉精确和不精确的中断和恢复的概念会发现这种材料有用，因为它们是理解[第 3 章]中更高级方法的关键（#_ bookmark93）。

[Section C.5](#extending-the-risc-v-integer-pipeline-to-handle-multicycle-operations) discusses how the five-stage pipeline can be extended to handle longer-running floating-point instructions. [Section C.6](#_bookmark521) puts these concepts together in a case study of a deeply pipelined processor, the MIPS R4000/4400, including both the eight-stage integer pipeline and the floating-point pipeline. The MIPS R40000 is similar to a single-issue embedded processor, such as the ARM Cortex-A5, which became available in 2010, and was used in several smart phones and tablets.

> [C.5 节]（＃扩展 risc-v-integer-pipeline to handle-multiclecle-operations）讨论了如何扩展五阶段管道以处理较长运行的浮点指令。[C.6 节]（#_ bookmark521）将这些概念放在一个深层管道处理器的案例研究中，MIPS R4000/4400，包括八阶段整数管道和浮点管道。MIPS R40000 类似于单发的嵌入式处理器，例如 ARM Cortex-A5，该处理器于 2010 年上市，并用于多个智能手机和平板电脑。

[Section C.7](#cross-cutting-issues-5) introduces the concept of dynamic scheduling and the use of scoreboards to implement dynamic scheduling. It is introduced as a cross-cutting issue, because it can be used to serve as an introduction to the core concepts in [Chapter 3](#_bookmark93), which focused on dynamically scheduled approaches. [Section C.7](#cross-cutting-issues-5) is also a gentle introduction to the more complex Tomasulo’s algorithm covered in [Chapter 3](#_bookmark93). Although Tomasulo’s algorithm can be covered and understood with- out introducing scoreboarding, the scoreboarding approach is simpler and easier to comprehend.

> [C.7]（＃cross-Cutting-Issues-5）介绍了动态调度的概念和计分板的使用来实现动态调度。它被引入作为跨切割问题，因为它可以用作[第 3 章]（#_ bookmark93）中的核心概念的介绍，该概念的重点是动态计划的方法。[C.7]（＃Cross-Cutting-Issues-5）也是对[第 3 章]（#_ bookmark93）涵盖的更复杂的 tomasulo 算法的温和介绍。尽管可以涵盖和理解 Tomasulo 的算法，并引入计分板，但记分板方法更容易理解。

### What Is Pipelining?

> ###什么是管道？

_Pipelining_ is an implementation technique whereby multiple instructions are over- lapped in execution; it takes advantage of parallelism that exists among the actions needed to execute an instruction. Today, pipelining is the key implementation tech- nique used to make fast processors, and even processors that cost less than a dollar are pipelined.

> _PIPELINGIND 是一种实现技术，在该技术中，执行多个指令；它利用了执行指令所需的行动之间存在的并行性。如今，管道是用于制造快速处理器的主要实施技术，甚至价格低于一美元的处理器也是管道。

A pipeline is like an assembly line. In an automobile assembly line, there are many steps, each contributing something to the construction of the car. Each step operates in parallel with the other steps, although on a different car. In a computer pipeline, each step in the pipeline completes a part of an instruction. Like the assembly line, different steps are completing different parts of different instruc- tions in parallel. Each of these steps is called a _pipe stage_ or a _pipe segment_. The stages are connected one to the next to form a pipe—instructions enter at one end, progress through the stages, and exit at the other end, just as cars would in an assembly line.

> 管道就像一个装配线。在汽车装配线中，有很多步骤，每个步骤都为汽车的建造做出了贡献。每个步骤都与其他步骤并行运行，尽管在不同的汽车上。在计算机管道中，管道中的每个步骤都完成了指令的一部分。像组装线一样，不同的步骤并行完成不同指令的不同部分。这些步骤中的每一个称为_ -Pipe stage_或_ -Pipe segment_。将一个阶段连接到一个形成管道的下一阶段 - 一端进入，从阶段进行进展，然后在另一端退出，就像汽车在装配线中一样。

In an automobile assembly line, _throughput_ is defined as the number of cars per hour and is determined by how often a completed car exits the assembly line. Like- wise, the throughput of an instruction pipeline is determined by how often an instruction exits the pipeline. Because the pipe stages are hooked together, all the stages must be ready to proceed at the same time, just as we would require in an assembly line. The time required between moving an instruction one step down the pipeline is a _processor cycle_. Because all stages proceed at the same time, the length of a processor cycle is determined by the time required for the slowest pipe stage, just as in an auto assembly line the longest step would determine the time between advancing cars in the line. In a computer, this processor cycle is almost always 1 clock cycle.

> 在汽车装配线中，_throughput_定义为每小时汽车数量，由完成的汽车退出装配线的频率确定。同样，指令管道的吞吐量取决于指令退出管道的频率。由于管道阶段被钩在一起，因此所有阶段都必须准备好同时进行，就像我们在装配线中所需的那样。向下移动说明所需的时间是_Processor Cycle_。由于所有阶段都同时进行，因此处理器周期的长度取决于最慢的管道阶段所需的时间，就像在自动装配线中一样，最长的步骤将决定线路中前进的汽车之间的时间。在计算机中，此处理器周期几乎总是 1 个时钟周期。

The pipeline designer’s goal is to balance the length of each pipeline stage, just as the designer of the assembly line tries to balance the time for each step in the process. If the stages are perfectly balanced, then the time per instruction on the pipelined processor—assuming ideal conditions—is equal to

> 管道设计人员的目标是平衡每个管道阶段的长度，就像装配线的设计师试图平衡过程中每个步骤的时间一样。如果阶段完美平衡，那么管道上的处理器上的每条指令（理想条件）等于

Time per instruction on unpipelined machine Number of pipe stages

> 在不封装机器数量的管道阶段的每指示度的时间

Under these conditions, the speedup from pipelining equals the number of pipe stages, just as an assembly line with _n_ stages can ideally produce cars _n_ times as fast. Usually, however, the stages will not be perfectly balanced; furthermore, pipelining does involve some overhead. Thus, the time per instruction on the pipe- lined processor will not have its minimum possible value, yet it can be close.

> 在这些条件下，管道上的加速等于管道阶段的数量，就像带有_n_阶段的装配线可以理想地生产汽车_n_乘以快速一样。但是，通常，阶段不会完全平衡。此外，管道确实涉及一些开销。因此，管道衬里处理器上的每个指令的时间将没有其最小值，但可以接近。

Pipelining yields a reduction in the average execution time per instruction. If the starting point is a processor that takes multiple clock cycles per instruction, then pipelining reduces the CPI. This is the primary view we will take.

> 管道上的平均执行时间减少。如果起点是每个指令进行多个时钟循环的处理器，则管道上的 CPI 减少。这是我们将采取的主要视图。

Pipelining is an implementation technique that exploits parallelism among the instructions in a sequential instruction stream. It has the substantial advantage that, unlike some speedup techniques (see [Chapter 4](#_bookmark165)), it is not visible to the programmer.

> 管道上是一种实现技术，可利用顺序指令流中指令之间的并行性。它具有很大的优势，即与某些加速技术不同（请参阅[第 4 章]（#_ bookmark165）），程序员看不到。

### The Basics of the RISC V Instruction Set

> ### RISC V 指令集的基础知识

Throughout this book we use RISC V, a load-store architecture, to illustrate the basic concepts. Nearly all the ideas we introduce in this book are applicable to other

> 在整本书中，我们使用负载商店体系结构 RISC V 来说明基本概念。我们在本书中介绍的几乎所有想法都适用于其他

processors, but the implementation may be much more complicated with complex instructions. In this section, we make use of the core of the RISC V architecture; see [Chapter 1](#_bookmark2) for a full description. Although we use RISC V, the concepts are significantly similar in that they will apply to any RISC, including the core archi- tectures of ARM and MIPS. All RISC architectures are characterized by a few key properties:

> 处理器，但是通过复杂的说明，实施可能会更加复杂。在本节中，我们利用 RISC V 架构的核心；有关完整说明，请参见[第 1 章]（#_ bookmark2）。尽管我们使用 RISC V，但这些概念非常相似，因为它们将适用于任何 RISC，包括 ARM 和 MIPS 的核心档案。所有 RISC 体系结构均以一些关键属性为特征：

- All operations on data apply to data in registers and typically change the entire register (32 or 64 bits per register).

> - 数据上的所有操作都适用于寄存器中的数据，并通常更改整个寄存器（每个寄存器 32 或 64 位）。

- The only operations that affect memory are load and store operations that move data from memory to a register or to memory from a register, respectively. Load and store operations that load or store less than a full register (e.g., a byte, 16 bits, or 32 bits) are often available.

> - 影响内存的唯一操作是将数据从内存转移到寄存器或从寄存器中移动到内存的负载和存储操作。加载和存储操作的加载或存储少于完整寄存器（例如，一个字节，16 位或 32 位）通常可用。

- The instruction formats are few in number, with all instructions typically being one size. In RISC V, the register specifiers: rs1, rs2, and rd are always in the same place simplifying the control.

> - 指令格式的数量很少，所有指令通常为一个尺寸。在 RISC V 中，寄存器指定器：RS1，RS2 和 RD 始终在同一位置简化控件。

These simple properties lead to dramatic simplifications in the implementation of pipelining, which is why these instruction sets were designed this way. [Chapter 1](#_bookmark2) contains a full description of the RISC V ISA, and we assume the reader has read [Chapter 1](#_bookmark2).

> 这些简单的属性导致了管道实施的戏剧性简化，这就是为什么这些指令集以这种方式设计的原因。[第 1 章]（#_ bookmark2）包含 RISC V ISA 的完整描述，我们假设读者已阅读[第 1 章]（#_ bookmark2）。

### A Simple Implementation of a RISC Instruction Set

> ### RISC 指令集的简单实现

To understand how a RISC instruction set can be implemented in a pipelined fashion, we need to understand how it is implemented _without_ pipelining. This section shows a simple implementation where every instruction takes at most 5 clock cycles. We will extend this basic implementation to a pipelined version, resulting in a much lower CPI. Our unpipelined implementation is not the most eco- nomical or the highest-performance implementation without pipelining. Instead, it is designed to lead naturally to a pipelined implementation. Implementing the instruction set requires the introduction of several temporary registers that are not part of the architecture; these are introduced in this section to simplify pipelin- ing. Our implementation will focus only on a pipeline for an integer subset of a RISC architecture that consists of load-store word, branch, and integer ALU operations.

> 要了解如何以管道方式实现 RISC 指令集，我们需要了解如何实现_without_管道。本节显示了一个简单的实现，每个指令最多进行 5 个时钟周期。我们将将此基本实现扩展到管道上的版本，从而导致 CPI 要低得多。我们的不封装实施不是没有管道的最具经济或最高绩效的实施。取而代之的是，它旨在自然地导致管道实现。实施指令集需要引入几个不属于体系结构的临时寄存器；在本节中介绍了这些内容，以简化管道。我们的实施仅专注于由负载商店，分支和整数 Alu 操作组成的 RISC 体系结构的整数子集的管道。

Every instruction in this RISC subset can be implemented in, at most, 5 clock cycles. The 5 clock cycles are as follows.

> RISC 子集中的每个指令最多都可以在 5 个时钟周期中实现。5 个时钟周期如下。

1. _Instruction fetch cycle_ (IF): Send the program counter (PC) to memory and fetch the current instruction from memory. Update the PC to the next sequential instruction by adding 4 (because each instruction is 4 bytes) to the PC.

> 1. _ Instruction 提取周期_（如果）：将程序计数器（PC）发送到内存并从内存中获取当前指令。通过向 PC 添加 4（因为每个指令为 4 个字节），将 PC 更新为下一个顺序指令。

2. _Instruction decode/register fetch cycle_ (ID): Decode the instruction and read the registers corresponding to register source specifiers from the register file. Do the equality test on the registers as they are read, for a possible branch. Sign-extend the offset field of the instruction in case it is needed. Compute the possible branch target address by adding the sign-extended offset to the incremented PC.

> 2. _ Instruction decode/寄存器提取周期_（id）：解码指令并读取与寄存器文件中寄存器源指定相对应的寄存器。在阅读时对寄存器进行平等测试，以便为一个可能的分支。如果需要，请登录说明的偏移字段。通过将签名扩展的偏移量添加到增量 PC 来计算可能的分支目标地址。

Decoding is done in parallel with reading registers, which is possible because the register specifiers are at a fixed location in a RISC architecture. This tech- nique is known as _fixed-field decoding_. Note that we may read a register we don’t use, which doesn’t help but also doesn’t hurt performance. (It does waste energy to read an unneeded register, and power-sensitive designs might avoid this.) For loads and ALU immediate operations, the immediate field is always in the same place, so we can easily sign extend it. (For a more complete implementation of RISC V, we would need to compute two different sign-extended values, because the immediate field for store is in a different location.)

> 解码与阅读寄存器并行进行，这是可能的，因为寄存器指定符位于 RISC 体系结构中的固定位置。该技术被称为_固定场解码_。请注意，我们可能会阅读我们不使用的登记册，这无济于事，但也不会损害性能。（读取不需要的寄存器，并且对功率敏感的设计可能会避免这种情况。（对于 RISC V 的更完整实现，我们需要计算两个不同的签名扩展值，因为存储的直接字段位于不同的位置。）

1. _Execution/effective address cycle_ (EX): The ALU operates on the operands prepared in the prior cycle, performing one of three functions, depending on the instruction type.

> 1. _Execution/有效地址周期_（ex）：ALU 在先前周期中准备的操作数上操作，根据指令类型执行三个功能之一。

- Memory reference—The ALU adds the base register and the offset to form the effective address.

> - 内存参考 -  ALU 添加了基本寄存器和偏移以形成有效地址。

- Register-Register ALU instruction—The ALU performs the operation spec- ified by the ALU opcode on the values read from the register file.

> - 寄存器注册 ALU 指令 -  ALU 对 Alu Opode 进行的操作对从寄存器文件读取的值进行了规格。

- Register-Immediate ALU instruction—The ALU performs the operation specified by the ALU opcode on the first value read from the register file and the sign-extended immediate.

> - 寄存器-Immediate Alu 指令 -  ALU 执行由 Alu Opcode 指定的操作，对从寄存器文件和签名扩展的即时读取的第一个值读取。

- Conditional branch—Determine whether the condition is true.

> - 条件分支 - 确定条件是否为真。

In a load-store architecture the effective address and execution cycles can be combined into a single clock cycle, because no instruction needs to simulta- neously calculate a data address and perform an operation on the data.

> 在负载商店体系结构中，有效的地址和执行周期可以合并为单个时钟周期，因为没有指令需要同时计算数据地址并对数据执行操作。

1. _Memory access_ (MEM): If the instruction is a load, the memory does a read using the effective address computed in the previous cycle. If it is a store, then the memory writes the data from the second register read from the register file using the effective address.

> 1. _ ememory Access_（mem）：如果指令是负载，则使用上一个周期中计算的有效地址进行读取。如果是商店，则记忆使用有效地址从寄存器文件中读取第二个寄存器的数据。

2. _Write-back cycle_ (WB):

> 2. _write-back Cycle_（WB）：

- Register-Register ALU instruction or load instruction: Write the result into the register file, whether it comes from the memory system (for a load) or from the ALU (for an ALU instruction).

> - 注册注册 ALU 指令或加载指令：将结果写入寄存器文件，无论是来自内存系统（用于负载）还是来自 ALU（用于 ALU 指令）。

In this implementation, branch instructions require three cycles, store instruc- tions require four cycles, and all other instructions require five cycles. Assuming a branch frequency of 12% and a store frequency of 10%, a typical instruction dis- tribution leads to an overall CPI of 4.66. This implementation, however, is not opti- mal either in achieving the best performance or in using the minimal amount of hardware given the performance level; we leave the improvement of this design as an exercise for you and instead focus on pipelining this version.

> 在此实施中，分支指令需要三个周期，商店的指令需要四个周期，所有其他指令都需要五个周期。假设分支频率为 12％，商店频率为 10％，则典型的指导分开导致总 CPI 为 4.66。但是，在达到最佳性能或使用最少的硬件鉴于性能水平的情况下，这种实现并不是很不可能的。我们将这种设计的改进作为您的练习，而是专注于管道上的版本。

### The Classic Five-Stage Pipeline for a RISC Processor

> ### RISC 处理器的经典五阶段管道

We can pipeline the execution described in the previous section with almost no changes by simply starting a new instruction on each clock cycle. (See why we chose this design?) Each of the clock cycles from the previous section becomes a _pipe stage_—a cycle in the pipeline. This results in the execution pattern shown in [Figure C.1](#_bookmark484), which is the typical way a pipeline structure is drawn. Although each instruction takes 5 clock cycles to complete, during each clock cycle the hardware will initiate a new instruction and will be executing some part of the five different instructions.

> 我们可以通过在每个时钟周期中启动新的指令来输入上一节中描述的执行，几乎没有更改。（请参阅为什么我们选择此设计？）上一节中的每个时钟循环变成_ pipe stage _-管道中的一个循环。这导致[图 C.1]（#_ bookmark484）中显示的执行模式，这是绘制管道结构的典型方式。尽管每个指令都需要 5 个时钟周期才能完成，但是在每个时钟周期中，硬件将启动新的指令，并将执行五个不同指令的某些部分。

You may find it hard to believe that pipelining is as simple as this; it’s not. In this and the following sections, we will make our RISC pipeline “real” by dealing with problems that pipelining introduces.

> 您可能会发现很难相信管道上的简单。它不是。在此及以下各节中，我们将通过处理管道引入的问题来使我们的 RISC 管道“真实”。

To start with, we have to determine what happens on every clock cycle of the processor and make sure we don’t try to perform two different operations with the same data path resource on the same clock cycle. For example, a single ALU can- not be asked to compute an effective address and perform a subtract operation at the same time. Thus, we must ensure that the overlap of instructions in the pipeline cannot cause such a conflict. Fortunately, the simplicity of a RISC instruction set makes resource evaluation relatively easy. [Figure C.2](#_bookmark485) shows a simplified version of a RISC data path drawn in pipeline fashion. As you can see, the major functional units are used in different cycles, and hence overlapping the execution of multiple

> 首先，我们必须确定处理器的每个时钟周期中发生的情况，并确保我们不会在同一时钟周期中使用相同的数据路径资源执行两个不同的操作。例如，不要求单个 ALU 计算有效的地址并同时执行减法操作。因此，我们必须确保管道中指令的重叠不会引起这种冲突。幸运的是，RISC 指令集的简单性使资源评估相对容易。[图 C.2]（#_ bookmark485）以管道方式绘制的 RISC 数据路径的简化版本。如您所见，主要功能单元用于不同的循环中，因此重叠多个循环

Figure C.1 Simple RISC pipeline. On each clock cycle, another instruction is fetched and begins its five-cycle execution. If an instruction is started every clock cycle, the performance will be up to five times that of a processor that is not pipelined. The names for the stages in the pipeline are the same as those used for the cycles in the unpipelined implementation: IF¼instruction fetch, ID¼instruction decode, EX ¼execution, MEM¼memory access, and WB¼write-back.

> 图 C.1 简单的 RISC 管道。在每个时钟周期中，获取另一个指令，并开始其五周期执行。如果每个时钟周期都启动了指令，则性能最多将是未管道的处理器的五倍。管道中的阶段的名称与未固定实现中的周期中使用的名称相同：if¼instructionfetch，ID¼InstructionDecode，Ex¼excuction，Mem¼Memory 访问和 WB¼Write-back。

<span id="_bookmark485" class="anchor"></span>Time (in clock cycles)

Figure C.2 The pipeline can be thought of as a series of data paths shifted in time. This figure shows the overlap among the parts of the data path, with clock cycle 5 (CC 5) showing the steady-state situation. Because the reg- ister file is used as a source in the ID stage and as a destination in the WB stage, it appears twice. We show that it is read in one part of the stage and written in another by using a solid line, on the right or left, respectively, and a dashed line on the other side. The abbreviation IM is used for instruction memory, DM for data memory, and CC for clock cycle.

> 图 C.2 可以将管道视为一系列数据路径随时间变化。该图显示了数据路径部分之间的重叠，其中时钟周期 5（CC 5）显示了稳态情况。由于 Reg-Ister 文件在 ID 阶段和 WB 阶段的目的地中用作源，因此似乎两次。我们表明，它是在舞台的一个部分中读取的，并通过使用坚固的线，分别在右侧或左侧和另一侧的虚线写在另一部分中。缩写 IM 用于指令内存，用于数据存储器的 DM 和 CC 用于时钟周期。

instructions introduces relatively few conflicts. There are three observations on which this fact rests.

> 说明引入了相对较少的冲突。这一事实有三个观察结果。

First, we use separate instruction and data memories, which we would typically implement with separate instruction and data caches (discussed in [Chapter 2](#_bookmark46)). The use of separate caches eliminates a conflict for a single memory that would arise between instruction fetch and data memory access. Notice that if our pipelined pro- cessor has a clock cycle that is equal to that of the unpipelined version, the memory system must deliver five times the bandwidth. This increased demand is one cost of higher performance.

> 首先，我们使用单独的指令和数据记忆，通常会使用单独的指令和数据缓存来实现（[第 2 章]（#_ bookmark46））。单独的缓存的使用消除了指令获取和数据内存访问之间将出现的单个内存的冲突。请注意，如果我们的管道操作器的时钟周期等于不符合版本的时钟周期，则内存系统必须提供带宽的五倍。这种增加的需求是较高性能的成本之一。

Second, the register file is used in the two stages: one for reading in ID and one for writing in WB. These uses are distinct, so we simply show the register file in two places. Hence, we need to perform two reads and one write every clock cycle.

> 其次，在两个阶段中使用寄存器文件：一个用于阅读 ID，一个用于 WB。这些用途是不同的，因此我们只是在两个地方显示寄存器文件。因此，我们需要执行两次读取，并且每个时钟周期都写入。

To handle reads and a write to the same register (and for another reason, which will become obvious shortly), we perform the register write in the first half of the clock cycle and the read in the second half.

> 要处理读取并写入相同的寄存器（并且由于另一个原因，这将很快变得很明显），我们在时钟周期的上半场进行寄存器写入，并在下半场读取。

Third, [Figure C.2](#_bookmark485) does not deal with the PC. To start a new instruction every clock, we must increment and store the PC every clock, and this must be done dur- ing the IF stage in preparation for the next instruction. Furthermore, we must also have an adder to compute the potential branch target address during ID. One further problem is that we need the ALU in the ALU stage to evaluate the branch condi- tion. Actually, we don’t really need a full ALU to evaluate the comparison between two registers, but we need enough of the function that it has to occur in this pipestage.

> 第三，[图 C.2]（#_ bookmark485）不涉及 PC。要启动每个时钟的新指令，我们必须在每个时钟上递增并存储 PC，这必须在 IF 阶段进行下一个指令做准备。此外，我们还必须有一个加法器来计算 ID 期间潜在的分支目标地址。另一个问题是，我们需要在 ALU 阶段的 ALU 来评估分支条件。实际上，我们实际上并不需要完整的 ALU 来评估两个寄存器之间的比较，但是我们需要足够的功能，必须在此管道中发生。

Although it is critical to ensure that instructions in the pipeline do not attempt to use the hardware resources at the same time, we must also ensure that instructions in different stages of the pipeline do not interfere with one another. This separation is done by introducing _pipeline registers_ between successive stages of the pipeline, so that at the end of a clock cycle all the results from a given stage are stored into a register that is used as the input to the next stage on the next clock cycle. [Figure C.3](#_bookmark486) shows the pipeline drawn with these pipeline registers.

> 尽管至关重要的是要确保管道中的说明不会同时尝试使用硬件资源，但我们还必须确保管道不同阶段的说明不会彼此干扰。通过在管道的连续阶段中引入_ pipeline registers _来完成此分离，以便在时钟周期结束时，给定阶段的所有结果都存储在一个寄存器中，该寄存器被用作下一个时钟的下一个阶段的输入循环。[图 C.3]（#_ bookmark486）显示了使用这些管道寄存器绘制的管道。

Although many figures will omit such registers for simplicity, they are required to make the pipeline operate properly and must be present. Of course, similar reg- isters would be needed even in a multicycle data path that had no pipelining (because only values in registers are preserved across clock boundaries). In the case of a pipelined processor, the pipeline registers also play the key role of carrying intermediate results from one stage to another where the source and destination may not be directly adjacent. For example, the register value to be stored during a store instruction is read during ID, but not actually used until MEM; it is passed through two pipeline registers to reach the data memory during the MEM stage. Likewise, the result of an ALU instruction is computed during EX, but not actually stored until WB; it arrives there by passing through two pipeline registers. It is sometimes useful to name the pipeline registers, and we follow the convention of naming them by the pipeline stages they connect, so the registers are called IF/ID, ID/EX, EX/MEM, and MEM/WB.

> 尽管许多数字会为了简单起见省略此类登记册，但它们必须使管道正常运行，并且必须存在。当然，即使在没有管道的多环数据路径中，也需要类似的重新计算机（因为只有在跨时钟边界上保留寄存器中的值）。对于管道处理的处理器，管道寄存器还起着从一个阶段到另一个阶段携带中间结果的关键作用，而源和目的地可能不会直接相邻。例如，在 ID 期间读取在商店指令中要存储的寄存器值，但直到 MEM 才真正使用；它通过两个管道寄存器通过，以在 MEM 阶段到达数据存储器。同样，在 EX 期间计算 ALU 指令的结果，但直到 WB 才真正存储。它通过通过两个管道寄存器到达那里。命名管道寄存器有时很有用，我们遵循按照它们连接的管道阶段命名它们的约定，因此将寄存器称为 IF/ID，ID/EX，EX/EX/MEM 和 MEM/WB。

### Basic Performance Issues in Pipelining

> ###管道中的基本绩效问题

Pipelining increases the processor instruction throughput—the number of instruc- tions completed per unit of time—but it does not reduce the execution time of an individual instruction. In fact, it usually slightly increases the execution time of each instruction due to overhead in the control of the pipeline. The increase in instruction throughput means that a program runs faster and has lower total exe- cution time, even though no single instruction runs faster!

> 管道式增加会增加处理器的指令吞吐量（每单位时间单位完成的仪器数量），但并不能减少单个指令的执行时间。实际上，由于管道控制中的开销，它通常会稍微增加每个指令的执行时间。指令吞吐量的增加意味着程序运行速度更快，并且总摄取时间较低，即使没有单个指令运行速度更快！

The fact that the execution time of each instruction does not decrease puts limits on the practical depth of a pipeline, as we will see in the next section. In addition to limitations arising from pipeline latency, limits arise from imbalance among the pipe stages and from pipelining overhead. Imbalance among the pipe stages reduces performance because the clock can run no faster than the time needed for the slowest pipeline stage. Pipeline overhead arises from the combina- tion of pipeline register delay and clock skew. The pipeline registers add setup time, which is the time that a register input must be stable before the clock signal that triggers a write occurs, plus propagation delay to the clock cycle. Clock skew, which is the maximum delay between when the clock arrives at any two registers, also contributes to the lower limit on the clock cycle. Once the clock cycle is as small as the sum of the clock skew and latch overhead, no further pipelining is useful, because there is no time left in the cycle for useful work. The interested reader should see [Kunkel and Smith (1986)](#_bookmark974).

> 正如我们将在下一节中看到的那样，每次指令的执行时间不减少的事实限制了管道的实际深度。除了管道延迟引起的局限性外，限制是由管道阶段之间的不平衡和管道上的开销产生的。管道阶段之间的不平衡会降低性能，因为时钟的运行速度不超过最慢的管道阶段所需的时间。管道开销是由管道寄存器延迟和时钟偏斜的组合产生的。管道寄存器添加设置时间，这是寄存器输入必须在触发写入的时钟信号之前稳定的时间，再加上传播延迟到时钟周期。时钟偏度是时钟到达任意两个寄存器之间的最大延迟，也有助于时钟周期的下限。一旦时钟周期与时钟偏斜和闩锁开销的总和一样小，就不会有进一步的管道有用，因为在周期中没有时间进行有用的工作。感兴趣的读者应该看到[Kunkel 和 Smith（1986）]（#_ Bookmark974）。

Figure C.3 A pipeline showing the pipeline registers between successive pipeline stages. Notice that the registers prevent interference between two different instructions in adjacent stages in the pipeline. The registers also play the critical role of carrying data for a given instruction from one stage to the other. The edge-triggered property of reg- isters—that is, that the values change instantaneously on a clock edge—is critical. Otherwise, the data from one instruction could interfere with the execution of another!

> 图 C.3 管道显示了连续管道阶段之间的管道寄存器。请注意，寄存器可以防止管道中相邻阶段的两个不同指令之间的干扰。寄存器还扮演着从一个阶段到另一个阶段的指令携带数据的关键作用。重新启用的边缘触发属性（即，值在时钟边缘上立即变化）至关重要。否则，来自一项指令的数据可能会干扰另一个指令的执行！

Example Consider the unpipelined processor in the previous section. Assume that it has a 4 GHz clock (or a 0.5 ns clock cycle) and that it uses four cycles for ALU oper- ations and branches and five cycles for memory operations. Assume that the rel- ative frequencies of these operations are 40%, 20%, and 40%, respectively. Suppose that due to clock skew and setup, pipelining the processor adds 0.1 ns of overhead to the clock. Ignoring any latency impact, how much speedup in the instruction execution rate will we gain from a pipeline?

> 示例考虑上一节中的不封装处理器。假设它具有 4 GHz 时钟（或 0.5 ns 时钟周期），并且使用四个循环用于 ALU 操作和分支，以及五个循环进行内存操作。假设这些操作的相关频率分别为 40％，20％和 40％。假设由于时钟偏斜和设置，管道管道在时钟上添加了 0.1 ns 的开销。忽略了任何延迟影响，我们将从管道中获得多少加速执行率？

In the pipelined implementation, the clock must run at the speed of the slowest stage plus overhead, which will be 0.5 + 0.1 or 0.6 ns; this is the average instruction execution time. Thus, the speedup from pipelining is

> 在管道的实现中，时钟必须以最慢的阶段加上开销的速度运行，该阶段的速度为 0.5 + 0.1 或 0.6 ns；这是平均指令执行时间。因此，管道加速是

The 0.1 ns overhead essentially establishes a limit on the effectiveness of pipelin- ing. If the overhead is not affected by changes in the clock cycle, Amdahl’s Law tells us that the overhead limits the speedup.

> 0.1 ns 的头顶基本上建立了管道效果的有效性的限制。如果开销不受时钟周期变化的影响，Amdahl 的定律告诉我们，开销限制了加速。

This simple RISC pipeline would function just fine for integer instructions if every instruction were independent of every other instruction in the pipeline. In reality, instructions in the pipeline can depend on one another; this is the topic of the next section.

> 如果每个指令都独立于管道中的其他所有说明，则此简单的 RISC 管道对于整数指令都可以正常工作。实际上，管道中的说明可以互相取决于彼此。这是下一节的主题。
