## Virtual Memory

… a system has been devised to make the core drum combination appear to the programmer as a single level store, the requisite transfers taking place automatically.
[Kilburn et al. (1962)](#_bookmark968)

> …已经设计了一个系统，以使程序员作为单一级别的商店出现核心鼓组合，并自动进行必要的传输。

At any instant in time computers are running multiple processes, each with its own address space. (Processes are described in the next section.) It would be too expen- sive to dedicate a full address space worth of memory for each process, especially because many processes use only a small part of their address space. Hence, there must be a means of sharing a smaller amount of physical memory among many processes.

> 在任何瞬间，计算机都在运行多个进程，每个流程都有自己的地址空间。(在下一部分中描述了过程。)为每个过程提供一个完整的地址空间，尤其是因为许多流程仅使用其地址空间的一小部分，这是太多的费用。因此，必须有一种在许多过程中共享较少的物理记忆的方法。

One way to do this, _virtual memory_, divides physical memory into blocks and allocates them to different processes. Inherent in such an approach must be a _pro- tection_ scheme that restricts a process to the blocks belonging only to that process. Most forms of virtual memory also reduce the time to start a program, because not all code and data need be in physical memory before a program can begin.

> *virtual Memory *将物理内存分为块并将其分配给不同的过程。这种方法固有的必须是* pro-tection*方案，该方案将一个过程限制在仅属于该过程的块中。大多数虚拟内存的形式还减少了启动程序的时间，因为并非所有代码和数据都需要在程序开始之前都需要在物理内存中。

Although protection provided by virtual memory is essential for current com- puters, sharing is not the reason that virtual memory was invented. If a program became too large for physical memory, it was the programmer’s job to make it fit. Programmers divided programs into pieces, then identified the pieces that were mutually exclusive, and loaded or unloaded these _overlays_ under user program control during execution. The programmer ensured that the program never tried to access more physical main memory than was in the computer, and that the proper overlay was loaded at the proper time. As you can well imagine, this respon- sibility eroded programmer productivity.

> 尽管虚拟内存提供的保护对于当前计算机至关重要，但共享并不是发明虚拟内存的原因。如果程序变得太大，无法进行物理记忆，那么程序员的工作是合适的。程序员将程序分为部分，然后确定了相互排斥的零件，并在执行过程中用户程序控制下加载或卸载了这些 *Overlays*。程序员确保该程序从未尝试过比计算机中更多的物理内存，并且在适当的时间加载了适当的覆盖层。您可以想象，这种呼应侵蚀了程序员的生产力。

Virtual memory was invented to relieve programmers of this burden; it auto- matically manages the two levels of the memory hierarchy represented by main memory and secondary storage. [Figure B.19](#_bookmark459) shows the mapping of virtual memory to physical memory for a program with four pages.

> 发明了虚拟记忆，以减轻程序员的负担；它可以自动管理以主内存和辅助存储为代表的内存层次结构的两个级别。[图 B.19](#_bookmark459) 显示了具有四个页面的程序的虚拟内存到物理内存的映射。

In addition to sharing protected memory space and automatically managing the memory hierarchy, virtual memory also simplifies loading the program for execu- tion. Called _relocation_, this mechanism allows the same program to run in any location in physical memory. The program in [Figure B.19](#_bookmark459) can be placed anywhere in physical memory or disk just by changing the mapping between them. (Prior to the popularity of virtual memory, processors would include a relocation register just for that purpose.) An alternative to a hardware solution would be software that changed all addresses in a program each time it was run.

> 除了共享受保护的内存空间并自动管理内存层次结构外，虚拟内存还简化了执行程序的加载。称为 *relocation*，此机制允许相同的程序在物理内存中的任何位置运行。[图 B.19]中的程序(#\_bookmark459)可以通过更改它们之间的映射来放置在物理内存中的任何位置或磁盘。(在虚拟内存的普及之前，处理器将仅出于此目的包括重定位寄存器。)硬件解决方案的替代方法是每次运行程序中所有地址的软件。

Figure B.19 The logical program in its contiguous virtual address space is shown on the left. It consists of four pages, A, B, C, and D. The actual location of three of the blocks is in physical main memory and the other is located on the disk.

> 图 B.19 左侧显示了其连续的虚拟地址空间中的逻辑程序。它由四页，A，B，C 和 D 组成。其中三个块的实际位置在物理主内存中，另一个位于磁盘上。

Several general memory hierarchy ideas from [Chapter 1](#_bookmark2) about caches are anal- ogous to virtual memory, although many of the terms are different. _Page_ or _seg- ment_ is used for block, and _page fault_ or _address fault_ is used for miss. With virtual memory, the processor produces _virtual addresses_ that are translated by a combi- nation of hardware and software to _physical addresses_, which access main mem- ory. This process is called _memory mapping_ or _address translation_. Today, the two memory hierarchy levels controlled by virtual memory are DRAMs and magnetic disks. [Figure B.20](#_bookmark460) shows a typical range of memory hierarchy parameters for vir- tual memory.

> [第 1 章](#_bookmark2)关于缓存的几个一般内存层次结构想法是对虚拟内存的肛门，尽管许多术语是不同的。*page* 或 *seg- ment* 用于块，*page FARD* 或 *ADDRESS FARD* 用于误解。使用虚拟内存，处理器会生成 *virtual 地址*，由硬件和软件的组合翻译成 *physical 地址*，可访问 main mem-ory。此过程称为* memory Mapping *或* address Translation*。如今，由虚拟内存控制的两个内存层次结构级别是 DRAM 和磁盘。[图 B.20](#_bookmark460) 显示了典型的存储器层次结构参数。

There are further differences between caches and virtual memory beyond those quantitative ones mentioned in [Figure B.20](#_bookmark460):

> 在[图 B.20]中提到的定量记忆之外，缓存和虚拟内存之间存在进一步的差异(#\_bookmark460)：

- Replacement on cache misses is primarily controlled by hardware, while vir- tual memory replacement is primarily controlled by the operating system. The longer miss penalty means it’s more important to make a good decision, so the operating system can be involved and take time deciding what to replace.

> - 高速缓存失误的替换主要由硬件控制，而纯记忆更换主要由操作系统控制。较长的罚款意味着做出一个好的决定更为重要，因此操作系统可以参与并花时间决定要替换的内容。

- The size of the processor address determines the size of virtual memory, but the cache size is independent of the processor address size.

> - 处理器地址的大小确定了虚拟内存的大小，但缓存大小与处理器地址大小无关。

- In addition to acting as the lower-level backing store for main memory in the hierarchy, secondary storage is also used for the file system. In fact, the file system occupies most of secondary storage. It is not usually in the address space.

> - 除了充当层次结构中主内存的低级备份商店外，还将辅助存储用于文件系统。实际上，文件系统占据了大部分辅助存储。通常不在地址空间中。

Figure B.20 Typical ranges of parameters for caches and virtual memory. Virtual memory parameters represent increases of 10–1,000,000 times over cache parame- ters. Usually, first-level caches contain at most 1 MiB of data, whereas physical memory contains 256 MiB to 1 TB.

> 图 B.20 缓存和虚拟内存参数的典型范围。虚拟内存参数代表比缓存对象的增加 10-1,000 亿倍。通常，第一级缓存最多包含 1 个 MIB 数据，而物理内存包含 256 MIB 至 1 TB。

Virtual memory also encompasses several related techniques. Virtual memory systems can be categorized into two classes: those with fixed-size blocks, called _pages_, and those with variable-size blocks, called _segments_. Pages are typically fixed at 4096–8192 bytes, while segment size varies. The largest segment sup- ported on any processor ranges from 2<sup>16</sup> bytes up to 2<sup>32</sup> bytes; the smallest segment is 1 byte. [Figure B.21](#_bookmark461) shows how the two approaches might divide code and data. The decision to use paged virtual memory versus segmented virtual memory affects the processor. Paged addressing has a single fixed-size address divided into page number and offset within a page, analogous to cache addressing. A single address does not work for segmented addresses; the variable size of segments requires 1 word for a segment number and 1 word for an offset within a segment, for a total of 2 words. An unsegmented address space is simpler for the compiler. The pros and cons of these two approaches have been well documented in oper- ating systems textbooks; [Figure B.22](#_bookmark462) summarizes the arguments. Because of the replacement problem (the third line of the figure), few computers today use pure segmentation. Some computers use a hybrid approach, called _paged segments_, in which a segment is an integral number of pages. This simplifies replacement because memory need not be contiguous, and the full segments need not be in main memory. A more recent hybrid is for a computer to offer multiple page sizes, with the larger sizes being powers of 2 times the smallest page size. The IBM

> 虚拟内存还包括几种相关的技术。虚拟内存系统可以分为两个类：具有固定尺寸块的 *pages* 和具有可变大小块的固定尺寸块的类别，称为 *segments*。页面通常固定在 4096–8192 字节上，而段的大小也有所不同。在任何处理器上提供的最大段范围从 2 <sup> 16 </sup>字节范围为 2 <sup> 32 </sup> bytes;最小的段是 1 个字节。[图 B.21](#_bookmark461) 显示了两种方法如何划分代码和数据。使用分页的虚拟内存与分段虚拟内存的决定会影响处理器。分类地址的单个固定大小地址分为页码和页面中的偏移，类似于缓存地址。单个地址不适用于分段地址；片段的可变大小需要 1 个字段编号和 1 个单词，在一个段内的偏移量，总共需要 2 个单词。对于编译器而言，未分段的地址空间更简单。这两种方法的优缺点在操作系统教科书中已得到充分记录。[图 B.22](#_bookmark462) 总结了参数。由于替代问题(图的第三行)，今天很少有计算机使用纯分段。一些计算机使用混合方法，称为 *paged segments*，其中段是一个集成的页面。这简化了替换，因为内存不必连续，并且整个段不必在主内存中。最新的混合动力车是为计算机提供多个页面尺寸，较大尺寸的功率是最小的页面尺寸的 2 倍。IBM

Figure B.21 Example of how paging and segmentation divide a program.

Figure B.22 Paging versus segmentation. Both can waste memory, depending on the block size and how well the segments fit together in main memory. Programming lan- guages with unrestricted pointers require both the segment and the address to be passed. A hybrid approach, called _paged segments_, shoots for the best of both worlds: segments are composed of pages, so replacing a block is easy, yet a segment may be treated as a logical unit.

> 图 B.22 分页与分割。两者都可以浪费内存，具体取决于块大小以及片段在主内存中的结合程度。带有无限制指针的编程语言需要通过细分市场和要通过的地址。一种称为* ppaged segments*的混合方法为两全其美而射击：段是由页面组成的，因此更换块很容易，但可以将段视为逻辑单元。

### Four Memory Hierarchy Questions Revisited

We are now ready to answer the four memory hierarchy questions for virtual memory.

##### _Q1: Where Can a Block be Placed in Main Memory?_

The miss penalty for virtual memory involves access to a rotating magnetic storage device and is therefore quite high. Given the choice of lower miss rates or a simpler placement algorithm, operating systems designers usually pick lower miss rates because of the exorbitant miss penalty. Thus, operating systems allow blocks to be placed anywhere in main memory. According to the terminology in [Figure B.2](#_bookmark440) on page B-8, this strategy would be labeled fully associative.

> 虚拟内存的罚款涉及访问旋转的磁性存储设备，因此很高。鉴于选择较低的错率或更简单的放置算法，操作系统设计人员通常会因高昂的罚款而选择较低的失误费率。因此，操作系统允许将块放置在主内存中的任何地方。根据[图 B.2](#_bookmark440) 中的术语，该策略将被标记为完全关联。

##### _Q2: How Is a Block Found If It Is in Main Memory?_

Both paging and segmentation rely on a data structure that is indexed by the page or segment number. This data structure contains the physical address of the block. For segmentation, the offset is added to the segment’s physical address to obtain the final physical address. For paging, the offset is simply concatenated to this physical page address (see [Figure B.23](#_bookmark463)).

> 分页和分割都取决于页面或段编号索引的数据结构。该数据结构包含块的物理地址。对于细分，将偏移添加到该细分市场的物理地址中以获得最终的物理地址。为了进行分页，简单地将偏移与此物理页面地址相连(请参见[图 B.23](#_bookmark463))。

This data structure, containing the physical page addresses, usually takes the form of a _page table._ Indexed by the virtual page number, the size of the table is the number of pages in the virtual address space. Given a 32-bit virtual address, 4 KiB pages, and 4 bytes per page table entry (PTE), the size of the page table would be (2<sup>32</sup>/2<sup>12</sup>) 2<sup>2</sup> 2<sup>22</sup> or 4 MiB.

> 该数据结构包含物理页面地址，通常采用\_page 表的形式。给定一个 32 位虚拟地址，4 个 KIB 页面和 4 个字节，每个页表条目(PTE)，页面的大小将为(2 <sup> 32 </sup>/2 <sup> 12 </sup> 12 </sup>>)2 <sup> 2 </sup> 2 <sup> 22 </sup>或 4 MIB。

To reduce the size of this data structure, some computers apply a hashing func- tion to the virtual address. The hash allows the data structure to be the length of the number of _physical_ pages in main memory. This number could be much smaller than the number of virtual pages. Such a structure is called an _inverted page table_. Using the previous example, a 512 MiB physical memory would only need 1 MiB (8 512 MiB/4 KiB) for an inverted page table; the extra 4 bytes per page table entry are for the virtual address. The HP/Intel IA-64 covers both bases by offering both traditional pages tables _and_ inverted page tables, leaving the choice of mech- anism to the operating system programmer.

> 为了减少此数据结构的大小，一些计算机将哈希函数应用于虚拟地址。哈希允许数据结构是主内存中 *physical* 页数的长度。这个数字可能比虚拟页面的数量小得多。这样的结构称为 *inverted Page table*。使用上一个示例，一个 512 MIB 物理内存只需要 1 个 MIB(8 512 MIB/4 KIB)才能倒置表；每个页面表条目的额外 4 个字节用于虚拟地址。HP/Intel IA-64 通过提供两个传统页面表*和*倒页表，将 Mech-Anism 的选择留给操作系统程序员，从而涵盖了这两个基础。

Figure B.23 The mapping of a virtual address to a physical address via a page table.

> 图 B.23 虚拟地址通过页面表映射到物理地址。

To reduce address translation time, computers use a cache dedicated to these address translations, called a _translation lookaside buffer_, or simply _translation buffer_, described in more detail shortly.

> 为了减少地址翻译时间，计算机使用专用于这些地址翻译的缓存，称为* translation lookAside buffer *或简单地* translation buffer*，更详细地描述。

##### _Q3: Which Block Should be Replaced on a Virtual Memory Miss?_

As mentioned earlier, the overriding operating system guideline is minimizing page faults. Consistent with this guideline, almost all operating systems try to replace the least recently used (LRU) block because if the past predicts the future, that is the one less likely to be needed.

> 如前所述，重大操作系统指南是最大程度地减少页面故障。与该准则一致，几乎所有操作系统都试图替换最不可用的(LRU)块，因为如果过去预测未来，那是不太需要的。

To help the operating system estimate LRU, many processors provide a _use bit_ or _reference bit_, which is logically set whenever a page is accessed. (To reduce work, it is actually set only on a translation buffer miss, which is described shortly.) The operating system periodically clears the use bits and later records them so it can determine which pages were touched during a particular time period. By keep- ing track in this way, the operating system can select a page that is among the least recently referenced.

> 为了帮助操作系统估算 LRU，许多处理器提供了 *use 位*或\_reference 位，每当访问页面时，该处理器都会在逻辑上设置。(为了减少工作，它实际上仅在翻译缓冲区遗漏上设置，这将很快描述。)操作系统会定期清除使用位并后来记录它们，以便可以确定在特定时间段内触摸了哪些页面。通过以这种方式保持轨道，操作系统可以选择一个最近引用的页面之一。

##### _Q4: What Happens on a Write?_

The level below main memory contains rotating magnetic disks that take millions of clock cycles to access. Because of the great discrepancy in access time, no one has yet built a virtual memory operating system that writes through main memory to disk on every store by the processor. (This remark should not be interpreted as an opportunity to become famous by being the first to build one!) Thus, the write strat- egy is always write-back.

> 主内存下方的水平包含旋转的磁盘，这些磁盘需要数百万个时钟周期才能访问。由于访问时间的差异很大，还没有人构建了通过处理器通过主内存来写入磁盘的虚拟内存操作系统。(这句话不应被解释为成为第一个建造一个人而闻名的机会！)因此，写道总是写下的。

Because the cost of an unnecessary access to the next-lower level is so high, virtual memory systems usually include a dirty bit. It allows blocks to be written to disk only if they have been altered since being read from the disk.

> 由于不必要访问下一个级别级别的成本是如此之高，因此虚拟内存系统通常包含一个肮脏的位。自从磁盘从磁盘上读取以来，它才能将块写入磁盘。

### Techniques for Fast Address Translation

Page tables are usually so large that they are stored in main memory and are some- times paged themselves. Paging means that every memory access logically takes at least twice as long, with one memory access to obtain the physical address and a second access to get the data. As mentioned in [Chapter 2](#_bookmark46), we use locality to avoid the extra memory access. By keeping address translations in a special cache, a memory access rarely requires a second access to translate the data. This special address translation cache is referred to as a _translation look aside buffer_ (TLB), also called a _translation buffer_ (TB).

> 页面表通常是如此之大，以至于它们存储在主内存中，并且有时会自己编写。分页意味着每个内存访问在逻辑上至少需要两倍的时间，其中一个内存访问以获取物理地址和第二个访问以获取数据。如[第 2 章](#_bookmark46)所述，我们使用局部性避免额外的内存访问。通过将地址翻译保存在特殊的缓存中，内存访问很少需要第二个访问来翻译数据。此特殊地址翻译缓存称为* translation Look fool Buffer*(TLB)，也称为 *translation Buffer*(TB)。

A TLB entry is like a cache entry where the tag holds portions of the virtual address and the data portion holds a physical page frame number, protection field, valid bit, and usually a use bit and dirty bit. To change the physical page frame number or protection of an entry in the page table, the operating system must make sure the old entry is not in the TLB; otherwise, the system won’t behave properly. Note that this dirty bit means the corresponding _page_ is dirty, not that the address translation in the TLB is dirty nor that a particular block in the data cache is dirty. The operating system resets these bits by changing the value in the page table and then invalidates the corresponding TLB entry. When the entry is reloaded from the page table, the TLB gets an accurate copy of the bits.

> TLB 条目就像一个缓存条目，该标签保存虚拟地址的一部分，并且数据部分保留一个物理页面框架号，保护字段，有效位，通常是使用位和肮脏位。要更改页面表中的物理页面框架号或条目的保护，操作系统必须确保旧条目不在 TLB 中；否则，系统将无法正确地行事。请注意，这个肮脏的位意味着相应的 *page* 是肮脏的，而不是 TLB 中的地址翻译是脏的，也不是数据缓存中的特定块是脏的。操作系统通过更改页面表中的值，然后使相应的 TLB 条目无效来重置这些位。从页面表重新加载条目时，TLB 将获得位的准确副本。

[Figure B.24](#_bookmark464) shows the Opteron data TLB organization, with each step of the translation labeled. This TLB uses fully associative placement; thus, the translation begins (steps 1 and 2) by sending the virtual address to all tags. Of course, the tag must be marked valid to allow a match. At the same time, the type of memory access is checked for a violation (also in step 2) against protection information in the TLB.

> [图 B.24](#_bookmark464) 显示了 opteron 数据 TLB 组织，并标记了翻译的每个步骤。该 TLB 使用完全关联的位置；因此，通过将虚拟地址发送到所有标签来开始翻译(步骤 1 和 2)。当然，标签必须有效标记以允许匹配。同时，检查内存访问的类型是否违反了 TLB 中的保护信息的违规行为(也在步骤 2 中)。

For reasons similar to those in the cache case, there is no need to include the 12 bits of the page offset in the TLB. The matching tag sends the corresponding phys- ical address through effectively a 40:1 multiplexor (step 3). The page offset is then combined with the physical page frame to form a full physical address (step 4). The address size is 40 bits.

> 由于与缓存情况类似的原因，无需在 TLB 中包含页面偏移的 12 位。匹配标签通过有效的 40：1 多路复用器(步骤 3)发送相应的物理地址。然后将页面偏移与物理页面框架结合使用以形成完整的物理地址(步骤 4)。地址大小为 40 位。

Address translation can easily be on the critical path determining the clock cycle of the processor, so the Opteron uses virtually addressed, physically tagged L1 caches.

> 地址翻译可以轻松地在确定处理器时钟周期的关键路径上，因此 Opteron 使用虚拟寻址的，物理标记的 L1 缓存。

### Selecting a Page Size

The most obvious architectural parameter is the page size. Choosing the page is a question of balancing forces that favor a larger page size versus those favoring a smaller size. The following favor a larger size:

> 最明显的架构参数是页面大小。选择页面是一个平衡力的问题，该问题偏爱较大的页面大小，而偏爱较小尺寸的力量。以下偏爱更大的尺寸：

![](../media/image487.png)

Figure B.24 Operation of the Opteron data TLB during address translation. The four steps of a TLB hit are shown as _circled numbers_. This TLB has 40 entries. [Section B.5](#protection-and-examples-of-virtual-memory) describes the various protection and access fields of an Opteron page table entry.

> 图 B.24 地址翻译过程中 Opteron 数据 TLB 的操作。TLB 命中的四个步骤显示为* circled number*。该 TLB 有 40 个条目。[B.5 节](Virtulual-Memory 的＃Protection-and-examples)描述了 Opteron Page 表条目的各种保护和访问字段。

- The size of the page table is inversely proportional to the page size; memory (or other resources used for the memory map) can therefore be saved by making the pages bigger.

> - 页面的大小与页面大小成反比；因此，可以通过使页面更大来保存内存(或用于内存图的其他资源)。

- As mentioned in [Section B.3](#six-basic-cache-optimizations), a larger page size can allow larger caches with fast cache hit times.

> - 如 [第 B.3 节](#six-basic-cache-optimizations) 中所述，较大的页面大小可以允许更大的缓存和更快的缓存命中时间。

- Transferring larger pages to or from secondary storage, possibly over a net- work, is more efficient than transferring smaller pages.

> - 将较大的页面转移到二次存储(可能是在网络上)比转移较小的页面更有效。

- The number of TLB entries is restricted, so a larger page size means that more memory can be mapped efficiently, thereby reducing the number of TLB misses.

> - TLB 条目的数量受到限制，因此较大的页面大小意味着可以有效地映射更多的内存，从而减少 TLB 错过的数量。

It is for this final reason that recent microprocessors have decided to support mul- tiple page sizes; for some programs, TLB misses can be as significant on CPI as the cache misses.

> 出于这个最终原因，最近的微处理器决定支持 Multiple Page 尺寸。对于某些程序，TLB 的失误在 CPI 上可能与缓存失误一样重要。

The main motivation for a smaller page size is conserving storage. A small page size will result in less wasted storage when a contiguous region of virtual memory is not equal in size to a multiple of the page size. The term for this unused memory in a page is _internal fragmentation_. Assuming that each process has three primary segments (text, heap, and stack), the average wasted storage per process will be 1.5 times the page size. This amount is negligible for computers with hun- dreds of megabytes of memory and page sizes of 4–8 KiB. Of course, when the page sizes become very large (more than 32 KiB), storage (both main and second- ary) could be wasted, as well as I/O bandwidth. A final concern is process start-up time; many processes are small, so a large page size would lengthen the time to invoke a process.

> 较小页面大小的主要动机是节省存储空间。当虚拟内存的连续区域大小不等于页面大小的倍数时，较小的页面大小将减少浪费的存储空间。页面中这种未使用的内存的术语是*内部碎片*。假设每个进程有三个主要段(文本、堆和堆栈)，每个进程平均浪费的存储空间将是页面大小的 1.5 倍。对于具有数百兆字节内存和 4-8 KiB 页面大小的计算机来说，这个数量可以忽略不计。当然，当页面大小变得非常大(超过 32 KiB)时，可能会浪费存储空间(主存储空间和辅助存储空间)以及 I/O 带宽。最后一个问题是进程启动时间；许多进程都很小，因此较大的页面大小会延长调用进程的时间。

### Summary of Virtual Memory and Caches

With virtual memory, TLBs, first-level caches, and second-level caches all map- ping portions of the virtual and physical address space, it can get confusing what bits go where. [Figure B.25](#_bookmark465) gives a hypothetical example going from a 64-bit virtual address to a 41-bit physical address with two levels of cache. This L1 cache is vir- tually indexed, and physically tagged because both the cache size and the page size are 8 KiB. The L2 cache is 4 MiB. The block size for both is 64 bytes.

> 由于虚拟内存、TLB、一级缓存和二级缓存都是虚拟和物理地址空间的映射部分，因此可能会混淆哪些位去哪里。[图 B.25](#_bookmark465) 给出了一个从 64 位虚拟地址到具有两级缓存的 41 位物理地址的假设示例。这个 L1 缓存是虚拟索引和物理标记的，因为缓存大小和页面大小都是 8 KiB。L2 缓存为 4 MiB。两者的块大小都是 64 字节。

Figure B.25 The overall picture of a hypothetical memory hierarchy going from virtual address to L2 cache access. The page size is 8 KiB. The TLB is direct mapped with 256 entries. The L1 cache is a direct-mapped 8 KiB, and the L2 cache is a direct-mapped 4 MiB. Both use 64-byte blocks. The virtual address is 64 bits and the physical address is 41 bits. The primary difference between this simple figure and a real cache is replication of pieces of this figure.

> 图 B.25 从虚拟地址到 L2 缓存访问的假设内存层次结构的整体图。页面大小为 8 KiB。TLB 直接映射有 256 个条目。L1 缓存是直接映射的 8 KiB，L2 缓存是直接映射的 4 MiB。两者都使用 64 字节块。虚拟地址为 64 位，物理地址为 41 位。这个简单的图和真正的缓存之间的主要区别是复制这个图的各个部分。

First, the 64-bit virtual address is logically divided into a virtual page number and page offset. The former is sent to the TLB to be translated into a physical address, and the high bit of the latter is sent to the L1 cache to act as an index. If the TLB match is a hit, then the physical page number is sent to the L1 cache tag to check for a match. If it matches, it’s an L1 cache hit. The block offset then selects the word for the processor.

> 首先，64 位虚拟地址在逻辑上分为虚拟页号和页偏移量。前者送入 TLB 翻译成物理地址，后者的高位送入 L1 缓存作为索引。如果 TLB 匹配成功，则将物理页码发送到 L1 缓存标记以检查匹配。如果匹配，则为 L1 缓存命中。块偏移然后为处理器选择字。

If the L1 cache check results in a miss, the physical address is then used to try the L2 cache. The middle portion of the physical address is used as an index to the 4 MiB L2 cache. The resulting L2 cache tag is compared with the upper part of the physical address to check for a match. If it matches, we have an L2 cache hit, and the data are sent to the processor, which uses the block offset to select the desired word. On an L2 miss, the physical address is then used to get the block from memory.

> 如果 L1 缓存检查结果未命中，则使用物理地址尝试 L2 缓存。物理地址的中间部分用作 4 MiB L2 缓存的索引。生成的 L2 缓存标记与物理地址的上半部分进行比较以检查是否匹配。如果匹配，我们就有一个 L2 缓存命中，数据被发送到处理器，处理器使用块偏移来选择所需的单词。在 L2 未命中时，物理地址然后用于从内存中获取块。

Although this is a simple example, the major difference between this drawing and a real cache is replication. First, there is only one L1 cache. When there are two L1 caches, the top half of the diagram is duplicated. Note that this would lead to two TLBs, which is typical. Hence, one cache and TLB is for instructions, driven from the PC, and one cache and TLB is for data, driven from the effective address.

> 尽管这是一个简单的示例，但此图纸与真实缓存之间的主要区别是复制。首先，只有一个 L1 缓存。当有两个 L1 缓存时，该图的上半部分被复制。请注意，这将导致两个 TLB，这是典型的。因此，一个缓存和 TLB 用于指令，由 PC 驱动，一个缓存和 TLB 用于数据，由有效地址驱动。

The second simplification is that all the caches and TLBs are direct mapped. If any were _n_-way set associative, then we would replicate each set of tag memory, comparators, and data memory _n_ times and connect data memories with an _n_:1 multiplexor to select a hit. Of course, if the total cache size remained the same, the cache index would also shrink by log 2*n* bits according to the formula in [Figure B.7](#_bookmark446) on page B-22.

> 第二个简化是直接映射所有缓存和 TLB。如果有的是 *n*-way 集关联，那么我们将复制每组标签内存，比较器和数据存储器 *n* Times，并使用 *n*：1 多路复用器连接数据记忆以选择命中。当然，如果总缓存大小保持不变，则根据[图 B.7](#_bogmark446) 中的公式，缓存索引也将按 log 2* n*位收缩。
