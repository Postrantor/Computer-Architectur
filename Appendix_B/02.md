## Cache Performance

> ##缓存性能

Because instruction count is independent of the hardware, it is tempting to evaluate processor performance using that number. Such indirect performance measures have waylaid many a computer designer. The corresponding temptation for eval- uating memory hierarchy performance is to concentrate on miss rate because it, too, is independent of the speed of the hardware. As we will see, miss rate can be just as misleading as instruction count. A better measure of memory hierarchy per- formance is the _average memory access time_:

> 由于指令数量与硬件无关，因此很容易使用该数字评估处理器性能。这种间接的绩效措施使许多计算机设计师都有车意。评估内存层次结构性能的相应诱惑是集中于错过率，因为它也独立于硬件的速度。正如我们将看到的那样，错率可能与指令数量一样具有误导性。更好地衡量内存层次结构的方法是_aaverage 内存访问时间_：

Average memory access time = Hit time + Miss rate ×Miss penalty where _hit time_ is the time to hit in the cache; we have seen the other two terms before. The components of average access time can be measured either in absolute time—say, 0.25–1.0 ns on a hit—or in the number of clock cycles that the proces- sor waits for the memory—such as a miss penalty of 150–200 clock cycles. Remember that average memory access time is still an indirect measure of perfor- mance; although it is a better measure than miss rate, it is not a substitute for execution time.

> 平均内存访问时间=命中时间 + 错率 × 失误罚款，其中_hit Time_是在缓存中击中的时间；我们以前见过其他两个任期。平均访问时间的组成部分可以在绝对时间内进行测量（例如，命中率为 0.25–1.0 ns），或者在 process-sor 等待内存的时钟周期数中，例如 150-200 的罚款。时钟周期。请记住，平均内存访问时间仍然是对表现的间接度量。尽管这比错过率更好，但它不能代替执行时间。

This formula can help us decide between split caches and a unified cache.

> 该公式可以帮助我们在分裂缓存和统一缓存之间做出决定。

Example Which has the lower miss rate: a 16 KiB instruction cache with a 16 KiB data cache or a 32 KiB unified cache? Use the miss rates in [Figure B.6](#_bookmark444) to help calculate the correct answer, assuming 36% of the instructions are data transfer instructions. Assume a hit takes 1 clock cycle and the miss penalty is 100 clock cycles. A load or store hit takes 1 extra clock cycle on a unified cache if there is only one cache port to satisfy two simultaneous requests. Using the pipelining terminology of [Chapter 3](#_bookmark93), the unified cache leads to a structural hazard. What is the average mem- ory access time in each case? Assume write-through caches with a write buffer and ignore stalls due to the write buffer.

> 示例哪个具有较低的错率：16 KIB 指令缓存，带有 16 KIB 数据缓存或 32 KIB 统一缓存？假设 36％的说明是数据传输说明，请使用[图 B.6]（#_ bokmark444）中的错率来帮助计算正确的答案。假设命中率是 1 个时钟周期，而错过的罚款为 100 个时钟周期。如果只有一个缓存端口可以满足两个同时的请求，则负载或商店命中量会在统一缓存上进行 1 个额外的时钟周期。使用[第 3 章]（#_ bookmark93）的管道术语，统一的缓存导致结构性危害。在每种情况下，平均内存访问时间是多少？假设用写缓冲区的写入缓存，并且由于写缓冲区而忽略了摊位。

Hence, the split caches in this example—which offer two memory ports per clock cycle, thereby avoiding the structural hazard—have a better average memory access time than the single-ported unified cache despite having a worse effective miss rate.

> 因此，在此示例中，拆分缓存（每个时钟周期都提供两个内存端口，从而避免结构性危险），尽管有效的失误率较差，但仍比单端口统一的统一缓存更好的平均内存访问时间。

### Average Memory Access Time and Processor Performance

> ###平均内存访问时间和处理器性能

An obvious question is whether average memory access time due to cache misses predicts processor performance.

> 一个明显的问题是，由于高速缓存失误而引起的平均内存访问时间是否可以预测处理器的性能。

First, there are other reasons for stalls, such as contention due to I/O devices using memory. Designers often assume that all memory stalls are due to cache mis- ses, because the memory hierarchy typically dominates other reasons for stalls. We use this simplifying assumption here, but be sure to account for _all_ memory stalls when calculating final performance.

> 首先，还有其他档位的原因，例如使用内存的 I/O 设备引起的争议。设计师通常认为所有内存摊位都是由于缓存错误引起的，因为内存层次结构通常主导了摊位的其他原因。我们在此处使用此简化的假设，但是在计算最终性能时，请确保考虑_ALL_内存失速。

Second, the answer also depends on the processor. If we have an in-order exe- cution processor (see [Chapter 3](#_bookmark93)), then the answer is basically yes. The processor stalls during misses, and the memory stall time is strongly correlated to average memory access time. Let’s make that assumption for now, but we’ll return to out-of-order processors in the next subsection.

> 其次，答案还取决于处理器。如果我们有一个内在的处理器（请参阅[第 3 章]（#_ bookmark93）），那么答案基本上是肯定的。在错过期间的处理器失速，并且内存失速时间与平均内存访问时间密切相关。让我们暂时做出假设，但是我们将在下一个小节中返回到订购外处理器。

As stated in the previous section, we can model CPU time as: CPU time = (CPU execution clock cycles + Memory stall clock cycles) ×Clock cycle time This formula raises the question of whether the clock cycles for a cache hit should be considered part of CPU execution clock cycles or part of memory stall clock cycles. Although either convention is defensible, the most widely accepted is to include hit clock cycles in CPU execution clock cycles.

> 如前一节所述，我们可以将 CPU 时间建模为：CPU 时间=（CPU 执行时钟循环 + 内存失速时钟循环）× 时钟周期时间此公式提出了一个问题，即缓存时钟周期是否应视为一部分 CPU 执行时钟周期或内存失速时钟周期的一部分。尽管两种约定都是可辩护的，但最广泛接受的是将命中时钟周期包括在 CPU 执行时钟周期中。

We can now explore the impact of caches on performance.

> 现在，我们可以探索缓存对性能的影响。

Example Let’s use an in-order execution computer for the first example. Assume that the cache miss penalty is 200 clock cycles, and all instructions usually take 1.0 clock cycles (ignoring memory stalls). Assume that the average miss rate is 2%, there is an average of 1.5 memory references per instruction, and the average number of cache misses per 1000 instructions is 30. What is the impact on performance when behavior of the cache is included? Calculate the impact using both misses per instruction and miss rate.

> 示例，让我们使用订单执行计算机进行第一个示例。假设缓存错过的惩罚为 200 个时钟周期，所有指令通常采用 1.0 时钟周期（忽略内存摊位）。假设平均失误率为 2％，平均每个说明中有 1.5 个内存参考，并且每 1000 个指令的平均缓存误差数为 30。当包括缓存行为时，对性能有什么影响？使用每指示和错过率都使用两次失误来计算影响。

The clock cycle time and instruction count are the same, with or without a cache. Thus, CPU time increases sevenfold, with CPI from 1.00 for a “perfect cache” to 7.00 with a cache that can miss. Without any memory hierarchy at all the CPI would increase again to 1.0 + 200 1.5 or 301—a factor of more than 40 times longer than a system with a cache!

> 有或没有缓存的时钟周期时间和指令计数是相同的。因此，CPU 时间增加了 7 倍，CPI 从 1.00 的“完美缓存”增加到 7.00，而缓存可能会错过。没有任何内存层次结构，CPI 将再次增加到 1.0 + 200 1.5 或 301，这是比具有缓存系统的系统长 40 倍以上！

As this example illustrates, cache behavior can have enormous impact on per- formance. Furthermore, cache misses have a double-barreled impact on a proces- sor with a low CPI and a fast clock:

> 如本示例所示，缓存行为可能会对能力产生巨大影响。此外，高速缓存遗失对低 CPI 和快速时钟的 Process 有双管的影响：

1. The lower the CPI<sub>execution</sub>, the higher the _relative_ impact of a fixed number of cache miss clock cycles.

> 1. cpi <sub>执行</sub>越低，固定数量的高速缓存钟周期的影响越高。

2. When calculating CPI, the cache miss penalty is measured in processor clock cycles for a miss. Therefore, even if memory hierarchies for two computers are identical, the processor with the higher clock rate has a larger number of clock cycles per miss and hence a higher memory portion of CPI.

> 2.计算 CPI 时，在处理器时钟周期中测量了缓存损失的惩罚。因此，即使两台计算机的内存层次结构是相同的，平时速率较高的处理器每位错过的时钟周期数量较大，因此 CPI 的内存部分更高。

The importance of the cache for processors with low CPI and high clock rates is thus greater, and, consequently, greater is the danger of neglecting cache behavior in assessing performance of such computers. Amdahl’s Law strikes again!

> 因此，缓存对于 CPI 低和高时钟速率的处理器的重要性更大，因此，忽略缓存行为在评估此类计算机性能中的危险是更大的危险。Amdahl 的法律再次罢工！

Although minimizing average memory access time is a reasonable goal—and we will use it in much of this appendix—keep in mind that the final goal is to reduce processor execution time. The next example shows how these two can differ.

> 尽管最小化平均内存访问时间是一个合理的目标，而我们将在本附录的大部分时间内使用它，但要记住，最终目标是减少处理器的执行时间。下一个示例显示了这两个如何不同。

Example What is the impact of two different cache organizations on the performance of a processor? Assume that the CPI with a perfect cache is 1.0, the clock cycle time is 0.35 ns, there are 1.4 memory references per instruction, the size of both caches is 128 KiB, and both have a block size of 64 bytes. One cache is direct mapped and the other is two-way set associative. [Figure B.5](#_bookmark443) shows that for set associative caches we must add a multiplexor to select between the blocks in the set depending on the tag match. Because the speed of the processor can be tied directly to the speed of a cache hit, assume the processor clock cycle time must be stretched 1.35 times to accommodate the selection multiplexor of the set associative cache. To the first approximation, the cache miss penalty is 65 ns for either cache orga- nization. (In practice, it is normally rounded up or down to an integer number of clock cycles.) First, calculate the average memory access time and then processor performance. Assume the hit time is 1 clock cycle, the miss rate of a direct-mapped 128 KiB cache is 2.1%, and the miss rate for a two-way set associative cache of the same size is 1.9%.

> 示例，两个不同的缓存组织对处理器性能有什么影响？假设具有完美缓存的 CPI 为 1.0，时钟周期时间为 0.35 ns，每个指令有 1.4 个内存引用，两个缓存的大小为 128 KIB，并且块大小为 64 个字节。一个缓存是直接映射的，另一个是双向集合的关联。[图 B.5]（#_ bookmark443）表明，对于集合的关联缓存，我们必须添加一个多路复用器，以根据标签匹配在集合中的块之间进行选择。由于处理器的速度可以直接绑定到缓存命中速度的速度，因此假设处理器时钟周期时间必须拉长 1.35 次，以适应集合关联缓存的选择多路复用器。在第一个近似值时，缓存或加拿大的罚款为 65 ns。（实际上，通常将其四舍五入到整数的时钟周期数。）首先，计算平均内存访问时间，然后计算处理器性能。假设命中时间为 1 个时钟周期，直接映射 128 KIB 缓存的错率为 2.1％，而相同大小的双向套件的关联缓存的错率为 1.9％。

In contrast to the results of average memory access time comparison, the direct- mapped cache leads to slightly better average performance because the clock cycle is stretched for _all_ instructions for the two-way set associative case, even if there are fewer misses. Because CPU time is our bottom-line evaluation and because direct mapped is simpler to build, the preferred cache is direct mapped in this example.

> 与平均内存访问时间比较的结果相反，直接映射的高速缓存会导致平均性能稍好一些，因为即使错过较少的错过，对于双向设置的关联案例的_ALL_指令就会伸展时钟周期。由于 CPU 时间是我们的底线评估，并且由于直接映射更简单地构建，因此在此示例中直接映射了首选缓存。

### Miss Penalty and Out-of-Order Execution Processors

> ###罚款和固定执行处理器

For an out-of-order execution processor, how do you define “miss penalty”? Is it the full latency of the miss to memory, or is it just the “exposed” or nonoverlapped latency when the processor must stall? This question does not arise in processors that stall until the data miss completes.

> 对于订单外执行处理器，您如何定义“小姐罚款”？这是错过到内存的全部延迟，还是当处理器必须停滞不前的“暴露”或不重叠的延迟？直到数据错过完成之前，这个问题才会在停滞不前的处理器中出现。

Let’s redefine memory stalls to lead to a new definition of miss penalty as non- overlapped latency:

> 让我们重新定义记忆摊位，导致对罚款小姐的新定义，因为非重叠的延迟：

Memory stall cycles Misses

> 记忆摊循环错过

Instruction = Instruction ×(Total miss latency — Overlapped miss latency)

> 指令=指令 ×（总数延迟 - 延迟延迟重叠）

Similarly, as some out-of-order processors stretch the hit time, that portion of the performance equation could be divided by total hit latency less overlapped hit latency. This equation could be further expanded to account for contention for memory resources in an out-of-order processor by dividing total miss latency into latency without contention and latency due to contention. Let’s just concentrate on miss latency.

> 同样，随着某些端外处理器延长了命中时间，性能方程的一部分可以由总命中延迟延迟较小的重叠率延迟划分。可以进一步扩展该方程式，以考虑到秩外处理器中的内存资源的争论，通过将延迟的总延迟分为延迟，而无需争夺争议而引起的争论和延迟。让我们只专注于延迟小姐。

We now have to decide the following:

> 现在，我们必须决定以下内容：

- _Length of memory latency_—What to consider as the start and the end of a mem- ory operation in an out-of-order processor.

> - 内存延迟的长度 - 要考虑的是在端外处理器中的开始操作的开始和结束。

- _Length of latency overlap_—What is the start of overlap with the processor (or, equivalently, when do we say a memory operation is stalling the processor)?

> - _延迟重叠的长度 - 与处理器重叠的开始是什么（或等效地，我们何时说内存操作正在停滞处理器）？

Given the complexity of out-of-order execution processors, there is no single cor- rect definition.

> 鉴于阶外执行处理器的复杂性，没有单个控制定义。

Because only committed operations are seen at the retirement pipeline stage, we say a processor is stalled in a clock cycle if it does not retire the maximum pos- sible number of instructions in that cycle. We attribute that stall to the first instruc- tion that could not be retired. This definition is by no means foolproof. For example, applying an optimization to improve a certain stall time may not always improve execution time because another type of stall—hidden behind the targeted stall—may now be exposed.

> 由于仅在退休管道阶段看到犯罪操作，因此我们说，如果处理器在该循环中没有退休最大可观的指令数量，则在时钟周期中停滞不前。我们将这种失速归因于无法退休的第一个指导。这个定义绝不是万无一失的。例如，应用优化来改善特定失速时间可能并不总是会改善执行时间，因为现在可能会暴露另一种类型的失速（固定在目标失速后面）。

For latency, we could start measuring from the time the memory instruction is queued in the instruction window, or when the address is generated, or when the instruction is actually sent to the memory system. Any option works as long as it is used in a consistent fashion.

> 对于延迟，我们可以从在指令窗口中排队或生成地址或实际发送到内存系统时开始测量内存指令的时间。只要以一致的方式使用，任何选项都可以使用。

Example Let’s redo the preceding example, but this time we assume the processor with the longer clock cycle time supports out-of-order execution yet still has a direct- mapped cache. Assume 30% of the 65 ns miss penalty can be overlapped; that is, the average CPU memory stall time is now 45.5 ns.

> 示例让我们重做前面的示例，但是这次我们假设具有较长时钟周期时间的处理器支持排序的执行，但仍具有直接映射的缓存。假设 65 NS 罚款中的 30％可以重叠；也就是说，平均 CPU 内存失速时间现在为 45.5 ns。

_Answer_ Average memory access time for the out-of-order (OOO) computer is

> _answer_ _answer 的平均内存访问时间（OOO）计算机为

Hence, despite a much slower clock cycle time and the higher miss rate of a direct- mapped cache, the out-of-order computer can be slightly faster if it can hide 30% of the miss penalty.

> 因此，尽管时钟周期时间较慢，并且直接映射的缓存的错率较高，但如果可以隐藏罚款的 30％，则端外计算机的速度可能会更快。

In summary, although the state of the art in defining and measuring memory stalls for out-of-order processors is complex, be aware of the issues because they signif- icantly affect performance. The complexity arises because out-of-order processors tolerate some latency due to cache misses without hurting performance. Conse- quently, designers usually use simulators of the out-of-order processor and mem- ory when evaluating trade-offs in the memory hierarchy to be sure that an improvement that helps the average memory latency actually helps program performance.

> 总而言之，尽管定义和测量序列处理器的内存摊位的最新状态很复杂，但要注意这些问题，因为它们显着影响性能。之所以出现复杂性，是因为越来越多的处理器可以忍受由于缓存而没有伤害性能而导致的一些延迟。因此，在评估内存层次结构中的权衡时，设计人员通常会使用排序外处理器的模拟器和 mem-yry，以确保有助于平均内存潜伏期的改进实际上有助于程序性能。

To help summarize this section and to act as a handy reference, [Figure B.7](#_bookmark446) lists the cache equations in this appendix.

> 为了帮助总结本节并充当方便的参考，[图 B.7]（#_ bookmark446）列出了此附录中的缓存方程。

Figure B.7 Summary of performance equations in this appendix. The first equation calculates the cache index size, and the rest help evaluate performance. The final two equations deal with multilevel caches, which are explained early in the next section. They are included here to help make the figure a useful reference.

> 图 B.7 本附录中的性能方程摘要。第一个方程计算缓存索引大小，其余方程有助于评估性能。最后两个方程式涉及多级缓存，在下一节初期对此进行了解释。它们在这里包括在内，以帮助使图成为有用的参考。
