## Distributed Shared-Memory and Directory-Based Coherence

As we saw in [Section 5.2](#centralized-shared-memory-architectures), a snooping protocol requires communication with all caches on every cache miss, including writes of potentially shared data. The absence of any centralized data structure that tracks the state of the caches is both the fundamental advantage of a snooping-based scheme, since it allows it to be inexpensive, as well as its Achilles’ heel when it comes to scalability.

> 正如我们在[第 5.2 节](＃集中式共享 - 内存 - 构造)中看到的那样，侦听协议需要与每个缓存失误的所有缓存，包括可能共享数据的写入。缺乏跟踪缓存状态的任何集中数据结构既是基于窥探的方案的基本优势，因为它允许其价格便宜，并且在可伸缩性方面也是其 Achilles 的脚跟。

For example, consider a multiprocessor consisting of four 4-core multicores capable of sustaining one data reference per clock and a 4 GHz clock. From the data in Section I.5 in Appendix I, we can see that the applications may require 4–170 GiB/s of memory bus bandwidth. The maximum memory bandwidth supported by the i7 with two DDR4 memory channels is 34 GiB/s. If several i7 multicore processors shared the same memory system, they would easily swamp it. In the last few years, the development of multicore processors forced all designers to shift to some form of distributed memory to support the bandwidth demands of the individual processors.

> 例如，考虑一个由四个 4 核多头器组成的多处理器，能够维持每个时钟和 4 GHz 时钟的一个数据参考。从附录 I 的 I.5 节中的数据中，我们可以看到应用程序可能需要 4-170 GIB/s 的内存总线带宽。i7 支持两个 DDR4 存储器通道支持的最大内存带宽为 34 GIB/s。如果几个 i7 多核处理器共享了相同的内存系统，则它们很容易淹没它。在过去的几年中，多层处理器的开发迫使所有设计人员都转移到某种形式的分布式内存，以支持单个处理器的带宽需求。

We can increase the memory bandwidth and interconnection bandwidth by distributing the memory, as shown in [Figure 5.2](#_bookmark217) on page 373; this immediately separates local memory traffic from remote memory traffic, reducing the bandwidth demands on the memory system and on the interconnection network. Unless we eliminate the need for the coherence protocol to broadcast on every cache miss, distributing the memory will gain us little.

> 我们可以通过分发内存来增加内存带宽和互连带宽，如[图 5.2](#_bookmark217) 在第 373 页上所示；这立即将本地内存流量与远程内存流量分开，从而减少了内存系统和互连网络上的带宽需求。除非我们消除了连贯协议在每个缓存遗漏上广播的需求，否则分发内存将使我们很少。

As we mentioned earlier, the alternative to a snooping-based coherence protocol is a _directory protocol_. A directory keeps the state of every block that may be cached. Information in the directory includes which caches (or collections of caches) have copies of the block, whether it is dirty, and so on. Within a multicore with a shared outermost cache (say, L3), it is easy to implement a directory scheme: simply keep a bit vector of the size equal to the number of cores for each L3 block. The bit vector indicates which private L2 caches may have copies of a block in L3, and invalidations are sent only to those caches. This works perfectly for a single multicore if L3 is inclusive, and this scheme is the one used in the Intel i7.

> 如前所述，基于窥探的连贯协议的替代方案是 *Directory 协议*。目录可以保留可能被缓存的每个块的状态。目录中的信息包括哪些缓存(或缓存的集合)具有该块的副本，无论是脏的，等等。在具有共享最外面缓存(例如 L3)的多核心内，很容易实现目录方案：只需保持一个大小的位向量等于每个 L3 块的内核数。位矢量指示哪些私有 L2 缓存可能具有 L3 中的块副本，并且仅发送给这些缓存。如果 L3 包含在内，则可以非常适合单个多核心，并且该方案是 Intel I7 中使用的方案。

The solution of a single directory used in a multicore is not scalable, even though it avoids broadcast. The directory must be distributed, but the distribution must be done in a way that the coherence protocol knows where to find the directory information for any cached block of memory. The obvious solution is to distribute the directory along with the memory so that different coherence requests can go to different directories, just as different memory requests go to different memories. If the information is maintained at an outer cache, like L3, which is multibanked, the directory information can be distributed with the different cache banks, effectively increasing the bandwidth.

> 在多项式中使用的单个目录的解决方案也无法扩展，即使它避免了广播。目录必须分布，但是必须以连贯协议知道在哪里找到任何缓存的内存块的目录信息的方式完成。显而易见的解决方案是将目录与内存一起分发目录，以便不同的连贯请求可以转到不同的目录，就像不同的内存请求转到不同的内存一样。如果该信息保持在外部缓存(例如多键的 L3)，则可以将目录信息与不同的缓存库分配，从而有效地增加带宽。

Figure 5.18 A directory is added to each node to implement cache coherence in a distributed-memory multiprocessor. In this case, a node is shown as a single multicore chip, and the directory information for the associated memory may reside either on or off the multicore. Each directory is responsible for tracking the caches that share the memory addresses of the portion of memory in the node. The coherence mechanism will handle both the maintenance of the directory information and any coherence actions needed within the multicore node.

> 图 5.18 将目录添加到每个节点中，以在分布式内存多处理器中实现缓存相干性。在这种情况下，一个节点显示为单个多层芯片，并且相关内存的目录信息可能存在于多项内或关闭多项。每个目录负责跟踪节点中内存部分的内存位置的缓存。相干机制将处理目录信息的维护和多项核心节点中所需的任何相干操作。

A distributed directory retains the characteristic that the sharing status of a block is always in a single known location. This property, together with the maintenance of information that says what other nodes may be caching the block, is what allows the coherence protocol to avoid broadcast. [Figure 5.18](#_bookmark236) shows how our distributed-memory multiprocessor looks with the directories added to each node.

> 分布式目录保留了一个块的共享状态始终位于一个已知位置的特征。该属性以及信息的维护，说明其他节点可能正在缓存该块，这是允许连贯协议避免广播的原因。[图 5.18](#_bookmark236) 显示了我们的分布式内存多处理器外观以及添加到每个节点的目录。

The simplest directory implementations associate an entry in the directory with each memory block. In such implementations, the amount of information is proportional to the product of the number of memory blocks (where each block is the same size as the L2 or L3 cache block) times the number of nodes, where a node is a single multicore processor or a small collection of processors that implements coherence internally. This overhead is not a problem for multiprocessors with less than a few hundred processors (each of which might be a multicore) because the directory overhead with a reasonable block size will be tolerable. For larger multiprocessors, we need methods to allow the directory structure to be efficiently scaled, but only supercomputer-sized systems need to worry about this.

> 最简单的目录实现将目录中的条目与每个内存块相关联。在此类实现中，信息量与内存块数量的乘积(每个块的大小与 L2 或 L3 缓存块相同)乘以节点的数量，其中节点是单个多层处理器或内部实现连贯性的一小部分处理器。对于少于几百个处理器(每个处理器可能是多层)的多处理器来说，该开销不是问题，因为具有合理块大小的目录开销将是可以忍受的。对于较大的多处理器，我们需要方法来有效地缩放目录结构，但是只有超级计算机大小的系统才需要担心这一点。

### Directory-Based Cache Coherence Protocols: The Basics

Just as with a snooping protocol, there are two primary operations that a directory protocol must implement: handling a read miss and handling a write to a shared, clean cache block. (Handling a write miss to a block that is currently shared is a simple combination of these two.) To implement these operations, a directory must track the state of each cache block. In a simple protocol, these states could be the following:

> 就像窥探协议一样，目录协议必须实现两个主要操作：处理读取错过并处理写入共享，干净的缓存块。(处理当前共享的块上的写入错过是这两者的简单组合。)要实现这些操作，目录必须跟踪每个缓存块的状态。在一个简单的协议中，这些状态可能是以下内容：

- _Shared_—One or more nodes have the block cached, and the value in memory is up to date (as well as in all the caches).
- _Uncached_—No node has a copy of the cache block.
- _Modified_—Exactly one node has a copy of the cache block, and it has written the block, so the memory copy is out of date. The processor is called the _owner_ of the block.

> - _shared_ - 一个或更多节点具有块缓存，并且内存中的值是最新的(以及所有缓存中)。
> - _uncached _-没有节点的副本，该副本的副本。
> - _modified _-表现出一个节点具有缓存块的副本，并且已写入块，因此内存副本已过时。处理器称为块的 *owner*。

In addition to tracking the state of each potentially shared memory block, we must track which nodes have copies of that block because those copies will need to be invalidated on a write. The simplest way to do this is to keep a bit vector for each memory block. When the block is shared, each bit of the vector indicates whether the corresponding processor chip (which is likely a multicore) has a copy of that block. We can also use the bit vector to keep track of the owner of the block when the block is in the exclusive state. For efficiency reasons, we also track the state of each cache block at the individual caches.

> 除了跟踪每个潜在共享内存块的状态外，我们还必须跟踪哪些节点具有该块的副本，因为这些副本需要在写入上无效。最简单的方法是为每个内存块保留一些位向量。当块共享时，向量的每个位都指示相应的处理器芯片(可能是多核)是否具有该块的副本。当块处于独家状态时，我们还可以使用 BIT 向量来跟踪块的所有者。出于效率原因，我们还跟踪各个缓存处每个缓存块的状态。

The states and transitions for the state machine at each cache are identical to what we used for the snooping cache, although the actions on a transition are slightly different. The processes of invalidating and locating an exclusive copy of a data item are different because they both involve communication between the requesting node and the directory and between the directory and one or more remote nodes. In a snooping protocol, these two steps are combined through the use of a broadcast to all the nodes.

> 每个缓存处的状态机的状态和过渡与我们用于窥探缓存的情况相同，尽管过渡上的动作略有不同。无效和定位数据项的独家副本的过程是不同的，因为它们既涉及请求节点和目录之间的通信以及目录和一个或多个远程节点之间的通信。在窥探协议中，这两个步骤通过使用向所有节点的广播使用来组合。

Before we see the protocol state diagrams, it is useful to examine a catalog of the message types that may be sent between the processors and the directories for the purpose of handling misses and maintaining coherence. [Figure 5.19](#_bookmark237) shows the types of messages sent among nodes. The _local node_ is the node where a request originates. The _home node_ is the node where the memory location and the directory entry of an address reside. The physical address space is statically distributed, so the node that contains the memory and directory for a given physical address is known. For example, the high-order bits may provide the node number, whereas the low-order bits provide the offset within the memory on that node. The local node may also be the home node. The directory must be accessed when the home node is the local node because copies may exist in yet a third node, called a _remote node_.

> 在看到协议状态图之前，检查处理器和目录之间可能发送的消息类型的目录是有用的，以处理错过和保持连贯性。[图 5.19](#_bookmark237) 显示了节点之间发送的消息的类型。*local Node* 是请求始发的节点。*home 节点*是地址位置的存储位置和目录条目的节点。物理地址空间是静态分布的，因此已知包含给定物理地址的内存和目录的节点。例如，高阶位可以提供节点编号，而低阶位则提供该节点上存储器内的偏移。本地节点也可以是主节点。当主节点是本地节点时，必须访问目录，因为副本可能存在于第三个节点中，称为 *remote node*。

Figure 5.19 The possible messages sent among nodes to maintain coherence, along with the source and destination node, the contents (where P 5 requesting node number, A 5 requested address, and D 5 data contents), and the function of the message. The first three messages are requests sent by the local node to the home. The fourth through sixth messages are messages sent to a remote node by the home when the home needs the data to satisfy a read or write miss request. Data value replies are used to send a value from the home node back to the requesting node. Data value write-backs occur for two reasons: when a block is replaced in a cache and must be written back to its home memory, and also in reply to fetch or fetch/invalidate messages from the home. Writing back the data value whenever the block becomes shared simplifies the number of states in the protocol because any dirty block must be exclusive and any shared block is always available in the home memory.

> 图 5.19 在节点之间发送的可能消息以及源节点和目标节点，内容(其中 p 5 请求节点号，5 个请求的地址和 D 5 个数据内容)以及消息的功能。前三个消息是本地节点发送到家庭的请求。第四到第六条消息是当房屋需要数据以满足读取或写入错过请求时，已将消息发送到远程节点的消息。数据值回复用于将主节点从主节点发送回请求节点。数据值写入后背现有两个原因：当块中替换为缓存并必须写回其家庭内存时，并且还回复了从家庭中获取或获取/无效消息。每当分享块共享时，请写回数据值，简化了协议中的状态数，因为任何肮脏的块都必须是独家的，并且在家庭内存中始终可用任何共享块。

A remote node is the node that has a copy of a cache block, whether exclusive (in which case it is the only copy) or shared. A remote node may be the same as either the local node or the home node. In such cases, the basic protocol does not change, but interprocessor messages may be replaced with intraprocessor messages.

> 远程节点是具有缓存块副本的节点，无论是独家(在这种情况下是唯一的副本)还是共享。远程节点可以与本地节点或主节点相同。在这种情况下，基本协议不会更改，但是可以用内部处理器消息替换处理处理器消息。

In this section, we assume a simple model of memory consistency. To minimize the type of messages and the complexity of the protocol, we make an assumption that messages will be received and acted upon in the same order they are sent. This assumption may not be true in practice and can result in additional complications, some of which we address in [Section 5.6](#models-of-memory-consistency-an-introduction) when we discuss memory consistency models. In this section, we use this assumption to ensure that invalidates sent by a node are honored before new messages are transmitted, just as we assumed in the discussion of implementing snooping protocols. As we did in the snooping case, we omit some details necessary to implement the coherence protocol. In particular, the serialization of writes and knowing that the invalidates for a write have completed are not as simple as in the broadcast-based snooping mechanism. Instead, explicit acknowledgments are required in response to write misses and invalidate requests. We discuss these issues in more detail in Appendix I.

> 在本节中，我们假设一个简单的内存一致性模型。为了最大程度地减少消息的类型和协议的复杂性，我们假设将收到消息并以相同的顺序接收消息。在实践中，此假设可能不是正确的，并且可能导致其他并发症，当我们讨论内存一致性模型时，我们在[第 5.6 节](第 5.6 节](第 5.6 节)中解决了一些并发症。在本节中，我们使用此假设来确保在传输新消息之前对节点发送的无效，就像我们在实施窥探协议的讨论中所假设的那样。就像我们在侦听情况下所做的那样，我们省略了实施连贯协议所需的一些细节。特别是，写作的序列化并知道写作的无效已经完成的不像基于广播的侦探机制那样简单。取而代之的是，需要明确的确认才能响应写信和无效的请求。我们在附录 I 中更详细地讨论了这些问题。

### An Example Directory Protocol

The basic states of a cache block in a directory-based protocol are exactly like those in a snooping protocol, and the states in the directory are also analogous to those we showed earlier. Thus we can start with simple state diagrams that show the state transitions for an individual cache block and then examine the state diagram for the directory entry corresponding to each block in memory. As in the snooping case, these state transition diagrams do not represent all the details of a coherence protocol; however, the actual controller is highly dependent on a number of details of the multiprocessor (message delivery properties, buffering structures, and so on). In this section, we present the basic protocol state diagrams. The knotty issues involved in implementing these state transition diagrams are examined in Appendix I.

> 基于目录的协议中的缓存块的基本状态与窥探协议中的基本状态一样，目录中的状态也类似于我们之前显示的那些。因此，我们可以从简单的状态图开始，这些状态图显示了单个缓存块的状态转换，然后检查与内存中每个块相对应的目录条目的状态图。与偷窥案一样，这些状态过渡图并不代表连贯协议的所有细节。但是，实际控制器高度取决于多处理器的许多细节(消息传递属性，缓冲结构等)。在本节中，我们介绍基本协议状态图。在附录 I 中检查了实施这些状态过渡图所涉及的打结问题。

[Figure 5.20](#_bookmark238) shows the protocol actions to which an individual cache responds. We use the same notation as in the last section, with requests coming from outside the node in gray and actions in bold. The state transitions for an individual cache are caused by read misses, write misses, invalidates, and data fetch requests; [Figure 5.20](#_bookmark238) shows these operations. An individual cache also generates read miss, write miss, and invalidate messages that are sent to the home directory. Read and write misses require data value replies, and these events wait for replies before changing state. Knowing when invalidates complete is a separate problem and is handled separately.

> [图 5.20](#_bookmark238) 显示了单个缓存响应的协议操作。我们使用与上一节中相同的符号，其中的请求来自灰色的节点外部，并以粗体为动作。单个缓存的状态转换是由阅读，写入错过，无效和数据提取请求引起的；[图 5.20](#_bookmark238) 显示了这些操作。单独的缓存还会生成读取，写失误和发送到主目录的消息无效。阅读和写入错过需要数据值的答复，这些事件在更改状态之前等待答复。知道何时完成无效是一个单独的问题，并分别处理。

The operation of the state transition diagram for a cache block in [Figure 5.20](#_bookmark238) is essentially the same as it is for the snooping case: the states are identical, and the stimulus is almost identical. The write miss operation, which was broadcast on the bus (or other network) in the snooping scheme, is replaced by the data fetch and invalidate operations that are selectively sent by the directory controller. Like the snooping protocol, any cache block must be in the exclusive state when it is written, and any shared block must be up to date in memory. In many multicore processors, the outermost level in the processor cache is shared among the cores (as is the L3 in the Intel i7, the AMD Opteron, and the IBM Power7), and hardware at that level maintains coherence among the private caches of each core on the same chip, using either an internal directory or snooping. Thus the on-chip multicore coherence mechanism can be used to extend coherence among a larger set of processors simply by interfacing to the outermost shared cache. Because this interface is at L3, contention between the processor and coherence requests is less of an issue, and duplicating the tags could be avoided.

> [图 5.20](#_bookmark238) 中缓存块的状态过渡图的操作本质上与侦听情况相同：状态是相同的，刺激几乎相同。在侦听方案中在总线(或其他网络)上广播的写入错过操作被数据获取和无效操作代替，这些操作由目录控制器选择性地发送。像窥探协议一样，任何缓存块编写时都必须处于独家状态，并且任何共享块都必须在内存中最新。在许多多层处理器中，处理器缓存中的最外层级别在核心之间共享(就像 Intel I7 中的 L3 一样使用内部目录或窥探同一芯片上的核心。因此，仅通过将最外面共享的高速缓存连接到较大的处理器之间，可以使用片上多层相干机制来扩展较大的处理器之间的连贯性。由于该接口处于 L3，因此处理器和连贯请求之间的争论不太重要，因此可以避免重复标签。

In a directory-based protocol, the directory implements the other half of the coherence protocol. A message sent to a directory causes two different types of actions: updating the directory state and sending additional messages to satisfy the request. The states in the directory represent the three standard states for a block; unlike in a snooping scheme, however, the directory state indicates the state of all the cached copies of a memory block, rather than for a single cache block.

> 在基于目录的协议中，目录实现了连贯协议的另一半。发送到目录的消息导致两种不同类型的操作：更新目录状态并发送其他消息以满足请求。目录中的各州代表一个块的三个标准状态；但是，与窥探方案不同，目录状态指示内存块的所有缓存副本的状态，而不是单个缓存块。

Figure 5.20 State transition diagram for an individual cache block in a directory-based system. Requests by the local processor are shown in _black_, and those from the home directory are shown in _gray_. The states are identical to those in the snooping case, and the transactions are very similar, with explicit invalidate and write-back requests replacing the write misses that were formerly broadcast on the bus. As we did for the snooping controller, we assume that an attempt to write a shared cache block is treated as a miss; in practice, such a transaction can be treated as an ownership request or upgrade request and can deliver ownership without requiring that the cache block be fetched.

> 图 5.20 基于目录的系统中单个缓存块的状态过渡图。本地处理器的请求显示在 *BLACK* 中，*gray* 中显示了主目录的请求。各州与偷窥案中的交易相同，交易非常相似，显式和写下请求取代了以前在公共汽车上播出的写作失误。就像我们为侦探控制器所做的那样，我们假设尝试编写共享缓存块的尝试被视为错过。实际上，这样的交易可以视为所有权请求或升级请求，并且可以交付所有权，而无需获取缓存块。

The memory block may be uncached by any node, cached in multiple nodes and readable (shared), or cached exclusively and writable in exactly one node. In addition to the state of each block, the directory must track the set of nodes that have a copy of a block; we use a set called _Sharers_ to perform this function. In multiprocessors with fewer than 64 nodes (each of which may represent four to eight times as many processors), this set is typically kept as a bit vector. Directory requests need to update the set Sharers and also read the set to perform invalidations.

> 内存块可以被任何节点未切除，以多个节点缓存并可读(共享)，也可以完全缓存，并在一个节点中完全缓存和写作。除了每个块的状态外，目录还必须跟踪具有块副本的节点集；我们使用一个名为 *sharers* 的集合来执行此功能。在少于 64 个节点的多处理器中(每个节点的四到八倍)，该集合通常保留为位向量。目录请求需要更新集合共享者，并读取该集以执行无效。

![](../media/image296.png)

Figure 5.21 The state transition diagram for the directory has the same states and structure as the transition diagram for an individual cache. All actions are in gray because they are all externally caused. _Bold_ indicates the action taken by the directory in response to the request.

> 图 5.21 目录的状态过渡图具有与单个缓存的过渡图相同的状态和结构。所有动作都是灰色的，因为它们都是外部引起的。*bold* 表示目录对请求的响应。

[Figure 5.21](#_bookmark239) shows the actions taken at the directory in response to messages received. The directory receives three different requests: read miss, write miss, and data write-back. The messages sent in response by the directory are shown in bold, and the updating of the set Sharers is shown in bold italics. Because all the stimulus messages are external, all actions are shown in gray. Our simplified protocol assumes that some actions are atomic, such as requesting a value and sending it to another node; a realistic implementation cannot use this assumption. To understand these directory operations, let’s examine the requests received and actions taken state by state. When a block is in the uncached state, the copy in memory is the current value, so the only possible requests for that block are

> [图 5.21](#_bookmark239) 显示了目录中对收到的消息采取的动作。该目录会收到三个不同的请求：阅读 MISS，WRITE MISS 和 DATA NOTER BACK。目录中发送的响应发送的消息以粗体显示，集合共享者的更新以粗体斜体显示。因为所有刺激消息都是外部的，所以所有动作均以灰色显示。我们的简化协议假设某些动作是原子，例如请求值并将其发送到另一个节点。现实的实现不能使用此假设。要了解这些目录操作，让我们检查一下按州采取的请求和采取的行动。当一个块处于未缓解状态时，内存中的副本是当前值，因此该块的唯一可能的请求是

- _Read miss_—The requesting node is sent the requested data from memory, and the requester is made the only sharing node. The state of the block is made shared.
- _Write miss_—The requesting node is sent the value and becomes the sharing node. The block is made exclusive to indicate that the only valid copy is cached. Sharers indicates the identity of the owner.

> - _read miss_ - 请求节点是从内存发送请求的数据，并将请求者成为唯一的共享节点。该块的状态被共享。
> - _write miss_-请求节点已发送值并成为共享节点。该块是独有的，以表明唯一的有效副本是缓存的。共享者表示所有者的身份。

When the block is in the shared state, the memory value is up to date, so the same two requests can occur:

- _Read miss_—The requesting node is sent the requested data from memory, and the requesting node is added to the sharing set.
- _Write miss_—The requesting node is sent the value. All nodes in the set Sharers are sent invalidate messages, and the Sharers set is to contain the identity of the requesting node. The state of the block is made exclusive.

When the block is in the exclusive state, the current value of the block is held in a cache on the node identified by the set Sharers (the owner), so there are three possible directory requests:

> 当块处于独家状态时，该块的当前值将在设置共享者(所有者)标识的节点上的缓存中保存，因此有三个可能的目录请求：

- _Read miss_—The owner is sent a data fetch message, which causes the state of the block in the owner’s cache to transition to shared and causes the owner to send the data to the directory, where it is written to memory and sent back to the requesting processor. The identity of the requesting node is added to the set Sharers, which still contains the identity of the processor that was the owner (since it still has a readable copy).

> - _read miss_-所有者发送了一条数据获取消息，该消息导致所有者缓存中的块状态过渡到共享，并导致所有者将数据发送到目录，并将其写入内存并发送回该目录请求处理器。请求节点的标识添加到集合共享者中，该共享者仍然包含所有者的处理器的身份(因为它仍然具有可读的副本)。

- _Data write-back_—The owner is replacing the block and therefore must write it back. This write-back makes the memory copy up to date (the home directory essentially becomes the owner), the block is now uncached, and the Sharers set is empty.

> - _data Write-back_-所有者正在替换块，因此必须将其写回。此写入后，使内存复制最新(主目录本质上是所有者)，块现在不适合，并且共享者集为空。

- _Write miss_—The block has a new owner. A message is sent to the old owner, causing the cache to invalidate the block and send the value to the directory, from which it is sent to the requesting node, which becomes the new owner. Sharers is set to the identity of the new owner, and the state of the block remains exclusive.

> - _write Miss_-块有一个新所有者。一条消息将发送给旧所有者，导致缓存无效块并将值发送到该目录，并将其发送到该请求节点，后者成为新所有者。共享者设置为新所有者的身份，并且该块的状态仍然是独有的。

This state transition diagram in [Figure 5.21](#_bookmark239) is a simplification, just as it was in the snooping cache case. In the case of a directory, as well as a snooping scheme implemented with a network other than a bus, our protocols will need to deal with nonatomic memory transactions. Appendix I explores these issues in depth.

> [图 5.21](#_bookmark239) 中的该状态过渡图是一个简化的，就像在侦听缓存案例中一样。在目录以及使用其他网络以外的其他网络实现的窥探方案的情况下，我们的协议将需要处理非原子记忆交易。附录一世深入探讨这些问题。

The directory protocols used in real multiprocessors contain additional optimizations. In particular, in this protocol when a read or write miss occurs for a block that is exclusive, the block is first sent to the directory at the home node. From there it is stored into the home memory and also sent to the original requesting node. Many of the protocols in use in commercial multiprocessors forward the data from the owner node to the requesting node directly (as well as performing the writeback to the home). Such optimizations often add complexity by increasing the possibility of deadlock and by increasing the types of messages that must be handled.

> 实际多处理器中使用的目录协议包含其他优化。特别是，在此协议中，当读取或写入错过的读数是针对独家的块时，该块首先将块发送到主节点的目录。从那里，它存储在家庭内存中，并发送到原始请求节点。商业多处理器中使用的许多协议将数据从所有者节点直接转发到请求节点(以及对家庭的写入)。这种优化通常通过增加僵局的可能性和增加必须处理的消息类型来增加复杂性。

Implementing a directory scheme requires solving most of the same challenges we discussed for snooping protocols. There are, however, new and additional problems, which we describe in Appendix I. In [Section 5.8](#_bookmark247), we briefly describe how modern multicores extend coherence beyond a single chip. The combinations of multichip coherence and multicore coherence include all four possibilities of snooping/snooping (AMD Opteron), snooping/directory, directory/snooping, and directory/directory! Many multiprocessors have chosen some form of snooping within a single chip, which is attractive if the outermost cache is shared and inclusive, and directories across multiple chips. Such an approach simplifies implementation because only the processor chip, rather than an individual core, need be tracked.

> 实施目录方案需要解决我们讨论的窥探协议的大多数相同挑战。但是，我们在附录 I 中描述的是新的和其他问题。Multichip 连贯性和多核相干性的组合包括所有四个可能性的窥探/侦听(AMD Opteron)，窥探/目录，目录/侦探以及目录/目录/目录！许多多处理器在单个芯片中选择了某种形式的窥探，如果共享最外面的高速缓存和包容性，以及跨多个芯片的目录很有吸引力。这种方法简化了实施，因为只有处理器芯片而不是单个核心才需要跟踪。
