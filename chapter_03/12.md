## Putting It All Together: The Intel Core i7 6700 and ARM Cortex-A53

> ##将所有内容放在一起：Intel Core i7 6700 和 Arm Cortex-A53

In this section, we explore the design of two multiple issue processors: the ARM Cortex-A53 core, which is used as the basis for several tablets and cell phones, and the Intel Core i7 6700, a high-end, dynamically scheduled, speculative processor intended for high-end desktops and server applications. We begin with the simpler processor.

> 在本节中，我们探讨了两个多个问题处理器的设计：ARM Cortex-A53 Core，用作几片平板电脑和手机的基础，以及 Intel Core i7 6700，高端，动态的，投机性的高端用于高端台式机和服务器应用程序的处理器。我们从更简单的处理器开始。

Figure 3.33 The speedup from using multithreading on one core on an i7 processor averages 1.28 for the Java benchmarks and 1.31 for the PARSEC benchmarks (using an unweighted harmonic mean, which implies a workload where the total time spent executing each benchmark in the single-threaded base set was the same). The energy efficiency averages 0.99 and 1.07, respectively (using the harmonic mean). Recall that anything above 1.0 for energy efficiency indicates that the feature reduces execution time by more than it increases average power. Two of the Java benchmarks experience little speedup and have significant negative energy efficiency because of this issue. Turbo Boost is off in all cases. These data were collected and analyzed by [Esmaeilzadeh et al. (2011)](#_bookmark945) using the Oracle (Sun) HotSpot build 16.3-b01 Java 1.6.0 Virtual Machine and the gcc v4.4.1 native compiler.

> 图 3.33 在 i7 处理器上使用多线程在 Java 基准测试的一个核心上使用多线程的速度和 PARSEC 基准测试的 1.31(1.31 基集是相同的)。能源效率平均分别为 0.99 和 1.07(使用谐波平均值)。回想一下，能源效率以上 1.0 以上的任何内容都表明该功能将执行时间减少超过其增加平均功率。由于这个问题，两个 Java 基准的速度很小，并且具有显着的负能量效率。在所有情况下，涡轮增压均未关闭。通过[Esmaeilzadeh 等人收集并分析了这些数据。(2011)](#_ bookmark945)使用 Oracle(Sun)Hotspot 构建 16.3-B01 Java 1.6.0 虚拟机和 GCC v4.4.1 本机编译器。

### The ARM Cortex-A53

> ###手臂 Cortex-A53

The A53 is a dual-issue, statically scheduled superscalar with dynamic issue detection, which allows the processor to issue two instructions per clock. [Figure 3.34](#_bookmark140) shows the basic pipeline structure of the pipeline. For nonbranch, integer instructions, there are eight stages: F1, F2, D1, D2, D3/ISS, EX1, EX2, and WB, as described in the caption. The pipeline is in order, so an instruction can initiate execution only when its results are available and when proceeding instructions have initiated. Thus, if the next two instructions are dependent, both can proceed to the appropriate execution pipeline, but they will be serialized when they get to the beginning of that pipeline. When the scoreboard-based issue logic indicates that the result from the first instruction is available, the second instruction can issue.

> A53 是带有动态问题检测的双标，静态安排的超级标准，可让处理器每个时钟发出两个说明。[图 3.34](#_ bookmark140)显示了管道的基本管道结构。对于非分支，整数说明，有八个阶段：F1，F2，D1，D2，D3/ISS，ISS，EX1，EX2 和 WB，如标题中所述。管道是按顺序进行的，因此只有在结果可用以及启动程序时，只有在其结果可用时才能启动执行。因此，如果接下来的两个说明取决于两个指令，则两者都可以进行适当的执行管道，但是当它们到达该管道的开头时，它们将被序列化。当基于计分板的问题逻辑指示第一个指令的结果可用时，第二个指令可以发出。

Figure 3.34 The basic structure of the A53 integer pipeline is 8 stages: F1 and F2 fetch the instruction, D1 and D2 do the basic decoding, and D3 decodes some more complex instructions and is overlapped with the first stage of the execution pipeline (ISS). After ISS, the Ex1, EX2, and WB stages complete the integer pipeline. Branches use four different predictors, depending on the type. The floating-point execution pipeline is 5 cycles deep, in addition to the 5 cycles needed for fetch and decode, yielding 10 stages in total.

> 图 3.34 A53 整数管道的基本结构是 8 个阶段：F1 和 F2 获取指令，D1 和 D2 进行基本解码，D3 解码一些更复杂的指令，并与执行管道的第一阶段重叠(ISS)(ISS)重叠。ISS 之后，EX1，EX2 和 WB 阶段完成整数管道。分支使用四个不同的预测指标，具体取决于类型。除了提取和解码所需的 5 个周期外，浮点执行管道深处是 5 个循环，总共产生了 10 个阶段。

The four cycles of instruction fetch include an address generation unit that produces the next PC either by incrementing the last PC or from one of four predictors:

> 指令获取的四个循环包括一个地址生成单元，该单元通过增加最后一个 PC 或从四个预测变量之一来产生下一辆 PC：

1. A single-entry branch target cache containing two instruction cache fetches (the next two instructions following the branch, assuming the prediction is correct). This target cache is checked during the first fetch cycle, if it hits; then the next two instructions are supplied from the target cache. In case of a hit and a correct prediction, the branch is executed with no delay cycles.

> 1.一个包含两个指令缓存提取器的单输入分支目标缓存(假设预测是正确的)。该目标缓存在第一个提取周期内检查，如果命中率；然后从目标缓存提供了接下来的两个说明。如果有命中率和正确的预测，则在没有延迟周期的情况下执行分支。

2. A 3072-entry hybrid predictor, used for all instructions that do not hit in the branch target cache, and operating during F3. Branches handled by this predictor incur a 2-cycle delay.

> 2. 3072-输入的混合预测指标，用于所有未在分支目标缓存中且在 F3 期间运行的指令。该预测因子处理的分支会产生 2 周期的延迟。

3. A 256-entry indirect branch predictor that operates during F4; branches predicted by this predictor incur a three-cycle delay when predicted correctly.

> 3.在 F4 期间运行的 256 个进入间接分支预测指标；该预测变量预测的分支在正确预测时会产生三周期延迟。

4. An 8-deep return stack, operating during F4 and incurring a three-cycle delay.

> 4.一个 8 深的返回堆栈，在 F4 期间运行，并产生三周期延迟。

Figure 3.35 Misprediction rate of the A53 branch predictor for SPECint2006.

> 图 3.35 Specint2006 的 A53 分支预测变量的错误预测率。

Branch decisions are made in ALU pipe 0, resulting in a branch misprediction penalty of 8 cycles. [Figure 3.35](#_bookmark141) shows the misprediction rate for SPECint2006. The amount of work that is wasted depends on both the misprediction rate and the issue rate sustained during the time that the mispredicted branch was followed. As [Figure 3.36](#_bookmark142) shows, wasted work generally follows the misprediction rate, though it may be larger or occasionally shorter.

> 分支决定是在 Alu 管道 0 中做出的，导致分支错误预测的罚款为 8 个周期。[图 3.35](#_ bookmark141)显示了 Specint2006 的错误预测率。浪费的工作数量取决于错误预测率和在遵循错误预测的分支机构期间所维持的问题。如[图 3.36](#_ bookmark142)所示，浪费的工作通常遵循错误预测率，尽管它可能更大或偶尔较短。

##### _Performance of the A53 Pipeline_

> ##### _ A53 管道的性能_

The A53 has an ideal CPI of 0.5 because of its dual-issue structure. Pipeline stalls can arise from three sources:

> A53 的理想 CPI 为 0.5，因为其双标结构。管道摊位可能来自三个来源：

1Functional hazards, which occur because two adjacent instructions selected for issue simultaneously use the same functional pipeline. Because the A53 is statically scheduled, the compiler should try to avoid such conflicts. When such

> 1 功能危害，这是因为选择了两个用于发行的相邻指令同时使用相同的功能管道。由于 A53 是静态安排的，因此编译器应尝试避免这种冲突。当这样

Figure 3.36 Wasted work due to branch misprediction on the A53. Because the A53 is an in-order machine, the amount of wasted work depends on a variety of factors, including data dependences and cache misses, both of which will cause a stall.

> 图 3.36 由于对 A53 的分支错误预测而浪费了工作。由于 A53 是一台订购机器，因此浪费的工作数量取决于各种因素，包括数据依赖性和缓存错过，这两者都会导致失速。

instructions appear sequentially, they will be serialized at the beginning of the execution pipeline, when only the first instruction will begin execution.

> 指令依次出现，它们将在执行管道的开头序列化，而只有第一个指令才能开始执行。

1. Data hazards, which are detected early in the pipeline and may stall either both instructions (if the first cannot issue, the second is always stalled) or the second of a pair. Again, the compiler should try to prevent such stalls when possible.

> 1.数据危害在管道中早期检测到，并且可能会停滞两种说明(如果第一次发行，第二个总是停滞)，则可能停滞不前。同样，编译器应尽可能防止此类失速。

2. Control hazards, which arise only when branches are mispredicted.

> 2.控制危害，仅当分支机构被错误预测时就会产生。

Both TLB misses and cache misses also cause stalls. On the instruction side, a TLB or cache miss causes a delay in filling the instruction queue, likely leading to a downstream stall of the pipeline. Of course, this depends on whether it is an L1 miss, which might be largely hidden if the instruction queue was full at the time of the miss, or an L2 miss, which takes considerably longer. On the data side, a cache or TLB miss will cause the pipeline to stall because the load or store that caused the miss cannot proceed down the pipeline. All other subsequent instructions will thus be stalled. [Figure 3.37](#_bookmark143) shows the CPI and the estimated contributions from various sources.

> TLB 错过和缓存失误也引起摊位。在指令侧，TLB 或缓存失误会导致延迟填充指令队列，这可能导致管道下游摊位。当然，这取决于它是 L1 失误，如果指示队列在失误时已满，或者是 L2 Miss，则可能会隐藏起来，这需要更长的时间。在数据侧，缓存或 TLB 失误将导致管道停滞，因为导致失误的负载或存储无法下水。因此，所有其他后续说明将停滞不前。[图 3.37](#_ bookmark143)显示了 CPI 和来自各种来源的估计贡献。

> Figure 3.37 The estimated composition of the CPI on the ARM A53 shows that pipeline stalls are significant but are outweighed by cache misses in the poorest performing programs. This estimate is obtained by using the L1 and L2 miss rates and penalties to compute the L1 and L2 generated stalls per instruction. These are subtracted from the CPI measured by a detailed simulator to obtain the pipeline stalls. Pipeline stalls include all three hazards.

>> 图 3.37 ARM A53 上 CPI 的估计组成表明，管道摊位很重要，但表现最差的程序中的 Cache 失误超过了。该估计是通过使用 L1 和 L2 损失率和罚款来计算 L1 和 L2 生成的摊位的估计值。这些是从详细的模拟器测量的 CPI 中减去这些管道摊位的。管道摊位包括所有三个危险。
>>

The A53 uses a shallow pipeline and a reasonably aggressive branch predictor, leading to modest pipeline losses, while allowing the processor to achieve high clock rates at modest power consumption. In comparison with the i7, the A53 consumes approximately 1/200 the power for a quad core processor!

> A53 使用浅水管道和合理的分支预测因子，导致管道损失适度，同时允许处理器在适度的功耗下实现高时钟速率。与 i7 相比，A53 消耗了大约 1/200 的 Quad Core 处理器的功率！

### The Intel Core i7

> ###英特尔核心 i7

The i7 uses an aggressive out-of-order speculative microarchitecture with deep pipelines with the goal of achieving high instruction throughput by combining multiple issue and high clock rates. The first i7 processor was introduced in 2008; the i7 6700 is the sixth generation. The basic structure of the i7 is similar, but successive generations have enhanced performance by changing cache strategies (e.g., the aggressiveness of prefetching), increasing memory bandwidth, expanding the number of instructions in flight, enhancing branch prediction, and improving graphics support. The early i7 microarchitectures used reservations stations and reorder buffers for their out-of-order, speculative pipeline. Later microarchitectures, including the i7 6700, use register renaming, with the reservations stations acting as functional unit queues and the reorder buffer simply tracking control information.

> i7 使用具有进取的投机性微体系结构，并具有深层管道，目的是通过结合多个问题和高时钟速率来实现高指导吞吐量。第一个 i7 处理器于 2008 年推出；i7 6700 是第六代。i7 的基本结构是相似的，但是连续的世代通过改变缓存策略(例如，预取的侵略性)，增加内存带宽，扩大飞行中的指令数量，增强分支预测和改善图形支持。i7 早期的微体系结构使用预订站和重新排序缓冲区作为货运，投机管道。后来的微体系结构(包括 i7 6700)使用寄存器重命名，预订站充当功能单位队列，重新排序缓冲区简单地跟踪控制信息。

[Figure 3.38](#_bookmark144) shows the overall structure of the i7 pipeline. We will examine the pipeline by starting with instruction fetch and continuing on to instruction commit, following steps labeled in the figure.

> [图 3.38](#_ bookmark144)显示了 i7 管道的整体结构。我们将通过指令获取并继续进行指令提交来检查管道，按照图中标记的步骤。

1. Instruction fetch—The processor uses a sophisticated multilevel branch predictor to achieve a balance between speed and prediction accuracy. There is also a return address stack to speed up function return. Mispredictions cause a penalty of about 17 cycles. Using the predicted address, the instruction fetch unit fetches 16 bytes from the instruction cache.

> 1.指令获取 - 处理器使用复杂的多级分支预测器，以在速度和预测准确性之间达到平衡。还有一个返回地址堆栈来加快函数返回。错误预测会导致大约 17 个周期的罚款。使用预测的地址，指令获取单元从指令缓存获取 16 个字节。

2. The 16 bytes are placed in the predecode instruction buffer—In this step, a process called macro-op fusion is executed. _Macro-op fusion_ takes instruction combinations such as compare followed by a branch and fuses them into a single operation, which can issue and dispatch as one instruction. Only certain special cases can be fused, since we must know that the only use of the first result is by the second instruction (i.e., compare and branch). In a study of the Intel Core architecture (which has many fewer buffers), [Bird et al. (2007)](#_bookmark929) discovered that macrofusion had a significant impact on the performance of integer programs resulting in an 8%–10% average increase in performance with a few programs showing negative results. There was little impact on FP programs; in fact, about half of the SPECFP benchmarks showed negative results from macro-op fusion. The predecode stage also breaks the 16 bytes into individual x86 instructions. This predecode is nontrivial because the length of an x86 instruction can be from 1 to 17 bytes and the predecoder must look through a number of bytes before it knows the instruction length. Individual x86 instructions (including some fused instructions) are placed into the instruction queue.

> 2.将 16 个字节放置在预码指令缓冲区中 - 在此步骤中，执行了称为宏观融合的过程。_macro-op Fusion_采用指令组合，例如比较，然后将它们融合到单个操作中，该操作可以发行并派遣作为一种指令。只有某些特殊情况可以融合，因为我们必须知道，第一个结果的唯一使用是通过第二个指令(即比较和分支)。在对英特尔核心体系结构(缓冲区较少)的研究中，[Bird 等。(2007)](#_ bookmark929)发现，宏福音对整数计划的性能产生了重大影响，导致绩效平均增长 8％–10％，其中一些程序显示了负面结果。对 FP 计划的影响很小。实际上，大约一半的 SPECFP 基准显示出宏观融合的负面结果。预码阶段还将 16 个字节分解为单个 X86 指令。该预码是不平凡的，因为 X86 指令的长度可以从 1 到 17 个字节，并且预码器必须在知道指令长度之前仔细阅读许多字节。将单个 X86 说明(包括一些融合说明)放入说明队列中。

3. Micro-op decode—Individual x86 instructions are translated into micro-ops. Micro-ops are simple RISC-V-like instructions that can be executed directly by the pipeline; this approach of translating the x86 instruction set into simple operations that are more easily pipelined was introduced in the Pentium Pro in 1997 and has been used since. Three of the decoders handle x86 instructions that translate directly into one micro-op. For x86 instructions that have more complex semantics, there is a microcode engine that is used to produce the micro-op sequence; it can produce up to four micro-ops every cycle and continues until the necessary micro-op sequence has been generated. The microops are placed according to the order of the x86 instructions in the 64-entry micro-op buffer.

> 3. Micro-OP 解码 - 个体 X86 指令被翻译成微型 OP。Micro-Ops 是可以直接通过管道执行的简单类似 RISC-V 的指令；这种方法将 X86 指令集转为简单操作，更容易管道，并于 1997 年在 Pentium Pro 中引入，此后已被使用。其中三个解码器处理 X86 指令，这些指令直接转换为一个微 OP。对于具有更复杂语义的 X86 指令，有一个用于生成微型 OP 序列的微码发动机。它可以在每个循环中最多产生四个微型操作，并继续进行，直到生成必要的微型 OP 序列为止。根据 64-输入微型 OP 缓冲液的 X86 指令的顺序放置微室。

> Figure 3.38 The Intel Core i7 pipeline structure shown with the memory system components. The total pipeline depth is 14 stages, with branch mispredictions typically costing 17 cycles, with the extra few cycles likely due to the time to reset the branch predictor. The six independent functional units can each begin execution of a ready micro-op in the same cycle. Up to four micro-ops can be processed in the register renaming table.

>> 图 3.38 带有内存系统组件的 Intel Core i7 管道结构。总管道深度为 14 个阶段，分支错误预测通常为 17 个周期，由于时间重置分支预测变量的时间可能会导致额外的周期。六个独立的功能单元可以分别在同一周期内开始执行即时的微型 OP。在寄存器重命名表中最多可以处理四个微型操作系统。
>>

1. The micro-op buffer preforms _loop stream detection_ and _microfusion_—If there is a small sequence of instructions (less than 64 instructions) that comprises a loop, the loop stream detector will find the loop and directly issue the micro-ops from the buffer, eliminating the need for the instruction fetch and instruction decode stages to be activated. Microfusion combines instruction pairs such as ALU operation and a dependent store and issues them to a single reservation station (where they can still issue independently), thus increasing the usage of the buffer. Micro-op fusion produces smaller gains for integer programs and larger ones for FP, but the results vary widely. The different results for integer and FP programs with macro and micro fusion, probably arise from the patterns recognized and fused and the frequency of occurrence in integer versus FP programs. In the i7, which has a much larger number of reorder buffer entries, the benefits from both techniques are likely to be smaller.

> 1. micro-op 缓冲区预成型_loop stream distion_和_microfusion_-如果有一系列指令(少于 64 个说明)包括一个循环，则循环流检测器将找到循环并直接从循环中发出微型操作系统。缓冲区，消除了对指令获取和指令解码阶段的需求。Microfusion 结合了指令对，例如 Alu 操作和依赖商店，并将其发行到单个预订站(它们仍然可以独立发行)，从而增加了缓冲区的使用情况。微型融合会为整数程序产生较小的收益，而 FP 则产生较大的融合计划，但结果差异很大。具有宏观和微型融合的整数和 FP 程序的不同结果，可能是由识别和融合的模式以及整数与 FP 程序中发生的频率产生的。在具有更大数量的重新订购缓冲区条目的 i7 中，这两种技术的好处都可能较小。

2. Perform the basic instruction issue—Looking up the register location in the register tables, renaming the registers, allocating a reorder buffer entry, and fetching any results from the registers or reorder buffer before sending the micro-ops to the reservation stations. Up to four micro-ops can be processed every clock cycle; they are assigned the next available reorder buffer entries.

> 2.执行基本指令问题 - 在寄存器表中查看寄存器位置，重命名寄存器，分配重新排序缓冲区条目，并从寄存器或重新排序缓冲区中获取任何结果，然后再将微型 OPS 发送到预订站。每个时钟周期都可以处理多达四个微型操作。他们将分配下一个可用的重新排序缓冲区条目。

3. The i7 uses a centralized reservation station shared by six functional units. Up to six micro-ops may be dispatched to the functional units every clock cycle.

> 3. i7 使用六个功能单元共享的集中预订站。每个时钟周期都可以将多达六个微型操作派遣到功能单元。

4. Micro-ops are executed by the individual function units, and then results are sent back to any waiting reservation station as well as to the register retirement unit, where they will update the register state once it is known that the instruction is no longer speculative. The entry corresponding to the instruction in the reorder buffer is marked as complete.

> 4. Micro-OPS 由单个功能单元执行，然后将结果发送回任何等待的预订站以及寄存器退休单元，一旦知道该说明已不再投机性。与重新订购缓冲区中指令相对应的条目标记为完整。

5. When one or more instructions at the head of the reorder buffer have been marked as complete, the pending writes in the register retirement unit are executed, and the instructions are removed from the reorder buffer.

> 5.当重新订购缓冲区头部的一个或多个说明标记为完整时，执行了寄存器退休单元中的待处理，并从重新订购缓冲区中删除说明。

In addition to the changes in the branch predictor, the major changes between the first generation i7 (the 920, Nehalem microarchitecture) and the sixth generation (i7 6700, Skylake microarchitecture) are in the sizes of the various buffers, renaming registers, and resources so as to allow many more outstanding instructions. [Figure 3.39](#_bookmark145) summarizes these differences.

> 除了分支预测因子的变化外，第一代 i7(920，Nehalem 微体系结构)和第六代(i7 6700，Skylake Microchittuction)之间的主要变化是各种缓冲器，重命名登记簿和资源的尺寸为了允许更多出色的说明。[图 3.39](#_ bookmark145)总结了这些差异。

##### _Performance of the i7_

> ##### _ i7_的表现

In earlier sections, we examined the performance of the i7’s branch predictor and also the performance of SMT. In this section, we look at single-thread pipeline performance. Because of the presence of aggressive speculation as well as nonblocking caches, it is difficult to accurately attribute the gap between idealized performance and actual performance. The extensive queues and buffers on the 6700 reduce the probability of stalls because of a lack of reservation stations, renaming registers, or reorder buffers significantly. Indeed, even on the earlier i7 920 with notably fewer buffers, only about 3% of the loads were delayed because no reservation station was available.

> 在较早的部分中，我们检查了 i7 的分支预测指标的性能以及 SMT 的性能。在本节中，我们查看单线程管道性能。由于存在激进的投机以及非封锁缓存，因此很难准确地归因于理想化的性能和实际表现之间的差距。由于缺乏预订站，重命名寄存器或重新订购缓冲区，因此 6700 上的大量队列和缓冲区降低了摊位的可能性。的确，即使在较早的 i7 920 中，缓冲液的缓冲液显着，也只有大约 3％的负载被延迟了，因为没有预订站。

> Figure 3.39 The buffers and queues in the first generation i7 and the latest generation i7. Nehalem used a reservation station plus reorder buffer organization. In later microarchitectures, the reservation stations serve as scheduling resources, and register renaming is used rather than the reorder buffer; the reorder buffer in the Skylake microarchitecture serves only to buffer control information. The choices of the size of various buffers and renaming registers, while appearing sometimes arbitrary, are likely based on extensive simulation.

>> 图 3.39 第一代 i7 和最新一代 i7 中的缓冲区和队列。Nehalem 使用了预订站加上重新订购缓冲组织。在以后的微体系结构中，预订站用作调度资源，并使用注册重命名，而不是重新订购缓冲区；Skylake 微体系结构中的重新排序缓冲区仅用于缓冲区控制信息。各种缓冲区和重命名寄存器的大小的选择有时是任意的，但可能是基于广泛的模拟。
>>

Thus most losses come either from branch mispredicts or cache misses. The cost of a branch mispredict is 17 cycles, whereas the cost of an L1 miss is about 10 cycles. An L2 miss is slightly more than three times as costly as an L1 miss, and an L3 miss costs about 13 times what an L1 miss costs (130–135 cycles). Although the processor will attempt to find alternative instructions to execute during L2 and L3 misses, it is likely that some of the buffers will fill before a miss completes, causing the processor to stop issuing instructions.

> 因此，大多数损失来自分支错误预测或缓存失误。分支错误预测的成本为 17 个周期，而 L1 失误的成本约为 10 个周期。L2 失误的成本略高于 L1 失误的三倍以上，而 L3 失误的成本约为 L1 错过的费用(130-135 个周期)的费用约为 13 倍。尽管处理器将试图在 L2 和 L3 错过期间找到替代说明，但某些缓冲区很可能会在错过完成之前填写，从而导致处理器停止发布说明。

[Figure 3.40](#_bookmark146) shows the overall CPI for the 19 SPECCPUint2006 benchmarks compared to the CPI for the earlier i7 920. The average CPI on the i7 6700 is 0.71, whereas it is almost 1.5 times better on the i7 920, at 1.06. This difference derives from improved branch prediction and a reduction in the demand miss rates (see Figure 2.26 on page 135).

> [图 3.40](#_ bookmark146)显示了与较早的 i7 920 相比，与 CPI 相比，19 个 SpecCpuint2006 基准的总 CPI 的总 CPI。i7 6700 的平均 CPI 为 0.71，而在 i7 920，在 i7 920，在 i7 920，在 i7 920，在 i7 920 中几乎是 1.5 倍。1.06。这种差异来自改进的分支预测和需求损失率的降低(请参见第 135 页的图 2.26)。

To understand how the 6700 achieves the significant improvement in CPI, let’s look at the benchmarks that achieve the largest improvement. [Figure 3.41](#_bookmark147) shows the five benchmarks that have a CPI ratio on the 920 that is at least 1.5 times higher than that of the 6700. Interestingly, three other benchmarks show a significant improvement in branch prediction accuracy (1.5 or more); however, those three benchmarks (HMMER, LIBQUANTUM, and SJENG) show equal or slightly higher L1 demand miss rates on the i7 6700. These misses likely arise because the aggressive prefetching is replacing cache blocks that are actually used. This type of behavior reminds designers of the challenges of maximizing performance in complex speculative multiple issue processors: rarely can significant performance be achieved by tuning only one part of the microarchitecture!

> 要了解 6700 如何实现 CPI 的重大改进，让我们看一下取得最大改进的基准。[图 3.41](#_ bookmark147)显示了在 920 上具有 CPI 比的五个基准，至少比 6700 的基准高 1.5 倍。有趣的是，其他三个基准测试显示出分支预测准确性的显着提高(1.5 或更多))；但是，这三个基准(Hmmer，libquantum 和 sjeng)在 i7 6700 上显示出等于或更高的 L1 需求率率。这些错过可能会出现，因为积极的预摘要是替代实际使用的高速缓存块。这种行为使设计师想起了在复杂的投机性多个问题处理器中最大化性能的挑战：通过仅调整微体系结构的一部分，很少能实现重要的性能！

> Figure 3.40 The CPI for the SPECCPUint2006 benchmarks on the i7 6700 and the i7 920. The data in this section were collected by Professor Lu Peng and PhD student Qun Liu, both of Louisiana State University.

>> 图 3.40 i7 6700 和 i7 920 的 Speccpuint2006 基准的 CPI。本节中的数据是由路易斯安那州立大学的 Lu Peng 教授和 PhD Student Liu 收集的。
>>

> Figure 3.41 An analysis of the five integer benchmarks with the largest performance gap between the i7 6700 and 920. These five benchmarks show an improvement in the branch prediction rate and a reduction in the L1 demand miss rate.

>> 图 3.41 对 i7 6700 和 920 之间最大性能差距的五个整数基准分析。这五个基准显示了分支预测率的提高和 L1 需求错过率的降低。
>>

Our few fallacies focus on the difficulty of predicting performance and energy efficiency and extrapolating from single measures such as clock rate or CPI. We also show that different architectural approaches can have radically different behaviors for different benchmarks.

> 我们的几个谬论集中在预测性能和能源效率以及从单一措施(例如时钟速率或 CPI)中推断的困难。我们还表明，不同的建筑方法可以针对不同的基准具有根本不同的行为。

Fallacy _It is easy to predict the performance and energy efficiency of two different versions of the same instruction set architecture, if we hold the technology constant._

> 如果我们保持技术常数，谬误_ IT 很容易预测两个不同版本的同一指令集架构的性能和能效。

Intel offers a processor for the low-end Netbook and PMD space called the Atom 230, which implements both the 64-bit and 32-bit versions of the x86 architecture. The Atom is a statically scheduled, 2-issue superscalar, quite similar in its microarchitecture to the ARM A8, a single-core predecessor of the A53. Interestingly, both the Atom 230 and the Core i7 920 have been fabricated in the same 45 nm Intel technology. [Figure 3.42](#_bookmark149) summarizes the Intel Core i7 920, the ARM CortexA8, and the Intel Atom 230. These similarities provide a rare opportunity to directly compare two radically different microarchitectures for the same instruction set while holding constant the underlying fabrication technology. Before we do the comparison, we need to say a little more about the Atom 230.

> 英特尔为低端上网本和 PMD 空间提供了称为 Atom 230 的处理器，该处理器同时实现了 X86 体系结构的 64 位和 32 位版本。该原子是一个静态的 2 号超标准，其微体系结构与 ARM A8 的微体系结构非常相似，ARM A8 是 A53 的单核前身。有趣的是，原子 230 和 Core i7 920 都在相同的 45 nm Intel 技术中制造。[图 3.42](#_ bookmark149)总结了 Intel Core i7 920，Arm Cortexa8 和 Intel Atom230。这些相似之处提供了一个罕见的机会，可以直接比较两种根本不同的微体系结构，同时保持固定的固定制造技术，以。在进行比较之前，我们需要对原子 230 进行更多的评价。

The Atom processors implement the x86 architecture using the standard technique of translating x86 instructions into RISC-like instructions (as every x86 implementation since the mid-1990s has done). Atom uses a slightly more powerful microoperation, which allows an arithmetic operation to be paired with a load or a store; this capability was added to later i7s by the use of macrofusion. This means that on average for a typical instruction mix, only 4% of the instructions require more than one microoperation. The microoperations are then executed in a 16-deep pipeline capable of issuing two instructions per clock, in order, as in the ARM A8. There are dual-integer ALUs, separate pipelines for FP add and other FP operations, and two memory operation pipelines, supporting more general dual execution than the ARM A8 but still limited by the in-order issue capability. The Atom 230 has a 32 KiB instruction cache and a 24 KiB data cache, both backed by a shared 512 KiB L2 on the same die. (The Atom 230 also supports multithreading with two threads, but we will consider only single-threaded comparisons.)

> 原子处理器使用将 X86 指令转换为类似 RISC 的指令的标准技术实现了 X86 体系结构(因为 1990 年代中期以来的每个 X86 实现)。Atom 使用了稍强的微型操作，这使算术操作可以与负载或商店配对；通过使用宏叶状，将此功能添加到后来的 i7s 中。这意味着，对于典型的指令组合，平均只有 4％的指令需要多个微型操作。然后，在 16 个深的管道中执行了微功能，能够按照 ARM A8 在 ARM A8 中的顺序发出两个说明。有双插图 Alus，用于 FP 添加和其他 FP 操作的单独的管道以及两个内存操作管道，比 ARM A8 支持更多的通用双执行，但仍然受到固定问题功能的限制。原子 230 具有 32 KIB 指令缓存和 24 KIB 数据缓存，均以同一模具上的共享 512 KIB L2 支持。(原子 230 还支持两个线程的多线程，但我们将仅考虑单线程比较。)

We might expect that these two processors, implemented in the same technology and with the same instruction set, would exhibit predictable behavior, in terms of relative performance and energy consumption, meaning that power and performance would scale close to linearly. We examine this hypothesis using three sets of benchmarks. The first set is a group of Java single-threaded benchmarks that come from the DaCapo benchmarks and the SPEC JVM98 benchmarks (see [Esmaeilzadeh et al. (2011)](#_bookmark945) for a discussion of the benchmarks and measurements). The second and third sets of benchmarks are from SPEC CPU2006 and consist of the integer and FP benchmarks, respectively.

> 我们可能希望这两个处理器在相同的技术中实施，并通过相同的指令集实现，在相对性能和能源消耗方面将表现出可预测的行为，这意味着功率和性能将在线性上缩小。我们使用三组基准测试了这一假设。第一组是来自 DACAPO 基准和 Spec JVM98 基准的一组 Java 单线程基准测试(请参阅[Esmaeilzadeh 等人(2011)](#_bookmark945)(#_ bookmark945)。第二组和第三组基准分别来自 Spec CPU2006，分别由整数和 FP 基准组成。

> Figure 3.42 An overview of the four-core Intel i7 920, an example of a typical ARM A8 processor chip (with a 256 MiB L2, 32 KiB L1s, and no floating point), and the Intel ARM 230, clearly showing the difference in design philosophy between a processor intended for the PMD (in the case of ARM) or netbook space (in the case of Atom) and a processor for use in servers and high-end desktops. Remember, the i7 includes four cores, each of which is higher in performance than the one-core A8 or Atom. All these processors are implemented in a comparable 45 nm technology.

>> 图 3.42 四核 Intel I7 920 的概述，一个典型的 ARM A8 处理器芯片的示例(带有 256 MIB L2，32 KIB L1，没有浮动点)和 Intel ARM 230，清楚地显示了差异在针对 PMD(ARM)或上网本空间(如果是原子)和用于服务器和高端台式机中使用的处理器之间的处理器之间的设计理念。请记住，i7 包括四个内核，每个内核的性能都比单核 A8 或原子高。所有这些处理器均在可比的 45 nm 技术中实现。
>>

As we can see in [Figure 3.43](#_bookmark150), the i7 significantly outperforms the Atom. All benchmarks are at least four times faster on the i7, two SPECFP benchmarks are over 10 times faster, and one SPECINT benchmark runs over eight times faster! Because the ratio of clock rates of these two processors is 1.6, most of the advantage comes from a much lower CPI for the i7 920: a factor of 2.8 for the Java benchmarks, a factor of 3.1 for the SPECINT benchmarks, and a factor of 4.3 for the SPECFP benchmarks.

> 正如我们在[图 3.43](#_ bookmark150)中看到的那样，i7 明显优于原子。所有基准测试在 i7 上的速度至少要快四倍，两个 SPECFP 基准测试速度超过 10 倍，一个 Specint Benchmark 的运行速度快八倍！因为这两个处理器的时钟率比为 1.6，所以大部分优势来自 i7 920 的 CPI：Java 基准测试的 2.8 倍，对于 Specint Benchmarks，倍数为 3.1，而一个因子为一个因子。4.3 对于 SPECFP 基准测试。

But the average power consumption for the i7 920 is just under 43 W, while the average power consumption of the Atom is 4.2 W, or about one-tenth of the power! Combining the performance and power leads to an energy efficiency advantage for the Atom that is typically more than 1.5 times better and often 2 times better! This comparison of two processors using the same underlying technology makes it clear that the performance advantages of an aggressive superscalar with dynamic scheduling and speculation come with a _significant disadvantage_ in energy efficiency.

> 但是，i7 920 的平均功耗不到 43 W，而原子的平均功耗为 4.2 W，约为功率的十分之一！结合性能和功率会为原子带来能量效率的优势，通常要好得多 1.5 倍以上，通常要好 2 倍！使用相同基础技术对两个处理器进行比较清楚地表明，具有动态调度和猜测的积极超级标准的性能优势在能源效率方面具有_SSISTIVENTIAND。

> Figure 3.43 The relative performance and energy efficiency for a set of single-threaded benchmarks shows the i7 920 is 4 to over 10 times faster than the Atom 230 but that it is about 2 times _less_ power-efficient on average! Performance is shown in the columns as i7 relative to Atom, which is execution time (i7)/execution time (Atom). Energy is shown with the line as Energy (Atom)/Energy (i7). The i7 never beats the Atom in energy efficiency, although it is essentially as good on four benchmarks, three of which are floating point. The data shown here were collected by [Esmaeilzadeh et al. (2011)](#_bookmark945). The SPEC benchmarks were compiled with optimization using the standard Intel compiler, while the Java benchmarks use the Sun (Oracle) Hotspot Java VM. Only one core is active on the i7, and the rest are in deep power saving mode. Turbo Boost is used on the i7, which increases its performance advantage but slightly decreases its relative energy efficiency.

>> 图 3.43 一组单线程基准的相对性能和能源效率表明，i7 920 比原子 230 快 4 至 10 倍以上，但平均约为_less_的 2 倍！性能在列中显示为 i7 相对于原子，这是执行时间(i7)/执行时间(ATOM)。能量以该线为能量(原子)/能量(i7)。i7 从未超过能源效率的原子，尽管在四个基准测试中基本上是好的，其中三个是浮点。[Esmaeilzadeh 等人收集了此处显示的数据。(2011)](#_ bookmark945)。使用标准 Intel 编译器对规格基准进行了优化，而 Java 基准测试使用 Sun(Oracle)热点 Java VM。i7 上只有一个核心活动，其余的处于深度节省模式。i7 上使用了涡轮增压，这增加了其性能优势，但略微降低了其相对能量效率。
>>

The key is that it is the product of CPI and clock rate that determines performance. A high clock rate obtained by deeply pipelining the processor must maintain a low CPI to get the full benefit of the faster clock. Similarly, a simple processor with a high clock rate but a low CPI may be slower.

> 关键是决定性能的是 CPI 和时钟速率的产物。通过深入管道处理的高时钟速率必须保持低 CPI，以使更快的时钟获得全部好处。同样，一个具有较高时钟速率但 CPI 低的简单处理器可能会慢。

As we saw in the previous fallacy, performance and energy efficiency can diverge significantly among processors designed for different environments even when they have the same ISA. In fact, large differences in performance can show up even within a family of processors from the same company all designed for high-end applications. [Figure 3.44](#_bookmark151) shows the integer and FP performance of two different implementations of the x86 architecture from Intel, as well as a version of the Itanium architecture, also by Intel.

> 正如我们在以前的谬论中所看到的那样，即使在具有相同的 ISA 的情况下，为不同环境设计的处理器之间的性能和能量效率也会显着分歧。实际上，即使在同一家公司的处理器家族中，绩效的巨大差异也可以显示出所有专为高端应用程序而设计的。[图 3.44](#_ bookmark151)显示了来自英特尔的 X86 体系结构的两个不同实现的整数和 FP 性能，以及 ITANIUM Architecture 的版本，也是 Intel 的版本。

The Pentium 4 was the most aggressively pipelined processor ever built by Intel. It used a pipeline with over 20 stages, had seven functional units, and cached micro-ops rather than x86 instructions. Its relatively inferior performance, given the aggressive implementation, was a clear indication that the attempt to exploit more ILP (there could easily be 50 instructions in flight) had failed. The Pentium’s power consumption was similar to the i7, although its transistor count was lower, as its primary caches were half as large as the i7, and it included only a 2 MiB secondary cache with no tertiary cache.

> 奔腾 4 是英特尔有史以来最积极的管道处理器。它使用了具有超过 20 个阶段的管道，具有 7 个功能单元，并且是 X86 指令。鉴于积极的实施，它的性能相对较低，这清楚地表明，试图利用更多 ILP(很容易在飞行中有 50 条说明)失败。奔腾的功耗与 i7 相似，尽管其晶体管计数较低，因为其主要缓存量与 i7 一样大，并且仅包括 2 个 MIB 次级缓存，没有三级缓存。

The Intel Itanium is a VLIW-style architecture, which despite the potential decrease in complexity compared to dynamically scheduled superscalars, never attained competitive clock rates with the mainline x86 processors (although it appears to achieve an overall CPI similar to that of the i7). In examining these results, the reader should be aware that they use different implementation technologies, giving the i7 an advantage in terms of transistor speed and hence clock rate for an equivalently pipelined processor. Nonetheless, the wide variation in performance—more than three times between the Pentium and i7—is astonishing. The next pitfall explains where a significant amount of this advantage comes from.

> Intel Itanium 是一种 VLIW 风格的体系结构，尽管与动态计划的 SuperScalars 相比，复杂性的潜力下降，但与 Mainline X86 处理器的竞争时钟速率从未达到竞争性时钟(尽管它似乎取得了与 I7 相似的总 CPI)。在检查这些结果时，读者应意识到他们使用不同的实施技术，从而使 i7 在晶体管速度方面具有优势，因此对于等管道的处理器而言，时钟速率。尽管如此，奔腾和 i7 之间的性能差异很大，超过三倍。下一个陷阱解释了大量优势的来源。

> Figure 3.44 Three different Intel processors vary widely. Although the Itanium processor has two cores and the i7 four, only one core is used in the benchmarks; the Power column is the thermal design power with estimates for only one core active in the multicore cases.

>> 图 3.44 三个不同的英特尔处理器差异很大。尽管 Itanium 处理器有两个核心和 i7 四，但在基准测试中只使用了一个核心。功率柱是热设计功率，其在多项式情况下仅估计一个核心。
>>

Much of the attention in the early 2000s went to building aggressive processors to exploit ILP, including the Pentium 4 architecture, which used the deepest pipeline ever seen in a microprocessor, and the Intel Itanium, which had the highest peak issue rate per clock ever seen. What quickly became clear was that the main limitation in exploiting ILP often turned out to be the memory system. Although speculative out-of-order pipelines were fairly good at hiding a significant fraction of the 10to 15-cycle miss penalties for a first-level miss, they could do very little to hide the penalties for a second-level miss that, when going to main memory, were likely to be 50–100 clock cycles.

> 在 2000 年代初期，许多关注都用于建立激进的处理器来利用 ILP，其中包括 Pentium 4 Architecture，它使用了微处理器中有史以来最深的管道和 Intel Itanium，而 Intel Itanium 则有史以来最高的峰值问题。。很快就显而易见的是，利用 ILP 的主要限制通常是记忆系统。尽管投机性的外管道方案相当擅长隐藏一小部分 15 周期小姐的罚款，但他们几乎无能为力地掩盖第二级小姐的处罚。对于主要记忆，可能是 50–100 时钟周期。

The result was that these designs never came close to achieving the peak instruction throughput despite the large transistor counts and extremely sophisticated and clever techniques. [Section 3.15](#historical-perspective-and-references) discusses this dilemma and the turning away from more aggressive ILP schemes to multicore, but there was another change that exemplified this pitfall. Instead of trying to hide even more memory latency with ILP, designers simply used the transistors to build much larger caches. Both the Itanium 2 and the i7 use three-level caches compared to the two-level cache of the Pentium 4, and the third-level caches are 9 and 8 MiB compared to the 2 MiB second-level cache of the Pentium 4. Needless to say, building larger caches is a lot easier than designing the 20+-stage Pentium 4 pipeline, and based on the data in [Figure 3.44](#_bookmark151), doing so seems to be more effective.

> 结果是，尽管晶体管的数量很高，而且非常复杂且聪明的技术，这些设计从未接近达到峰值指导吞吐量。[第 3.15 节](＃历史透视和引用)讨论了这一难题，并从更具侵略性的 ILP 方案转向多学科，但发生了另一种变化，这例证了这一陷阱。设计师没有尝试使用 ILP 隐藏更多的内存延迟，而只是使用晶体管来构建更大的缓存。与五角星 4 的两级缓存相比，ITANIUM 2 和 i7 都使用三级缓存，而第三级缓存为 9 和 8 MIB，与五个 MIB 的 2 MIB 二级缓存相比。可以说，构建较大的缓存比设计 20+ 阶段五角管 4 管道要容易得多，并且基于[图 3.44](#_ bookmark151)中的数据，这样做似乎更有效。

Pitfall _And sometimes smarter is better than bigger and dumber._

> 陷阱_有时更聪明的比更大和笨蛋更好。

One of the more surprising results of the past decade has been in branch prediction. The emergence of hybrid tagged predictors has shown that a more sophisticated predictor can outperform the simple gshare predictor with the same number of bits (see [Figure 3.8](#_bookmark105) on page 171). One reason this result is so surprising is that the tagged predictor actually stores fewer predictions, because it also consumes bits to store tags, whereas gshare has only a large array of predictions. Nonetheless, it appears that the advantage gained by not misusing a prediction for one branch on another branch more than justifies the allocation of bits to tags versus predictions.

> 过去十年中，最令人惊讶的结果之一是分支机构预测。混合标记的预测因子的出现表明，更复杂的预测变量可以胜过具有相同数量位数的简单 gshare 预测变量(请参见第 171 页的[图 3.8](#_kmark105))。这个结果如此令人惊讶的一个原因是，标记的预测器实际上存储了较少的预测，因为它也消耗了存储标签的位，而 Gshare 只有大量的预测。尽管如此，似乎通过不滥用另一个分支的预测而获得的优势远远超出了对标签与预测的分配的合理性。

Pitfall _Believing that there are large amounts of ILP available, if only we had the right techniques._

> 陷阱_ believing 只有我们有正确的技术，就有大量的 ILP 可用。

The attempts to exploit large amounts of ILP failed for several reasons, but one of the most important ones, which some designers did not initially accept, is that it is hard to find large amounts of ILP in conventionally structured programs, even with speculation. A famous study by David Wall in 1993 (see [Wall, 1993](#_bookmark1016)) analyzed the amount of ILP available under a variety of idealistic conditions. We summarize his results for a processor configuration with roughly five to ten times the capability of the most advanced processors in 2017. Wall’s study extensively documented a variety of different approaches, and the reader interested in the challenge of exploiting ILP should read the complete study.

> 出于多种原因，利用大量 ILP 的尝试失败了，但是一些设计师最初不接受的最重要的尝试之一是，即使有猜测，也很难在常规结构化的程序中找到大量的 ILP。大卫·沃尔(David Wall)在 1993 年进行的一项著名研究(参见[Wall，1993](#_ bookmark1016))分析了在各种理想主义条件下可用的 ILP 的数量。我们总结了他对处理器配置的结果，大约是 2017 年最先进的处理器能力的五到十倍。Wall 的研究广泛记录了各种不同的方法，并且对利用 ILP 的挑战感兴趣的读者应阅读完整的研究。

The aggressive processor we consider has the following characteristics:

> 我们考虑的积极处理器具有以下特征：

1. Up to 64 instruction issues and dispatches per clock with _no_ issue restrictions, or 8 times the total issue width of the widest processor in 2016 (the IBM Power8) and with up to 32 times as many loads and stores allowed per clock! As we have discussed, there are serious complexity and power problems with large issue rates.

> 1.使用_no_发行限制，最多可达 64 个指令问题和调度，或 2016 年最宽的处理器的总发行宽度(IBM Power8)和最多 32 倍的 32 倍的负载和商店，每个时钟允许的最多 32 倍！正如我们已经讨论的那样，大问题率存在严重的复杂性和功率问题。

2. A tournament predictor with 1K entries and a 16-entry function return predictor. This predictor is comparable to the best predictors in 2016; the predictor is not a primary bottleneck. Mispredictions are handled in one cycle, but they limit the ability to speculate.

> 2.具有 1K 条目和 16 输入功能返回预测器的比赛预测指标。该预测因子与 2016 年最佳预测指标相当；预测因子不是主要的瓶颈。错误预测是在一个周期中处理的，但它们限制了推测能力。

3. Perfect disambiguation of memory references done dynamically—this is ambitious but perhaps attainable for small window sizes.

> 3.对记忆参考的完美歧义是动态完成的 - 这是雄心勃勃的，但对于小窗户尺寸而言也许可以实现。

4. Register renaming with 64 additional integer and 64 additional FP registers, which is somewhat less than the most aggressive processor in 2011. Because the study assumes a latency of only one cycle for all instructions (versus 15 or more on processors like the i7 or Power8), the effective number of rename registers is about five times larger than either of those processors.

> 4.注册登记使用 64 个额外整数和 64 个额外的 FP 寄存器，这比 2011 年最激进的处理器要小。POWER8)，有效的重命名寄存器数量大约是任何一个处理器中的五倍。

[Figure 3.45](#_bookmark152) shows the result for this configuration as we vary the window size. This configuration is more complex and expensive than existing implementations, especially in terms of the number of instruction issues. Nonetheless, it gives a useful upper limit on what future implementations might yield. The data in these figures are likely to be very optimistic for another reason. There are no issue restrictions among the 64 instructions: for example, they may all be memory references. No one would even contemplate this capability in a processor for the near future. In addition, remember that in interpreting these results, cache misses and non-unit latencies were not taken into account, and both these effects have significant impacts.

> [图 3.45](#_ bookmark152)显示了这种配置的结果，因为我们改变了窗口大小。这种配置比现有实现更复杂和昂贵，尤其是在指令问题的数量方面。尽管如此，它为未来的实施可能产生了有用的上限。由于另一个原因，这些数字中的数据可能非常乐观。在 64 个说明中没有任何问题限制：例如，它们可能是内存引用。在不久的将来，没有人会在处理器中考虑这种功能。此外，请记住，在解释这些结果时，没有考虑到缓存的错过和非单位潜伏期，这两种影响都有重大影响。

The most startling observation in [Figure 3.45](#_bookmark152) is that with the preceding realistic processor constraints, the effect of the window size for the integer programs is not as severe as for FP programs. This result points to the key difference between these two types of programs. The availability of loop-level parallelism in two of the FP programs means that the amount of ILP that can be exploited is higher, but for integer programs other factors—such as branch prediction, register renaming, and less parallelism, to start with—are all important limitations. This observation is critical because most of the market growth in the past decade—transaction processing, web servers, and the like—depended on integer performance, rather than floating point.

> [图 3.45](#_ bookmark152)中最令人震惊的观察结果是，在前面的逼真的处理器约束下，窗口大小对整数程序的影响不如 FP 程序那样严重。该结果指出了这两种类型的程序之间的关键区别。在两个 FP 程序中，循环级并行性的可用性意味着可以利用的 ILP 的数量更高，但是对于整数程序而言，其他因素(例如分支机构预测，注册重命名和较少的并行性)，从而开始 - 从所有重要的局限性。这一观察结果至关重要，因为过去十年中的大部分市场增长(Transaction Processing，Web 服务器等)依靠整数性能而不是浮点。

Wall’s study was not believed by some, but 10 years later, the reality had sunk in, and the combination of modest performance increases with significant hardware resources and major energy issues coming from incorrect speculation forced a change in direction. We will return to this discussion in our concluding remarks.

> 沃尔的研究并不相信，但是十年后，现实已经沉入了，而适度的性能的结合随着重大硬件资源和不正确的推测带来的重大能源问题的增加而增加了方向的变化。我们将在我们的总结中回到讨论。

Figure 3.45 The amount of parallelism available versus the window size for a variety of integer and floatingpoint programs with up to 64 arbitrary instruction issues per clock. Although there are fewer renaming registers than the window size, the fact that all operations have 1-cycle latency and that the number of renaming registers equals the issue width allows the processor to exploit parallelism within the entire window.

> 图 3.45 可用的平行性与各种整数和浮点程序的窗口大小，每个时钟最多有 64 个任意指令问题。尽管重命名寄存器的重命名量少于窗口大小，但所有操作都具有 1 周期延迟，重命名寄存器的数量等于问题宽度，使处理器可以利用整个窗口中的并行性。
