## Historical Perspectives and References

Section M.9 (available online) covers the development of DSAs.

## Case Studies and Exercises by Cliff Young

### Case Study: Google’s Tensor Processing Unit and Acceleration of Deep Neural Networks

##### _Concepts illustrated by this case study_

- Structure of matrix multiplication operations
- Capacities of memories and rates of computations (“speeds and
  feeds”) for a simple neural network model
- Construction of a special-purpose ISA
- Inefficiencies in mapping convolutions to TPU hardware
- Fixed-point arithmetic
- Function approximation

  1. \[10/20/10/25/25\] &lt; 7.3,7.4 &gt;Matrix multiplication is a
     key operation supported in hardware by the TPU. Before going
     into details of the TPU hardware, it’s worth
     analyzing the matrix multiplication calculation itself. One common way to depict matrix multiplication is with the following triply nested loop:

float a\[M\]\[K\], b\[K\]\[N\], c\[M\]\[N\];

// M, N, and K are constants. for (int i = 0; i &lt; M; ++i)

for (int j = 0; j &lt; N; ++j)

for (int k = 0; k &lt; K; ++k)

c\[i\]\[j\] += a\[i\]\[k\] \* b\[k\]\[j\];

1. \[10\] Suppose that M, N, and K are all equal. What is the asymptotic complexity in time of this algorithm? What is the asymptotic complexity in space of the arguments? What does this mean for the operational intensity of matrix multi- plication as M, N, and K grow large?
2. \[20\] Suppose that M= 3, N= 4, and K= 5, so that each of the dimensions are relatively prime. Write out the order of accesses to memory locations in each of the three matrices A, B, and C (you might start with two-dimensional indices, then translate those to memory addresses or offsets from the start of each matrix). For which matrices are the elements accessed sequentially? Which are not? Assume row-major (C-language) memory ordering.
3. \[10\] Suppose that you transpose matrix B, swapping its indices so that they are B\[N\]\[K\] instead. So, now the innermost statement of the loop looks like:
   c\[i\]\[j\] += a\[i\]\[k\] \* b\[j\]\[k\];

Now, for which matrices are the elements accessed sequentially?

1. \[25\] The innermost (k-indexed) loop of our original routine performs a dot-product operation. Suppose that you are a given a hardware unit that can perform an 8-element dot-product more efficiently than the raw C code, behav- ing effectively like this C function:
   void hardware_dot(float \*accumulator,

const float \*a_slice, const float \*b_slice) { float total = 0.;

for (int k = 0; k &lt; 8; ++k) {

total += a_slice\[k\] \* b_slice\[k\];

}

\*accumulator += total;

}

How would you rewrite the routine with the transposed B matrix from part (c) to use this function?

1. \[25\] Suppose that instead, you are given a hardware unit that performs an 8-element “saxpy” operation, which behaves like this C function:
   void hardware_saxpy(float \*accumulator, float a, const float \*input) {

for (int k = 0; k &lt; 8; ++k) {

accumulator\[k\] += a \* input\[k\];

}

}
Write another routine that uses the saxpy primitive to deliver equivalent results to the original loop, without the transposed memory ordering for the B matrix.

1. \[15/10/10/20/15/15/20/20\] &lt;7.3,7.4 &gt;Consider the neural network model MLP0 from [Figure 7.5](#_bookmark331). That model has 20 M weights in five fully connected layers (neural
   network researchers count the input layer as if it were a layer in the stack, but it has no

weights associated with it). For simplicity, let’s assume that those layers are each the same size, so each layer holds 4 M weights. Then assume that each layer has identical geometry, so each group of 4 M weights represents a 2 K\*2 K matrix. Because the TPU typically uses 8-bit numerical values, 20 M weights take up 20 MB.

1. \[15\] For batch sizes of 128, 256, 512, 1024, and 2048, how big are the input activations for each layer of the model (which, except for the input layer, are also the output activations of the previous layer)? Now considering the whole mode (i.e., there’s just the input to the first layer and the output from the last layer), for each batch size, what is the transfer time for input and output over PCIe Gen3 x16, which has a transfer speed of about 100 Gibit/s?
2. \[10\] Given the memory system speed of 30 GiB/s, give a lower bound for the time the TPU takes to read the weights of MLP0 from memory. How much time does it take for the TPU to read a 256 × 256 “tile” of weights from memory?
3. \[10\] Show how to calculate the TPU’s 92 T operations/second, given that we know that the systolic array matrix multiplier has 256 256 elements, each of which performs an 8-bit multiply-accumulate operation (MAC) each cycle. In high-performance-computing marketing terms, a MAC counts as two operations.
4. \[20\] Once a weight tile has been loaded into the matrix unit of the TPU, it can be reused to multiply a 256-element input vector by the 256 256 weight matrix represented by the tile to produce a 256-element output vector every cycle. How many cycles pass during the time it takes to load a weight tile? This is the “break-even” batch size, where compute and memory-load times are equal, also known as the “ridge” of the roofline.
5. \[15\] The compute peak for the Intel Haswell x86 server is about 1 T FLOPS, while the compute peak for the NVIDIA K80 GPU is about 3 T FLOPS. Sup- posing that they hit these peak numbers, calculate their best-case compute time for batch size 128. How do these times compare to the time the TPU takes to load all 20 M weights from memory?
6. \[15\] Assuming that the TPU program does not overlap computation with I/O over PCIe, calculate the time elapsed from when the CPU starts to send the first byte of data to the TPU until the time that the last byte of output is returned. What fraction of PCIe bandwidth is used?
7. \[20\] Suppose that we deployed a configuration where one CPU was connected to five TPUs across a single PCIe Gen3 x16 bus (with appropriate PCIe switches). Assume that we parallelize by placing one layer of MLP0 on each TPU, and that the TPUs can communicate directly with each other over PCIe. At batch = 128, what is the best-case latency for calculating a single inference, and what throughput, in terms of inferences per second, would such a config- uration deliver? How does this compare to a single TPU?
8. \[20\] Suppose that each example in a batch of inferences requires 50 core-micro- seconds of processing time on the CPU. How many cores on the host CPU will be required to drive a single-TPU configuration at batch = 128?

   1. \[20/25/25/25/Discussion\] &lt;7.3,7.4 &gt;Consider a
      pseudo-assembly language for the TPU, and consider the program
      that handles a batch of size 2048 for a tiny fully
      connected layer with a 256 256 weight matrix. If there were no constraints on the sizes or alignments of computations in each instruction, the entire program for that layer might look like the following:

read_host u#0, 256\*2048 read_weights w#0, 256\*256

// matmul weights are implicitly read from the FIFO. activate u#256\*2048, a#0, 256\*2048

write_host, u#256\*2048, 256\*2048

In this pseudo-assembly language, a prefix of “u#” refers to a memory address in the unified buffer; a prefix of “w#” refers to a memory address in the off-chip weight DRAM, and a prefix of “a#” refers to an accumulator address. The last argument of each assembly instruction describes the number of bytes to be operated upon.

Let’s walk through the program instruction by instruction:

- The read_host instruction reads 512 KB of data from host memory, storing it at the very beginning of the unified buffer (u#0).
- The read_weights instruction tells the weight fetching unit to read 64 KB of weights, loading them into the on-chip weight FIFO. These 64 KB of weights represent a single, 256 256 matrix of weights, which we will call a “weight tile.”
- The matmul instruction reads the 512 KB of input data from address 0 in the unified buffer, performs a matrix multiplication with the tile of weights, and stores the resulting 256\*2048 = 524,288, 32-bit activations at accumulator address 0 (a#0). We have intentionally glossed over the details of the ordering of weights; the exercise will expand on these details.
- The activate instruction takes those 524,288 32-bit accumulators at a#0, applies an activation function to them, and stores the resulting 524,288, 8-bit output values at the next free location in the unified buffer, u#524288.
- The write_host instruction writes the 512 KB of output activations, starting at u#524288, back to the host CPU.
  We will progressively add realistic details to the pseudo-assembly language to explore some aspects of TPU design.

1. \[20\] While we have written our pseudo-code in terms of bytes and byte addresses (or in the case of the accumulators, in terms of addresses to 32-bit values), the TPU operates on a natural vector length of 256. This means that the unified buffer is typically addressed at 256-byte boundaries, the accumula- tors are addressed in groups of 256 32-bit values (or at 1 KB boundaries), and weights are loaded in groups of 65,536 8-bit values. Rewrite the program’s addresses and transfer sizes to take these vector and weight-tile lengths into account. How many 256-element vectors of input activations will be read by the program? How many bytes of accumulated values will be used while computing the results? How many 256-element vectors of output activations will be written back to the host?
2. \[25\] Suppose that the application requirements change, and instead of a multiplication by a 256 256 weight matrix, the shape of the weight matrix now becomes 1024 256. Think of the matmul instruction as putting the weights as the right argument of the matrix multiplication operator, so 1024 corresponds to K, the dimension in which the matrix multiplication adds up values. Suppose that there are now two variants of the accumulate instruction, one of which overwrites the accumulators with its results, and the other of which adds the matrix multiplication results to the specified accumulator. How would you change the program to handle this 1024 256 matrix? Do you need more accumulators? The size of the matrix unit remains the same
   at 256 × 256; how many 256 × 256 weight tiles does your program need?
3. \[25\] Now write the program to handle a multiplication by a weight matrix of size 256 512. Does your program need more accumulators? Can you rewrite your program so that it uses only 2048, 256-entry accumulators? How many weight tiles does your program need? In what order should they be stored in the weight DRAM?
4. \[25\] Next, write the program to handle a multiplication by a weight matrix of size 1024 768. How many weight tiles does your program need? Write your program so that it uses only 2048, 256-entry accumulators. In what order should the weight tiles be stored in the weight DRAM? For this calculation, how many times did each input activation get read?
5. \[Discussion\] What would it take to build an architecture that reads each 256-element set of input activations just once? How many accumulators would that require? If you did it that way, how big would the accumulator memory have to be? Contrast this approach with the TPU, which uses 4096 accumula- tors, so that one set of 2048 accumulators can be written by the matrix unit while another is being used for activations.

<!-- -->

1. \[15/15/15\] &lt;7.3,7.4 &gt;Consider the first convolutional layer
   of AlexNet, which uses a 7 × 7 convolutional kernel, with an input
   feature depth of 3 and an output feature depth of 48. The original
   image width is 220 × 220.

   1. \[15\] Ignore the 7 7 convolutional kernel for the moment, and

      > consider just the center element of that kernel. A 1 1
      > convolutional kernel is mathemati- cally equivalent to a
      > matrix multiplication, using a weight matrix that is
      > input_depth output_depth in dimensions. With these depths,
      > and using a stan- dard matrix multiplication, what fraction of
      > the TPU’s 65,536 ALUs can be used?
      >
   2. \[15\] For convolutional neural networks, the spatial dimensions

      > are also sources of weight reuse, since the convolutional
      > kernel gets applied to many different (x,y) coordinate
      > positions. Suppose that the TPU reaches balanced compute and
      > memory at a batch size of 1400 (as you might have computed in
      > exercise 1d). What is the smallest square image size that the
      > TPU can process efficiently at a batch size of 1?
      >
   3. \[15\] The first convolutional layer of AlexNet implements a > _kernel stride_ of 4, which means that rather than moving by > one X or Y pixel at each application,
      the 7 7 kernel moves by 4 pixels at a time. This striding means that we can permute the input data from 220 220 3 to be 55 55 48 (dividing the X and Y dimensions by 4 and multiplying the input depth by 16), and simulta- neously we can restack the 7 7 3 48 convolutional weights to be

2 2 48 48 (just as the input data gets restacked by 4 in X and Y, we do the same to the 7 7 elements of the convolutional kernel, ending up with ceiling(7/4) = 2 elements in each of the X and Y dimensions). Because the kernel is now 2 2, we need to perform only four matrix multiplication oper- ations, using weight matrices of size 48 48. What is the fraction of the 65,536 ALUs that can be used now?

1. \[15/10/20/20/20/25\] &lt; 7.3 &gt;The TPU uses _fixed-point arithmetic_ (sometimes also called _quantized arithmetic_, with overlapping and conflicting definitions), where integers are used to represent values on the real number line. There are a number of
   different schemes for fixed-point arithmetic, but they share the common theme that there is an affine projection from the integer used by hardware to the real number that the integer represents. An affine projection has the form r=i\*s+b, where i is the integer, r is the represented real value, and s and b are a scale and bias. You can of course write the projection in either direction, from integers to reals or vice versa (although you need to round when converting from reals to integers).
2. \[15\] The simplest activation function supported by the TPU is “ReLUX,” which is a rectified linear unit with a maximum of X. For example, ReLU6 is defined by
   Relu6(x) ={ 0, when x&lt; 0; x, when 0 &lt;=x&lt;= 6; and 6, when x&gt; 6 }. So 0.0 and

6.0 on the real number line are the minimum and maximum values that Relu6

might produce. Assume that you use an 8-bit unsigned integer in hardware, and that you want to make 0 map to 0.0 and 255 map to 6.0. Solve for s and b.

1. \[10\] How many values on the real number line are exactly representable by an 8-bit quantized representation of ReLU6 output? What is the real-number spacing between them?
2. \[20\] The difference between representable values is sometimes called a “unit in the least place,” or _ulp_, when performing numerical analysis. If you map a real number to its fixed-point representation, then map back, you only rarely get back the original real number. The difference between the original number and its representation is called the _quantization error_. When mapping a real number in the range \[0.0,6.0\] to an 8-bit integer, show that the worst-case quantization error is one-half of an ulp (make sure you round to the nearest rep- resentable value). You might consider graphing the errors as a function of the original real number.
3. \[20\] Keep the real-number range \[0.0,6.0\] for an 8-bit integer from the last step. What 8-bit unsigned integer represents 1.0? What is the quantization error for 1.0? Suppose that you ask the TPU to add 1.0 to 1.0. What answer do you get back, and what is the error in that result?
4. \[20\] If you pick a random number uniformly in the range \[0.0, 6.0\], then quantize it to an 8-bit unsigned integer, what distribution would you expect to see for the 256 integer values?
5. \[25\] The hyperbolic tangent function, _tanh_, is another commonly used activation function in deep learning: tanh _x_ <u><sup>1</sup> <sup>—</sup> _<sup>e</sup>_—2*x*</u>
   Tanh also has a bounded range, mapping the entire real number line to the interval

1.0, 1.0 . Solve for s and b for this range, using an 8-bit unsigned representation. Then solve for s and b using an 8-bit two’s complement representation. For both

cases, what real number does the integer 0 represent? Which integer represents the real number 0.0? Can you imagine any issues that might result from the quantiza- tion error incurred when representing 0.0?

1. \[20/25/15/15/30/30/30/40/40/25/20/Discussion\] &lt;7.3 &gt;In
   addition to tanh, another s-shaped smooth function, the logistic
   sigmoid function _y_ = 1 / (1 +exp(—_x_)),
   _logistic_\__sigmoid x_ 1

1+ _e_—_x_

is commonly used as an activation function in neural networks. A common way to implement them in fixed-point arithmetic uses a piecewise quadratic approxima- tion, where the most significant bits of the input value select which table entry to use. Then the least significant bits of the input value are sent to a degree-2 polyno- mial that describes a parabola that is fit to the subrange of the approximated function.

1. \[20\] Using a graphing tool (we like [www.desmos.com/calculator](http://www.desmos.com/calculator)), draw the graphs for the logistic sigmoid and tanh functions.
2. \[25\] Now draw the graph of _y_ =tanh(_x_/2)/2. Compare that graph with the logis- tic sigmoid function. How much do they differ by? Build an equation that shows how to transform one into the other. Prove that your equation is correct.
3. \[15\] Given this algebraic identity, do you need to use two different sets of coef- ficients to approximate logistic sigmoid and tanh?
4. \[15\] Tanh is an odd function, meaning that f( _x_)= f(_x_). Can you exploit this fact to save table space?
5. \[30\] Let’s focus our attention on approximating _tanh_ over the interval _x_ 0.0, 6.4 on the number line. Using floating-point arithmetic, write a pro- gram that divides the interval into 64 subintervals (each of length 0.1), and then
   approximates the value of _tanh_ over each subinterval using a single constant floating-point value (so you’ll need to pick 64 different floating-point values, one for each subinterval). If you spot-check 100 different values (randomly chosen is fine) within each subinterval, what is the worst-case approximation error you see over all subintervals? Can you choose your constant to minimize the approximation error for each subinterval?
6. \[30\] Now consider building a floating-point linear approximation for each sub- interval. In this case, you want to pick a pair of floating-point values _m_ and _b_, for the traditional line equation _y mx_ + _b_, to approximate each of the 64 sub- intervals. Come up with a strategy that you think is reasonable to build this lin- ear interpolation over 64 subintervals for tanh. Measure the worst-case approximation error over the 64 intervals. Is your approximation monotonic when it reaches a boundary between subintervals?
7. \[40\] Next, build a quadratic approximation, using the standard formula _y ax_<sup>2</sup> + _bx_ + _c_. Experiment with a number of different ways to fit the formula. Try fitting the parabola to the endpoints and midpoint of the bucket, or using a
   Taylor approximation around a single point in the bucket. What worst-case error do you get?
8. \[40\] (extra credit) Let’s combine the numerical approximations of this exercise with the fixed-point arithmetic of the previous exercise. Suppose that the input _x_ 0.0, 6.4 is represented by a 15-bit unsigned value, with 0x0000 represent-
   ing 0.0 and 0x7FFF representing 6.4. For the output, similarly use a 15-bit

unsigned value, with 0x0000 representing 0.0 and 0x7FFF representing 1.0. For each of your constant, linear, and quadratic approximations, calculate the combined effect of approximation and quantization errors. Since there are so few input values, you can write a program to check them exhaustively.

1. \[25\] For the quadratic, quantized approximation, is your approximation mono- tonic within each subinterval?
2. \[20\] A difference of one ulp in the output scale should correspond to an error of
   1.0 / 32767. How many ulps of error are you seeing in each case?
3. \[Discussion\] By choosing to approximate the interval \[0.0, 6.4\], we effectively clipped the “tail” of the hyperbolic tangent function, for values of _x_ &gt; 6.4. It’s not an unreasonable approximation to set the output value for all of the tail to
   1.0. What’s the worst-case error, in terms of both real numbers and ulps, of treating the tail this way? Is there a better place we might have clipped the tail to improve our accuracy?

### Exercises

1. \[10/20/10/15\] &lt;7.2,7.5 &gt;One popular family of FPGAs, the Virtex-7 series, is built by Xilinx. A Virtex-7 XC7VX690T FPGA contains 3,600 25x18-bit
   integer multiply-add “DSP slices.” Consider building a TPU-style design on such an FPGA.
2. \[10\] Using one 25 18 integer multiplier per systolic array cell, what’s the larg- est matrix multiplication unit one could construct? Assume that the matrix mul- tiplication unit must be square.
3. \[20\] Suppose that you could build a rectangular, nonsquare matrix multiplica- tion unit. What implications would such a design have for hardware and soft- ware? (Hint: think about the vector length that software must handle.)
4. \[10\] Many FPGA designs are lucky to reach 500 MHz operation. At that speed, calculate the peak 8-bit operations per second that such a device might achieve. How does that compare to the 3 T FLOPS of a K80 GPU?
5. \[15\] Assume that you can make up the difference between 3600 and 4096 DSP slices using LUTs, but that doing so will reduce your clock rate to 350 MHz. Is this a worthwhile trade-off to make?

<!-- -->

1. \[15/15/15\] &lt;7.9 &gt;Amazon Web Services (AWS) offers a wide
   variety of “com- puting instances,” which are machines configured to
   target different applications
   and scales. AWS prices tell us useful data about the Total Cost of Ownership (TCO) of various computing devices, particularly as computer equipment is often depreciated[<sup>1</sup>](#_bookmark390) on a 3-year schedule. As of July 2017, a dedicated, compute-oriented “c4” computing instance includes two x86 chips with 20 physical cores in total. It rents on-demand for $1.75/hour, or $17,962 for 3 years. In contrast, a dedicated “p2” computing instance also has two x86 chips but with 36 cores in total, and adds 16 NVIDIA K80 GPUs. A p2 rents on-demand for $15.84/hour, or $184,780 for 3 years.
2. \[15\] The c4 instance uses Intel Xeon E5-2666 v3 (Haswell) processors. The p2 instance uses Intel Xeon E5-2686 v4 (Broadwell) processors. Neither part num- ber is listed officially on Intel’s product website, which suggests that these parts are specially built for Amazon by Intel. The E5-2660 v3 part has a similar core count to the E5-2666 v3 and has a street price of around $1500. The E5-2697 v4 part has a similar core count to the E5-2686 v4 and has a street price of around
   $3000. Assume that the non-GPU portion of the p2 instance would have a price proportional to the ratio of street prices. What is the TCO, over 3 years, for a single K80 GPU?
3. \[15\] Suppose that you have a compute- and throughput-dominated workload that runs at rate 1 on the c4 instance and at rate T on the GPU-accelerated p2 instance. How large must T be for the GPU-based solution to be more cost-effective? Suppose that each general-purpose CPU core can compute at a rate of about 30G single-precision FLOPS. Ignoring the CPUs of the p2 instance, what fraction of peak K80 FLOPs would be required to reach the same rate of computation as the c4 instance?
4. \[15\] AWS also offers “f1” instances that include 8 Xilinx Ultrascale +VU9P FPGAs. They rent at $13.20/hour, or $165,758 for 3 years. Each VU9P device includes 6840 DSP slices, which can perform 27 18-bit integer multiply- accumulate operations (recall that one multiply-accumulate counts as two “operations”). At 500 MHz, what is the peak multiply-accumulate opera- tions/cycle that an f1-based system might achieve, counting all 8 FPGAs toward the computation total? Assuming that the integer operations on the FPGAs can substitute for floating-point operations, how does this compare to the peak single-precision multiply-accumulate operations/cycle of the GPUs of the p2 instance? How do they compare in terms of cost-effectiveness?

<!-- -->

1. \[20/20/25\] &lt;7.7 &gt;As shown in Figure 7.34 (but simplified to
   fewer PEs), each Pixel Visual Core includes a 16 × 16 set of full
   processing elements, surrounded
   <span id="_bookmark390" class="anchor"></span><sup>1</sup>Capital expenses are accounted for over the lifetime of an asset, using a “depreciation schedule.” Rather than taking a one-time charge at the point where an asset is acquired, standard accounting practice spreads out the capital cost over the lifetime of the asset. So one might account for a $30,000 device that has a useful life of 3 years by assigning

$10,000 in depreciation to each year.

by an additional two layers of “simplified” processing elements. Simplified PEs can store and communicate data but omit the computation hardware of full PEs. Simplified PEs store copies of data that might be the “home data” of a neighboring core, so there are (16 + 2 + 2)<sup>2</sup> = 400 PEs in total, 256 full and 144 simplified.

1. \[20\] Suppose that you wanted to process a 64 32 grayscale image with a 5 5 stencil using 8 Pixel Visual Cores. For now, assume that the image is laid out in raster-scan order (pixels that are adjacent in X are adjacent in memory, while pixels that are adjacent in Y are 64 memory locations apart). For each of the 8 cores, describe the memory region that the core should import to handle its part of the image. Make sure to include the halo region. Which parts of the halo region should be zeroed by software to ensure correct operation? You may find it convenient to refer to subregions of the image using a 2D slice notation, where for example image\[2:5\]\[6:13\] refers to the set of pixels whose x
   component is 2 &lt;=x&lt; 5 and whose y component is 6 &lt;=y&lt; 13 (the slices are half-open following Python slicing practice).
2. \[20\] If we change to a 3 3 stencil, how do the regions imported from memory change? How many halo-simplified PEs go unused?
3. \[25\] Now consider how to support a 7 7 stencil. In this case, we don’t have as many hardware-supported simplified PEs as we need to cover the three pixels worth of halo data that “belong to” neighboring cores. To handle this, we use the outermost ring of full PEs as if they were simplified PEs. How many pixels can we handle in a single core using this strategy? How many “tiles” are now required to handle our 64 32 input image? What is the utilization of our full PEs over the complete processing time for the 7 7 stencil over the 64 32 image?

<!-- -->

1. \[20/20/20/25/25\] &lt;7.7 &gt;Consider a case in which each of the eight cores on a Pixel Visual Core device is connected through a four-port switch to a 2D SRAM,
   forming a core +memory unit. The remaining two ports on the switch link these units in a ring, so that each core is able to access any of the eight SRAMs. However, this ring-based network-on-chip topology makes some data access patterns more efficient than others.

Core
Core

Core

Core

1. \[20\] Suppose that each link in the NOC has the same bandwidth B, and that each link is full-duplex, so it can simultaneously transfer bandwidth B in each direction. Links connect the core to the switch, the switch to SRAM, and pairs of switches in the ring. Assume that each local memory has at least B band- width, so it can saturate its link. Consider a memory access pattern where each of the eight PEs access only the closest memory (the one connected via the switch of the core +memory unit). What is the maximum memory bandwidth that the core will be able to achieve?
2. \[20\] Now consider an off-by-one access pattern, where core _i_ accesses memory _i_ + 1, going through three links to reach that memory (core 7 will access mem- ory 0, because of the ring topology). What is the maximum memory bandwidth that the core will be able to achieve in this case? To achieve that bandwidth, do you need to make any assumptions about the capabilities of the 4-port switch? What if the switch can only move data at rate B?
3. \[20\] Consider an off-by-two access pattern, where core _i_ access memory _i_ + 2. Once again, what is the maximum memory bandwidth that the core will be able to achieve in this case? Where are the bottleneck links in the network-on-chip?
4. \[25\] Consider a uniform random memory access pattern, where each core uses each of the SRAMs for ⅛ of its memory requests. Assuming this traffic pattern, how much traffic traverses a switch-to-switch link, compared to the amount of traffic between a core and its associated switch or between an SRAM and its associated switch?
5. \[25\] (advanced) Can you conceive of a case (workload) where this network can deadlock? From the standpoint of software-only solutions, what should the compiler do to avoid such a scenario? If you can make changes to hardware, what changes in routing topology (and routing scheme) would guarantee no deadlocks?

   1. &lt;7.2 &gt;The first Anton molecular dynamics supercomputer
      typically simulated a box of water that was 64 Å on a side. The
      computer itself might be approximated as a box with 1 m side
      length. A single simulation step represented 2.5 fs of simula-
      tion time, and took about 10 μs of wall-clock time. The physics models used in molecular dynamics act as if every particle in the system exerts a force on every other particle in the system on each (“outer”) time step, requiring what amounts to a global synchronization across the entire computer.
6. Calculate the spatial expansion factor from simulation space to hardware in real space.
7. Calculate the temporal slowdown factor from simulated time to wall-clock time.
8. These two numbers come out surprisingly close. Is this just a coincidence, or is there some other limit that constrains them in some way? (Hint: the speed of light applies to both the simulated chemical system and the hardware that does the simulation.)
9. Given these limits, what would it take to use a warehouse-scale supercomputer to perform molecular dynamics simulations at Anton rates? That is, what’s the fastest simulation step time that might be achieved with a machine 10<sup>2</sup> or 10<sup>3</sup> m on a side? What about simulating on a world-spanning Cloud service?

<!-- -->

1. &lt;7.2 &gt;The Anton communication network is a 3D, 8 8 8 torus, where each node in the system has six links to neighboring nodes. Latency for a packet to tran-
   sit single link is about 50 ns. Ignore on-chip switching time between links for this exercise.
2. What is the diameter (maximum number of hops between a pair of nodes) of the communication network? Given that diameter, what is the shortest latency required to broadcast a single value from one node of the machine to all 512 nodes of the machine?
3. Assuming that adding up two values takes zero time, what is the shortest latency to add up a sum over 512 values to a single node, where each value starts on a different node of the machine?
4. Once again assume that you want to perform the sum over 512 values, but you want each of the 512 nodes of the system to end up with a copy of the sum. Of course you could perform a global reduction followed by a broadcast. Can you do the combined operation in less time? This pattern is called an _all-reduce_. Compare the times of your all-reduce pattern to the time of a broadcast from a single node or a global sum to a single node. Compare the bandwidth used by the all-reduce pattern with the other patterns.
