## Characteristics of Scientific Applications

The primary use of scalable shared-memory multiprocessors is for true parallel programming, as opposed to multiprogramming or transaction-oriented comput- ing. The primary target of parallel computing is scientific and technical applica- tions. Thus, understanding the design issues requires some insight into the behavior of such applications. This section provides such an introduction.

> 可伸缩共享内存多处理器的主要用途是真正的并行编程，而不是多道程序设计或面向事务的计算。并行计算的主要目标是科学和技术应用。因此，理解设计问题需要深入了解此类应用程序的行为。本节提供这样的介绍。

### Characteristics of Scientific Applications

Our scientific/technical parallel workload consists of two applications and two computational kernels. The kernels are fast Fourier transformation (FFT) and an LU decomposition, which were chosen because they represent commonly used techniques in a wide variety of applications and have performance characteristics typical of many parallel scientific applications. In addition, the kernels have small code segments whose behavior we can understand and directly track to specific architectural characteristics. Like many scientific applications, I/O is essentially nonexistent in this workload.

> 我们的科学/技术并行工作负载由两个应用程序和两个计算内核组成。内核是快速傅里叶变换 (FFT) 和 LU 分解，之所以选择它们是因为它们代表了广泛应用中的常用技术，并且具有许多并行科学应用的典型性能特征。此外，内核具有小代码段，我们可以理解其行为并直接跟踪到特定的体系结构特征。与许多科学应用程序一样，此工作负载中基本上不存在 I/O。

The two applications that we use in this appendix are Barnes and Ocean, which represent two important but very different types of parallel computation. We briefly describe each of these applications and kernels and characterize their basic behavior in terms of parallelism and communication. We describe how the prob- lem is decomposed for a distributed shared-memory multiprocessor; certain data decompositions that we describe are not necessary on multiprocessors that have a single, centralized memory.

> 我们在本附录中使用的两个应用程序是 Barnes 和 Ocean，它们代表两种重要但截然不同的并行计算类型。我们简要描述了这些应用程序和内核中的每一个，并在并行性和通信方面描述了它们的基本行为。我们描述了如何为分布式共享内存多处理器分解问题；我们描述的某些数据分解在具有单个集中式内存的多处理器上是不必要的。

##### `The FFT Kernel`

The FFT is the key kernel in applications that use spectral methods, which arise in fields rangingfrom signalprocessingtofluid flowtoclimate modeling. The FFT appli- cation we study here is a one-dimensional version of a parallel algorithm for a complex number FFT. It has a sequential execution time for `n` data points of `n` log `n`. The algo- rithm uses a high radix (equal to p*n*) that minimizes communication. The measurements shown in this appendix are collected for a million-point input data set.

> FFT 是使用频谱方法的应用程序的关键内核，这些应用程序出现在从信号处理到流体流动到气候建模的各个领域。我们在这里研究的 FFT 应用是复数 FFT 并行算法的一维版本。它对 `n` log `n` 的 `n` 数据点有顺序执行时间。该算法使用高基数(等于 p*n*)来最小化通信。本附录中显示的测量值是针对百万点输入数据集收集的。

There are three primary data structures: the input and output arrays of the data being transformed and the roots of unity matrix, which is precomputed and only read during the execution. All arrays are organized as square matrices. The six steps in the algorithm are as follows:

> 存在三种主要的数据结构：被转换数据的输入和输出数组以及单位矩阵的根，它是预先计算的并且只在执行期间读取。所有阵列都组织为方阵。算法的六个步骤如下：

1. Transpose data matrix.
2. Perform 1D FFT on each row of data matrix.
3. Multiply the roots of unity matrix by the data matrix and write the result in the data matrix.
4. Transpose data matrix.
5. Perform 1D FFT on each row of data matrix.
6. Transpose data matrix.

The data matrices and the roots of unity matrix are partitioned among processors in contiguous chunks of rows, so that each processor’s partition falls in its own local memory. The first row of the roots of unity matrix is accessed heavily by all pro- cessors and is often replicated, as we do, during the first step of the algorithm just shown. The data transposes ensure good locality during the individual FFT steps, which would otherwise access nonlocal data.

> 数据矩阵和 Unity 矩阵的根部在连续的行中分配在处理器之间，因此每个处理器的分区都属于其本地内存。统一矩阵的根的第一行均由所有专业人员大量访问，并且在刚刚显示的算法的第一步中，经常像我们一样复制。数据转置确保在单个 FFT 步骤中确保良好的位置，否则将访问非本地数据。

The only communication is in the transpose phases, which require all-to-all communication of large amounts of data. Contiguous subcolumns in the rows assigned to a processor are grouped into blocks, which are transposed and placed into the proper location of the destination matrix. Every processor transposes one block locally and sends one block to each of the other processors in the system. Although there is no reuse of individual words in the transpose, with long cache blocks it makes sense to block the transpose to take advantage of the spatial locality afforded by long blocks in the source matrix.

> 唯一的通信是在转置阶段，需要全面交流大量数据。分配给处理器的行中的连续子列分组为块，这些块被转置并将其放置在目标矩阵的正确位置中。每个处理器在本地转换一个块，并将一个块发送到系统中的每个其他处理器。尽管在转置中没有单个单词的重复使用，但在长缓存块中，阻止转置以利用源矩阵中长块提供的空间位置是有意义的。

##### `The LU Kernel`

LU is an LU factorization of a dense matrix and is representative of many dense linear algebra computations, such as QR factorization, Cholesky factorization, and eigenvalue methods. For a matrix of size `n n` the running time is `n` 3 and the parallelism is proportional to `n` 2. Dense LU factorization can be performed efficiently by blocking the algorithm, using the techniques in [Chapter 2](#_bookmark46), which leads to highly efficient cache behavior and low communication. After blocking the algorithm, the dominant computation is a dense matrix multiply that occurs in the innermost loop. The block size is chosen to be small enough to keep the cache miss rate low and large enough to reduce the time spent in the less parallel parts of the computation. Relatively small block sizes (8 8 or 16 16) tend to satisfy both criteria.

> lu 是密集矩阵的 LU 分解，代表了许多密集的线性代数计算，例如 QR 分解，Cholesky 分解和特征值方法。对于大小 *n n* 的矩阵，运行时间为 *n* 3 ，并且并行性与 *n* 2 成正比。通过[第 2 章](#_bookmark46)中的技术阻止算法，可以通过阻止算法来有效地进行密集的 lu 分解，从而导致高效的高速缓存行为和较低的通信。阻止算法后，主要计算是在最内向环中发生的密集矩阵倍数。选择块大小足够小，以使高速缓存率低率低和足够大，以减少计算不平行部分所花费的时间。相对较小的块大小(8 8 或 16 16)倾向于满足这两个标准。

Two details are important for reducing interprocessor communication. First, the blocks of the matrix are assigned to processors using a 2D tiling: The `n` × `n`

> 两个细节对于减少处理器交流很重要。首先，矩阵的块使用 2D 瓷砖分配给处理器：`×`

(where each block is `B B`) matrix of blocks is allocated by laying a grid of size `p p` over the matrix of blocks in a cookie-cutter fashion until all the blocks are allocated to a processor. Second, the dense matrix multiplication is performed by the processor that owns the `destination` block. With this blocking and allocation scheme, communication during the reduction is both regular and predictable. For the measurements in this appendix, the input is a 512 512 matrix and a block of 16 16 is used.

> (每个块为 *b b*)块的矩阵通过以 cookie-cutter 方式将大小 *p p* 的网格分配给块的矩阵，直到将所有块分配给处理器。其次，密集矩阵乘法由拥有 *destination* 块的处理器执行。通过这种阻止和分配方案，减少期间的通信既是规则的，也是可预测的。对于本附录中的测量值，输入是 512 512 矩阵，使用了 16 16 的块。

A natural way to code the blocked LU factorization of a 2D matrix in a shared address space is to use a 2D array to represent the matrix. Because blocks are allo- cated in a tiled decomposition, and a block is not contiguous in the address space in a 2D array, it is very difficult to allocate blocks in the local memories of the processors that own them. The solution is to ensure that blocks assigned to a processor are allocated locally and contiguously by using a 4D array (with the first two dimensions specifying the block number in the 2D grid of blocks, and the next two specifying the element in the block).

> 在共享地址空间中 2D 矩阵的阻塞 LU 分解的一种自然方法是使用 2D 数组来表示矩阵。由于块是在瓷砖分解中分配的，并且在 2D 数组中的地址空间中不连续，因此很难在拥有它们的处理器的本地记忆中分配块。该解决方案是确保通过使用 4D 数组在本地和连续分配给处理器的块(前两个维度在块的 2D 网格中指定块号，而下两个在块中指定了元素)。

##### `The Barnes Application`

Barnes is an implementation of the Barnes-Hut `n`-body algorithm solving a problem in galaxy evolution. `N-body algorithms` simulate the interaction among a large number of bodies that have forces interacting among them. In this instance, the bodies represent collections of stars and the force is gravity. To reduce the computational time required to model completely all the individual interactions among the bodies, which grow as `n` 2, `n`-body algorithms take advan- tage of the fact that the forces drop off with distance. (Gravity, for example, drops off as 1/\_d_2, where `d` is the distance between the two bodies.) The Barnes-Hut algorithm takes advantage of this property by treating a collection of bodies that are "far away" from another body as a single point at the center of mass of the collection and with mass equal to the collection. If the body is far enough from any body in the collection, then the error introduced will be negligible. The collections are structured in a hierarchical fashion, which can be represented in a tree. This algorithm yields an `n` log `n` running time with par- allelism proportional to `n`.

> Barnes 是解决星系演化问题的 Barnes-Hut `n`-body 算法的实现。"N 体算法" 模拟大量物体之间的相互作用，这些物体之间存在相互作用力。在这种情况下，物体代表恒星的集合，力是引力。为了减少完全模拟物体之间所有个体相互作用所需的计算时间，这些相互作用增长为 `n` 2，`n`-body 算法利用了力随距离而下降的事实。(例如，重力下降为 1/\_d_2，其中 d 是两个物体之间的距离。)Barnes-Hut 算法通过处理 "远离" 物体的集合来利用此属性 另一个物体作为集合质量中心的单个点，质量等于集合。如果主体与集合中的任何主体都足够远，则引入的错误可以忽略不计。集合以分层方式构建，可以用树表示。该算法产生 `n` log `n` 运行时间，并行度与 `n` 成正比。

The Barnes-Hut algorithm uses an octree (each node has up to eight children) to represent the eight cubes in a portion of space. Each node then represents the collection of bodies in the subtree rooted at that node, which we call a `cell`. Because the density of space varies and the leaves represent individual bodies, the depth of the tree varies. The tree is traversed once per body to compute the net force acting on that body. The force calculation algorithm for a body starts at the root of the tree. For every node in the tree it visits, the algorithm determines if the center of mass of the cell represented by the subtree rooted at the node is "far enough away" from the body. If so, the entire subtree under that node is approximated by a single point at the center of mass of the cell, and the force that this center of mass exerts on the body is computed. On the other hand, if the center of mass is not far enough away, the cell must be "opened" and each of its subtrees visited. The distance between the body and the cell, together with the error tolerances, determines which cells must be opened. This force calcula- tion phase dominates the execution time. This appendix takes measurements using 16K bodies; the criterion for determining whether a cell needs to be opened is set to the middle of the range typically used in practice.

> Barnes-Hut 算法使用八叉树(每个节点最多有八个子节点)来表示一部分空间中的八个立方体。然后，每个节点代表以该节点为根的子树中的物体集合，我们称之为 "细胞" 。因为空间的密度不同，树叶代表个体，所以树的深度不同。每个物体遍历一次树以计算作用在该物体上的合力。物体的力计算算法从树的根部开始。对于它访问的树中的每个节点，该算法都会确定以该节点为根的子树所代表的细胞的质心是否离身体 "足够远" 。如果是这样，则该节点下的整个子树近似为单元质量中心的单个点，并计算该质量中心施加在身体上的力。另一方面，如果质心距离不够远，则必须 "打开" 单元格并访问其每个子树。主体和单元格之间的距离以及误差容限决定了必须打开哪些单元格。这个力计算阶段支配着执行时间。本附录使用 16K 主体进行测量；确定是否需要打开单元格的标准设置为通常在实践中使用的范围的中间值。

Obtaining effective parallel performance on Barnes-Hut is challenging because the distribution of bodies is nonuniform and changes over time, making partition- ing the work among the processors and maintenance of good locality of reference difficult. We are helped by two properties: (1) the system evolves slowly, and (2) because gravitational forces fall off quickly, with high probability, each cell requires touching a small number of other cells, most of which were used on the last time step. The tree can be partitioned by allocating each processor a subtree. Many of the accesses needed to compute the force on a body in the subtree will be to other bodies in the subtree. Since the amount of work associated with a subtree varies (cells in dense portions of space will need to access more cells), the size of the subtree allocated to a processor is based on some measure of the work it has to do (e.g., how many other cells it needs to visit), rather than just on the number of nodes in the subtree. By partitioning the octree representation, we can obtain good load balance and good locality of reference, while keeping the partitioning cost low. Although this partitioning scheme results in good locality of reference, the resulting data references tend to be for small amounts of data and are unstructured. Thus, this scheme requires an efficient implementation of shared-memory communication.

> 在 Barnes-Hut 上获得有效的并行性能具有挑战性，因为主体的分布是不均匀的并且会随时间变化，这使得在处理器之间划分工作和维护良好的参考位置变得困难。我们得到两个特性的帮助：(1)系统演化缓慢，以及(2)由于引力下降很快，很有可能每个细胞需要接触少量其他细胞，其中大部分是上次使用的 步。可以通过为每个处理器分配一个子树来对树进行分区。计算子树中一个物体上的力所需的许多访问将是对子树中其他物体的访问。由于与子树相关的工作量各不相同(空间密集部分的单元将需要访问更多单元)，分配给处理器的子树的大小基于对其必须完成的工作的某种度量(例如，如何 它需要访问的许多其他单元)，而不仅仅是子树中的节点数。通过对八叉树表示进行分区，我们可以获得良好的负载平衡和良好的参考局部性，同时保持较低的分区成本。尽管这种分区方案产生了良好的引用局部性，但生成的数据引用往往是针对少量数据并且是非结构化的。因此，该方案需要高效实现共享内存通信。

##### `The Ocean Application`

Ocean simulates the influence of eddy and boundary currents on large-scale flow in the ocean. It uses a restricted red-black Gauss-Seidel multigrid technique to solve a set of elliptical partial differential equations. `Red-black Gauss-Seidel` is an iteration technique that colors the points in the grid so as to consistently update each point based on previous values of the adjacent neighbors. `Multigrid methods` solve finite difference equations by iteration using hierarchical grids. Each grid in the hierarchy has fewer points than the grid below and is an approximation to the lower grid. A finer grid increases accuracy and thus the rate of convergence, while requiring more execution time, since it has more data points. Whether to move up or down in the hierarchy of grids used for the next iteration is deter- mined by the rate of change of the data values. The estimate of the error at every time step is used to decide whether to stay at the same grid, move to a coarser grid, or move to a finer grid. When the iteration converges at the finest level, a solution has been reached. Each iteration has `n` 2 work for an `n n` grid and the same amount of parallelism.

> 海洋模拟涡流和边界流对海洋中大尺度流动的影响。它使用受限制的红黑 Gauss-Seidel 多重网格技术来求解一组椭圆偏微分方程。`Red-black Gauss-Seidel` 是一种迭代技术，它为网格中的点着色，以便根据相邻邻居的先前值一致地更新每个点。"多重网格方法" 通过使用分层网格的迭代来求解有限差分方程。层次结构中的每个网格比下面的网格具有更少的点，并且是对较低网格的近似。更精细的网格可以提高准确性，从而提高收敛速度，同时需要更多的执行时间，因为它有更多的数据点。在用于下一次迭代的网格层次结构中向上还是向下移动取决于数据值的变化率。每个时间步的误差估计用于决定是留在同一网格、移动到更粗的网格还是移动到更精细的网格。当迭代收敛于最精细的水平时，就达到了解决方案。每次迭代都有 `n` 2 个工作用于 `n n` 网格和相同数量的并行度。

The arrays representing each grid are dynamically allocated and sized to the particular problem. The entire ocean basin is partitioned into square subgrids (as close as possible) that are allocated in the portion of the address space corre- sponding to the local memory of the individual processors, which are assigned responsibility for the subgrid. For the measurements in this appendix we use an input that has 130 130 grid points. There are five steps in a time iteration. Since data are exchanged between the steps, all the processors present synchronize at the end of each step before proceeding to the next. Communication occurs when the boundary points of a subgrid are accessed by the adjacent subgrid in nearest- neighbor fashion.

> 代表每个网格的数组是根据特定问题动态分配和调整大小的。整个海洋盆地被划分为正方形子网格(尽可能接近)，这些子网格分配在与各个处理器的本地内存相对应的地址空间部分中，这些处理器被分配了对子网格的责任。对于本附录中的测量，我们使用具有 130 130 个网格点的输入。时间迭代有五个步骤。由于数据在步骤之间交换，所有当前的处理器在每个步骤结束时同步，然后再继续下一步。当相邻子网格以最近邻方式访问子网格的边界点时，就会发生通信。

##### `Computation/Communication for the Parallel Programs`

A key characteristic in determining the performance of parallel programs is the ratio of computation to communication. If the ratio is high, it means the application has lots of computation for each datum communicated. As we saw in [Section I.2](#interprocessor-communication-the-critical-performance-issue), communication is the costly part of parallel computing; therefore, high computation-to-communication ratios are very beneficial. In a parallel processing environment, we are concerned with how the ratio of computation to communica- tion changes as we increase either the number of processors, the size of the prob- lem, or both. Knowing how the ratio changes as we increase the processor count sheds light on how well the application can be sped up. Because we are often inter- ested in running larger problems, it is vital to understand how changing the data set size affects this ratio.

> 决定并行程序性能的一个关键特征是计算与通信的比率。如果比率很高，则意味着应用程序对每个通信的数据进行大量计算。正如我们在 [第 I.2 节](#interprocessor-communication-the-critical-performance-issue)中看到的那样，通信是并行计算中代价高昂的部分；因此，高计算通信比率是非常有益的。在并行处理环境中，我们关心计算与通信的比率如何随着处理器数量、问题规模或两者的增加而变化。了解当我们增加处理器数量时该比率如何变化可以阐明应用程序可以加速到什么程度。因为我们通常对运行更大的问题感兴趣，所以了解改变数据集大小如何影响这个比率是至关重要的。

To understand what happens quantitatively to the computation-to- communication ratio as we add processors, consider what happens separately to computation and to communication as we either add processors or increase prob- lem size. Figure I.1 shows that as we add processors, for these applications, the amount of computation per processor falls proportionately and the amount of com- munication per processor falls more slowly. As we increase the problem size, the computation scales as the `O`( ) complexity of the algorithm dictates. Communica- tion scaling is more complex and depends on details of the algorithm; we describe the basic phenomena for each application in the caption of Figure I.1.

> 要了解当我们添加处理器时计算与通信的比率在数量上会发生什么变化，请考虑当我们添加处理器或增加问题规模时计算和通信分别发生了什么。图 I.1 显示，随着我们添加处理器，对于这些应用程序，每个处理器的计算量按比例下降，而每个处理器的通信量下降得更慢。随着我们增加问题的大小，计算会随着算法的 `O`( ) 复杂度的要求而扩展。通信缩放更复杂，取决于算法的细节；我们在图 I.1 的标题中描述了每个应用程序的基本现象。

Application Scaling of computation Scaling of communication

> 通信计算缩放的应用程序缩放

Figure I.1 Scaling of computation, of communication, and of the ratio are critical factors in determining perfor- mance on parallel multiprocessors. In this table, `p` is the increased processor count and `n` is the increased dataset size. Scaling is on a per-processor basis. The computation scales up with `n` at the rate given by `O`( ) analysis and scales down linearly as `p` is increased. Communication scaling is more complex. In FFT, all data points must interact, so communication increases with `n` and decreases with `p`. In LU and Ocean, communication is proportional to the boundary of a block, so it scales with dataset size at a rate proportional to the side of a square with `n` points, namely, pn; for the same reason communication in these two applications scales inversely to pp. Barnes has the most complex scaling properties. Because of the fall-off of interaction between bodies, the basic number of interactions among bodies that require communication scales as pn. An additional factor of log `n` is needed to maintain the relationships among the bodies. As processor count is increased, communication scales inversely to pp.

> 图 I.1 计算、通信和比率 ​​ 的缩放是决定并行多处理器性能的关键因素。在此表中，"p" 是增加的处理器数量，"n" 是增加的数据集大小。缩放是基于每个处理器的。计算按 `O`( ) 分析给出的速率随 `n` 扩大，并随着 `p` 的增加而线性缩小。通信缩放更复杂。在 FFT 中，所有数据点都必须交互，因此通信随 "n" 增加而随 "p" 减少。在 LU 和 Ocean 中，通信与块的边界成正比，因此它以与具有 "n" 个点的正方形的边成正比的速率随数据集大小缩放，即 pn；出于同样的原因，这两个应用程序中的通信与 pp 成反比。Barnes 具有最复杂的缩放属性。由于身体之间相互作用的下降，需要交流的身体之间相互作用的基本数量按 pn 缩放。需要一个额外的 log `n` 因子来维护主体之间的关系。随着处理器数量的增加，通信与 pp 成反比。

The overall computation-to-communication ratio is computed from the indi- vidual growth rate in computation and communication. In general, this ratio rises slowly with an increase in dataset size and decreases as we add processors. This reminds us that performing a fixed-size problem with more processors leads to increasing inefficiencies because the amount of communication among processors grows. It also tells us how quickly we must scale dataset size as we add processors to keep the fraction of time in communication fixed. The following example illus- trates these trade-offs.

> 总体计算与通信比率是根据计算和通信中的个体增长率计算的。一般来说，这个比率随着数据集大小的增加而缓慢上升，并随着我们添加处理器而下降。这提醒我们，使用更多处理器执行固定大小的问题会导致效率越来越低，因为处理器之间的通信量会增加。它还告诉我们，在添加处理器以保持通信时间比例固定时，我们必须以多快的速度扩展数据集大小。下面的例子说明了这些权衡。

Example Suppose we know that for a given multiprocessor the Ocean application spends 20% of its execution time waiting for communication when run on four processors. Assume that the cost of each communication event is independent of processor count, which is not true in general, since communication costs rise with processor count. How much faster might we expect Ocean to run on a 32-processor machine with the same problem size? What fraction of the execution time is spent on com- munication in this case? How much larger a problem should we run if we want the fraction of time spent communicating to be the same?

> 示例假设我们知道，对于给定的多处理器，海洋应用程序的执行时间为 20％，等待四个处理器运行时的通信。假设每个通信事件的成本独立于处理器计数，这通常是不正确的，因为沟通成本随处理器计数而上升。我们期望海洋在具有相同问题大小的 32 处理器上运行多少速度？在这种情况下，在通讯上花费了哪一部分执行时间？如果我们想要花费的时间相同的时间相同，我们应该遇到多少问题？

_Answer_ The computation-to-communication ratio for Ocean is p*n*/p*p*, so if the problem size is the same, the communicatipon frequency scales by `p`. This means that recognizing that the computation is decreased but the communication time is increased. If `T`<sub>4</sub> is the total execution time for four processors, then the execution time for 32 processors is

> `ANSWER` 海洋的计算与通信比为 p*n*/p*p*，因此，如果问题大小相同，则可以通过 *p* 进行通信频率缩放。这意味着认识到计算减少，但通信时间增加。如果 *t* <sub> 4 </sub>是四个处理器的总执行时间，则 32 处理器的执行时间为

For the fraction of the communication time to remain the same, we must keep the computation-to-communication ratio the same, so the problem size must scale at the same rate as the processor count. Notice that, because we have changed the problem size, we cannot fairly compare the speedup of the original problem and the scaled problem. We will return to the critical issue of scaling applications for mul- tiprocessors in [Section I.6](#performance-measurement-of-parallel-processors-with-scientific-applications).

> 对于保持相同时间的通信时间的比例，我们必须保持计算与通信比率相同，因此问题大小必须以与处理器计数相同的速率扩展。请注意，由于我们已经改变了问题大小，因此我们无法公平地比较原始问题的加速和缩放问题。我们将返回[I.6 节]中的缩放应用程序的关键问题(＃与 - 科学应用程序的＃performance-Measurement-seperument-seperement-seperement-secientific-applications)。
