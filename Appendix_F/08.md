## Examples of Interconnection Networks

> ##互连网络的示例

To further provide mass to the concepts described in the previous sections, we look at five example networks from the four interconnection network domains consid- ered in this appendix. In addition to one for each of the OCN, LAN, and WAN areas, we look at two examples from the SAN area: one for system area networks and one for system/storage area networks. The first two examples are proprietary networks used in high-performance systems; the latter three examples are network standards widely used in commercial systems.

> 为了进一步提供前面部分中描述的概念的质量，我们研究了本附录中考虑的四个互连网络域的五个示例网络。除了每个 OCN，LAN 和 WAN 区域中的一个外，我们还研究了 SAN 区域的两个示例：一个用于系统区域网络，一个用于系统/存储区域网络。前两个示例是在高性能系统中使用的专有网络。后三个示例是在商业系统中广泛使用的网络标准。

### On-Chip Network: Intel Single-Chip Cloud Computer

> ###片网络：英特尔单芯片云计算机

With continued increases in transistor integration as predicted by Moore’s law, processor designers are under the gun to find ways of combating chip-crossing wire delay and other problems associated with deep submicron technology scaling. Multicore microarchitectures have gained popularity, given their advantages of simplicity, modularity, and ability to exploit parallelism beyond that which can be achieved through aggressive pipelining and multiple instruction/data issuing on a single core. No matter whether the processor consists of a single core or mul- tiple cores, higher and higher demands are being placed on intrachip communica- tion bandwidth to keep pace—not to mention interchip bandwidth. This has spurred a great amount of interest in OCN designs that efficiently support commu- nication of instructions, register operands, memory, and I/O data within and between processor cores both on and off the chip. Here we focus on one such on-chip network: The Intel Single-chip Cloud Computer prototype.

> 如摩尔定律所预测的那样，随着晶体管整合的持续增加，处理器设计师正在枪支中寻找打击芯片交叉的电线延迟以及与深 subsicron 技术扩展相关的其他问题的方法。鉴于它们的简单性，模块化和利用并行性的能力超出了可以通过激进的管道管道和多个核心上发出的多个指令/数据来实现的能力，多核微体系结构已获得了流行。无论处理器是由单个核心还是模拟核心组成，都将在内部通信带宽上提出越来越高的需求，以保持步伐，更不用说互动带宽了。这引起了 OCN 设计的极大兴趣，这些设计有效地支持说明，注册操作数，内存和 I/O 数据内外的处理器内部和芯片之间的 I/O 数据。在这里，我们专注于这样的片网络：英特尔单芯片云计算机原型。

The Single-chip Cloud Computer (SCC) is a prototype chip multiprocessor with 48 Intel IA-32 architecture cores. Cores are laid out (see [Figure F.28](#_bookmark626)) on a network with a 2D mesh topology (6 4). The network connects 24 tiles, 4 on- die memory controllers, a voltage regulator controller (VRC), and an external system interface controller (SIF). In each tile two cores are connected to a router. The four memory controllers are connected at the boundaries of the mesh, two on each side, while the VRC and SIF controllers are connected at the bottom border of the mesh.

> 单芯片云计算机（SCC）是具有 48 个 Intel IA-32 体系结构内核的原型芯片多处理器。在具有 2D 网格拓扑的网络上布局（参见[图 F.28]（#_ bookmark626））布局（参见图 F.28]（#_ bookmark626））（6 4）。该网络连接 24 个瓷砖，4 个模具控制器，电压调节器控制器（VRC）和外部系统接口控制器（SIF）。在每个瓷砖中，两个核心连接到路由器。四个存储器控制器连接在网格的边界上，两侧两个，而 VRC 和 SIF 控制器在网格的底部边框连接。

Each memory controller can address two DDR3 DIMMS, each up to 8 GB of memory, thus resulting in a maximum of 64 GB of memory. The VRC controller allows any core or the system interface to adjust the voltage in any of the six pre- defined regions configuring the network (two 2-tile regions). The clock can also be adjusted at a finer granularity with each tile having its own operating frequency. These regions can be turned off or scaled down for large power savings. This method allows full application control of the power state of the cores. Indeed, applications have an API available to define the voltage and the frequency of each region. The SIF controller is used to communicate the network from outside the chip.

> 每个内存控制器都可以解决两个 DDR3 DIMM，每个 DIMM 最多可达 8 GB 的内存，从而最大值 64 GB 的内存。VRC 控制器允许任何核心或系统接口调整配置网络的六个预定区域中的任何一个（两个 2 英尺区域）。时钟也可以以更精细的粒度进行调整，每个瓷砖都有其自身的工作频率。这些区域可以关闭或缩放以节省大量功率。此方法允许对核心的功率状态进行完整的应用控制。实际上，应用程序具有可用的 API 来定义每个区域的电压和频率。SIF 控制器用于从芯片外部传达网络。

Each of the tiles includes two processor cores (P54C-based IA) with associated L1 16 KB data cache and 16 KB instruction cache and a 256 KB L2 cache (with the associated controller), a 5-port router, traffic generator (for testing purposes only), a mesh interface unit (MIU) handling all message passing requests, memory look-up tables (with configuration registers to set the mapping of a core’s physical addresses to the extended memory map of the system), a message-passing buffer, and circuitry for the clock generation and synchronization for crossing asynchro- nous boundaries.

> 每个瓷砖都包含两个处理器内核（基于 P54C 的 IA），其中包括关联的 L1 16 KB 数据缓存和 16 KB 指令缓存和 256 kb L2 CACHE（带有关联的控制器），一个 5 端口路由器，流量生成器（用于测试仅目的），一个网格接口单元（MIU）处理所有消息传递请求，内存查找表（配置寄存器都将核心物理地址的映射设置为系统的扩展存储器映射），一个消息缓冲器，以及用于交叉异步边界的时钟生成和同步的电路。

<img src="./media/image704.png" style="width:4.13485in;height:3.29919in" />
<img src="./media/image301.png" style="width:5.97466in" />

Figure F.28 SCC Top-level architecture. From Howard, J. et al., *IEEE International Solid-State Circuits Conference Digest of Technical Papers*, pp. 58–59.

> 图 F.28 SCC 顶级体系结构。摘自 Howard，J。等人， *IEEE 国际固态电路会议技术论文摘要 *，第 58-59 页。

Focusing on the OCN, the MIU unit is in charge of interfacing the cores to the network, including the packetization and de-packetization of large messages; com- mand translation and address decoding/lookup; link-level flow control and credit management; and arbiter decisions following a round-robin scheme. A credit- based flow control mechanism is used together with virtual cut-through switching (thus making it necessary to split long messages into packets). The routers are con- nected in a 2D mesh layout, each on its own power supply and clock source. Links connecting routers have 16B+ 2B side bands running at 2 GHz. Zero-load latency is set to 4 cycles, including link traversal. Eight virtual channels are used for per- formance (6 VCs) and protocol-level deadlock handling (2 VCs). A message-level arbitration is implemented by a wrapped wave-front arbiter. The dimension-order XY routing algorithm is used and pre-computation of the output port is performed at every router.

> MIU 单元专注于 OCN，负责将核心接口到网络，包括大消息的包装和去包装；命令翻译和地址解码/查找；链接级流量控制和信用管理；和仲裁计划之后的仲裁者决策。基于信用的流量控制机制与虚拟切割开关一起使用（因此有必要将长消息分为数据包）。路由器以 2D 网格布局连接，每个布局都有其自身的电源和时钟源。连接路由器的链接具有 16b+ 2b 的侧带，以 2 GHz 运行。零负载延迟设置为 4 个周期，包括链接遍历。八个虚拟通道用于实现（6 个 VC）和协议级僵局处理（2 VC）。消息级仲裁由包裹的波 - 前仲裁员实施。使用尺寸订单 XY 路由算法，并在每个路由器上执行输出端口的预计算。

Besides the tiles having regions defined for voltage and frequency, the network (made of routers and links) has its own single region. Thus, all the network com- ponents run at the same speed and use the same power supply. An asynchronous clock transition is required between the router and the tile.

> 除了为电压和频率定义区域的瓷砖外，网络（由路由器和链接制成）具有其自己的单个区域。因此，所有网络组合都以相同的速度运行并使用相同的电源。路由器和瓷砖之间需要异步时钟过渡。

One of the distinctive features of the SCC architecture is the support for a messaging-based communication protocol rather than hardware cache-coherent memory for inter-core communication. Message passing buffers are located on every router and APIs are provided to take full control of MPI structures. Cache coherency can be implemented by software.

> SCC 体系结构的独特功能之一是支持基于消息传递的通信协议，而不是用于核心间通信的硬件缓存连接内存。消息传递缓冲区位于每个路由器上，并且提供了 API，以完全控制 MPI 结构。可以通过软件实现缓存相干性。

The SCC router represents a significant improvement over the Teraflops pro- cessor chip in the implementation of a 2D on-chip interconnect. Contrasted with the 2D mesh implemented in the Teraflops processor, this implementation is tuned for a wider data path in a multiprocessor interconnect and is more latency, area, and power optimized for such a width. It targets a lower 2-GHz frequency of operation compared to the 5 GHz of its predecessor Teraflops processor, yet with a higher- performance interconnect architecture.

> SCC 路由器在实施 2D 片上互连时对 TERAFlops Prosessor 芯片表示显着改善。与 Teraflops 处理器中实现的 2D 网格形成对比，此实现是为多处理器互连中更宽的数据路径调整的，并且更延迟，区域和功率为这种宽度而优化。与其前身 TERAFlops 处理器的 5 GHz 相比，它的靶向 2-GHz 的操作频率较低，但具有更高的性能互连体系结构。

### System Area Network: IBM Blue Gene/L 3D Torus Network

> ###系统区域网络：IBM 蓝色基因/L 3D 圆环网络

The IBM BlueGene/L was the largest-scaled, highest-performing computer system in the world in 2005, according to _[www.top500.org](http://www.top500.org/)._ With 65,536 dual-processor compute nodes and 1024 I/O nodes, this 360 TFLOPS (peak) supercomputer has a system footprint of approximately 2500 square feet. Both processors at each node can be used for computation and can handle their own communication protocol processing in virtual mode or, alternatively, one of the processors can be used for computation and the other for network interface processing. Packets range in size from 32 bytes to a maximum of 256 bytes, and 8 bytes are used for the header. The header includes routing, virtual channel, link-level flow control, packet size, and other such information, along with 1 byte for CRC to protect the header. Three bytes are used for CRC at the packet level, and 1 byte serves as a valid indicator.

> 根据_ [www.top500.org]（[www.top500.org/](http://www.top500.org/)），IBM Bluegene/L 是 2005 年全球最大，表现最高的计算机系统。处理器计算节点和 1024 I/O 节点，这个 360 TFLOPS（峰）超级计算机的系统足迹约为 2500 平方英尺。每个节点上的两个处理器都可以用于计算，并且可以以虚拟模式处理自己的通信协议处理，或者，其中一个处理器可用于计算，而另一个处理器则用于网络接口处理。数据包的大小从 32 个字节到最多 256 个字节，并且将 8 个字节用于标题。标题包括路由，虚拟通道，链接级流量控制，数据包大小和其他此类信息，以及 1 个字节以保护标头。三个字节用于数据包级别的 CRC，1 个字节用作有效指示器。

The main interconnection network is a proprietary 32 32 64 3D torus SAN that interconnects all 64 K nodes. Each node switch has six 350 MB/sec bidirec- tional links to neighboring torus nodes, an injection bandwidth of 612.5 MB/sec from the two node processors, and a reception bandwidth of 1050 MB/sec to the two node processors. The reception bandwidth from the network equals the inbound bandwidth across all switch ports, which prevents reception links from bottlenecking network performance. Multiple packets can be sunk concurrently at each destination node because of the higher reception link bandwidth.

> 主要的互连网络是专有 32 32 64 3D 圆环 SAN，可互连所有 64 K 节点。每个节点开关都有六个 350 MB/sec 的竞标链接，可从两个节点处理器进行 612.5 mb/sec 的注射带宽，以及两个节点处理器的接收带宽为 1050 MB/sec。网络的接收带宽等于所有开关端口的入站带宽，这阻止了接收链接的瓶颈网络性能。由于较高的接收链路带宽，可以同时在每个目的地节点同时沉没多个数据包。

Two nodes are implemented on a 2 1 1 compute card, 16 compute cards and 2 I/O cards are implemented on a 4 4 2 node board, 16 node boards are implemented on an 8 8 8 midplane, and 2 midplanes form a 1024-node rack with physical dimensions of 0.9 0.9 1.9 cubic meters. Links have a maximum physical length of 8.6 meters, thus enabling efficient link-level flow control with reasonably low buffering requirements. Low latency is achieved by implementing virtual cut-through switching, distributing arbitration at switch input and output ports, and precomputing the current routing path at the previous switch using a finite-state machine so that part of the routing delay is removed from the critical path in switches. High effective bandwidth is achieved using input-buffered switches with dual read ports, virtual cut-through switching with four virtual chan- nels, and fully adaptive deadlock-free routing based on bubble flow control.

> 在 2 1 1 的计算卡上实现了两个节点，16 张计算卡和 2 个 I/O 卡在 4 4 2 节点板上实现，在 8 8 8 8 8 8 8 中平面上实现了 16 个节点板，并形成 2 个中间平面。物理尺寸为 0.9 0.9 1.9 立方米的节点架。链接的最大物理长度为 8.6 米，因此可以实现有效的链接级流量控制，并具有相当低的缓冲要求。通过实现虚拟切割开关，在开关输入和输出端口分配仲裁以及使用有限状态机器在上一个开关处的当前路由路径来实现较低的延迟，从而实现了延迟。在开关中。使用带有双读取端口的输入缓冲开关，带有四个虚拟 Channels 的虚拟切割开关以及基于气泡流量控制的完全自适应的无僵路路由，可以实现高效带宽。

A key feature in networks of this size is fault tolerance. Failure rate is reduced by using a relatively low link clock frequency of 700 MHz (same as processor clock) on which both edges of the clock are used (i.e., 1.4 Gbps or 175 MB/sec transfer rate is supported for each bit-serial network link in each direction), but failures may still occur in the network. In case of failure, the midplane node boards containing the fault(s) are switched off and bypassed to isolate the fault, and com- putation resumes from the last checkpoint. Bypassing is done using separate bypass switch boards associated with each midplane that are additional to the set of torus node boards. Each bypass switch board can be configured to connect either to the corresponding links in the midplane node boards or to the next bypass board, effectively removing the corresponding set of midplane node boards. Although the number of processing nodes is reduced to some degree in some net- work dimensions, the machine retains its topological structure and routing algorithm.

> 此大小网络中的一个关键功能是容错。使用相对较低的链路时钟频率为 700 MHz（与处理器时钟相同），可以降低故障率，在该频率上都使用了两个时钟边缘（即支持每个位串行网络的 1.4 Gbps 或 175 MB/sec 的传输速率在每个方向上的链接），但是网络中仍可能发生故障。如果发生故障，则关闭包含故障的中平面节点板并绕过以隔离故障，并从最后一个检查点恢复恢复。使用与每个中间平面相关联的单独旁路开关板进行绕过，这些桥梁是圆环节点板的其他。每个旁路开关板可以配置为连接到中平面节点板中的相应链接，也可以连接到下一个旁路板，有效地删除了相应的中平面节点板。尽管在某些净工作维度中，处理节点的数量在某种程度上减少了，但该机器保留了其拓扑结构和路由算法。

Some collective communication operations such as barrier synchronization, broadcast/multicast, reduction, and so on are not performed well on the 3D torus as the network would be flooded with traffic. To remedy this, two separate tree networks with higher per-link bandwidth are used to implement collective and combining operations more efficiently. In addition to providing support for efficient synchronization and broadcast/multicast, hardware is used to perform some arithmetic reduction operations in an efficient way (e.g., to compute the sum or the maximum value of a set of values, one from each processing node). In addition to the 3D torus and the two tree networks, the Blue Gene/L imple- ments an I/O Gigabit Ethernet network and a control system Fast Ethernet net- work of lower bandwidth to provide for parallel I/O, configuration, debugging, and maintenance.

> 一些集体沟通操作，例如屏障同步，广播/多播，减少等等，因为网络将充斥着流量，因此在 3D 圆圈上表现不佳。为了解决这个问题，使用两个具有较高每链接带宽的单独的树网络用于更有效地实现集体和组合操作。除了提供有效同步和广播/多播的支持外，硬件还用于以有效的方式执行一些算术减少操作（例如，计算一组值的总和或最大值，一个从每个处理节点中）。除了 3D 圆环和两个树网络外，蓝色基因/l 触发了 I/O 千兆以太网网络和控制系统快速以太网网络 - 较低带宽以提供并行的 I/O，配置，调试，调试和维护。

### System/Storage Area Network: InfiniBand

> ###系统/存储区网络：Infiniband

InfiniBand is an industrywide _de facto_ networking standard developed in October 2000 by a consortium of companies belonging to the InfiniBand Trade Associa- tion. InfiniBand can be used as a system area network for interprocessor commu- nication or as a storage area network for server I/O. It is a switch-based interconnect technology that provides flexibility in the topology, routing algo- rithm, and arbitration technique implemented by vendors and users. InfiniBand supports data transmission rates of 2 to 120 Gbp/link per direction across distances of 300 meters. It uses cut-through switching, 16 virtual channels and service levels, credit-based link-level flow control, and weighted round-robin fair scheduling and implements programmable forwarding tables. It also includes features useful for increasing reliability and system availability, such as communication subnet man- agement, end-to-end path establishment, and virtual destination naming.

> Infiniband 是一家行业范围内的 FACTO_网络标准，由属于 Infiniband 交易协会的一组公司于 2000 年 10 月开发。Infiniband 可以用作解释器通信的系统区域网络，也可以用作服务器 I/O 的存储区域网络。它是一种基于开关的互连技术，可在供应商和用户实施的拓扑，路由算法和仲裁技术方面提供灵活性。Infiniband 支持每个方向的数据传输速率在 300 米的距离上为 2 至 120 GBP/链路。它使用切割开关，16 个虚拟渠道和服务级别，基于信用的链接级流量控制以及加权的圆形旋转公平计划，并实现可编程转发表。它还包括用于提高可靠性和系统可用性的功能，例如通信子网管理，端到端路径建立和虚拟目的地命名。

Figure F.29 Characteristics of on-chip networks implemented in recent research and commercial processors. Some processors implement multiple on-chip networks (not all shown)—for example, two in the MIT Raw and eight in the TRIP Edge.

> 图 F.29 在最近的研究和商业处理器中实施的片网络的特征。一些处理器实施了多个芯片网络（并非全部显示） - 例如，在 MIT RAW 中有两个，在旅行边缘中有八个。

[Figure F.30](#_bookmark627) shows the packet format for InfiniBand juxtaposed with two other net- work standards from the LAN and WAN areas. Figure F.31 compares various char- acteristics of the InfiniBand standard with two proprietary system area networks widely used in research and commercial high-performance computer systems.

> [图 F.30]（#_ bookmark627）显示了 Infiniband 的数据包格式与 LAN 和 WAN 区域的另外两个净工作标准并列。图 F.31 将 Infiniband 标准的各种特征与在研究和商业高性能计算机系统中广泛使用的两个专有系统区域进行了比较。

Figure F.30 Packet format for InfiniBand, Ethernet, and ATM. ATM calls their messages “cells” instead of packets, so the proper name is ATM cell format. The width of each drawing is 32 bits. All three formats have destination addres- sing fields, encoded differently for each situation. All three also have a checksum field to catch transmission errors, although the ATM checksum field is calculated only over the header; ATM relies on higher-level protocols to catch errors in the data. Both InfiniBand and Ethernet have a length field, since the packets hold a variable amount of data, with the former counted in 32-bit words and the latter in bytes. InfiniBand and ATM headers have a type field (T) that gives the type of packet. The remaining Ethernet fields are a preamble to allow the receiver to recover the clock from the self-clocking code used on the Ethernet, the source address, and a pad field to make sure the smallest packet is 64 bytes (including the header). InfiniBand includes a version field for protocol version, a sequence number to allow in- order delivery, a field to select the destination queue, and a partition key field. Infiniband has many more small fields not shown and many other packet formats; above is a simplified view. ATM’s short, fixed packet is a good match to real-time demand of digital voice.

> 图 F.30 Infiniband，以太网和 ATM 的数据包格式。ATM 称他们的消息为“单元格”，而不是数据包，因此专有名称是 ATM 单元格式。每个图的宽度为 32 位。所有三种格式都有目的地插件字段，在每种情况下均以不同的方式编码。尽管 ATM 校验和字段仅在标题上计算出来，但这三个都有一个校验和字段来捕获传输错误。ATM 依靠高级协议来捕获数据中的错误。Infiniband 和以太网都有一个长度字段，因为数据包具有可变量的数据，前者以 32 位单词计数，而后者则以字节为单位。Infiniband 和 ATM 标头具有一个类型字段（T），可提供数据包的类型。其余的以太网字段是一个序言，可以允许接收器从以太网，源地址和 PAD 字段上使用的自锁定代码中恢复时钟，以确保最小的数据包为 64 个字节（包括标头）。Infiniband 包括一个用于协议版本的版本字段，一个允许输送的序列号，选择目标队列的字段和分区密钥字段。Infiniband 有更多未显示的小领域，还有许多其他数据包格式。以上是简化的视图。ATM 的简短固定数据包与数字声音的实时需求相匹配。

Figure F.31 Characteristics of system area networks implemented in various top 10 supercomputer clusters in 2005.

> 图 F.31 2005 年在各种十大超级计算机群集中实现的系统区域网络的特征。

InfiniBand offers two basic mechanisms to support user-level communica- tion: send/receive and remote DMA (RDMA). With send/receive, the receiver has to explicitly post a receive buffer (i.e., allocate space in its channel adapter network interface) before the sender can transmit data. With RDMA, the sender can remotely DMA data directly into the receiver device’s memory. For exam- ple, for a nominal packet size of 4 bytes measured on a Mellanox MHEA28-XT channel adapter connected to a 3.4 GHz Intel Xeon host device, sending and receiving overhead is 0.946 and 1.423 μs, respectively, for the send/receive mechanism, whereas it is 0.910 and 0.323 μs, respectively, for the RDMA mechanism.

> Infiniband 提供了两种基本机制来支持用户级通信：发送/接收和远程 DMA（RDMA）。在发送/接收的情况下，接收器必须明确发布接收缓冲区（即在其通道适配器网络接口中分配空间），然后发件人可以传输数据。使用 RDMA，发件人可以将 DMA 数据直接远程数据远程数据远程数据。对于考试，对于连接到 3.4 GHz Intel Xeon 主机设备的 Mellanox MheA28-X 频道适配器的标称数据包大小为 4 个字节，发送和接收开销分别为 0.946 和 1.423μs，用于发送/接收机制，而 RDMA 机制分别为 0.910 和 0.323μs。

As discussed in [Section F.2](#interconnecting-two-devices), the packet size is important in getting full benefit of the network bandwidth. One might ask, “What is the natural size of messages?” [Figure F.32(a)](#_bookmark628) shows the size of messages for a commercial fluid dynamics sim- ulation application, called Fluent, collected on an InfiniBand network at The Ohio State University’s Network-Based Computer Laboratory. One plot is cumulative in messages sent and the other is cumulative in data bytes sent. Messages in this graph are message passing interface (MPI) units of information, which gets divided into InfiniBand maximum transfer units (packets) transferred over the network. As shown, the maximum message size is over 512 KB, but approximately 90% of the messages are less than 512 bytes. Messages of 2 KB represent approximately 50% of the bytes transferred. An Integer Sort application kernel in the NAS Parallel

> 如[F.2]（＃互连两个设备）中所述，数据包大小对于获得网络带宽的全部受益很重要。有人可能会问：“消息的自然大小是多少？”[图 F.32（a）]（#_ bookmark628）显示了商业流体动力学模拟应用程序的消息大小，称为 Fluent，该应用程序在俄亥俄州立大学基于网络的计算机实验室的 Infiniband 网络上收集。一个图是在发送的消息中累积的，另一个图是在发送的数据字节中累积的。此图中的消息是信息传递接口（MPI）单位的信息，该单元被分为通过网络传输的 Infiniband 最大传输单元（数据包）。如图所示，最大消息大小超过 512 kb，但大约 90％的消息小于 512 字节。2 kb 的消息约占传输字节的 50％。NAS 平行中的整数排序应用程序内核

Figure F.32 Data collected by D.K. Panda, S. Sur, and L. Chai (2005) in the Network-Based Computing Laboratory at The Ohio State University. (a) Cumulative percentage of messages and volume of data transferred as message size varies for the Fluent application (_[www.fluent.com](http://www.fluent.com)_). Each _x_-axis entry includes all bytes up to the next one; for example, 128 represents 1 byte to 128 bytes. About 90% of the messages are less than 512 bytes, which represents about 40% of the total bytes transferred. (b) Effective bandwidth versus message size measured on SDR and DDR InfiniBand networks running MVAPICH ([_http://nowlab.cse.ohio-state.edu/projects/mpi-iba_)](http://nowlab.cse.ohio-state.edu/projects/mpi-iba)) with OS bypass (native) and without (IPoIB).

> 图 F.32 D.K.收集的数据 Panda，S。Sur 和 L. Chai（2005）在俄亥俄州立大学的基于网络的计算实验室中。（a）消息大小随消息大小的累积百分比和流利应用程序（_ [www.fluent.com]（[www.fluent.com](http://www.fluent.com)）_ _）的累计量变化。每个_x_-axis 条目都包含所有字节，直到下一个字节；例如，128 表示 1 个字节至 128 个字节。大约 90％的消息小于 512 个字节，约占传输总字节的 40％。（b）在运行 MVAPICH 的 SDR 和 DDR Infiniband 网络上测量的有效带宽与消息大小-state.edu/projects/mpi-iba））带有 OS 旁路（本机）和没有（iPOIB）。

Benchmark suite is also measured to have about 75% of its messages below 512 bytes (plots not shown). Many applications send far more small messages than large ones, particularly since requests and acknowledgments are more frequent than data responses and block writes.

> 基准套件还测量了其消息的 75％以下，低于 512 个字节（未显示图）。许多应用程序发送的消息比大型消息要多得多，尤其是因为请求和确认比数据响应和块写入更频繁。

InfiniBand reduces protocol processing overhead by allowing it to be off- loaded from the host computer to a controller on the InfiniBand network inter- face card. The benefits of protocol offloading and bypassing the operating system are shown in [Figure F.32(b)](#_bookmark628) for MVAPICH, a widely used implemen- tation of MPI over InfiniBand. Effective bandwidth is plotted against message size for MVAPICH configured in two modes and two network speeds. One mode runs IPoIB, in which InfiniBand communication is handled by the IP layer implemented by the host’s operating system (i.e., no OS bypass). The other mode runs MVAPICH directly over VAPI, which is the native Mellanox InfiniBand interface that offloads transport protocol processing to the channel adapter hardware (i.e., OS bypass). Results are shown for 10 Gbps single data rate (SDR) and 20 Gbps double data rate (DDR) InfiniBand networks. The results clearly show that offloading the protocol processing and bypassing the OS significantly reduce sending and receiving overhead to allow near wire-speed effective bandwidth to be achieved.

> Infiniband 通过允许将其从主机计算机中卸载到 Infiniband Network Inter-Face Card 上的控制器来减少协议处理开销。协议卸载和绕过操作系统的好处在[图 F.32（b）]（#_ bookmark628）中显示了 MVAPICH，这是 MVAPICH（MVAPICH），这是 MPI 在 Infiniband 上的广泛使用。绘制有效的带宽，以两种模式和两个网络速度配置的 MVAPICH 的消息大小绘制。一种模式运行 IPOIB，其中 Infiniband 通信由主机操作系统实现的 IP 层处理（即无 OS 旁路）。另一种模式直接通过 VAPI 运行 MVAPICH，VAPI 是本机 Mellanox Infiniband 接口，可将传输协议处理转移到通道适配器硬件（即 OS Bypass）。显示了 10 Gbps 单数据速率（SDR）和 20 Gbps 双重数据速率（DDR）Infiniband 网络的结果。结果清楚地表明，卸载协议处理并绕过 OS 可显着减少发送和接收开销，以实现接近电线的有效带宽。

### Ethernet: _The_ Local Area Network

> ###以太网：_the_局部区域网络

Ethernet has been extraordinarily successful as a LAN—from the 10 Mbit/sec stan- dard proposed in 1978 used practically everywhere today to the more recent 10 Gbit/sec standard that will likely be widely used. Many classes of computers include Ethernet as a standard communication interface. Ethernet, codified as IEEE standard 802.3, is a packet-switched network that routes packets using the desti- nation address. It was originally designed for coaxial cable but today uses primarily Cat5E copper wire, with optical fiber reserved for longer distances and higher bandwidths. There is even a wireless version (802.11), which is testimony to its ubiquity.

> 以太网作为局域网一直取得了极大的成功，从 1978 年在 1978 年提出的 10 mbit/sec standard 几乎在当今无处不在，到最近的 10 Gbit/sec 标准，可能会被广泛使用。许多类别的计算机都将以太网作为标准通信接口。以太网编码为 IEEE Standard 802.3，是一个包装开关的网络，该网络使用 Desti-nister 地址路由数据包。它最初是为同轴电缆设计的，但如今主要使用 CAT5E 铜线，其光纤预留用于更长的距离和更高的带宽。甚至有无线版本（802.11），这是其无处不在的证词。

Over a 20-year span, computers became thousands of times faster than they were in 1978, but the shared media Ethernet network remained the same. Hence, engineers had to invent temporary solutions until a faster, higher-bandwidth net- work became available. One solution was to use multiple Ethernets to interconnect machines and to connect those Ethernets with internetworking devices that could transfer traffic from one Ethernet to another, as needed. Such devices allow indi- vidual Ethernets to operate in parallel, thereby increasing the aggregate intercon- nection bandwidth of a collection of computers. In effect, these devices provide similar functionality to the switches described previously for point-to-point networks.

> 在 20 年的时间里，计算机的变化速度比 1978 年快数千倍，但是共享的媒体以太网网络保持不变。因此，工程师必须发明临时解决方案，直到可以使用更快，更高的宽宽网络。一种解决方案是使用多个以太网互连机器，并将这些以太网与可以根据需要将流量从一个以太网转移到另一个以太网的互联网工程设备将其连接到另一个。这些设备允许单独的以太网并联运行，从而增加了计算机集合的总互音带宽。实际上，这些设备提供了与先前针对点对点网络所描述的开关相似的功能。

[Figure F.33](#_bookmark629) shows the potential parallelism that can be gained. Depending on how they pass traffic and what kinds of interconnections they can join together, these devices have different names:

> [图 F.33]（#_ bookmark629）显示了可以获得的潜在平行性。根据它们如何通过流量以及可以将哪些互连连接在一起，这些设备具有不同的名称：

Figure F.33 The potential increased bandwidth of using many Ethernets and bridges.

> 图 F.33 使用许多以太网和桥梁的带宽可能增加。

- _Bridges_—These devices connect LANs together, passing traffic from one side to another depending on the addresses in the packet. Bridges operate at the Ethernet protocol level and are usually simpler and cheaper than routers, dis- cussed next. Using the notation of the OSI model described in the next section (see Figure F.36 on page F-85), bridges operate at layer 2, the data link layer.

> - _bridges_  - 这些设备将 LAN 连接在一起，根据数据包中的地址从一侧传递到另一侧。桥梁在以太网协议级别上运行，通常比路由器更简单，更便宜，接下来引起了讨论。使用下一节中描述的 OSI 模型的表示法（请参见第 F-85 页的图 F.36），桥梁在数据链路层的第 2 层工作。

- Routers or _gateways_—These devices connect LANs to WANs, or WANs to WANs, and resolve incompatible addressing. Generally slower than bridges, they operate at OSI layer 3, the network layer. WAN routers divide the network into separate smaller subnets, which simplifies manageability and improves security.

> - 路由器或_gateways_-这些设备将兰斯连接到 wans，或将 wans 连接到万物，并解决不兼容的寻址。它们通常比桥梁慢，它们在网络层的 OSI 第 3 层运行。WAN 路由器将网络分为单独的较小子网，从而简化可管理性并提高安全性。

The final internetworking devices are _hubs_, but they merely extend multiple seg- ments into a single LAN. Thus, hubs do not help with performance, as only one message can transmit at a time. Hubs operate at OSI layer 1, called the physical

> 最终的 Internetworking 设备是_HUBS_，但它们仅扩展到单个 LAN。因此，集线器无助于性能，因为只有一条消息一次可以传输。集线器在 OSI 第 1 层操作，称为物理

Figure F.34 The connection established between mojave.stanford.edu and mammoth.berkeley.edu (1995). FDDI is a 100 Mbit/sec LAN, while a T1 line is a 1.5 Mbit/sec telecommunications line and a T3 is a 45 Mbit/sec telecom- munications line. BARRNet stands for Bay Area Research Network. Note that inr-111-cs2.Berkeley.edu is a router with two Internet addresses, one for each port.

> 图 F.34 Mojave.stanford.edu 和 Mammoth.berkeley.edu（1995）之间建立的联系。FDDI 是 100 mbit/sec lan，而 T1 线是 1.5 mbit/sec 的电信线，T3 是 45 mbit/sec 电信线路。Barrnet 代表湾区研究网络。请注意，INR-111-CS2.Berkeley.edu 是一个路由器，每个端口一个都有两个互联网地址。

Figure F.35 The role of internetworking. The width indicates the relative number of items at each level.

> 图 F.35 互联网工作的作用。宽度表示每个级别的相对数量。

Figure F.36 The OSI model layers. Based on [*www.geocities.com/SiliconValley/Monitor/3131/ne/osimodel.html*.](http://www.geocities.com/SiliconValley/Monitor/3131/ne/osimodel.html)

> 图 F.36 OSI 模型层。基于[*[www.geocities.com/siliconvalley/monitor/3131/ne/osimodel.html](http://www.geocities.com/siliconvalley/monitor/3131/ne/osimodel.html)*]。

layer. Since these devices were not planned as part of the Ethernet standard, their ad hoc nature has added to the difficulty and cost of maintaining LANs.

> 层。由于这些设备不是作为以太网标准的一部分计划的，因此它们的临时性质增加了维护 LAN 的困难和成本。

As of 2011, Ethernet link speeds are available at 10, 100, 10,000, and 100,000 Mbits/sec. Although 10 and 100 Mbits/sec Ethernets share the media with multiple devices, 1000 Mbits/sec and above Ethernets rely on point-to-point links and switches. Ethernet switches normally use some form of store-and-forward.

> 截至 2011 年，以太网链路速度为 10、100、10,000 和 100,000 mbits/sec。尽管 10 和 100 mbits/sec 以太网与多个设备共享媒体，但 1000 mbits/sec 及更高的以太网依靠点对点链接和开关。以太网开关通常使用某种形式的存储和前向。

Ethernet has no real flow control, dating back to its first instantiation. It orig- inally used carrier sensing with exponential back-off (see page F-23) to arbitrate for the shared media. Some switches try to use that interface to retrofit their version of flow control, but flow control is not part of the Ethernet standard.

> 以太网没有真正的流控制，可以追溯到其第一个实例化。它与指数向后传感（请参阅第 23 页）进行原始使用的载波传感，以仲裁共享媒体。一些开关试图使用该接口对其流量控制版本进行改造，但流控制并不是以太网标准的一部分。

### Wide Area Network: ATM

> ###广域网络：ATM

_Asynchronous Transfer Mode_ (ATM) is a wide area networking standard set by the telecommunications industry. Although it flirted as competition to Ethernet as a LAN in the 1990s, ATM has since retreated to its WAN stronghold.

> _ASYNCHRONOUS 转移模式_（ATM）是电信行业设定的广泛区域网络标准。尽管它在 1990 年代作为 LAN 作为 LAN 的竞争而调情，但此后，ATM 已退回到 WAN 据点。

The telecommunications standard has scalable bandwidth built in. It starts at 155 Mbits/sec and scales by factors of 4 to 620 Mbits/sec, 2480 Mbits/sec, and so on. Since it is a WAN, ATM’s medium is fiber, both single mode and multimode. Although it is a switched medium, unlike the other examples it relies on virtual con- nections for communication. ATM uses virtual channels for routing to multiplex dif- ferent connections on a single network segment, thereby avoiding the inefficiencies of conventional connection-based networking. The WAN focus also led to store- and-forward switching. Unlike the other protocols, [Figure F.30](#_bookmark627) shows ATM has a small, fixed-sized packet with 48 bytes of payload. It uses a credit-based flow con- trol scheme as opposed to IP routers that do not implement flow control.

> 电信标准具有内置的可扩展带宽。它以 155 mbits/sec 的速度开始，并按照 4 至 620 mbits/sec，2480 mbits/sec 等因素开始。由于它是 WAN，因此 ATM 的介质是光纤，无论是单模式还是多模。尽管它是一种切换介质，但与其他示例不同，它依赖于虚拟连接进行通信。ATM 使用虚拟通道来路由单个网络段上的多路复用连接，从而避免了基于常规连接的网络的效率低下。WAN 的焦点还导致了存储和前向切换。与其他协议不同，[图 F.30]（#_ bookmark627）显示 ATM 具有一个小的，固定尺寸的数据包，带有 48 个字节有效载荷。它使用基于信用的流程控制方案，而不是不实施流量控制的 IP 路由器。

The reason for virtual connections and small packets is quality of service. Since the telecommunications industry is concerned about voice traffic, predictability matters as well as bandwidth. Establishing a virtual connection has less variability than connectionless networking, and it simplifies store-and-forward switching. The small, fixed packet also makes it simpler to have fast routers and switches. Toward that goal, ATM even offers its own protocol stack to compete with TCP/IP. Surprisingly, even though the switches are simple, the ATM suite of pro- tocols is large and complex. The dream was a seamless infrastructure from LAN to WAN, avoiding the hodgepodge of routers common today. That dream has faded from inspiration to nostalgia.

> 虚拟连接和小数据包的原因是服务质量。由于电信行业关注语音流量，因此可预测性和带宽也很重要。建立虚拟连接的可变性较小，而无连接网络则简化了存储和前向切换。小型固定数据包还使拥有快速路由器和开关变得更加简单。为了实现这一目标，ATM 甚至提供了自己的协议堆栈以与 TCP/IP 竞争。令人惊讶的是，即使开关很简单，供应托运的 ATM 套件又大又复杂。梦想是从 LAN 到 WAN 的无缝基础设施，避免了今天的路由器的大概。这个梦想从灵感逐渐消失了。
