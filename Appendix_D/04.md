## I/O Performance, Reliability Measures, and Benchmarks

> ## i/o 性能，可靠性措施和基准

I/O performance has measures that have no counterparts in design. One of these is diversity: Which I/O devices can connect to the computer system? Another is capacity: How many I/O devices can connect to a computer system?

> I/O 性能的措施在设计方面没有对应。其中之一是多样性：哪个 I/O 设备可以连接到计算机系统？另一个是容量：有多少个 I/O 设备可以连接到计算机系统？

In addition to these unique measures, the traditional measures of performance (namely, response time and throughput) also apply to I/O. (I/O throughput is some- times called _I/O bandwidth_ and response time is sometimes called _latency._) The next two figures offer insight into how response time and throughput trade off against each other. [Figure D.8](#_bookmark549) shows the simple producer-server model. The pro- ducer creates tasks to be performed and places them in a buffer; the server takes tasks from the first in, first out buffer and performs them.

> 除了这些独特的措施外，传统的绩效指标(即响应时间和吞吐量)也适用于 I/O。(i/o 吞吐量有时称为_i/o bandWidth_，响应时间有时称为_latency._)接下来的两个数字提供了有关响应时间和吞吐量如何相互交易的洞察力。[图 D.8](#_ bookmark549)显示了简单的生产者 - 服务器模型。程序创建要执行的任务，并将其放置在缓冲区中。该服务器从第一个 IN 进行任务，首先输出缓冲区并执行它们。

![](./media/image601.png)

Figure D.8 The traditional producer-server model of response time and throughput. Response time begins when a task is placed in the buffer and ends when it is completed by the server. Throughput is the number of tasks completed by the server in unit time.

> 图 D.8 响应时间和吞吐量的传统生产者服务器模型。当任务放在缓冲区中并在服务器完成时结束时，响应时间开始。吞吐量是服务器在单位时间内完成的任务数。

Response time is defined as the time a task takes from the moment it is placed in the buffer until the server finishes the task. Throughput is simply the average num- ber of tasks completed by the server over a time period. To get the highest possible throughput, the server should never be idle, thus the buffer should never be empty. Response time, on the other hand, counts time spent in the buffer, so an empty buffer shrinks it.

> 响应时间定义为任务从将其放置在缓冲区中到服务器完成任务的时间。吞吐量仅仅是服务器在一段时间内完成的任务的平均数量。为了获得最高的吞吐量，服务器绝不应该是空闲的，因此缓冲区绝不应该为空。另一方面，响应时间计算在缓冲区中花费的时间，因此空的缓冲区会缩小它。

Another measure of I/O performance is the interference of I/O with processor execution. Transferring data may interfere with the execution of another process. There is also overhead due to handling I/O interrupts. Our concern here is how much longer a process will take because of I/O for another process.

> I/O 性能的另一个度量是 I/O 对处理器执行的干扰。传输数据可能会干扰另一个过程的执行。由于处理 I/O 的中断，也有开销。我们在这里担心的是，由于 I/O 进行另一个过程，过程将花费多长时间。

### Throughput versus Response Time

> ###吞吐量与响应时间

[Figure D.9](#_bookmark550) shows throughput versus response time (or latency) for a typical I/O system. The knee of the curve is the area where a little more throughput results in much longer response time or, conversely, a little shorter response time results in much lower throughput.

> [图 D.9](#_ bookmark550)显示了典型 I/O 系统的吞吐量与响应时间(或延迟)。曲线的膝盖是导致响应时间更长的吞吐量的区域，或者相反，较短的响应时间会导致吞吐量较低。

How does the architect balance these conflicting demands? If the computer is interacting with human beings, [Figure D.10](#_bookmark551) suggests an answer. An interaction, or _transaction_, with a computer is divided into three parts:

> 工程师如何平衡这些相互矛盾的需求？如果计算机与人类的交互，[图 D.10](#_ Bookmark551)提出了答案。与计算机的交互或_transaction_分为三个部分：

1. _Entry time_—The time for the user to enter the command.

> 1. _entry Time_-用户输入命令的时间。

2. _System response time_—The time between when the user enters the

> 2. _system 响应时间_-用户输入何时

command and the complete response is displayed.

> 命令和完整响应显示。

3. _Think time_—The time from the reception of the response until the user begins to enter the next command.

> 3. _思考时间_-从响应接收到用户开始输入下一个命令的时间。

The sum of these three parts is called the _transaction time_. Several studies report that user productivity is inversely proportional to transaction time. The results in [Figure D.10](#_bookmark551) show that cutting system response time by 0.7 seconds saves 4.9 seconds (34%) from the conventional transaction and 2.0 seconds (70%) from the graphics transaction. This implausible result is explained by human nature: People need less time to think when given a faster response. Although this study is 20 years old, response times are often still much slower than 1 second, even if processors are 1000 times faster. Examples of long delays include starting an appli- cation on a desktop PC due to many disk I/Os, or network delays when clicking on Web links.

> 这三个部分的总和称为_transaction 时间_。几项研究报告说，用户生产率与交易时间成反比。[图 D.10](#_ bookmark551)中的结果表明，将系统响应时间缩短 0.7 秒可节省 4.9 秒(34％)，从常规交易中节省了 4.9 秒(34％)，而图形交易中的 2.0 秒(70％)节省了 2.0 秒(70％)。这种令人难以置信的结果是由人性解释的：人们需要更少的时间来思考。尽管这项研究已有 20 年的历史，但响应时间通常仍比 1 秒钟慢得多，即使处理器的速度快 1000 倍。长时间延迟的示例包括由于许多磁盘 I/OS 或单击 Web 链接时的网络延迟，在台式 PC 上启动应用程序。

Figure D.9 Throughput versus response time. Latency is normally reported as response time. Note that the minimum response time achieves only 11% of the throughput, while the response time for 100% throughput takes seven times the min- imum response time. Note also that the independent variable in this curve is implicit; to trace the curve, you typically vary load (concurrency). Chen et al. \[1990\] collected these data for an array of magnetic disks.

> 图 D.9 吞吐量与响应时间。延迟通常报告为响应时间。请注意，最小响应时间仅达到吞吐量的 11％，而 100％吞吐量的响应时间是最小响应时间的七倍。还请注意，此曲线中的自变量是隐式的。要追踪曲线，您通常会改变负载(并发)。Chen 等。\ [1990 \]为一系列磁盘收集了这些数据。

Figure D.10 A user transaction with an interactive computer divided into entry time, system response time, and user think time for a conventional system and graphics system. The entry times are the same, independent of system response time. The entry time was 4 seconds for the conventional system and 0.25 seconds for the graphics sys- tem. Reduction in response time actually decreases transaction time by more than just the response time reduction. (From Brady \[1986\].)

> 图 D.10 具有交互式计算机的用户交易分为输入时间，系统响应时间以及用户考虑传统系统和图形系统的时间。进入时间是相同的，与系统响应时间无关。传统系统的输入时间为 4 秒，图形系统为 0.25 秒。响应时间的减少实际上会减少交易时间不仅减少响应时间。(来自 Brady \ [1986 \]。)。

Figure D.11 Response time restrictions for three I/O benchmarks.

> 图 D.11 三个 I/O 基准测试的响应时间限制。

To reflect the importance of response time to user productivity, I/O bench- marks also address the response time versus throughput trade-off. Figure D.11 shows the response time bounds for three I/O benchmarks. They report maximum throughput given either that 90% of response times must be less than a limit or that the average response time must be less than a limit.

> 为了反映响应时间对用户生产率的重要性，I/O 台上标记还解决了响应时间与吞吐量权衡问题。图 D.11 显示了三个 I/O 基准测试的响应时间边界。他们报告的最大吞吐量给定，要么 90％的响应时间必须小于限制，要么平均响应时间必须小于限制。

Let’s next look at these benchmarks in more detail.

> 接下来，让我们更详细地研究这些基准。

### Transaction-Processing Benchmarks

> ###交易处理基准

_Transaction processing_ (TP, or OLTP for online transaction processing) is chiefly concerned with _I/O rate_ (the number of disk accesses per second), as opposed to _data rate_ (measured as bytes of data per second). TP generally involves changes to a large body of shared information from many terminals, with the TP system guaranteeing proper behavior on a failure. Suppose, for example, that a bank’s computer fails when a customer tries to withdraw money from an ATM. The TP system would guarantee that the account is debited if the customer received the money _and_ that the account is unchanged if the money was not received. Air- line reservations systems as well as banks are traditional customers for TP.

> _transaction 处理_(用于在线交易处理的 TP 或 OLTP)主要与_i/O 速率_(每秒磁盘访问的数量)有关，而不是_DATA 速率_(以每秒数据为单位)。TP 通常涉及从许多终端的大量共享信息的变化，而 TP 系统可以保证失败的适当行为。例如，假设当客户试图从 ATM 提取资金时，银行的计算机会失败。TP 系统将保证如果客户收到的钱_和_未收到款项，则帐户将不变。航空保留系统和银行是 TP 的传统客户。

As mentioned in [Chapter 1](#_bookmark2), two dozen members of the TP community con- spired to form a benchmark for the industry and, to avoid the wrath of their legal departments, published the report anonymously \[Anon. et al. 1985\]. This report led to the _Transaction Processing Council_, which in turn has led to eight benchmarks since its founding. Figure D.12 summarizes these benchmarks.

> 如[第 1 章](#_ bookmark2)中提到的，TP 社区的两十名成员构成了该行业的基准，为了避免其法律部门的愤怒，请匿名发布该报告。等。1985 \]。该报告导致了_transaction 处理委员会，这又导致了自成立以来的八个基准。图 D.12 总结了这些基准。

Let’s describe TPC-C to give a flavor of these benchmarks. TPC-C uses a data- base to simulate an order-entry environment of a wholesale supplier, including entering and delivering orders, recording payments, checking the status of orders, and monitoring the level of stock at the warehouses. It runs five concurrent trans- actions of varying complexity, and the database includes nine tables with a scalable range of records and customers. TPC-C is measured in transactions per minute (tpmC) and in price of system, including hardware, software, and three years of maintenance support. [Figure 1.17](#_bookmark2) on page 42 in [Chapter 1](#_bookmark2) describes the top sys- tems in performance and cost-performance for TPC-C.

> 让我们描述 TPC-C 给出这些基准测试的味道。TPC-C 使用数据库模拟批发供应商的订单输入环境，包括输入和交付订单，记录付款，检查订单状态以及监视仓库的库存水平。它运行了五个以及不同复杂性的并发传输，并且数据库包括九个具有可扩展记录和客户的表。TPC-C 以每分钟交易(TPMC)和系统价格(包括硬件，软件和三年的维护支持)来衡量。[图 1.17](#_ bookmark2)第 42 页[第 1 章](#_ bookmark2)描述了 TPC-C 的性能和成本表现的顶级系统。

Figure D.12 Transaction Processing Council benchmarks. The summary results include both the performance met- ric and the price-performance of that metric. TPC-A, TPC-B, TPC-D, and TPC-R were retired.

> 图 D.12 交易处理委员会的基准。摘要结果包括该指标的性能和价格绩效。退休了 TPC-A，TPC-B，TPC-D 和 TPC-R。

These TPC benchmarks were the first—and in some cases still the only ones— that have these unusual characteristics:

> 这些 TPC 基准是具有这些不寻常特征的第一个(在某些情况下仍然是唯一的)：

- _Price is included with the benchmark results._ The cost of hardware, software, and maintenance agreements is included in a submission, which enables eval- uations based on price-performance as well as high performance.

> - _ price 包括基准结果。

- _The dataset generally must scale in size as the throughput increases_. The benchmarks are trying to model real systems, in which the demand on the sys- tem and the size of the data stored in it increase together. It makes no sense, for example, to have thousands of people per minute access hundreds of bank accounts.

> - _数据集通常必须随着吞吐量的增加而扩展大小。这些基准试图建模真实系统，其中对系统的需求和存储在其中的数据的大小增加在一起。例如，每分钟访问数百个银行帐户毫无意义。

- _The benchmark results are audited_. Before results can be submitted, they must be approved by a certified TPC auditor, who enforces the TPC rules that try to make sure that only fair results are submitted. Results can be challenged and disputes resolved by going before the TPC.

> - _基准结果进行了审核_。在提交结果之前，必须由经过认证的 TPC 审核员批准，该审计师执行了 TPC 规则，以确保仅提交公平的结果。结果可能会受到挑战，并通过在 TPC 之前解决问题。

- _Throughput is the performance metric, but response times are limited_. For example, with TPC-C, 90% of the new order transaction response times must be less than 5 seconds.

> - _ throughput 是性能指标，但响应时间有限_。例如，使用 TPC-C，新订单交易响应时间的 90％必须小于 5 秒。

- _An independent organization maintains the benchmarks_. Dues collected by TPC pay for an administrative structure including a chief operating office. This organization settles disputes, conducts mail ballots on approval of changes to benchmarks, holds board meetings, and so on.

> - _独立组织维护基准_。TPC 收集的会费为包括首席运营办公室在内的行政结构付费。该组织解决争议，在批准基准更改，举行董事会会议等方面进行邮件选票等。

### SPEC System-Level File Server, Mail, and Web Benchmarks

> ###规格系统级文件服务器，邮件和 Web 基准测试

The SPEC benchmarking effort is best known for its characterization of processor performance, but it has created benchmarks for file servers, mail servers, and Web servers.

> 规格基准测试工作以其处理器性能的特征而闻名，但它为文件服务器，邮件服务器和 Web 服务器创建了基准。

Seven companies agreed on a synthetic benchmark, called SFS, to evaluate systems running the Sun Microsystems network file service (NFS). This bench- mark was upgraded to SFS 3.0 (also called SPEC SFS97_R1) to include support for NFS version 3, using TCP in addition to UDP as the transport protocol, and making the mix of operations more realistic. Measurements on NFS systems led to a synthetic mix of reads, writes, and file operations. SFS supplies default param- eters for comparative performance. For example, half of all writes are done in 8 KB blocks and half are done in partial blocks of 1, 2, or 4 KB. For reads, the mix is 85% full blocks and 15% partial blocks.

> 七家公司商定了一个称为 SFS 的合成基准测试，以评估运行 Sun Microsystems 网络文件服务(NFS)的系统。此基准标记已升级到 SFS 3.0(也称为 SPEC SFS97_R1)，以包括对 NFS 版本 3 的支持，除了使用 UDP 作为传输协议外，还使用 TCP，并使操作的组合更现实。NFS 系统的测量导致读取，写入和文件操作的合成组合。SFS 为比较性能提供默认参数。例如，所有写作的一半是在 8 kb 的块中完成的，一半是在 1、2 或 4 kb 的部分块中完成的。对于读取，混合物为 85％的完整块和 15％的部分块。

Like TPC-C, SFS scales the amount of data stored according to the reported throughput: For every 100 NFS operations per second, the capacity must increase by 1 GB. It also limits the average response time, in this case to 40 ms. [Figure D.13](#_bookmark552)

> 像 TPC-C 一样，SFS 根据报告的吞吐量缩放存储的数据量：对于每秒 100 个 NFS 操作，容量必须增加 1 GB。在这种情况下，它还将平均响应时间限制为 40 ms。[图 D.13](#_ bookmark552)

Figure D.13 SPEC SFS97_R1 performance for the NetApp FAS3050c NFS servers in two configurations. Two processors reached 34,089 operations per second and four processors did 47,927. Reported in May 2005, these systems used the Data ONTAP 7.0.1R1 operating system, 2.8 GHz Pentium Xeon microprocessors, 2 GB of DRAM per processor, 1 GB of nonvolatile memory per system, and 168 15 K RPM, 72 GB, Fibre Channel disks. These disks were connected using two or four QLogic ISP-2322 FC disk controllers.

> 图 D.13 NetApp FAS3050C NFS 服务器的 SPES SFS97_R1 性能在两种配置中。两个处理器每秒达到 34,089 次操作，四个处理器进行了 47,927。这些系统在 2005 年 5 月报道，这些系统使用数据 ONTAP 7.0.1R1 操作系统，2.8 GHz Pentium Xeon 微处理器，每个处理器 2 GB DRAM，1 GB 每个系统的非挥发性存储器和 168 15 K rpm，72 GB，72 GB，纤维通道磁盘，纤维通道磁盘。这些磁盘使用两个或四个 QLOGIC ISP-2322 FC 磁盘控制器连接。

shows average response time versus throughput for two NetApp systems. Unfor- tunately, unlike the TPC benchmarks, SFS does not normalize for different price configurations.

> 显示两个 NetApp 系统的平均响应时间与吞吐量。与 TPC 基准分析不同，SFS 不适合不同的价格配置。

SPECMail is a benchmark to help evaluate performance of mail servers at an Internet service provider. SPECMail2001 is based on the standard Internet proto- cols SMTP and POP3, and it measures throughput and user response time while scaling the number of users from 10,000 to 1,000,000.

> SpecMail 是一个基准，可帮助评估 Internet 服务提供商的邮件服务器的性能。SpecMail2001 基于标准的 Internet 原始 COLS SMTP 和 POP3，它测量吞吐量和用户响应时间，同时将用户数量从 10,000 到 1,000,000。

SPECWeb is a benchmark for evaluating the performance of World Wide Web servers, measuring number of simultaneous user sessions. The SPECWeb2005 workload simulates accesses to a Web service provider, where the server supports home pages for several organizations. It has three workloads: Banking (HTTPS), E-commerce (HTTP and HTTPS), and Support (HTTP).

> SpecWeb 是评估万维网服务器的性能，测量同时的用户会话数量的基准。SpecWeb2005 Workload 模拟了对 Web 服务提供商的访问，服务器支持多个组织的主页。它具有三个工作负载：银行(HTTPS)，电子商务(HTTP 和 HTTPS)和支持(HTTP)。

### Examples of Benchmarks of Dependability

> ###可靠性基准的示例

The TPC-C benchmark does in fact have a dependability requirement. The bench- marked system must be able to handle a single disk failure, which means in practice that all submitters are running some RAID organization in their storage system.

> TPC-C 基准实际上确实具有可靠性要求。台式系统必须能够处理单个磁盘故障，这意味着在实际上，所有提交者都在其存储系统中运行某些 RAID 组织。

Efforts that are more recent have focused on the effectiveness of fault tolerance in systems. Brown and Patterson \[2000\] proposed that availability be measured by examining the variations in system quality-of-service metrics over time as faults are injected into the system. For a Web server, the obvious metrics are performance (measured as requests satisfied per second) and degree of fault tolerance (measured as the number of faults that can be tolerated by the storage subsystem, network connection topology, and so forth).

> 最近的努力集中在系统在系统中的有效性。Brown 和 Patterson \ [2000 \]提出，可以通过检查系统服务质量指标的变化来衡量可用性，因为将故障注入系统中。对于 Web 服务器，明显的指标是性能(根据每秒满足的请求来衡量)和容错程度(以存储子系统，网络连接拓扑等能够耐受的故障数来衡量)。

The initial experiment injected a single fault—such as a write error in disk sec- tor—and recorded the system’s behavior as reflected in the quality-of-service met- rics. The example compared software RAID implementations provided by Linux, Solaris, and Windows 2000 Server. SPECWeb99 was used to provide a workload and to measure performance. To inject faults, one of the SCSI disks in the software RAID volume was replaced with an emulated disk. It was a PC running software using a SCSI controller that appears to other devices on the SCSI bus as a disk. The disk emulator allowed the injection of faults. The faults injected included a variety of transient disk faults, such as correctable read errors, and permanent faults, such as disk media failures on writes.

> 最初的实验注射了一个故障，例如在磁盘上的写入错误，并记录了系统的行为，如服务质量的情况所示。该示例比较了 Linux，Solaris 和 Windows 2000 服务器提供的软件 RAID 实现。SpecWeb99 用于提供工作量和衡量性能。为了注入故障，软件 RAID 卷中的 SCSI 磁盘之一被模拟磁盘替换。它是使用 SCSI 控制器运行的 PC 运行软件，该软件显示在 SCSI 总线上的其他设备作为磁盘。磁盘模拟器允许注射故障。注入的故障包括各种瞬态磁盘故障，例如可更正的读取错误和永久性故障，例如磁盘媒体失败。

[Figure D.14](#_bookmark553) shows the behavior of each system under different faults. The two top graphs show Linux (on the left) and Solaris (on the right). As RAID systems can lose data if a second disk fails before reconstruction completes, the longer the reconstruction (MTTR), the lower the availability. Faster reconstruction implies decreased application performance, however, as reconstruction steals I/O resources from running applications. Thus, there is a policy choice between taking a performance hit during reconstruction or lengthening the window of vulnerability and thus lowering the predicted MTTF.

> [图 D.14](#_ bookmark553)显示了每个系统在不同故障下的行为。两个顶部图显示了 Linux(左侧)和 Solaris(右侧)。由于 RAID 系统在重建完成之前会失败，因此 RAID 系统可能会丢失数据，重建时间越长，可用性就越低。更快的重建速度意味着应用程序性能降低，但是，由于重建从运行应用程序中窃取了 I/O 资源。因此，在重建过程中进行绩效打击或延长脆弱性窗口，从而降低预测的 MTTF 之间存在政策选择。

Figure D.14 Availability benchmark for software RAID systems on the same computer running Red Hat 6.0 Linux, Solaris 7, and Windows 2000 operating systems. Note the difference in philosophy on speed of reconstruction of Linux versus Windows and Solaris. The _y_-axis is behavior in hits per second running SPECWeb99. The arrow indicates time of fault insertion. The lines at the top give the 99% confidence interval of performance before the fault is inserted. A 99% confidence interval means that if the variable is outside of this range, the probability is only 1% that this value would appear.

> 图 D.14 在运行红色帽子 6.0 Linux，Solaris 7 和 Windows 2000 操作系统的同一台计算机上的软件 RAID 系统的可用性基准。注意 Linux 与 Windows 和 Solaris 重建速度的哲学差异。_y_轴是每秒运行 SpecWeb99 的命中行为。箭头表示故障插入的时间。顶部的线在插入故障之前给出了 99％的性能置信区间。99％的置信区间意味着，如果该变量不在此范围内，则该值仅出现该值的概率仅为 1％。

Although none of the tested systems documented their reconstruction policies outside of the source code, even a single fault injection was able to give insight into those policies. The experiments revealed that both Linux and Solaris initiate auto- matic reconstruction of the RAID volume onto a hot spare when an active disk is taken out of service due to a failure. Although Windows supports RAID

> 尽管未经测试的系统都没有记录其在源代码之外的重建策略，但即使是单个故障注入也能够深入了解这些策略。实验表明，当由于故障而取消活动磁盘时，Linux 和 Solaris 都会启动 RAID 体积自动重建在热备件上。尽管 Windows 支持 RAID

reconstruction, the reconstruction must be initiated manually. Thus, without human intervention, a Windows system that did not rebuild after a first failure remains susceptible to a second failure, which increases the window of vulnerabil- ity. It does repair quickly once told to do so.

> 重建，必须手动启动重建。因此，在没有人类干预的情况下，第一次失败后未重建的窗户系统仍然容易受到第二次故障的影响，这增加了脆弱性的窗口。它确实迅速修复了这样做。

The fault injection experiments also provided insight into other availability policies of Linux, Solaris, and Windows 2000 concerning automatic spare utiliza- tion, reconstruction rates, transient errors, and so on. Again, no system documented their policies.

> 故障注入实验还洞悉了 Linux，Solaris 和 Windows 2000 的其他可用性策略，这些策略涉及自动备用利用，重建率，瞬态错误等。同样，没有系统记录他们的政策。

In terms of managing transient faults, the fault injection experiments revealed that Linux’s software RAID implementation takes an opposite approach than do the RAID implementations in Solaris and Windows. The Linux implementation is paranoid—it would rather shut down a disk in a controlled manner at the first error, rather than wait to see if the error is transient. In contrast, Solaris and Win- dows are more forgiving—they ignore most transient faults with the expectation that they will not recur. Thus, these systems are substantially more robust to tran- sients than the Linux system. Note that both Windows and Solaris do log the tran- sient faults, ensuring that the errors are reported even if not acted upon. When faults were permanent, the systems behaved similarly.

> 在管理瞬态故障方面，故障注入实验表明，Linux 的软件 RAID 实现与 Solaris 和 Windows 中的 RAID 实现相反。Linux 实现是偏执的 - 宁愿在第一个错误以受控方式关闭磁盘，而不是等待查看错误是否是瞬态。相比之下，Solaris 和 Windows 更加宽容 - 他们忽略了最短暂的故障，因为他们期望它们不会复发。因此，这些系统比 Linux 系统更适合转移。请注意，Windows 和 Solaris 都会记录偏差的故障，以确保即使不采取行动也会报告错误。当故障是永久性时，系统的行为类似。
