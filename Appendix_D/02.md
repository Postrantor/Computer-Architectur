## Advanced Topics in Disk Storage

> ##磁盘存储中的高级主题

The disk industry historically has concentrated on improving the capacity of disks. Improvement in capacity is customarily expressed as improvement in _areal density_, measured in bits per square inch:

> 磁盘行业历史上一直集中在提高磁盘的能力上。容量的提高通常表示为_areal 密度的改进，以每平方英寸的位数测量：

Areal density ¼ Trackson a disk surface × Bits on a track

> 面积密度 ¼ 跟踪轨道上的磁盘表面 × 轨道位

Through about 1988, the rate of improvement of areal density was 29% per year, thus doubling density every 3 years. Between then and about 1996, the rate improved to 60% per year, quadrupling density every 3 years and matching the traditional rate of DRAMs. From 1997 to about 2003, the rate increased to 100%, doubling every year. After the innovations that allowed this renaissances had largely played out, the rate has dropped recently to about 30% per year. In 2011, the highest density in commercial products is 400 billion bits per square inch. Cost per gigabyte has dropped at least as fast as areal density has increased, with smaller diameter drives playing the larger role in this improve- ment. Costs per gigabyte improved by almost a factor of 1,000,000 between 1983 and 2011.

> 到 1988 年左右，面积密度的改善速度为每年 29％，因此每 3 年加倍密度。从那时到 1996 年左右，每年的比率提高到每年 60％，每 3 年加密四倍，并与传统的 DRAM 相匹配。从 1997 年到 2003 年左右，汇率增加到 100％，每年翻了一番。在允许这种复兴的创新大部分结束后，该利率最近下降到每年约 30％。2011 年，商业产品的最高密度为每平方英寸 4000 亿位。每千兆字节的成本至少降低了，而面积密度的增加，直径较小的驱动器在这种改进中起着更大的作用。在 1983 年至 2011 年之间，每千兆字节的成本几乎提高了 100 万倍。

Magnetic disks have been challenged many times for supremacy of secondary storage. [Figure D.1](#_bookmark543) shows one reason: the fabled _access time gap_ between disks and DRAM. DRAM latency is about 100,000 times less than disk, and that per- formance advantage costs 30 to 150 times more per gigabyte for DRAM.

> 磁盘已受到次要存储至高无上的挑战。[图 D.1](#_ bookmark543)显示了一个原因：传说中的_access 时间间隙_在磁盘和 DRAM 之间。DRAM 等待时间比磁盘少 100,000 倍，并且这种功能优势的价格为 DRAM 的每 GB 次数高 30 至 150 倍。

The bandwidth gap is more complex. For example, a fast disk in 2011 transfers at 200 MB/sec from the disk media with 600 GB of storage and costs about $400. A 4 GB DRAM module costing about $200 in 2011 could transfer at 16,000 MB/ sec (see [Chapter 2](#_bookmark46)), giving the DRAM module about 80 times higher bandwidth than the disk. However, the bandwidth per GB is 6000 times higher for DRAM, and the bandwidth per dollar is 160 times higher.

> 带宽差距更为复杂。例如，2011 年的快速磁盘从磁盘媒体以 200 MB/秒的价格转移，存储量为 600 GB，费用约为 400 美元。一个 4 GB DRAM 模块在 2011 年的价格约为 200 美元，可以以 16,000 MB/秒的价格转移(请参阅[第 2 章](#_ bookmark46))，使 DRAM 模块的带宽比磁盘高约 80 倍。但是，DRAM 的每 GB 带宽高 6000 倍，每美元的带宽是 160 倍。

Many have tried to invent a technology cheaper than DRAM but faster than disk to fill that gap, but thus far all have failed. Challengers have never had a prod- uct to market at the right time. By the time a new product ships, DRAMs and disks have made advances as predicted earlier, costs have dropped accordingly, and the challenging product is immediately obsolete.

> 许多人试图发明比 DRAM 便宜的技术，但比磁盘更快以填补该空白，但到目前为止，所有技术都失败了。挑战者从来没有在正确的时间进行推销。到新产品，DRAM 和磁盘如前所述取得了进步时，成本已经相应下降，并且具有挑战性的产品立即过时。

The closest challenger is Flash memory. This semiconductor memory is non- volatile like disks, and it has about the same bandwidth as disks, but latency is 100 to 1000 times faster than disk. In 2011, the price per gigabyte of Flash was 15 to 20 times cheaper than DRAM. Flash is popular in cell phones because it comes in much smaller capacities and it is more power efficient than disks, despite the cost per gigabyte being 15 to 25 times higher than disks. Unlike disks and DRAM,

> 最接近的挑战者是闪存。该半导体内存像磁盘一样不稳定，并且具有与磁盘大致相同的带宽，但延迟比磁盘快 100 至 1000 倍。2011 年，Flash 的每千兆字节的价格比 DRAM 便宜 15 至 20 倍。Flash 在手机中很受欢迎，因为它具有小得多的能力，并且比磁盘更有效，尽管每 GB 的费用比磁盘高 15 至 25 倍。与磁盘和 DRAM 不同，

![](./media/image599.png)

Figure D.1 Cost versus access time for DRAM and magnetic disk in 1980, 1985, 1990, 1995, 2000, and 2005. The two-order-of-magnitude gap in cost and five-order-of-magnitude gap in access times between semiconductor mem- ory and rotating magnetic disks have inspired a host of competing technologies to try to fill them. So far, such attempts have been made obsolete before production by improvements in magnetic disks, DRAMs, or both. Note that between 1990 and 2005 the cost per gigabyte DRAM chips made less improvement, while disk cost made dra- matic improvement.

> 图 D.1 在 1980 年，1985 年，1990 年，1995 年，2000 年和 2005 年的 DRAM 和磁盘的成本与访问时间。在半导体的内存和旋转磁盘之间，启发了许多竞争技术，试图填充它们。到目前为止，通过改进磁盘，DRAM 或两者的改善，已经使此类尝试过时了。请注意，在 1990 年至 2005 年之间，每千兆头 DRAM 芯片的成本降低了改善，而磁盘成本则取得了进步。

Flash memory bits wear out—typically limited to 1 million writes—and so they are not popular in desktop and server computers.

> 闪存位磨损(通常限制为 100 万写字)，因此它们在台式机和服务器计算机中不受欢迎。

While disks will remain viable for the foreseeable future, the conventional sector-track-cylinder model did not. The assumptions of the model are that nearby blocks are on the same track, blocks in the same cylinder take less time to access since there is no seek time, and some tracks are closer than others.

> 尽管磁盘在可预见的未来将保持可行性，但常规部门轨道缸模型却不可行。该模型的假设是，附近的块在同一轨道上，同一气缸中的块需要较少的时间来访问，因为没有寻求时间，并且有些轨道比其他轨道更接近。

First, disks started offering higher-level intelligent interfaces, like ATA and SCSI, when they included a microprocessor inside a disk. To speed up sequential transfers, these higher-level interfaces organize disks more like tapes than like ran- dom access devices. The logical blocks are ordered in serpentine fashion across a single surface, trying to capture all the sectors that are recorded at the same bit den- sity. (Disks vary the recording density since it is hard for the electronics to keep up with the blocks spinning much faster on the outer tracks, and lowering linear den- sity simplifies the task.) Hence, sequential blocks may be on different tracks. We will see later in [Figure D.22](#_bookmark563) on page D-45 an illustration of the fallacy of assuming the conventional sector-track model when working with modern disks.

> 首先，磁盘开始提供更高级别的智能接口，例如 ATA 和 SCSI，当时它们在磁盘中包括微处理器。为了加快顺序传输的速度，这些高级接口组织的磁盘更像是磁带，而不是像 Ran-dom Access 设备一样。逻辑块在单个表面上以蛇形的方式排序，试图捕获所有以相同位置记录的扇区。(磁盘改变了记录密度，因为电子设备很难跟上外部轨道上旋转速度更快的块，并且降低线性降低可简化任务。)因此，顺序块可能在不同的轨道上。我们将在[图 D.22](#_ bookmark563)的稍后在第 D-45 页上看到，以说明使用现代磁盘时假设传统部门模型的谬论。

Second, shortly after the microprocessors appeared inside disks, the disks included buffers to hold the data until the computer was ready to accept it, and later caches to avoid read accesses. They were joined by a command queue that allowed the disk to decide in what order to perform the commands to maximize perfor- mance while maintaining correct behavior. [Figure D.2](#_bookmark544) shows how a queue depth of 50 can double the number of I/Os per second of random I/Os due to better sched- uling of accesses. Although it’s unlikely that a system would really have 256 com- mands in a queue, it would triple the number of I/Os per second. Given buffers, caches, and out-of-order accesses, an accurate performance model of a real disk is much more complicated than sector-track-cylinder.

> 其次，在微处理器出现在磁盘内不久之后，磁盘包括缓冲区以保存数据，直到计算机准备好接受它，然后避免使用读取访问。他们加入了命令队列，该命令队允许磁盘决定执行命令以最大程度地提高性能的顺序，同时保持正确的行为。[图 D.2](#_ bookmark544)显示了 50 的队列深度如何将随机 I/OS 的每秒 I/O 的数量增加一倍，这是由于访问的时间安排更好。尽管系统不太可能在队列中真正拥有 256 个命令，但它将每秒 I/OS 的数量增加三倍。给定缓冲液，缓存和越常访问，实际磁盘的准确性能模型比扇形轨道缸更复杂。

Figure D.2 Throughput versus command queue depth using random 512- byte reads. The disk performs 170 reads per second starting at no command queue and doubles performance at 50 and triples at 256 \[Anderson 2003\].

> 图 D.2 使用随机 512-字节读取。该磁盘每秒以 No 命令队列的启动执行 170 次读取，并以 50 的速度加倍，并在 256 \ [Anderson 2003 \]时进行三倍。

Finally, the number of platters shrank from 12 in the past to 4 or even 1 today, so the cylinder has less importance than before because the percentage of data in a cylinder is much less.

> 最后，拼盘的数量从过去的 12 个缩水到今天的 4 个甚至 1 个，因此圆柱体的重要性比以前的重要性较小，因为气缸中的数据百分比要少得多。

### Disk Power

> ###磁盘功率

Power is an increasing concern for disks as well as for processors. A typical ATA disk in 2011 might use 9 watts when idle, 11 watts when reading or writing, and 13 watts when seeking. Because it is more efficient to spin smaller mass, smaller- diameter disks can save power. One formula that indicates the importance of rota- tion speed and the size of the platters for the power consumed by the disk motor is the following \[Gurumurthi et al. 2005\]:

> 对磁盘和处理器的电源越来越关注。2011 年的典型 ATA 磁盘可能会使用 9 瓦，当闲置时闲置，阅读或写作时 11 瓦以及在寻找时 13 瓦。因为旋转较小的质量更有效，所以较小的直径磁盘可以节省动力。一种表明旋转速度和盘子大小对磁盘电机消耗的功率的重要性的公式是以下\ [Gurumurthi 等人。2005 \]：

Figure D.3 shows the specifications of two 3.5-inch disks in 2011. The _Serial_

> 图 D.3 显示了 2011 年两个 3.5 英寸磁盘的规格。

_ATA_ (SATA) disks shoot for high capacity and the best cost per gigabyte, so the 2000 GB drives cost less than $0.05 per gigabyte. They use the widest platters that fit the form factor and use four or five of them, but they spin at 5900 RPM and seek relatively slowly to allow a higher areal density and to lower power. The corre- sponding _Serial Attach SCSI_ (SAS) drive aims at performance, so it spins at 15,000 RPM and seeks much faster. It uses a lower areal density to spin at that high rate. To reduce power, the platter is much narrower than the form factor. This combination reduces capacity of the SAS drive to 600 GB.

> _ATA_(SATA)磁盘以高容量和每 GB 的最佳成本拍摄，因此 2000 GB 驱动器的价格低于每 GB 的 0.05 美元。他们使用适合外形并使用其中四个或五个的最宽的拼盘，但它们以 5900 rpm 的速度旋转，并寻求相对较慢的速度，以允许更高的面积密度和降低功率。Corre-Sponding _serial 附加 SCSI_(SAS)驱动器针对性能，因此它以 15,000 rpm 的速度旋转，并寻求更快的速度。它使用较低的面积密度以高速率旋转。为了降低功率，盘子比外形要窄得多。这种组合将 SAS 驱动器的容量降低到 600 GB。

The cost per gigabyte is about a factor of five better for the SATA drives, and, conversely, the cost per I/O per second or MB transferred per second is about a factor of five better for the SAS drives. Despite using smaller platters and many fewer of them, the SAS disks use twice the power of the SATA drives, due to the much faster RPM and seeks.

> 对于 SATA 驱动器而言，每千兆字节的成本约为五倍，相反，每秒转移的每秒 I/O 成本约为 SAS 驱动器的五倍。尽管使用了较小的拼盘，而 SAS 磁盘的使用速度更快，但 SAS 磁盘使用了 SATA 驱动器的两倍。

Figure D.3 Serial ATA (SATA) versus Serial Attach SCSI (SAS) drives in 3.5-inch form factor in 2011. The I/Os per second were calculated using the average seek plus the time for one-half rotation plus the time to transfer one sector of 512 KB.

> 图 D.3 串行 ATA(SATA)与串行连接 SCSI(SAS)驱动器在 2011 年为 3.5 英寸外形驱动器。使用平均搜索和一半旋转的时间，以及时间到转移一个 512 kb 的扇区。

### Advanced Topics in Disk Arrays

> ###磁盘阵列中的高级主题

An innovation that improves both dependability and performance of storage systems is _disk arrays_. One argument for arrays is that potential throughput can be increased by having many disk drives and, hence, many disk arms, rather than fewer large drives. Simply spreading data over multiple disks, called _striping_, auto- matically forces accesses to several disks if the data files are large. (Although arrays improve throughput, latency is not necessarily improved.) As we saw in [Chapter 1](#_bookmark2), the drawback is that with more devices, dependability decreases: _N_ devices generally have 1/_N_ the reliability of a single device.

> 提高存储系统可靠性和性能的创新是_Disk Arrays_。阵列的一个论点是，可以通过拥有许多磁盘驱动器，因此可以增加许多磁盘臂，而不是更少的大型驱动器来增加潜在的吞吐量。仅将数据传播到多个磁盘上，称为_Striping_，如果数据文件大，则自动强制访问几个磁盘。(尽管数组改善了吞吐量，但不一定会改善延迟。)正如我们在[第 1 章](#_ bookmark2)中看到的，缺点是，使用更多设备，可靠性降低：_n_设备通常具有 1/_n_的可靠性。。

Although a disk array would have more faults than a smaller number of larger disks when each disk has the same reliability, dependability is improved by adding redundant disks to the array to tolerate faults. That is, if a single disk fails, the lost information is reconstructed from redundant information. The only danger is in having another disk fail during the _mean time to repair_ (MTTR). Since the _mean time to failure_ (MTTF) of disks is tens of years, and the MTTR is measured in hours, redundancy can make the measured reliability of many disks much higher than that of a single disk.

> 尽管当每个磁盘具有相同的可靠性时，磁盘阵列比较小数量的较大磁盘会有更多的故障，但是通过将冗余磁盘添加到阵列中以忍受故障来提高可靠性。也就是说，如果单个磁盘失败，则丢失的信息将从冗余信息中重建。唯一的危险是在_的时间修理时间(MTTR)中让另一个磁盘失败。由于圆盘的失败时间_(MTTF)是数十年的数十年，并且 MTTR 以小时数测量，因此冗余可以使许多磁盘的可靠性远高于单个磁盘的可靠性。

Such redundant disk arrays have become known by the acronym _RAID_, which originally stood for _redundant array of inexpensive disks_, although some prefer the word _independent_ for _I_ in the acronym. The ability to recover from failures plus the higher throughput, measured as either megabytes per second or I/Os per second, make RAID attractive. When combined with the advantages of smaller size and lower power of small-diameter drives, RAIDs now dominate large-scale storage systems. Figure D.4 summarizes the five standard RAID levels, showing how eight disks of user data must be supplemented by redundant or check disks at each RAID level, and it lists the pros and cons of each level. The standard RAID levels are well documented, so we will just do a quick review here and discuss advanced levels in more depth.

> 这样的冗余磁盘数组已通过首字母缩写_RAID_知道，最初代表了廉价的磁盘的 redlection 阵列，尽管有些人在首字母缩写词中更喜欢_independent_ for _i_的单词。从失败加上较高的吞吐量中恢复的能力，以每秒兆字节或每秒 I/OS 的速度测量，使突袭有吸引力。当与小直径驱动器的尺寸和较低功率的优势结合使用时，突袭现在主导了大规模存储系统。图 D.4 总结了五个标准的 RAID 级别，显示了如何在每个 RAID 级别上用冗余或检查磁盘补充八个磁盘，并列出了每个级别的优缺点。标准的突袭级别有充分的文献记载，因此我们将在此处快速回顾，并更深入地讨论高级级别。

- _RAID 0_—It has no redundancy and is sometimes nicknamed _JBOD_, for _just a bunch of disks_, although the data may be striped across the disks in the array. This level is generally included to act as a measuring stick for the other RAID levels in terms of cost, performance, and dependability.

> - _raid 0_-它没有冗余，有时被昵称为_jbod_，对于一堆磁盘_，尽管数据可能会在数组中的磁盘上加入。通常，该水平在成本，绩效和可靠性方面充当其他 RAID 水平的测量棒。

- _RAID 1_—Also called _mirroring_ or _shadowing_, there are two copies of every piece of data. It is the simplest and oldest disk redundancy scheme, but it also has the highest cost. Some array controllers will optimize read performance by allowing the mirrored disks to act independently for reads, but this optimiza- tion means it may take longer for the mirrored writes to complete.

> - _raid 1_-也称为_ mirroring_或_ shadowing_，每个数据有两份副本。它是最简单，最古老的磁盘冗余方案，但成本也最高。一些数组控制器将通过允许镜像磁盘独立起作用来进行读取，从而优化读取性能，但是这种优化意味着镜像的写入可能需要更长的时间才能完成。

- _RAID 2_—This organization was inspired by applying memory-style error- correcting codes (ECCs) to disks. It was included because there was such a disk array product at the time of the original RAID paper, but none since then as other RAID organizations are more attractive.

> - _RAID 2_-该组织的启发是通过将内存式错误纠正代码(ECC)应用于磁盘的启发。它之所以包括在原始 RAID 纸时有这样的磁盘阵列产品，但是从那时起，由于其他突袭组织更具吸引力。

- _RAID 3_—Since the higher-level disk interfaces understand the health of a disk, it’s easy to figure out which disk failed. Designers realized that if one extra disk RAID level

> - _RAID 3_  - 由于高级磁盘接口了解磁盘的健康，因此很容易找出哪些磁盘失败。设计师意识到，如果一个额外的磁盘突袭级别

Figure D.4 RAID levels, their fault tolerance, and their overhead in redundant disks. The paper that introduced the term _RAID_ \[Patterson, Gibson, and Katz 1987\] used a numerical classification that has become popular. In fact, the non- redundant disk array is often called _RAID 0_, indicating that the data are striped across several disks but without redun- dancy. Note that mirroring (RAID 1) in this instance can survive up to eight disk failures provided only one disk of each mirrored pair fails; worst case is both disks in a mirrored pair fail. In 2011, there may be no commercial implementations of RAID 2; the rest are found in a wide range of products. RAID 0 + 1, 1 + 0, 01, 10, and 6 are discussed in the text.

> 图 D.4 突袭水平，其容错性以及其在冗余磁盘中的开销。引入术语_raid_ \ [Patterson，Gibson 和 Katz 1987 \]的论文使用了已流行的数值分类。实际上，非冗余磁盘阵列通常称为_raid 0_，表明数据是在几个磁盘上条纹，但没有重新播放。请注意，在这种情况下，镜像(RAID 1)最多可以生存八个磁盘故障，只提供了每个镜像对失败的一个磁盘；最坏的情况是镜像对中的两个磁盘失败。在 2011 年，可能没有突袭 2 的商业实施；其余的是各种产品。文本中讨论了 RAID 0 + 1、1 + 0、01、10 和 6。

contains the parity of the information in the data disks, a single disk allows recovery from a disk failure. The data are organized in stripes, with _N_ data blocks and one parity block. When a failure occurs, we just “subtract” the good data from the good blocks, and what remains is the missing data. (This works whether the failed disk is a data disk or the parity disk.) RAID 3 assumes that the data are spread across all disks on reads and writes, which is attractive when reading or writing large amounts of data.

> 包含数据磁盘中信息的均衡性，单个磁盘允许从磁盘故障中恢复。数据以条纹组织，具有_n_数据块和一个奇偶校验块。发生故障时，我们只是从良好的块中“减去”好数据，而剩下的就是丢失的数据。(这可以通过失败的磁盘为数据磁盘还是奇偶磁盘。)RAID 3 假设数据分布在读取和写入的所有磁盘上，这在读取或编写大量数据时很有吸引力。

- _RAID 4_—Many applications are dominated by small accesses. Since sectors have their own error checking, you can safely increase the number of reads per second by allowing each disk to perform independent reads. It would seem that writes would still be slow, if you have to read every disk to calculate parity. To increase the number of writes per second, an alternative approach involves only two disks. First, the array reads the old data that are about to be overwrit- ten, and then calculates what bits would change before it writes the new data. It then reads the old value of the parity on the check disks, updates parity accord- ing to the list of changes, and then writes the new value of parity to the check disk. Hence, these so-called “small writes” are still slower than small reads— they involve four disks accesses—but they are faster than if you had to read all disks on every write. RAID 4 has the same low check disk overhead as RAID 3, and it can still do large reads and writes as fast as RAID 3 in addition to small reads and writes, but control is more complex.

> - _RAID 4_-许多应用程序以小型访问为主导。由于扇区有自己的错误检查，因此您可以通过允许每个磁盘执行独立读取来安全地增加每秒读数的数量。如果您必须阅读每个磁盘才能计算平等，看来写作仍然很慢。为了增加每秒写入的数量，另一种方法仅涉及两个磁盘。首先，该数组读取即将覆盖的旧数据，然后在编写新数据之前计算出来的更改。然后，它读取支票磁盘上奇偶校验的旧值，根据更改列表的更新均衡，然后写入与支票磁盘的均等价值。因此，这些所谓的“小写作”仍然比小读数慢 - 它们涉及四个磁盘访问 - 但是它们比您必须在每本写入时都要阅读所有磁盘要快。RAID 4 具有与 RAID 3 相同的低检查磁盘顶开：它仍然可以进行大量读取和写入 RAID 3 之外的速度和写入，除了小读写和写入外，但控制更为复杂。

- _RAID 5_—Note that a performance flaw for small writes in RAID 4 is that they all must read and write the same check disk, so it is a performance bottleneck. RAID 5 simply distributes the parity information across all disks in the array, thereby removing the bottleneck. The parity block in each stripe is rotated so that parity is spread evenly across all disks. The disk array controller must now calculate which disk has the parity for when it wants to write a given block, but that can be a simple calculation. RAID 5 has the same low check disk overhead as RAID 3 and 4, and it can do the large reads and writes of RAID 3 and the small reads of RAID 4, but it has higher small write bandwidth than RAID 4. Nevertheless, RAID 5 requires the most sophisticated controller of the classic RAID levels.

> - _RAID 5_-注意 RAID 4 中的小写入性能缺陷是，他们都必须读取并编写相同的检查磁盘，因此这是性能瓶颈。RAID 5 只需在阵列中的所有磁盘上分发奇偶校验信息，从而消除了瓶颈。每个条带中的奇偶校验块旋转，因此平均值均匀地散布在所有磁盘上。磁盘数组控制器现在必须计算哪个磁盘在要编写给定块时具有奇偶元，但这可以简单地计算。RAID 5 具有与 RAID 3 和 4 相同的低支票磁盘高架，它可以进行 RAID 3 和 RAID 4 的小读物的大读物和写作，但是它比 RAID 4 具有更高的写入带宽。需要经典突袭级别中最复杂的控制器。

Having completed our quick review of the classic RAID levels, we can now look at two levels that have become popular since RAID was introduced.

> 完成了对经典突袭级别的快速回顾后，我们现在可以研究自引入 RAID 以来已经流行的两个级别。

##### _RAID 10 versus 01 (or 1 + 0 versus RAID 0 + 1)_

> ##### _raid 10 对 01(或 1 + 0 对 RAID 0 + 1)_

One topic not always described in the RAID literature involves how mirroring in RAID 1 interacts with striping. Suppose you had, say, four disks’ worth of data to store and eight physical disks to use. Would you create four pairs of disks—each organized as RAID 1—and then stripe data across the four RAID 1 pairs? Alter- natively, would you create two sets of four disks—each organized as RAID 0—and then mirror writes to both RAID 0 sets? The RAID terminology has evolved to call the former RAID 1 + 0 or RAID 10 (“striped mirrors”) and the latter RAID 0 + 1 or RAID 01 (“mirrored stripes”).

> RAID 文献中并非总是描述的一个主题涉及 RAID 1 中的镜像如何与条纹相互作用。假设您有四个磁盘的数据值存储，并且可以使用八个物理磁盘。您是否会创建四对磁盘(以 RAID 1 为组织)，然后在四个 RAID 1 对中进行条纹数据？一方面，您是否会创建两组四个磁盘(以 RAID 0 为组织)，然后镜像写入两套 RAID 0 套装？突袭术语已经演变为将前突袭 1 + 0 或 RAID 10(“条纹镜子”)和后者突袭 0 + 1 或 RAID 01(“镜像条纹”)。

##### _RAID 6: Beyond a Single Disk Failure_

> ##### _raid 6：超越单个磁盘故障_

The parity-based schemes of the RAID 1 to 5 protect against a single self- identifying failure; however, if an operator accidentally replaces the wrong disk during a failure, then the disk array will experience two failures, and data will be lost. Another concern is that since disk bandwidth is growing more slowly than disk capacity, the MTTR of a disk in a RAID system is increasing, which in turn increases the chances of a second failure. For example, a 500 GB SATA disk could take about 3 hours to read sequentially assuming no interference. Given that the damaged RAID is likely to continue to serve data, reconstruction could be stretched considerably, thereby increasing MTTR. Besides increasing reconstruc- tion time, another concern is that reading much more data during reconstruction means increasing the chance of an uncorrectable media failure, which would result in data loss. Other arguments for concern about simultaneous multiple failures are the increasing number of disks in arrays and the use of ATA disks, which are slower and larger than SCSI disks.

> RAID 1 至 5 的基于平价的方案可以防止单一的自我识别失败；但是，如果操作员在故障过程中意外替换了错误的磁盘，则磁盘阵列将遇到两个故障，并且数据将丢失。另一个问题是，由于磁盘带宽的增长比磁盘容量较慢，因此 RAID 系统中磁盘的 MTTR 正在增加，进而增加了第二次故障的机会。例如，500 GB SATA 磁盘可能需要大约 3 个小时才能顺序读取没有干扰。鉴于受损的突袭可能会继续提供数据，因此可以大大扩展重建，从而增加 MTTR。除了增加重新构成时间外，另一个问题是，在重建过程中阅读更多数据意味着增加了无法纠正的媒体故障的机会，这将导致数据丢失。关于同时多次故障的其他争论是，阵列中的磁盘数量增加，并且使用 ATA 磁盘的使用速度较慢，并且比 SCSI 磁盘较慢且大。

Hence, over the years, there has been growing interest in protecting against more than one failure. Network Appliance (NetApp), for example, started by build- ing RAID 4 file servers. As double failures were becoming a danger to customers, they created a more robust scheme to protect data, called _row-diagonal parity_ or _RAID-DP_ \[Corbett et al. 2004\]. Like the standard RAID schemes, row-diagonal parity uses redundant space based on a parity calculation on a per-stripe basis. Since it is protecting against a double failure, it adds two check blocks per stripe of data. Let’s assume there are _p_ + 1 disks total, so _p_ 1 disks have data. [Figure D.5](#_bookmark545) shows the case when _p_ is 5.

> 因此，多年来，人们对保护多个失败的兴趣越来越大。例如，网络设备(NETAPP)是通过构建 RAID 4 文件服务器开始的。随着双重失败对客户的危险，他们创建了一个更强大的方案来保护数据，称为_ROW-DIAGONAL PARITY_或_RAID-DP_ \ [CORBETT 等人。2004 \]。像标准 RAID 方案一样，行式奇偶校验基于平等计算的冗余空间。由于它可以防止双重故障，因此每个数据条添加了两个检查块。假设总共有_p_ + 1 个磁盘，所以_p_ 1 个磁盘具有数据。[图 D.5](#_ bookmark545)显示了_p_为 5 的情况。

The row parity disk is just like in RAID 4; it contains the even parity across the other four data blocks in its stripe. Each block of the diagonal parity disk contains the even parity of the blocks in the same diagonal. Note that each diagonal does not cover one disk; for example, diagonal 0 does not cover disk 1. Hence, we need just _p_ 1 diagonals to protect the _p_ disks, so the disk only has diagonals 0 to 3 in [Figure D.5](#_bookmark545).

> 行均值磁盘就像突袭 4 中一样；它包含其条纹中其他四个数据块的平均值。对角均衡磁盘的每个块都包含相同对角线中块的平均值。请注意，每个对角线都不覆盖一个磁盘；例如，对角线 0 不涵盖磁盘 1.因此，我们只需要_p_ 1 对角即可保护_p_磁盘，因此磁盘仅在[图 D.5]中具有对角线 0 到 3(#_ bookmark545)。

Let’s see how row-diagonal parity works by assuming that data disks 1 and 3 fail in [Figure D.5](#_bookmark545). We can’t perform the standard RAID recovery using the first row using row parity, since it is missing two data blocks from disks 1 and 3. How- ever, we can perform recovery on diagonal 0, since it is only missing the data block associated with disk 3. Thus, row-diagonal parity starts by recovering one of the four blocks on the failed disk in this example using diagonal parity. Since each diagonal misses one disk, and all diagonals miss a different disk, two diagonals are only missing one block. They are diagonals 0 and 2 in this example, so we next restore the block from diagonal 2 from failed disk 1. When the data for those blocks have been recovered, then the standard RAID recovery scheme can be used to recover two more blocks in the standard RAID 4 stripes 0 and 2, which in turn allows us to recover more diagonals. This process continues until two failed disks are completely restored.

> 让我们通过假设数据磁盘 1 和 3 在[图 D.5]中失败(#_ bookmark545)，从而查看行 - 划分均等的工作原理。我们无法使用第一行使用行奇偶校验执行标准 RAID 恢复，因为它缺少磁盘 1 和 3 的两个数据块。但是，我们可以在对角线 0 上执行恢复，因为它仅缺少数据块因此，与磁盘 3 相关联，因此，在本示例中，使用对角线奇偶校验在本示例中恢复了失败磁盘上的四个块之一，首先是行为。由于每个对角线都会错过一个磁盘，并且所有对角线都会错过另一个磁盘，因此两个对角线仅缺少一个块。它们是对角线 0 和 2 在此示例中，因此我们接下来从对角 2 中恢复了失败磁盘 1 的块。标准 RAID 4 条纹 0 和 2，这又使我们能够恢复更多的对角线。这个过程一直持续到两个失败的磁盘完全恢复为止。

Figure D.5 Row diagonal parity for _p_ 55, which protects four data disks from double failures \[Corbett et al. 2004\]. This figure shows the diagonal groups for which parity is calculated and stored in the diagonal parity disk. Although this shows all the check data in separate disks for row parity and diagonal parity as in RAID 4, there is a rotated version of row-diagonal parity that is analogous to RAID 5. Parameter _p_ must be prime and greater than 2; however, you can make _p_ larger than the number of data disks by assum- ing that the missing disks have all zeros and the scheme still works. This trick makes it easy to add disks to an existing system. NetApp picks _p_ to be 257, which allows the sys- tem to grow to up to 256 data disks.

> 图 D.5 _p_ 55 的行对角线奇偶校验，它保护四个数据磁盘免受双重故障\ [Corbett 等。2004 \]。该图显示了在对角线均衡磁盘中计算和存储的对角线组。尽管这显示了 RAID 4 中的行奇偶校验和对角线奇偶校验中的单独磁盘中的所有检查数据，但有一个类似于 RAID 5 的行旋转版本的行式旋转版本。但是，您可以通过假设缺失的磁盘具有所有零，并且该方案仍然有效，可以使_p_大于数据磁盘数量。此技巧使得在现有系统中添加磁盘变得容易。NetApp 选择_p_为 257，允许系统生长到最多 256 个数据磁盘。

The EVEN-ODD scheme developed earlier by researchers at IBM is similar to row diagonal parity, but it has a bit more computation during operation and recovery \[Blaum 1995\]. Papers that are more recent show how to expand EVEN-ODD to protect against three failures \[Blaum, Bruck, and Vardy 1996; Blaum et al. 2001\].

> IBM 研究人员较早开发的均匀 ODD 方案类似于行对角线均等，但是在操作和恢复过程中，它具有更多的计算\ [Blaum 1995 \]。最近的论文显示了如何扩展偶数偶数以防止三个失败\ [Blaum，Bruck 和 Vardy 1996；Blaum 等。2001 \]。
