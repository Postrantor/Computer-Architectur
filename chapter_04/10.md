## Historical Perspective and References

Section M.6 (available online) features a discussion on the Illiac IV (a representative of the early SIMD architectures) and the Cray-1 (a representative of vector architec- tures). We also look at multimedia SIMD extensions and the history of GPUs.

## Case Study and Exercises by Jason D. Bakos

### Case Study: Implementing a Vector Kernel on a Vector Processor and GPU

##### _Concepts illustrated by this case study_

- Programming Vector Processors
- Programming GPUs
- Performance Estimation

MrBayes is a popular computational biology application for inferring the evolu- tionary histories among a set of input species based on their prealigned DNA sequence data of length _n_. MrBayes works by performing an heuristic search over the space of all binary tree topologies for which the inputs are the leaves. In order to evaluate a particular tree, the application must compute an _n_ 4 conditional like- lihood table (named clP) for each interior node. The table is a function of the con- ditional likelihood tables of the node’s two descendent nodes (clL and clR, single precision floating point) and the 4 4 transition probability table (tiPL and tiPR, single precision floating point). One of this application’s kernels is the com- putation of this conditional likelihood table and is shown as follows:

Write code for RISC-V and RV64V. Assume the starting addresses of tiPL, tiPR, clL, clR, and clP are in RtiPL, RtiPR, RclL, RclR, and RclP, respectively. Do not unroll the loop. To facilitate vector addition reductions, assume that we add the following instructions to RV64V:

Vector Summation Reduction Single Precision:

This instruction performs a summation reduction on a vector register Vs, writ- ing to the sum into scalar register Fd.

1. [5] <4.1, 4.2> Assuming seq_length == 500, what is the dynamic instruction count for both implementations?
2. [25] <4.1, 4.2> Assume that the vector reduction instruction is executed on the vector functional unit, similar to a vector add instruction. Show how the code sequence lays out in convoys assuming a single instance of each vector functional unit. How many chimes will the code require? How many cycles per FLOP are needed, ignoring vector instruction issue overhead?
3. [15] <4.1, 4.2> Consider the possibility of unrolling the loop and mapping mul- tiple iterations to vector operations. Assume that you can use scatter-gather loads and stores (vldi and vsti). How does this affect the way you can write the RV64Vcode for this kernel?
4. [25] <4.4> Now assume we want to implement the MrBayes kernel on a GPU using a single thread block. Rewrite the C code of the kernel using CUDA. Assume that pointers to the conditional likelihood and transition probability tables are specified as parameters to the kernel. Invoke one thread for each iter- ation of the loop. Load any reused values into shared memory before performing operations on it.
5. [15] <4.4> With CUDA we can use coarse-grain parallelism at the block level to compute the conditional likelihood of multiple nodes in parallel. Assume that we want to compute the conditional likelihood from the bottom of the tree up. Assume seq*length 500 for all notes and that the group of tables for each of the 12 leaf nodes is stored in consecutive memory locations in the order of node number (e.g., the mth element of clP on node n is at clP [\_n*\*4\*seq*length + \_m*\*4]). Assume that we want to compute the conditional likelihood for nodes 12–17, as shown in [Figure 4.35](#_bookmark212). Change the method by which you compute the array indices in your answer from Exercise 4.5 to include the block number.
6. [15] <4.4> Convert your code from Exercise 4.6 into PTX code. How many instructions are needed for the kernel?
7. [10] <4.4> How well do you expect this code to perform on a GPU? Explain your answer. Figure 4.35 Sample tree.

### Exercises

Assume that the processor runs at 700 MHz and has a maximum vector length of 64. The load/store unit has a start-up overhead of 15 cycles; the multiply unit, 8 cycles; and the add/subtract unit, 5 cycles.

1. [10] <4.3> What is the arithmetic intensity of this kernel? Justify your answer.
2. [20] <4.2> Convert this loop into RV64V assembly code using strip mining.
3. [20] <4.2> Assuming chaining and a single memory pipeline, how many chimes are required? How many clock cycles are required per complex result value, including start-up overhead?
4. [15] <4.2> If the vector sequence is chained, how many clock cycles are required per complex result value, including overhead?
5. [15] <4.2> Now assume that the processor has three memory pipelines and chaining. If there are no bank conflicts in the loop’s accesses, how many clock cycles are required per result?
6. [30] <4.2,4.3,4.4> In this problem, we will compare the performance of a vector processor with a hybrid system that contains a scalar processor and a GPU-based coprocessor.

In the hybrid system, the host processor has superior scalar perfor- mance to the GPU, so in this case all scalar code is executed on the host processor while all vector code is executed on the GPU. We will refer to the first system as the vector computer and the second system as the hybrid computer. Assume that your target application contains a vector kernel with an arithmetic intensity of 0.5 FLOPs per DRAM byte accessed; however, the application also has a scalar com- ponent that must be performed before and after the kernel in order to prepare the input vectors and output vectors, respectively. For a sample dataset, the scalar por- tion of the code requires 400 ms of execution time on both the vector processor and the host processor in the hybrid system. The kernel reads input vectors consisting of 200 MB of data and has output data consisting of 100 MB of data. The vector processor has a peak memory bandwidth of 30 GB/s and the GPU has a peak mem- ory bandwidth of 150 GB/s. The hybrid system has an additional overhead that requires all input vectors to be transferred between the host memory and GPU local memory before and after the kernel is invoked. The hybrid system has a direct memory access (DMA) bandwidth of 10 GB/s and an average latency of 10 ms. Assume that both the vector processor and GPU are performance bound by mem- ory bandwidth. Compute the execution time required by both computers for this application.

1. [15/25/25] <4.4, 4.5> [Section 4.5](#detecting-and-enhancing-loop-level-parallelism) discussed the reduction operation that reduces a vector down to a scalar by repeated application of an operation. A reduction is a special type of a loop recurrence. An example is shown as follows:

> ===

A vectorizing compiler might apply a transformation called _scalar expansion_, which expands dot into a vector and splits the loop such that the multiply can be performed with a vector operation, leaving the reduction as a separate scalar operation:

> ===

As mentioned in [Section 4.5](#detecting-and-enhancing-loop-level-parallelism), if we allow the floating-point addition to be asso- ciative, there are several techniques available for parallelizing the reduction.

1. [15] <4.4, 4.5> One technique is called recurrence doubling, which adds sequences of progressively shorter vectors (ie, two 32-element vectors, then two 16-element vectors, and so on). Show how the C code would look for executing the second loop in this way.
2. [25] <4.4, 4.5> In some vector processors, the individual elements within the vec- tor registers are addressable. In this case, the operands to a vector operation may be two different parts of the same vector register. This allows another solution for the reduction called _partial sums_. The idea is to reduce the vector to _m_ sums where _m_ is the total latency through the vector functional unit, including the operand read and write times. Assume that the VMIPS vector registers are addressable (e.g., you can initiate a vectoroperation withthe operand V1(16), indicating that the input oper- and begins with element 16). Also, assume that the total latency for adds, including the operand read and result write, is eight cycles. Write a VMIPS code sequence that reduces the contents of V1 to eight partial sums.
3. [25] <4.4, 4.5> When performing a reduction on a GPU, one thread is associ- ated with each element in the input vector. The first step is for each thread to write its corresponding value into shared memory. Next, each thread enters a loop that adds each pair of input values. This reduces the number of elements by half after each iteration, meaning that the number of active threads also reduces by half after each iteration. In order to maximize the performance of the reduction, the number of fully populated warps should be maximized throughout the course of the loop. In other words, the active threads should be contiguous. Also, each thread should index the shared array in such a way as to avoid bank conflicts in the shared memory. The following loop vio- lates only the first of these guidelines and also uses the modulo operator, which is very expensive for GPUs:

> ===

Rewrite the loop to meet these guidelines and eliminate the use of the modulo operator. Assume that there are 32 threads per warp and a bank conflict occurs whenever two or more threads from the same warp reference an index whose modulo by 32 are equal.

1. [10/10/10/10] <4.3> The following kernel performs a portion of the finite- difference time-domain (FDTD) method for computing Maxwell’s equations in a three-dimensional space, part of one of the SPEC06fp benchmarks:

> ===

Assume that dH1, dH2, Hy, Hz, dy, dz, Ca, Cb, and Ex are all single- precision floating-point arrays. Assume IDx is an array of unsigned int.

1. [10] <4.3> What is the arithmetic intensity of this kernel?
2. [10] <4.3> Is this kernel amenable to vector or SIMD execution? Why or why not?
3. [10] <4.3> Assume this kernel is to be executed on a processor that has 30 GB/s of memory bandwidth. Will this kernel be memory bound or compute bound?
4. [10] <4.3> Develop a roofline model for this processor, assuming it has a peak computational throughput of 85 GFLOP/s.

<!-- -->

1. [10/15] <4.4> Assume a GPU architecture that contains 10 SIMD processors. Each SIMD instruction has a width of 32 and each SIMD processor contains 8 lanes for single-precision arithmetic and load/store instructions, meaning that each nondi- verged SIMD instruction can produce 32 results every 4 cycles. Assume a kernel that has divergent branches that causes, on average, 80% of threads to be active. Assume that 70% of all SIMD instructions executed are single-precision arithmetic and 20% are load/store. Because not all memory latencies are covered, assume an average SIMD instruction issue rate of 0.85. Assume that the GPU has a clock speed of 1.5 GHz.
2. [10] <4.4> Compute the throughput, in GFLOP/s, for this kernel on this GPU.
3. [15] <4.4> Assume that you have the following choices:

   1. Increasing the number of single-precision lanes to 16
   2. Increasing the number of SIMD processors to 15 (assume this change doesn’t affect any other performance metrics and that the code scales to the additional processors)
   3. Adding a cache that will effectively reduce memory latency by > 40%, which will increase instruction issue rate to 0.95 What is speedup in throughput for each of these improvements?
4. [10/15/15] <4.5> In this exercise, we will examine several loops and analyze their potential for parallelization.
5. [15] <4.5> In the following loop, find all the true dependences, output depen- dences, and antidependences. Eliminate the output dependences and antidependences by renaming.

Are there dependences between S1 and S2? Is this loop parallel? If not, show how to make it parallel.

1. [10] <4.4> List and describe at least four factors that influence the performance of GPU kernels. In other words, which runtime behaviors that are caused by the ker- nel code cause a reduction in resource utilization during kernel execution?
2. [10] <4.4> Assume a hypothetical GPU with the following characteristics:

- Clock rate 1.5 GHz
- Contains 16 SIMD processors, each containing 16 single-precision floating- point units
- Has 100 GB/s off-chip memory bandwidth

Without considering memory bandwidth, what is the peak single-precision floating-point throughput for this GPU in GLFOP/s, assuming that all memory latencies can be hidden? Is this throughput sustainable given the memory band- width limitation?

1. [60] <4.4> For this programming exercise, you will write and characterize the behavior of a CUDA kernel that contains a high amount of data-level parallelism but also contains conditional execution behavior. Use the NVIDIA CUDA Toolkit along with GPU-SIM from the University of British Columbia ([http://www.gpgpu-](http://www.gpgpu-sim.org/) [sim.org*/*](http://www.gpgpu-sim.org/)) or the CUDA Profiler to write and compile a CUDA kernel that performs 100 iterations of Conway’s Game of Life for a 256 256 game board and returns the final state of the game board to the host. Assume that the board is initialized by the host. Associate one thread with each cell. Make sure you add a barrier after each game iteration. Use the following game rules:

- Any live cell with fewer than two live neighbors dies.
- Any live cell with two or three live neighbors lives on to the next generation.
- Any live cell with more than three live neighbors dies.
- Any dead cell with exactly three live neighbors becomes a live cell.

After finishing the kernel answer the following questions:
