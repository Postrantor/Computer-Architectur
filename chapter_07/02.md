## Guidelines for DSAs

Here are five principles that generally guided the designs of the four DSAs we’ll see in [Sections 7.4–7.7](#googles-tensor-processing-unit-an-inference-data-center-accelerator). Not only do these five guidelines lead to increased area and energy efficiency, they also provide two valuable bonus effects. First, they lead to simpler designs, which reduce the cost of NRE of DSAs (see the fallacy in [Section 7.10](#_bookmark385)). Second, for user-facing applications that are commonplace with DSAs, accelerators that follow these principles are a better match to the 99th- percentile response-time deadlines than the time-varying performance optimiza- tions of traditional processors, as we will see in [Section 7.9](#putting-it-all-together-cpus-versus-gpus-versus-dnn-accelerators). [Figure 7.3](#_bookmark328) shows how the four DSAs followed these guidelines.

> 以下是我们将在 [第 7.4–7.7 节](#googles-tensor-processing-unit-an-inference-data-center-accelerator) 中看到的四个 DSA 设计的五个总体指导原则。这五项准则不仅可以增加面积和能源效率，还可以提供两种有价值的奖励效果。首先，它们导致更简单的设计，从而降低 DSA 的 NRE 成本(参见 [第 7.10 节](#_bookmark385) 中的谬误)。其次，对于 DSA 常见的面向用户的应用程序，遵循这些原则的加速器比传统处理器的时变性能优化更符合 99% 的响应时间期限，正如我们将在 [ 第 7.9 节](#putting-it-all-together-cpus-versus-gpus-versus-dnn-accelerators)。[图 7.3](#_bookmark328) 显示了四个 DSA 如何遵循这些指南。

1. _Use dedicated memories to minimize the distance over which data is moved_. The many levels of caches in general-purpose microprocessors use a great deal of area and energy trying to move data optimally for a program. For example, a two-way set associative cache uses 2.5 times as much energy as an equivalent software-controlled scratchpad memory. By definition, the compiler writers and programmers of DSAs understand their domain, so there is no need for the hardware to try to move data for them. Instead, data movement is reduced with software-controlled memories that are dedicated to and tailored for specific functions within the domain.

> 1. 1._使用专用存储器来最小化数据移动的距离_。通用微处理器中的多级高速缓存使用大量区域和能量来尝试为程序最佳地移动数据。例如，双向组关联高速缓存使用的能量是等效的软件控制暂存器内存的 2.5 倍。根据定义，DSA 的编译器编写者和程序员了解他们的领域，因此硬件没有必要尝试为他们移动数据。相反，数据移动通过软件控制的存储器减少，这些存储器专用于域内的特定功能并为其量身定制。

2. _Invest the resources saved from dropping advanced microarchitectural optimi- zations into more arithmetic units or bigger memories_.

> 2. _将通过高级微架构优化节省下来的资源投资到更多的算术单元或更大的内存中_。

As [Section 7.1](#introduction-5) describes, architects turned the bounty from Moore’s Law into the resource-intensive optimizations for CPUs and GPUs (out-of-order execu- tion, multithreading, multiprocessing, prefetching, address coalescing, etc.).

> 正如 [7.1](%EF%BC%83resoped-5) 所描述的那样，工程师将赏金从摩尔定律转变为用于 CPU 和 GPU 的资源密集型优化(额外执行，多线程，多线程，多处处理，预及，预购，地址合并等等)。)。

Figure 7.3 The four DSAs in this chapter and how closely they followed the five guidelines. Pixel Visual Core typ- ically has 2–16 cores. The first implementation of Pixel Visual Core does not support 8-bit arithmetic.

> 图 7.3 本章中的四个 DSA 及其遵循五个准则的紧密关系。像素视觉核心典型上有 2-16 个核心。像素视觉核心的首次实现不支持 8 位算术。

Given the superior understanding of the execution of programs in these nar- rower domains, these resources are much better spent on more processing units or larger on-chip memory.

> 鉴于对这些 Nar-Rower 域中程序执行的高度了解，这些资源在更多的处理单元或更大的芯片内存储器上花费了得多。

1. _Use the easiest form of parallelism that matches the domain_. Target domains for DSAs almost always have inherent parallelism. The key deci- sions for a DSA are how to take advantage of that parallelism and how to expose it to the software. Design the DSA around the natural granularity of the parallelism of the domain and expose that parallelism simply in the programming model. For example, with respect to data-level parallelism, if SIMD works in the domain, it’s certainly easier for the programmer and the compiler writer than MIMD. Simi- larly, if VLIW can express the instruction-level parallelism for the domain, the design can be smaller and more energy-efficient than out-of-order execution.

> 1. 1._使用与领域匹配的最简单的并行形式_。DSA 的目标域几乎总是具有内在的并行性。DSA 的关键决策是如何利用这种并行性以及如何将其公开给软件。围绕域并行性的自然粒度设计 DSA，并在编程模型中简单地公开该并行性。例如，关于数据级并行性，如果 SIMD 在域中工作，对于程序员和编译器编写者来说肯定比 MIMD 更容易。同样，如果 VLIW 可以表达域的指令级并行性，则设计可以比乱序执行更小、更节能。

2. _Reduce data size and type to the simplest needed for the domain_. As we will see, applications in many domains are typically memory-bound, so you can increase the effective memory bandwidth and on-chip memory utiliza- tion by using narrower data types. Narrower and simpler data also let’s you pack more arithmetic units into the same chip area.

> 2. \_Reduce 数据大小和键入域所需的最简单。正如我们将看到的那样，许多域中的应用程序通常是内存的，因此您可以通过使用较窄的数据类型来增加有效的内存带宽和芯片内存内存利用。较窄，更简单的数据还可以使您将更多的算术单元包装到同一芯片区域。

3. _Use a domain-specific programming language to port code to the DSA_. As [Section 7.1](#introduction-5) mentions, a classic challenge for DSAs is getting applications to run on your novel architecture. A long-standing fallacy is assuming that your new computer is so attractive that programmers will rewrite their code just for your hardware. Fortunately, domain-specific programming languages were becoming popular even before architects were forced to switch their attention to DSAs. Examples are Halide for vision processing and TensorFlow for DNNs ([Ragan-Kelley et al., 2013; Abadi et al., 2016](#_bookmark994)). Such languages make porting applications to your DSA much more feasible. As previously mentioned, only a small, compute-intensive portion of the application needs to run on the DSA in some domains, which also simplifies porting.

> 3. 3._使用特定领域的编程语言将代码移植到 DSA_。正如 [第 7.1 节](#introduction-5) 提到的，DSA 面临的一个典型挑战是让应用程序在您的新颖架构上运行。一个长期存在的谬论是假设您的新计算机非常有吸引力，以至于程序员会只为您的硬件重写他们的代码。幸运的是，甚至在架构师被迫将注意力转移到 DSA 之前，特定领域的编程语言就开始流行了。例如用于视觉处理的 Halide 和用于 DNN 的 TensorFlow([Ragan-Kelley 等人，2013 年；Abadi 等人，2016 年](#_bookmark994))。这些语言使将应用程序移植到 DSA 变得更加可行。如前所述，在某些域中，只有一小部分计算密集型应用程序需要在 DSA 上运行，这也简化了移植。

DSAs introduce many new terms, mostly from the new domains but also from novel architecture mechanisms not seen in conventional processors. As we did in [Chapter 4](#_bookmark165), [Figure 7.4](#_bookmark330) lists the new acronyms, terms, and short explanations to aid the reader.

> DSA 介绍了许多新术语，主要来自新领域，也来自传统处理器中未见的新型体系结构机制。如我们在[第 4 章](#_bookmark165)中所做的那样，[图 7.4](#_bookmark330) 列出了新的首字母缩写词，术语和简短的解释，以帮助读者。
