## Exercises

1. \[12\] &lt;J.2 &gt;Using _n_ bits, what is the largest and smallest integer that can be repre- sented in the two’s complement system?
2. \[20/25\] &lt;J.2 &gt;In the subsection “Signed Numbers” (page J-7), it was stated that two’s complement overflows when the carry into the high-order bit position is dif-
   ferent from the carry-out from that position.
3. \[20\] &lt;J.2 &gt;Give examples of pairs of integers for all four combinations of carry-in and carry-out. Verify the rule stated above.
4. \[25\] &lt;J.2 &gt;Explain why the rule is always true.

   1. \[12\] &lt;J.2 &gt;Using 4-bit binary numbers, multiply —8 ×— 8
      using Booth recoding.
   2. \[15\] &lt;J.2 &gt;[Equations J.2.1](#_bookmark766) and
      [J.2.2](#_bookmark767) are for adding two _n_-bit numbers.
      Derive similar equations for subtraction, where there will be a
      borrow instead
      of a carry.
5. \[25\] &lt;J.2 &gt;On a machine that doesn’t detect integer overflow
   in hardware, show how you would detect overflow on a signed addition
   operation in software.
6. \[15/15/20\] &lt;J.3 &gt;Represent the following numbers as
   single-precision and double-precision IEEE floating-point numbers:

   1. \[15\] &lt;J.3 &gt; 10.
   2. \[15\] &lt;J.3 &gt; 10.5.
   3. \[20\] &lt;J.3 &gt; 0.1.
7. \[12/12/12/12/12\] &lt;J.3 &gt;Below is a list of floating-point
   numbers. In single preci- sion, write down each number in binary, in
   decimal, and give its representation in
   IEEE arithmetic.
8. \[12\] &lt;J.3 &gt;The largest number less than 1.
9. \[12\] &lt;J.3 &gt;The largest number.
10. \[12\] &lt;J.3 &gt;The smallest positive normalized number.
11. \[12\] &lt;J.3 &gt;The largest denormal number.
12. \[12\] &lt;J.3 &gt;The smallest positive number.

<!-- -->

1. \[15\] &lt;J.3 &gt;Is the ordering of nonnegative floating-point
   numbers the same as inte- gers when denormalized numbers are also
   considered?
2. \[20\] &lt;J.3 &gt;Write a program that prints out the bit patterns
   used to represent floating-point numbers on your favorite computer.
   What bit pattern is used
   for NaN?
3. \[15\] &lt;J.4 &gt;Using _p_ = 4, show how the binary floating-point
   multiply algorithm computes the product of 1.875 × 1.875.
4. \[12/10\] &lt;J.4 &gt;Concerning the addition of exponents in
   floating-point multiply:

   1. \[12\] &lt;J.4 &gt;What would the hardware that implements the

      > addition of expo- nents look like?
      >
   2. \[10\] &lt;J.4 &gt;If the bias in single precision were 129

      > instead of 127, would addi- tion be harder or easier to
      > implement?
      >
5. \[15/12\] &lt;J.4 &gt;In the discussion of overflow detection for
   floating-point multipli- cation, it was stated that (for single
   precision) you can detect an overflowed expo-
   nent by performing exponent addition in a 9-bit adder.
6. \[15\] &lt;J.4 &gt;Give the exact rule for detecting overflow.
7. \[12\] &lt;J.4 &gt;Would overflow detection be any easier if you used a 10-bit adder instead?

<!-- -->

1. \[15/10\] &lt;J.4 &gt;Floating-point multiplication:

   1. \[15\] &lt;J.4 &gt;Construct two single-precision floating-point

      > numbers whose prod- uct doesn’t overflow until the final
      > rounding step.
      >
   2. \[10\] &lt;J.4 &gt;Is there any rounding mode where this

      > phenomenon cannot occur?
      >
2. \[15\] &lt;J.4 &gt;Give an example of a product with a denormal operand but a normal- ized output. How large was the final shifting step? What is the maximum possible
   shift that can occur when the inputs are double-precision numbers?
3. \[15\] &lt;J.5 &gt;Use the floating-point addition algorithm on page J-23 to compute 1.0102 —.10012 (in 4-bit precision).
4. \[10/15/20/20/20\] &lt;J.5 &gt;In certain situations, you can be sure that _a_ + _b_ is exactly representable as a floating-point number, that is, no rounding is necessary.

   1. \[10\] &lt;J.5 &gt;If _a_, _b_ have the same exponent and
      > different signs, explain why _a_ > \+ _b_ is exact. This was used in the subsection “Speeding Up Addition” on page
      >

J-25.

1. \[15\] &lt;J.5 &gt;Give an example where the exponents differ by 1, _a_ and _b_ have dif- ferent signs, and _a_ + _b_ is not exact.
2. \[20\] &lt;J.5 &gt;If _a b_ 0, and the top two bits of _a_ cancel when computing _a b_, explain why the result is exact (this fact is mentioned on page J-22).
3. \[20\] &lt;J.5 &gt;If _a b_ 0, and the exponents differ by 1, show that _a b_ is exact unless the high order bit of _a b_ is in the same position as that of _a_ (mentioned
   in “Speeding Up Addition,” page J-25).
4. \[20\] &lt;J.5 &gt;If the result of _a b_ or _a_ + _b_ is denormal, show that the result is exact (mentioned in the subsection “Underflow,” on page J-36).

<!-- -->

1. \[15/20\] &lt;J.5 &gt;Fast floating-point addition (using parallel adders) for _p_ = 5.

   1. \[15\] &lt;J.5 &gt;Step through the fast addition algorithm for
      > _a_ + _b,_ where
      > _a_ = 1.01112 and _b_ =.110112.
      >
2. \[20\] &lt;J.5 &gt;Suppose the rounding mode is toward + ∞. What complication arises in the above example for the adder that assumes a carry-out? Suggest
   a solution.
3. \[12\] &lt;J.4, J.5 &gt;How would you use two parallel adders to avoid the final round-up addition in floating-point multiplication?
4. \[30/10\] &lt;J.5 &gt;This problem presents a way to reduce the number of addition steps in floating-point addition from three to two using only a single adder.

   1. \[30\] &lt;J.5 &gt;Let _A_ and _B_ be integers of opposite
      > signs, with _a_ and _b_ their mag- nitudes. Show that the
      > following rules for manipulating the unsigned numbers _a_
      > and _b_ gives _A_ + _B._
      >
5. Complement one of the operands.
6. Use end-around carry to add the complemented operand and the other (uncomplemented) one.
7. If there was a carry-out, the sign of the result is the sign associated with the uncomplemented operand.
8. Otherwise, if there was no carry-out, complement the result, and give it the sign of the complemented operand.

   1. \[10\] &lt;J.5 &gt;Use the above to show how steps 2 and 4 in
      > the floating-point addi- tion algorithm on page J-23 can be
      > performed using only a single addition.
      >

   <!-- -->

   1. \[20/15/20/15/20/15\] &lt;J.6 &gt;Iterative square root.

      1. \[20\] &lt;J.6 &gt;Use Newton’s method to derive an

         > iterative algorithm for square root. The formula will
         > involve a division.
         >
      2. \[15\] &lt;J.6 &gt;What is the fastest way you can think of

         > to divide a floating-point number by 2?
         >
      3. \[20\] &lt;J.6 &gt;If division is slow, then the iterative
         square root routine will also be slow. Use Newton’s method
         on _f_(_x_) 1/_x_2 \_a_ to derive a method that
         doesn’t
         use any divisions.
9. \[15\] &lt;J.6 &gt;Assume that the ratio division by 2 : floating-point add : floating- point multiply is 1:2:4. What ratios of multiplication time to divide time makes
   each iteration step in the method of part (c) faster than each iteration in the method of part (a)?
10. \[20\] &lt;J.6 &gt;When using the method of part (a), how many bits
    need to be in the initial guess in order to get double-precision
    accuracy after three iterations?
    (You may ignore rounding error.)
11. \[15\] &lt;J.6 &gt;Suppose that when spice runs on the TI 8847, it
    spends 16.7% of its time in the square root routine (this percentage
    has been measured on other
    machines). Using the values in Figure J.36 and assuming three iterations, how much slower would spice run if square root were implemented in software using the method of part(a)?
12. \[10/20/15/15/15\] &lt;J.6 &gt;Correctly rounded iterative division.
    Let _a_ and _b_ be floating-point numbers with _p_-bit significands
    (_p_ 53 in double precision). Let _q_ be the exact quotient _q
    a_/_b_,1 _q_ &lt; 2. Suppose that _q_ is the result of an iteration
    process, that _q_ has a few extra bits of precision, and that 0 &lt; _q q_ &lt; 2—_p_. For the

following, it is important that _q_ &lt; _q_, even when _q_ can be exactly represented as a floating-point number.

1. \[10\] &lt;J.6 &gt;If _x_ is a floating-point number, and 1 _x_ &lt; 2, what is the next rep- resentable number after _x_?
2. \[20\] &lt;J.6 &gt;Show how to compute _q_' from _q_, where _q_' has _p_ + 1 bits of precision and \|_q_ — _q_'\|&lt; 2—_p_.
3. \[15\] &lt;J.6 &gt;Assuming round to nearest, show that the
   correctly rounded quo- tient is either _q_',
   _q_' — 2—_p_, or _q_' +
   2—_p_.
4. \[15\] &lt;J.6 &gt;Give rules for computing the correctly rounded quotient from _q_'
   based on the low-order bit of _q_' and the sign of _a_ — _bq_'.
5. \[15\] &lt;J.6 &gt;Solve part (c) for the other three rounding modes.

<!-- -->

1. \[15\] &lt;J.6 &gt;Verify the formula on page J-30. (_Hint_: If _xn_ = _x_0(2 — \_x_0_b_)×
   Π h1+ (1 — _x b_)2*i* i, then 2 — _x b_ = 2 — _x b_(2 — _x b_)Πh1+ (1 — _x b_ 2*i* i

2 — h1 — (1 — _x b_)2iΠh1+ (1 — _x b_)2*i* i.)

1. \[15\] &lt;J.7 &gt;Our example that showed that double rounding can give a different answer from rounding once used the round-to-even rule. If halfway cases are
   always rounded up, is double rounding still dangerous?
2. \[10/10/20/20\] &lt;J.7 &gt;Some of the cases of the italicized statement in the “Preci- sions” subsection (page J-33) aren’t hard to demonstrate.

   1. \[10\] &lt;J.7 &gt;What form must a binary number have if

      > rounding to _q_ bits followed by rounding to _p_ bits gives a
      > different answer than rounding directly to _p_ bits?
      >
   2. \[10\] &lt;J.7 &gt;Show that for multiplication of _p_-bit

      > numbers, rounding to _q_ bits followed by rounding to _p_ bits
      > is the same as rounding immediately to _p_ bits if _q_ ≥ 2*p*.
      >
   3. \[20\] &lt;J.7 &gt;If _a_ and _b_ are _p_-bit numbers with the

      > same sign, show that rounding _a_ + _b_ to _q_ bits followed
      > by rounding to _p_ bits is the same as rounding immedi- ately
      > to _p_ bits if _q_ ≥ 2*p* + 1.
      >
   4. \[20\] &lt;J.7 &gt;Do part (c) when _a_ and _b_ have opposite

      > signs.
      >
3. \[Discussion\] &lt;J.7 &gt;In the MIPS approach to exception handling, you need a test for determining whether two floating-point operands could cause an exception.
   This should be fast and also not have too many false positives. Can you come up with a practical test? The performance cost of your design will depend on the distribution of floating-point numbers. This is discussed in [Knuth \[1981\]](#_bookmark821) and the Hamming paper in [Swartzlander \[1990\]](#_bookmark834).
4. \[12/12/10\] &lt;J.8 &gt;Carry-skip adders.

   1. \[12\] &lt;J.8 &gt;Assuming that time is proportional to logic
      > levels, how long does it take an _n_-bit adder divided into
      > (fixed) blocks of length _k_ bits to perform an
      > addition?
      >
5. \[12\] &lt;J.8 &gt;What value of _k_ gives the fastest adder?
6. \[10\] &lt;J.8 &gt;Explain why the carry-skip adder takes time 0(,_n_).

<!-- -->

1. \[10/15/20\] &lt;J.8 &gt;Complete the details of the block diagrams for the following adders.

   1. \[10\] &lt;J.8 &gt;In [Figure J.15](#_bookmark787), show how to

      > implement the “1” and “2” boxes in terms of AND and OR gates.
      >
   2. \[15\] &lt;J.8 &gt;In [Figure J.19](#_bookmark791), what signals

      > need to flow from the adder cells in the top row into the “C”
      > cells? Write the logic equations for the “C” box.
      >
   3. \[20\] &lt;J.8 &gt;Show how to extend the block diagram in J.17

      > so it will produce the carry-out bit \_c_8.
      >
2. \[15\] &lt;J.9 &gt;For ordinary Booth recoding, the multiple of _b_
   used in the *i*th step is simply _ai_—1
   _ai_. Can you find a similar formula for radix-4 Booth
   recoding (overlapped triplets)?
3. \[20\] &lt;J.9 &gt;Expand [Figure J.29](#_bookmark798) in the
   fashion of J.27, showing the individual adders.
4. \[25\] &lt;J.9 &gt;Write out the analog of Figure J.25 for radix-8
   Booth recoding.
5. \[18\] &lt;J.9 &gt;Suppose that _an_—1 …
   _a_1_a_0 and \_bn_—1 …
   _b_1_b_0 are being added in a signed-digit
   adder as illustrated in the example on page J-53. Write a formula
   for the *i*th bit of the sum, \_si_, in terms of
   _ai_, _ai_—1,
   _ai_—2, _bi_,
   _bi_—1, and _bi_—2.
6. \[15\] &lt;J.9 &gt;The text discussed radix-4 SRT division with
   quotient digits of —2,
   —1, 0, 1, 2. Suppose that 3 and —3 are also allowed as quotient digits. What relation replaces \|_ri_\|≤ 2*b*/3?
7. \[25/20/30\] &lt;J.9 &gt;Concerning the SRT division table, Figure
   J.34:

   1. \[25\] &lt;J.9 &gt;Write a program to generate the results of
      Figure J.34.
   2. \[20\] &lt;J.9 &gt;Note that Figure J.34 has a certain symmetry

      > with respect to pos- itive and negative values of P. Can you
      > find a way to exploit the symmetry and
      > only store the values for positive P?
      >
8. \[30\] &lt;J.9 &gt;Suppose a carry-save adder is used instead of a
   propagate adder. The input to the quotient lookup table will be _k_
   bits of divisor and _l_ bits of
   remainder, where the remainder bits are computed by summing the top _l_ bits of the sum and carry registers. What are _k_ and _l_? Write a program to generate the analog of Figure J.34.
9. \[12/12/12\] &lt;J.9, J.12 &gt;The first several million Pentium
   chips produced had a flaw that caused division to sometimes return
   the wrong result. The Pentium uses
   a radix-4 SRT algorithm similar to the one illustrated in the example on page J-56 (but with the remainder stored in carry-save format; see Exercise J.33(c)). Accord- ing to Intel, the bug was due to five incorrect entries in the quotient lookup table.
10. \[12\] &lt;J.9, J.12 &gt;The bad entries should have had a quotient of plus or minus 2, but instead had a quotient of 0. Because of redundancy, it’s conceivable that the
    algorithm could “recover” from a bad quotient digit on later iterations. Show that this is not possible for the Pentium flaw.
11. \[12\] &lt;J.9, J.12 &gt;Since the operation is a floating-point divide rather than an integer divide, the SRT division algorithm on page J-45 must be modified in
    two ways. First, step 1 is no longer needed, since the divisor is already normal- ized. Second, the very first remainder may not satisfy the proper bound ( _r_ 2*b*/3 for Pentium; see page J-55). Show that skipping the very first left shift in step 2(a) of the SRT algorithm will solve this problem.
12. \[12\] &lt;J.9, J.12 &gt;If the faulty table entries were indexed by
    a remainder that could occur at the very first divide step (when the
    remainder is the divisor), ran-
    dom testing would quickly reveal the bug. This didn’t happen. What does that tell you about the remainder values that index the faulty entries?
13. \[12\] &lt;J.6, J.9 &gt;The discussion of the remainder-step instruction assumed that division was done using a bit-at-a-time algorithm. What would have to change
    if division were implemented using a higher-radix method?
14. \[25\] &lt;J.9 &gt;In the array of [Figure J.28](#_bookmark797), the fact that an array can be pipelined is not exploited. Can you come up with a design that feeds the output of the bottom CSA
    into the bottom CSAs instead of the top one, and that will run faster than the arrangement of [Figure J.28](#_bookmark797)?
