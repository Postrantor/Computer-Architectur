## SIMD Instruction Set Extensions for Multimedia

> ## SIMD 指令集的多媒体扩展

SIMD Multimedia Extensions started with the simple observation that many media applications operate on narrower data types than the 32-bit processors were optimized for. Graphics systems would use 8 bits to represent each of the three primary colors plus 8 bits for transparency. Depending on the application, audio samples are usually represented with 8 or 16 bits. By partitioning the carry chains within, say, a 256-bit adder, a processor could perform simultaneous operations on short vectors of thirty-two 8-bit operands, sixteen 16-bit operands, eight 32-bit operands, or four 64-bit operands. The additional cost of such partitioned adders was small. [Figure 4.8](#_bookmark177) summarizes typical multimedia SIMD instructions. Like vector instructions, a SIMD instruction specifies the same operation on vectors of data. Unlike vector machines with large register files such as the RISC-V RV64V vector registers, which can hold, say, thirty-two 64-bit elements in each of 32 vector registers, SIMD instructions tend to specify fewer operands and thus use much smaller register files. In contrast to vector architectures, which offer an elegant instruction set that is intended to be the target of a vectorizing compiler, SIMD extensions have three major omissions: no vector length register, no strided or gather/scatter data transfer instructions, and no mask registers.

> SIMD 多媒体扩展始于一个简单的观察，即许多媒体应用程序在窄数据类型上运行的数据比 32 位处理器优化了。图形系统将使用 8 位表示三种原色中的每一个以及 8 位以供透明。根据应用程序，音频样品通常用 8 或 16 位表示。通过对携带链进行分区（例如 256 位加法器），处理器可以同时对 32 个 8 位操作数的短矢量进行同时操作操作数。此类分区加法器的额外费用很小。[图 4.8]（#\_ bookmark177）总结了典型的多媒体 SIMD 指令。像向量说明一样，SIMD 指令在数据向量上指定了相同的操作。与具有大寄存器文件的矢量机不同，例如 RISC-V RV64V 矢量寄存器，可以在 32 个向量寄存器中每一个中容纳 32 个 64 位元素，SIMD 说明倾向于指定较少的操作数，从而使用较小的寄存器。文件。与矢量体系结构相反，矢量体系结构提供了一个旨在作为矢量化编译器的目标的优雅指令集，SIMD 扩展程序具有三个主要的遗漏：无矢量长度寄存器，没有踩踏或收集/分散数据传输指令，也没有掩码寄存器登记册。

Figure 4.8 Summary of typical SIMD multimedia support for 256-bit-wide operations. Note that the IEEE 754-2008 floating-point standard added half-precision (16bit) and quad-precision (128-bit) floating-point operations.

> 图 4.8 典型的 SIMD 多媒体支持对 256 位宽的操作的摘要。请注意，IEEE 754-2008 浮点标准添加了半精确（16 位）和 Quad-Precision（128 位）浮点操作。

1. Multimedia SIMD extensions fix the number of data operands in the opcode, which has led to the addition of hundreds of instructions in the MMX, SSE, and AVX extensions of the x86 architecture. Vector architectures have a vector-length register that specifies the number of operands for the current operation. These variable-length vector registers easily accommodate programs that naturally have shorter vectors than the maximum size the architecture supports. Moreover, vector architectures have an implicit maximum vector length in the architecture, which combined with the vector length register avoids the use of many opcodes.

> 1. Multimedia Simd 扩展程序修复了 OpCode 中数据操作数的数量，这导致在 X86 体系结构的 MMX，SSE 和 AVX 扩展中添加了数百个指令。向量体系结构具有向量长度寄存器，该寄存器指定当前操作的操作数。这些可变长度向量寄存器可以轻松容纳比体系结构支持的最大尺寸自然具有较短矢量的程序。此外，向量体系结构在体系结构中具有隐式最大向量长度，该架构与向量长度寄存器结合使用，避免使用许多操作编码。

2. Up until recently, multimedia SIMD did not offer the more sophisticated addressing modes of vector architectures, namely strided accesses and gather-scatter accesses. These features increase the number of programs that a vector compiler can successfully vectorize (see [Section 4.7](#_bookmark198)).

> 2.直到最近，Multimedia Simd 还没有提供更复杂的矢量体系结构的寻址模式，即访问访问和聚会片段访问。这些功能增加了向量编译器可以成功向量化的程序数（请参阅[4.7]（#\_ bookmark198））。

3. Although this is changing, multimedia SIMD usually did not offer the mask registers to support conditional execution of elements as in vector architectures.

> 3.尽管情况正在发生变化，但多媒体 SIMD 通常没有提供蒙版寄存器来支持矢量体系结构中的元素的有条件执行。

Such omissions make it harder for the compiler to generate SIMD code and increase the difficulty of programming in SIMD assembly language.

> 这样的遗漏使编译器更难生成 SIMD 代码并增加 SIMD 汇编语言中编程的难度。

For the x86 architecture, the MMX instructions added in 1996 repurposed the 64-bit floating-point registers, so the basic instructions could perform eight 8-bit operations or four 16-bit operations simultaneously. These were joined by parallel MAX and MIN operations, a wide variety of masking and conditional instructions, operations typically found in digital signal processors, and ad hoc instructions that were believed to be useful in important media libraries. Note that MMX reused the floating-point data-transfer instructions to access memory.

> 对于 X86 体系结构，MMX 指令在 1996 年添加了 64 位浮点寄存器，因此基本说明可以同时执行八个 8 位操作或四个 16 位操作。这些通过并行最大操作，多种掩盖和有条件的说明，通常在数字信号处理器中找到的操作以及据信在重要媒体库中有用的临时说明加入了这些操作。请注意，MMX 将浮点数据传输指令重复使用以访问内存。

The Streaming SIMD Extensions (SSE) successor in 1999 added 16 separate registers (XMM registers) that were 128 bits wide, so now instructions could simultaneously perform sixteen 8-bit operations, eight 16-bit operations, or four 32-bit operations. It also performed parallel single-precision floating-point arithmetic. Because SSE had separate registers, it needed separate data transfer instructions. Intel soon added double-precision SIMD floating-point data types via SSE2 in 2001, SSE3 in 2004, and SSE4 in 2007. Instructions with four single-precision floating-point operations or two parallel double-precision operations increased the peak floating-point performance of the x86 computers, as long as programmers placed the operands side by side. With each generation, they also added ad hoc instructions whose aim was to accelerate specific multimedia functions perceived to be important.

> 1999 年的流媒体 SIMD 扩展名（SSE）后继添加了 16 个单独的寄存器（XMM 寄存器），宽 128 位，因此现在指令可以同时执行 16 个 8 位操作，8 个 16 位操作，或 4 个 32 位操作。它还执行了平行的单精度浮点算术。由于 SSE 有单独的寄存器，因此需要单独的数据传输说明。英特尔很快在 2001 年通过 SSE2，2004 年的 SSE3 和 2007 年的 SSE4 添加了双精度 SIMD 浮点数据类型。具有四个单精制浮点操作或两次平行的双重精确操作的指示提高了峰值浮点功能在 X86 计算机中，只要程序员并排放置操作数即可。他们还添加了临时指令，其目的是加速特定的多媒体功能，认为这很重要。

The Advanced Vector Extensions (AVX), added in 2010, doubled the width of the registers again to 256 bits (YMM registers) and thereby offered instructions that double the number of operations on all narrower data types. [Figure 4.9](#_bookmark178) shows AVX instructions useful for double-precision floating-point computations. AVX2 in 2013 added 30 new instructions such as gather (VfiATHER) and vector shifts (VPSLL, VPSRL, VPSRA). AVX-512 in 2017 doubled the width again to 512 bits (ZMM registers), doubled the number of the registers again to 32, and added about 250 new instructions including scatter (VPSCATTER) and mask registers (OPMASK). AVX includes preparations to extend registers to 1024 bits in future editions of the architecture.

> 2010 年添加的高级矢量扩展名（AVX）将寄存器的宽度再次翻了一番，达到 256 位（YMM 寄存器），从而提供了指令，使所有较窄的数据类型的操作数量增加一倍。[图 4.9]（#\_ bookmark178）显示了对双精度浮点计算有用的 AVX 指令。AVX2 在 2013 年添加了 30 个新说明，例如收集（VFIATHER）和矢量偏移（VPSLL，VPSRL，VPSRA）。2017 年的 AVX-512 再次将宽度翻了一番，达到 512 位（ZMM 寄存器），将寄存器的数量再次翻了一番，并增加到 32 位，并增加了大约 250 个新说明，包括 STACTER（VPSCATTER）和 MAKEN 寄存器（OPMASK）。AVX 包括准备将寄存器扩展到未来版本中的 1024 位的准备工作。

In general, the goal of these extensions has been to accelerate carefully written libraries rather than for the compiler to generate them (see Appendix H), but recent x86 compilers are trying to generate such code, particularly for floating-pointintensive applications. Since the opcode determines the width of the SIMD register, every time the width doubles, so must the number of SIMD instructions.

> 通常，这些扩展程序的目的是加速仔细的书面库，而不是为了使编译器生成它们（请参阅附录 H），但是最近的 X86 编译器正在尝试生成此类代码，尤其是对于浮动量的应用程序。由于 OPCODE 确定 SIMD 寄存器的宽度，每次宽度加倍时，SIMD 指令的数量也必须。

Figure 4.9 AVX instructions for x86 architecture useful in double-precision floating-point programs. Packeddouble for 256-bit AVX means four 64-bit operands executed in SIMD mode. As the width increases with AVX, it is increasingly important to add data permutation instructions that allow combinations of narrow operands from different parts of the wide registers. AVX includes instructions that shuffle 32-bit, 64-bit, or 128-bit operands within a 256-bit register. For example, BROADCAST replicates a 64-bit operand four times in an AVX register. AVX also includes a large variety of fused multiply-add/subtract instructions; we show just two here.

> 图 4.9 X86 体系结构的 AVX 指令在双精度浮点程序中有用。256 位 AVX 的 PackedDouble 表示以 SIMD 模式执行的四个 64 位操作数。随着宽度随 AVX 的增加，添加数据排列指令允许从宽寄存器不同部分组合狭窄操作数的组合变得越来越重要。AVX 包括在 256 位寄存器中进行 32 位，64 位或 128 位操作数的说明。例如，广播在 AVX 寄存器中四次复制了 64 位操作数。AVX 还包括各种各样的熔融倍数/减法指令；我们在这里只显示两个。

Given these weaknesses, why are multimedia SIMD extensions so popular? First, they initially cost little to add to the standard arithmetic unit and they were easy to implement. Second, they require scant extra processor state compared to vector architectures, which is always a concern for context switch times. Third, you need a lot of memory bandwidth to support a vector architecture, which many computers don’t have. Fourth, SIMD does not have to deal with problems in virtual memory when a single instruction can generate 32 memory accesses and any of which can cause a page fault. The original SIMD extensions used separate data transfers per SIMD group of operands that are aligned in memory, and so they cannot cross page boundaries. Another advantage of short, fixedlength “vectors” of SIMD is that it is easy to introduce instructions that can help with new media standards, such as instructions that perform permutations or instructions that consume either fewer or more operands than vectors can produce. Finally, there was concern about how well vector architectures can work with caches. More recent vector architectures have addressed all of these problems. The overarching issue, however, is that due the overiding importance of backwards binary compatability, once an architecture gets started on the SIMD path it’s very hard to get off it.

> 考虑到这些弱点，为什么多媒体 Simd 扩展如此受欢迎？首先，最初，它们的成本很少，而添加到标准算术单元中，并且易于实现。其次，与矢量体系结构相比，它们需要额外的处理器状态，这始终是上下文切换时间的关注点。第三，您需要大量的内存带宽来支持许多计算机所没有的矢量体系结构。第四，当单个指令可以生成 32 个内存访问并且任何可能导致页面故障时，SIMD 不必处理虚拟内存中的问题。原始的 SIMD 扩展名使用单独的数据传输，每个 SIMD 在内存中对齐的操作数组，因此它们无法跨页边界。SIMD 的简短，固定长度“向量”的另一个优点是，很容易介绍可以帮助新媒体标准的说明，例如执行置换或指令所消耗的操作数或更多操作数的说明，而不是向量可以产生的操作数。最后，人们担心矢量体系结构如何与缓存配合使用。最新的矢量体系结构已经解决了所有这些问题。然而，总体问题是，由于在 SIMD 路径上启动架构后，很难脱离它，因此倒数二进制兼容性的重要性。

Example To get an idea about what multimedia instructions look like, assume we added a 256-bit SIMD multimedia instruction extension to RISC-V, tentatively called RVP for “packed.” We concentrate on floating-point in this example. We add the suffix “4D” on instructions that operate on four double-precision operands at once. Like vector architectures, you can think of a SIMD Processor as having lanes, four in this case. RV64P expands the F registers to be the full width, in this case 256 bits. This example shows the RISC-V SIMD code for the DAXPY loop, with the changes to the RISC-V code for SIMD underlined. We assume that the starting addresses of X and Y are in x5 and x6, respectively.

> 示例以了解多媒体指令的外观，假设我们在 RISC-V 中添加了 256 位 SIMD 多媒体指令扩展名，暂定称为“包装”的 RVP。在此示例中，我们专注于浮点。我们将后缀“ 4D”添加到一次在四个双重精神操作数上操作的说明。像矢量体系结构一样，您可以将 SIMD 处理器视为有车道，在这种情况下为四个。RV64P 将 F 寄存器扩展为完整宽度，在这种情况下为 256 位。此示例显示了 Daxpy 循环的 RISC-V SIMD 代码，并更改了 SIMD 下划线的 RISC-V 代码。我们假设 X 和 Y 的起始地址分别在 X5 和 X6 中。

The changes were replacing every RISC-V double-precision instruction with its 4D equivalent, increasing the increment from 8 to 32, and adding the splat instruction that makes 4 copies of a in the 256 bits of f0. While not as dramatic as the 32 reduction of dynamic instruction bandwidth of RV64V, RISC-V SIMD does get almost a 4 reduction: 67 versus 258 instructions executed for RV64G. This code knows the number of elements. That number is often determined at run time, which would require an extra strip-mine loop to handle the case when the number is not a modulo of 4.

> 这些更改是用其 4D 等效替代了每个 RISC-V 双重精确指令，将增量从 8 增加到 32，并添加了 256 位 F0 位中 4 份 A 副本的 SPLAT 指令。虽然不如 RV64V 的动态指令带宽减少 32 个，但 RISC-V SIMD 确实几乎降低了 4 个：67 对 RV64G 执行的 258 个指令。该代码知道元素的数量。该数字通常是在运行时间确定的，当数字不是 4 个模型时，这将需要一个额外的带状循环来处理外壳。

### Programming Multimedia SIMD Architectures

> ###编程多媒体 SIMD 架构

Given the ad hoc nature of the SIMD multimedia extensions, the easiest way to use these instructions has been through libraries or by writing in assembly language. Recent extensions have become more regular, giving compilers a more reasonable target. By borrowing techniques from vectorizing compilers, compilers are starting to produce SIMD instructions automatically. For example, advanced compilers today can generate SIMD floating-point instructions to deliver much higher performance for scientific codes. However, programmers must be sure to align all the data in memory to the width of the SIMD unit on which the code is run to prevent the compiler from generating scalar instructions for otherwise vectorizable code.

> 鉴于 SIMD 多媒体扩展的临时性质，使用这些说明的最简单方法是通过库或用汇编语言写作。最近的扩展已变得更加规律，为编译器提供了更合理的目标。通过从矢量化编译器借用技术，编译器开始自动产生 SIMD 指令。例如，当今高级编译器可以生成 SIMD 浮点指令，以提供更高的科学代码性能。但是，程序员必须确保将存储器中的所有数据与运行代码的 SIMD 单元的宽度对齐，以防止编译器生成标量指令，以供其他矢量化代码。

### The Roofline Visual Performance Model

> ###车顶线视觉性能模型

One visual, intuitive way to compare potential floating-point performance of variations of SIMD architectures is the Roofline model ([Williams et al., 2009](#_bookmark1017)). The horizontal and diagonal lines of the graphs it produces give this simple model its name and indicate its value (see [Figure 4.11](#_bookmark180)). It ties together floating-point performance, memory performance, and arithmetic intensity in a two-dimensional graph.

> 一种视觉，直观的方式来比较 SIMD 体系结构变化的潜在浮点性能是车顶线模型（[Williams 等，2009]（#_ bookmark1017））。它产生的图形的水平和对角线赋予了此简单模型的名称并指示其值（请参见[图 4.11]（#_ bookmark180））。它将二维图中的浮点性能，内存性能和算术强度联系在一起。

![](./media/image236.png)

> ！[]（./ Media/image236.png）

Figure 4.10 Arithmetic intensity, specified as the number of floating-point operations to run the program divided by the number of bytes accessed in main memory ([Williams et al., 2009](#_bookmark1017)). Some kernels have an arithmetic intensity that scales with problem size, such as a dense matrix, but there are many kernels with arithmetic intensities independent of problem size.

> 图 4.10 算术强度，指定为运行程序的浮点操作数量除以主内存中访问的字节数（[Williams 等，2009]（#\_ Bookmark1017））。有些内核具有算术强度，其尺寸与问题大小（例如密集的矩阵）相扩展，但是有许多核心具有算术强度的内核，而与问题大小无关。

_Arithmetic intensity_ is the ratio of floating-point operations per byte of memory accessed. It can be calculated by taking the total number of floating-point operations for a program divided by the total number of data bytes transferred to main memory during program execution. [Figure 4.10](#_bookmark179) shows the relative arithmetic intensity of several example kernels.

> *arithmetic intense* 是访问内存的每个字节的浮点操作的比率。可以通过获取程序的浮点操作总数除以在程序执行过程中传输到主内存的数据字节总数。[图 4.10]（#\_ bookmark179）显示了几个示例内核的相对算术强度。

Peak floating-point performance can be found using the hardware specifications. Many of the kernels in this case study do not fit in on-chip caches, so peak memory performance is defined by the memory system behind the caches. Note that we need the peak memory bandwidth that is available to the processors, not just at the DRAM pins as in [Figure 4.27](#_bookmark200) on page 328. One way to find the (delivered) peak memory performance is to run the Stream benchmark.

> 使用硬件规格可以找到峰值浮点性能。在本案例研究中，许多内核都不适合片上缓存，因此峰值记忆性能由缓存背后的内存系统定义。请注意，我们需要处理器可用的峰值内存带宽，而不仅仅是在第 328 页[图 4.27]（#\_ bookmark200）中的 DRAM 引脚。流基准。

[Figure 4.11](#_bookmark180) shows the Roofline model for the NEC SX-9 vector processor on the left and the Intel Core i7 920 multicore computer on the right. The vertical Yaxis is achievable floating-point performance from 2 to 256 GFLOPS/s. The horizontal X-axis is arithmetic intensity, varying from 1/8 FLOP/DRAM byte accessed to 16 FLOP/DRAM byte accessed in both graphs. Note that the graph is a log-log scale, and that Rooflines are done just once for a computer.

> [图 4.11]（#\_ bookmark180）显示了左侧的 NEC SX-9 矢量处理器的车顶线模型，右侧显示了 Intel Core i7 920 多功能计算机。垂直 Yaxis 是 2 至 256 Gflops/s 的浮点性能。水平 X 轴是算术强度，从访问的 1/8 flop/dram 字节到两个图中访问的 16 个 flop/dram 字节。请注意，该图是对数量表的刻度，并且屋顶线仅用于计算机一次。

For a given kernel, we can find a point on the X-axis based on its arithmetic intensity. If we drew a vertical line through that point, the performance of the kernel on that computer must lie somewhere along that line. We can plot a horizontal line showing peak floating-point performance of the computer. Obviously, the actual floating-point performance can be no higher than the horizontal line because that is a hardware limit.

> 对于给定的内核，我们可以根据其算术强度找到 X 轴的点。如果我们在该点上绘制了一条垂直线，则该计算机上的内核的性能必须位于该线路的某个地方。我们可以绘制一条水平线，显示计算机的峰值浮点性能。显然，实际的浮点性能不高于水平线，因为这是硬件限制。

How could we plot the peak memory performance? Because the X-axis is FLOP/byte and the Y-axis is FLOP/s, bytes/s is just a diagonal line at a 45-degree angle in this figure. Thus we can plot a third line that gives the maximum floatingpoint performance that the memory system of that computer can support for a given

> 我们如何绘制峰值内存性能？因为 X 轴是插槽/字节，而 y 轴是触发器/s，所以字节/s 只是该图中 45 度角的对角线。因此，我们可以绘制第三行，该行能为该计算机的内存系统提供最大的浮点性能

![](./media/image237.png)

> ！[]（./ Media/image237.png）

![](./media/image245.png)128

> ！[]（./ Media/Image245.png）128

Figure 4.11 Roofline model for one NEC SX-9 vector processor on the left and the Intel Core i7 920 multicore computer with SIMD extensions on the right ([Williams et al., 2009](#_bookmark1017)). This Roofline is for unit-stride memory accesses and double-precision floating-point performance. NEC SX-9 is a vector supercomputer announced in 2008 that cost millions of dollars. It has a peak DP FP performance of 102.4 GFLOP/s and a peak memory bandwidth of 162 GB/s from the Stream benchmark. The Core i7 920 has a peak DP FP performance of 42.66 GFLOP/s and a peak memory bandwidth of 16.4 GB/s. The dashed vertical lines at an arithmetic intensity of 4 FLOP/byte show that both processors operate at peak performance. In this case, the SX-9 at 102.4 FLOP/s is 2.4 faster than the Core i7 at arithmetic intensity. We can express the limits as a formula to plot these lines in the graphs in [Figure 4.11](#_bookmark180):

> 图 4.11 左侧的一个 NEC SX-9 矢量处理器的车顶线模型和右侧具有 SIMD 扩展的 Intel Core i7 920 多核计算机（[[Williams 等，2009]（#_ bookmark1017））。该屋顶线适用于单位式内存访问和双精度浮点功能。NEC SX-9 是 2008 年宣布的矢量超级计算机，耗资数百万美元。它的峰值 DP FP 性能为 102.4 GFLOP/s，峰值存储器带宽为 162 GB/s，来自流基准。核心 i7 920 的峰值 DP FP 性能为 42.66 GFLOP/s，峰值存储器带宽为 16.4 GB/s。在 4 flop/byte 的算术强度下，虚线的垂直线表明，两个处理器在峰值性能下运行。在这种情况下，在 102.4 flop/s 处的 SX-9 比算术强度的核心 i7 快 2.4。我们可以将限制表示为公式，以在[图 4.11]（#_ bookmark180）中的图中绘制这些行：

either it hits the flat part of the roof, which means performance is computationally limited, or it hits the slanted part of the roof, which means performance is ultimately limited by memory bandwidth. In [Figure 4.11](#_bookmark180), the vertical dashed line on the right (arithmetic intensity of 4) is an example of the former and the vertical dashed line on the left (arithmetic intensity of 1/4) is an example of the latter. Given a Roofline model of a computer, you can apply it repeatedly, because it doesn’t vary by kernel.

> 它要么撞到屋顶的平坦部分，这意味着性能在计算上有限，要么撞击屋顶的倾斜部分，这意味着性能最终受到内存带宽的限制。在[图 4.11]（#\_ bookmark180）中，右侧的垂直虚线（算术强度为 4）是前者的一个示例，左侧的垂直虚线（算术强度为 1/4）是一个例子。后者。给定计算机的车顶线模型，您可以反复应用它，因为它不会因内核而异。

Note that the “ridge point,” where the diagonal and horizontal roofs meet, offers an interesting insight into a computer. If it is far to the right, then only kernels with very high arithmetic intensity can achieve the maximum performance of that computer. If it is far to the left, then almost any kernel can potentially hit the maximum performance. As we will see, this vector processor has both much higher memory bandwidth and a ridge point far to the left as compared to other SIMD Processors.

> 请注意，对角线和水平屋顶相遇的“山脊点”为计算机提供了有趣的见解。如果距离很远，那么只有非常高的算术强度的内核才能达到该计算机的最大性能。如果距离左右，那么几乎所有内核都可以达到最高性能。正如我们将看到的那样，与其他 SIMD 处理器相比，该矢量处理器具有更高的内存带宽和距离距离的脊。

[Figure 4.11](#_bookmark180) shows that the peak computational performance of the SX-9 is 2.4 faster than Core i7, but the memory performance is 10 faster. For programs with an arithmetic intensity of 0.25, the SX-9 is 10 faster (40.5 versus 4.1 GFLOP/s). The higher memory bandwidth moves the ridge point from 2.6 in the Core i7 to 0.6 on the SX-9, which means many more programs can reach the peak computational performance on the vector processor.

> [图 4.11]（#\_ bookmark180）表明，SX-9 的峰值计算性能比 Core i7 快 2.4，但内存性能速度快 10。对于算术强度为 0.25 的程序，SX-9 的速度快 10 速度（40.5 对 4.1 GFLOP/s）。较高的内存带宽将山脊点从核心 i7 中的 2.6 移至 SX-9 上的 0.6，这意味着更多程序可以达到矢量处理器上的峰值计算性能。
