## Cross-Cutting Issues

> ##交叉切割问题

Net gear is the SUV of the datacenter.

> NetGear 是数据中心的 SUV。

James [Hamilton (2009)](#_bookmark956)

> 詹姆斯[汉密尔顿（2009）]（#\_ bookmark956）

### Preventing the WSC Network From Being a Bottleneck

> ###防止 WSC 网络成为瓶颈

[Figure 6.22](#_bookmark298) shows the network demands doubling every 12–15 months for Google, resulting in a 50 growth in traffic from the servers in Google’s fleet of WSCs in just 7 years. Clearly, without great care, the WSC network could easily become a performance or cost bottleneck.

> [图 6.22]（#\_ bookmark298）显示，该网络要求 Google 每 12-15 个月增加每 12-15 个月的需求，从而在短短 7 年内从 Google 的 WSC 舰队中的服务器增长了 50 个。显然，如果不小心，WSC 网络很容易成为性能或成本瓶颈。

In the previous edition, we pointed out that a data center switch could cost almost $1 million, or more than 50 times as much as a Top of Rack switch. Not only was such a switch expensive, the resulting oversubscription affected the design of software and the placement of services and data within the WSC. The WSC network bottlenecks constrained data placement, which in turn complicated WSC software. Because this software is one of the most valuable assets of a WSC company, the cost of this added complexity was significant.

> 在上一版中，我们指出，数据中心开关的价格接近 100 万美元，是机架开关顶部的 50 倍以上。如此开关不仅昂贵，而且产生的超额订阅影响了软件的设计以及在 WSC 中的服务和数据的放置。WSC 网络瓶颈限制了数据放置，这反过来又复杂了 WSC 软件。由于该软件是 WSC 公司最有价值的资产之一，因此这种增加的复杂性的成本很大。

The ideal WSC network would be a black box whose topology and band- width are uninteresting because there are no restrictions: any workload could be placed anywhere and optimized for server utilization rather than network traffic locality. [Vahdat et al. (2010)](#_bookmark1014) proposed borrowing networking technology from supercomputing to overcome the price and performance problems. The lat- ter proposed a networking infrastructure that could scale to 100,000 ports and 1 Pbit/s of bisection bandwidth. A major benefit of these novel data center switches is to simplify the software challenges because of oversubscription.

> 理想的 WSC 网络将是一个黑匣子，其拓扑和带宽无趣，因为没有限制：任何工作负载都可以在任何地方放置并针对服务器使用而不是网络流量区域进行了优化。[Vahdat 等。（2010）]（#\_ bookmark1014）从超级计算中提出的借贷网络技术来克服价格和绩效问题。该拉特尔提出了一个网络基础架构，该基础架构可以扩展到 100,000 个端口和 1 个 PBIT/s 的双分配带宽。这些新型数据中心开关的一个主要好处是，由于超额检查而简化了软件挑战。

![](./media/image342.png)

Figure 6.22 Network traffic from all the servers in Google’s WSCs over 7 years (Singh et al., 2015).

> 图 6.22 Google WSC 中所有服务器的网络流量超过 7 年（Singh 等，2015）。

Since that time, many companies with WSC have designed their own switches to overcome these challenges ([Hamilton, 2014](#_bookmark958)). [Singh et al. (2015)](#_bookmark1003) reported on several generations of custom networks used inside Google WSCs, which [Figure 6.23](#_bookmark299) lists.

> 从那时起，许多拥有 WSC 的公司设计了自己的开关来克服这些挑战（[Hamilton，2014]（#_ bookmark958））。[Singh 等。（2015）]（#_ bookmark1003）报告了 Google WSC 中使用的几代自定义网络，其中[图 6.23]（#\_ bookmark299）列表。

To keep costs down, they built their switches from standard commodity switch chips. They found that the features of traditional data center switches that were used in part to justify their high costs—such as decentralized network routing and pro- tocols to manage support of arbitrary deployment scenarios—were unnecessary in a WSC because the network topology could be planned in advance of deployment and the network had only a single operator. Google instead used centralized control that relied on a common configuration that was copied to all data center switches. The modular hardware design and robust software control allowed these switches to be used both for inside the WSC and for wide area networks between WSCs. Google scaled the bandwidth of its WSCs networks by 100X in a decade, and offered more than 1 Pbit/s of bisection bandwidth in 2015.

> 为了降低成本，他们从标准商品开关芯片中建立了开关。他们发现，在 WSC 中，网络拓扑可以计划，因此不必要地计划了传统数据中心开关的特征，部分用来证明其高成本（例如分散的网络路由和支持任意部署场景的支持），例如分散的网络路由和支持任意部署方案的支持，因为网络拓扑是不必要的在部署之前，网络只有一个操作员。相反，Google 使用了依赖于所有数据中心开关的常见配置的集中控制。模块化硬件设计和稳健的软件控制允许在 WSC 内部和 WSC 之间的大区域内使用这些开关。Google 在十年内将其 WSCS 网络的带宽缩小了 100 倍，并在 2015 年提供了超过 1 个 PBIT/s 的双分配带宽。

Figure 6.23 Six generations of network switches deployed at Google WSCs (Singh et al., 2015). The Four-Post CRs used commercial 512 port, 1 Gbit/s Ethernet switches, and 48-port, 1 Gbit/s Ethernet Top of Rack (ToR) switches, which allowed 20,000 servers in the array. The goal of Firehose 1.0 was to deliver 1 Gbps of nonblocking bisection bandwidth to each of 10,000 servers, but it ran into problems with the low connectivity of the ToR switch that caused problems when links failed. Firehose 1.1 was the first custom-designed switch with better connectivity in the ToR switch. Watchtower and Saturn followed in the same footsteps, but used new, faster merchant switch chips. Jupiter uses 40 Gbps links and switches to deliver more than 1 Pbit/s of bisection bandwidth. [Section 6.7](#_bookmark300) describes the Jupi- ter switch and the Edge Aggregation and Spine Blocks of Clos networks in more detail.

> 图 6.23 在 Google WSCS 部署的六代网络交换机（Singh 等，2015）。四位 CRS 使用了商业 512 端口，1 Gbit/s 以太网开关和 48 端口，1 Gbit/s 架子的顶部（TOR）开关，该开关允许阵列中的 20,000 台服务器。Firehose 1.0 的目标是将 1 Gbps 的非封锁一分配带宽输送到 10,000 台服务器中的每台，但由于 TOR 开关的低连接性，在链接失败时引起问题的问题遇到了问题。Firehose 1.1 是第一个定制设计的开关，在 TOR 开关中具有更好的连接性。守望台和土星跟随相同的脚步，但使用了新的，更快的商人开关芯片。木星使用 40 GBPS 链接和开关提供超过 1 个 PBIT/s 的双分配带宽。[第 6.7 节]（#\_ bookmark300）更详细地描述了关闭网络的 jupi-ter 开关以及边缘聚合和脊柱块。

### Using Energy Efficiently Inside the Server

> ###在服务器内有效地使用能源

Although PUE measures the efficiency of a WSC, it has nothing to say about what goes on inside the IT equipment. Thus, another source of electrical inefficiency is the power supply _inside_ the server, which converts input of high voltage to the voltages that chips and disks use. In 2007 many power supplies were 60%– 80% efficient, which meant there were greater losses inside the server than there were going through the many steps and voltage changes from the high-voltage lines at the utility tower to supply the low-voltage lines at the server. One reason was that the power supply was often oversized in watts for what was on the motherboard. Moreover, such power supplies were typically at their worst effi- ciency at 25% load or less, even though, as [Figure 6.3](#_bookmark275) on page 441 shows, many WSC servers operate in that range. Computer motherboards also have voltage reg- ulator modules (VRMs), and they can have relatively low efficiency as well.

> 尽管 Pue 衡量了 WSC 的效率，但对于 IT 设备内部发生的事情，它无话可说。因此，电效率的另一个来源是电源 *inside* 服务器，该服务器将高压的输入转换为芯片和磁盘使用的电压。在 2007 年，许多电源效率为 60％ - 80％，这意味着服务器内部的损失要比经历了多个步骤和电压的变化，而电压的变化来自 Utility Tower 的高压线路以提供低压线路在服务器上。原因之一是，电源通常是瓦特的超大供应主板上的东西。此外，这种电源通常在 25％或更少的情况下以其最差的效率，即使[图 6.3]（#\_ bookmark275）在第 441 页上显示，许多 WSC 服务器在该范围内运行。计算机主板还具有电压调节模块（VRMS），它们的效率也相对较低。

[Barroso and H€](#_bookmark923)olzle [(2007)](#_bookmark923) said the goal for the whole server should be _energy proportionality_; that is, servers should consume energy in proportion to the amount of work performed. A decade later, we’ve gotten close but have not hit that ideal goal. For example, the best-rated SPECpower servers in [Chapter 1](#_bookmark2) still use about 20% of the full power when idle and almost 50% of full power at just 20% load. That represents huge progress since 2007 when an idle computer used 60% of full power and 70% at a 20% load, but there is still room to improve.

> [Barroso and H€]（#_ bookmark923）Olzle [（2007）]（#_ bookmark923）说，整个服务器的目标应为*energy 比例*;也就是说，服务器应与执行的工作量成比例地消耗能量。十年后，我们已经接近了，但没有达到理想的目标。例如，[第 1 章]（#\_ bookmark2）中最优惠的规格服务器在空闲时仍使用约 20％的全功率，而几乎只有 20％的负载，几乎全功率的 50％。这代表了自 2007 年以来的巨大进步，当时闲置的计算机使用了 60％的全功率和 70％的负载，但仍有改进的空间。

Systems software is designed to use all of an available resource if it potentially improves performance, without concern for the energy implications. For example, operating systems use all of memory for program data or for file caches, although much of the data will likely never be used. Software architects need to consider energy as well as performance in future designs ([Carter and Rajamani, 2010](#_bookmark933)).

> 系统软件旨在使用所有可用资源，如果它有可能提高性能，而无需担心能源影响。例如，操作系统将所有内存用于程序数据或文件缓存，尽管许多数据可能永远不会使用。软件架构师需要在未来设计中考虑能量和性能（[[Carter and Rajamani，2010]（#\_ bookmark933））。

Given the background from these six sections, we are now ready to appreciate the work of the Google WSC architects.

> 鉴于这六个部分的背景，我们现在准备欣赏 Google WSC Architects 的工作。

Because many companies with WSCs are competing vigorously in the market- place, most have been reluctant to share their latest innovations with the public (and each other). Fortunately, Google has continued its tradition of providing details on recent WSCs for new editions of this book, once again making this edition likely the most up-to-date public description of a Google WSC, which is representative of the current state-of-the-art.

> 由于许多拥有 WSC 的公司都在市场上积极竞争，因此大多数公司都不愿与公众（彼此之间）分享其最新创新。幸运的是，Google 一直在其传统上提供有关本书新版本的最新 WSC 的详细信息，再次使该版本成为 Google WSC 的最新公开描述，该版本代表了当前的现状。艺术。

### Power Distribution in a Google WSC

> ### Google WSC 中的发电

We start with power distribution. Although there are many variations deployed, in North America electric power typically goes through multiple voltage changes on the way to the server, starting with the high-voltage lines at the utility tower of over 110,000 V.

> 我们从配电开始。尽管部署了许多变化，但在北美，电力通常会在通往服务器的途中经历多个电压变化，从超过 110,000 V 的 Utility Tower 的高压线开始。

For large-scale sites with multiple WSCs, power is delivered to on-site substa- tions ([Figure 6.24](#_bookmark301)). The substations are sized for hundreds of megawatts of power. The voltage is reduced to between 10,000 and 35,000 V for distribution to WSCs on the site.

> 对于具有多个 WSC 的大型站点，将功率传递到现场取代（[图 6.24]（#\_ bookmark301））。这些变电站的大小适用于数百兆瓦的权力。电压降至 10,000 至 35,000 V 之间，以分配到现场的 WSC。

Near the buildings of the WSC, the voltage is further reduced to around 400 V ([Figure 6.25](#_bookmark302)) for distribution to the rows of servers on the data center floor. (480 V is common in North America, but 400 V in the rest of the world; Google uses 415 V.) To prevent the whole WSC from going offline if power is lost, WSCs have their version of an uninterruptible power supply (UPS), just as most servers do in conventional data centers. Diesel generators are connected to the power distribu- tion system at this level to provide power in the event of an issue with the utility power. Although most outages are less than a few minutes, WSCs store thousands of gallons of diesel on site for an extended event. The operators even make pro- visions with local fuel companies for continuous delivery of diesel should a site need to operate from generators for days or weeks.

> 在 WSC 的建筑物的建筑物附近，电压进一步降低至 400 V（[图 6.25]（#\_ bookmark302）），以分配到数据中心楼层的一排服务器。（480 V 在北美很常见，但在世界其他地区 400 V； Google 使用 415 V.）为防止整个 WSC 丢失，如果损失了电力，WSC 的 WSCS 具有不间断的电源版本（UPS）。，就像大多数服务器在常规数据中心一样。柴油发电机连接到此级别的功率分布系统，以便在实用功率发行的情况下提供电源。尽管大多数停电不到几分钟，但 WSC 在现场存储了数千加仑的柴油，以进行延长活动。运营商甚至与当地燃料公司进行策划，以连续交付柴油机，如果现场需要在发电机几天或几周内从发电机操作。

Inside the WSC, power is delivered to the racks via copper bus ducts that run above each row of racks, as [Figure 6.26](#_bookmark303) shows. The last step splits the three-phase power into three separate single-phase powers of 240–277 V delivered by power cables to the rack. Near the top of the rack, power converters turn the 240 V AC current into 48 V DC to bring the voltage down to what boards can use.

> 在 WSC 内部，如[图 6.26]（#\_ bookmark303）所示，通过在每行架子上方运行的铜总线管传递电源。最后一步将三相功率分为三个单独的单相功率，分别是由电缆传递到机架的 240-277 V。在机架顶部附近，电源转换器将 240 V AC 电流变成 48 V DC，以使电压降低到可以使用的板。

<img src="./media/image371.jpeg" style="width:3.93326in;height:2.21in" />

Figure 6.24 An on-site substation.

> 图 6.24 现场变电站。

<img src="./media/image372.jpeg" style="width:3.94189in;height:2.205in" />

Figure 6.25 This image shows transformers, switch gear, and generators in close proximity to a WSC.

> 图 6.25 此图显示了与 WSC 紧密近距离的变压器，开关齿轮和发电机。

<img src="./media/image373.jpeg" style="width:3.94461in;height:2.215in" />

Figure 6.26 Row of servers with the copper bus ducts above that distribute 400 V to the servers. Although hard to see, they are above the shelf on the right side of the photo. It also shows a cold aisle that operators use to service the equipment.

> 图 6.26 用铜总线管将 400 V 分配到服务器上方的铜总线管。尽管很难看到，但它们位于照片右侧的架子上方。它还显示了一个寒冷的过道，该过道使用了操作员为设备提供服务。

In summary, power is distributed in a hierarchy in a WSC, with each level of the hierarchy corresponding to a distinct failure and maintenance unit: the whole WSC, arrays, rows, and racks. Software is aware of the hierarchy, and it spreads work and storage topographically to increase dependability.

> 总而言之，功率分布在 WSC 中的层次结构中，每个层次结构的每个级别都对应于独特的故障和维护单元：整个 WSC，阵列，行，行和机架。软件意识到层次结构，并在地形上传播工作和存储以提高可靠性。

WSCs around the world have different distribution voltages and frequencies, but the overall design is similar. The primary places for improvement in power efficiency are in the voltage transformers at each step, but these are highly optimized components, so there is little opportunity left.

> 世界各地的 WSC 具有不同的分配电压和频率，但总体设计相似。提高功率效率的主要位置是在每个步骤的电压变压器中，但是这些是高度优化的组件，因此几乎没有机会。

### Cooling in a Google WSC

> ###在 Google WSC 中冷却

Now that we can deliver power from the utility poles to the floor of the WSC, we need to remove the heat generated from using it. There are considerably more opportunities for improvement in the cooling infrastructure.

> 现在，我们可以将电力从公用电线杆传递到 WSC 的地板，我们需要去除使用它产生的热量。有更多的改善冷却基础设施的机会。

One of the easiest ways to improve energy efficiency is simply to run the IT equipment at higher temperatures so that the air does not need to be cooled as much. Google runs its equipment at 80+°F (27+°C), which is considerably higher than traditional data centers that are so cold that you need to wear a jacket.

> 提高能源效率的最简单方法之一就是仅在较高温度下运行 IT 设备，以免空气冷却太多。Google 在 80+°F（27+°C）的设备上运行，该设备比传统数据中心要高得多，这些数据中心非常冷，需要穿夹克。

Airflow is carefully planned for the IT equipment, even using Computational Fluid Dynamics simulation to design the facility. Efficient designs preserve the temperature of the cool air by reducing the chances of it mixing with hot air.

> 即使使用计算流体动力学仿真来设计设施，也要为 IT 设备进行精心计划。有效的设计通过减少与热空气混合的机会来保留凉爽空气的温度。

For example, most WSCs today have alternating aisles of hot air and cold air by orienting servers in opposite directions in alternating rows of racks so that hot exhaust blows in alternating directions. They are referred to as _hot aisles_ and *cold aisle*s. [Figure 6.26](#_bookmark303) shows a cold aisle that people use to service the servers, and [Figure 6.27](#_bookmark304) shows the hot aisle. The hot air from the hot aisle rises through ducts into the ceiling.

> 例如，如今，大多数 WSC 都有交替的热空气和冷空气过道，通过将服务器朝相反的方向定向，以交替行架，以便在交替的方向上进行热排气吹动。它们被称为* hot eisles*和 *冷通道 *s。[图 6.26]（#_ bookmark303）显示了人们用来为服务器服务的冷道，[图 6.27]（#_ bookmark304）显示了热过道。来自热过道的热空气通过管道升入天花板。

<img src="./media/image374.jpeg" style="width:5.01868in;height:3.345in" />

Figure 6.27 Hot aisle in a Google data center, which is clearly not designed to accommodate people.

> 图 6.27 Google 数据中心中的热过道，显然不是为了容纳人们而设计的。

<img src="./media/image375.jpeg" style="width:3.15585in;height:2.3in" />

Figure 6.28 The cool air blows into the room containing the aisles of servers. The hot air goes through large vents into the ceilings where it is cooled before returning to these fans.

> 图 6.28 凉爽的空气吹入装有服务器过道的房间。热空气通过大型通风孔进入天花板，然后再返回这些风扇。

In conventional data centers, each server relies on internal fans to ensure a suf- ficient flow of cool air over the hot chips to maintain their temperature. These mechanical fans are one of the weakest components in servers; for example, the MTBF of fans is 150,000 h versus 1,200,000 h for disks. In a Google WSC, the server fans work synergistically with dozens of giant fans in the room to ensure airflow for the whole room ([Figure 6.28](#_bookmark305)). This division of labor means the small server fans use as little power as possible while delivering maximum performance at the worst-case power and ambient conditions. The large fans are controlled using air pressure as the control variable. The fan speeds are adjusted to maintain a min- imum pressure difference between the hot and cold aisles.

> 在传统的数据中心中，每个服务器都依靠内部风扇来确保在热芯片上充分流动凉爽的空气以保持其温度。这些机械风扇是服务器中最弱的组件之一。例如，磁盘的粉丝 MTBF 为 150,000 h，而磁盘为 1,200,000 h。在 Google WSC 中，服务器粉丝在房间中与数十个巨型风扇协同工作，以确保整个房间的气流（[图 6.28]（#\_ bookmark305））。这种人工划分意味着小型服务器风扇在最差的功率和环境条件下提供最大的性能，同时使用尽可能少的功率。大型风扇以气压作为控制变量进行控制。调整风扇速度，以保持热和冷通道之间的最小压力差。

To cool this hot air, they add large-scale fan-coils at either end of the rows of racks. Hot air from the racks is delivered to the fan-coils above via a horizontal plenum inside the hot aisle. (Two rows share the pair of cooling coils, as they are placed above the cold aisle between the two rows.) The cooled air is sent via a plenum in the ceiling to the wall with the big fans in [Figure 6.28](#_bookmark305), which return the cooled air to the room containing the racks.

> 为了冷却这种热空气，它们在架子的两端都添加大型风扇线条。架子上的热空气通过热过道内部的水平式充气传递到上方的风扇线圈。（两排共享两排的冷却线圈，因为它们放在两行之间的寒冷过道上方。）冷却空气是通过天花板中的一个宽度发送到墙壁的，并在图 6.28 中带有大风扇（#\_ bookmark305）），将冷却的空气返回到装有架子的房间。

We’ll describe how to remove the heat from the water in the cooling coils shortly, but let’s reflect on the architecture so far. It separates the racks from the cooling capacity provided by the fan-coils, which allows for sharing of cooling across two rows of racks in the WSC. Thus, it efficiently provides more cooling to high-power racks and less to low-power racks. With thousands of racks in a WSC, they are unlikely to be identical, so power variability between racks is common, which this design accommodates.

> 我们将描述如何尽快从冷却线圈中的水中去除热量，但让我们回顾到目前为止的建筑。它将架子与风扇线圈提供的冷却能力分开，从而可以在 WSC 中的两排架子上共享冷却。因此，它有效地为高功率架提供了更多的冷却，而更少到低功率架。在 WSC 中有成千上万个机架，它们不可能是相同的，因此架子之间的功率可变性很常见，因此该设计可容易。

Cool water is supplied to the individual fan-coils via a network of pipes from a cooling plant. Heat is transferred into the water via forced convection in the cooling coils, and warm water returns to a cooling plant.

> 通过冷却厂的管道网络向单个风扇芯提供冷水。通过在冷却线圈中的强制对流将热量转移到水中，温水返回冷却厂。

To improve the efficiency of WSCs, architects try to use the local environment to remove the heat whenever possible. Evaporative _cooling towers_ are common in WSCs to leverage the colder outside air to cool the water instead of it being chilled mechanically. The temperature that matters is called the _wet-bulb temperature_, which is the lowest temperature that can be achieved by evaporating water with air. It is the temperature a parcel of air would have if it were cooled to saturation (100% relative humidity) by the evaporation of water into it, with the latent heat being supplied by the parcel. Wet-bulb temperature is measured by blowing air at the bulb end of a thermometer that has water on it.

> 为了提高 WSC 的效率，建筑师试图尽可能使用本地环境去除热量。在 WSC 中，蒸发 *Cooling 塔很常见，以利用更冷的外部空气来冷却水，而不是机械冷却。重要的温度称为\_wet-ulb 温度*，这是通过用空气蒸发水来达到的最低温度。如果将水蒸发到饱和度（100％相对湿度）的情况下，将会有一块空气的温度，将水蒸发到饱和度（100％相对湿度）中，并且包裹提供了潜热。通过在温度计的灯泡末端吹出水的温度，可以测量湿块温度。

Warm water is sprayed inside in the cooling tower and collected in pools at the bottom, transferring heat to the outside air via evaporation and thereby cooling the water. This technique is called _water-side economization_. [Figure 6.29](#_bookmark306) shows the steam rising above cooling towers. An alternative is to use cold water instead of crisp air. Google’s WSC in Finland uses a water-to-water heat exchanger that takes the frigid water from the Gulf of Finland to chill the warm water from inside the WSC.

> 温水被喷在冷却塔中，并收集在底部的水池中，通过蒸发将热量转移到外部空气中，从而冷却水。该技术称为 *WATER 侧节省。[图 6.29]（#* bookmark306）显示了蒸汽升高的冷却塔。另一种选择是使用冷水代替酥脆的空气。Google 在芬兰的 WSC 使用了水到水的热交换器，该热交换器从芬兰海湾占据寒冷的水，从 WSC 内部冷却热水。

The cooling tower system uses water caused by evaporation in the cooling towers. For example, an 8-MW facility might need 70,000–200,000 gallons of water per day, thus the desire for the WSC to be located near ample sources of water.

> 冷却塔系统使用冷却塔中蒸发引起的水。例如，一个 8 兆瓦的设施每天可能需要 70,000–200,000 加仑的水，因此希望 WSC 位于充足的水源附近。

Although the cooling plant is designed so that heat can be removed without artificial cooling most of the time, mechanical chillers aid in rejecting the heat in some regions when the weather is warm.

> 尽管设计冷却厂可以在大多数情况下可以去除热量而无需人造冷却，但机械冷却器有助于在天气温暖时拒绝某些地区的热量。

<img src="./media/image376.jpeg" style="width:4.99708in;height:2.805in" />

Figure 6.29 Steam rising from the cooling towers that transfer heat to the air from the water used to cool equipment.

> 图 6.29 蒸汽从冷却塔中升起，这些冷却塔将热量从用于冷却设备的水中传递到空气中。

### Racks of a Google WSC

> ### Google WSC 的架子

We saw how Google gets power to rack and how it cools the hot air that exhausts from the rack. Now we’re ready to explore the rack itself. [Figure 6.30](#_bookmark307) shows a typical rack found inside a Google WSC. To put this rack into context, a WSC consists of multiple arrays (which Google calls clusters). Although arrays vary in size, some have one to two dozen rows with each row holding two to three dozen racks.

> 我们看到了 Google 如何获得电源，以及如何冷却从机架上排气的热空气。现在，我们准备探索机架本身。[图 6.30]（#\_ bookmark307）显示了 Google WSC 中发现的典型机架。为了将该机架置于上下文中，WSC 由多个阵列（Google 调用簇）组成。尽管阵列的大小各不相同，但有些阵列有一到二十排，每行拿着两到三打架子。

The 20 slots shown the middle of the rack in [Figure 6.30](#_bookmark307) hold the servers. Depending on their width, up to four servers can be placed in a single tray. The power converters near the top of the rack turn the 240 V AC current into 48 V DC, which is run on copper bus bars down the back of the rack to power the servers.

> 在[图 6.30]（#\_ Bookmark307）中显示的 20 个插槽显示了架子的中间。根据其宽度，最多可以将四个服务器放入一个托盘中。机架顶部附近的电源转换器将 240 V AC 电流变成 48 V DC，该电源在铜线杆的背面沿机架的背面运行，以便服务器供电。

The diesel generators that provide backup power for the whole WSC take tens of seconds before they can offer power. Instead of populating a large room with enough batteries to power the whole WSC for several minutes—which was a com- mon practice in the early WSCs—Google puts small batteries at the bottom of each rack. Because UPS is distributed to each rack, the cost is incurred only as racks are deployed, instead of paying upfront for the UPS capacity of a full WSC. These batteries are also better than the traditional batteries because they are on the DC side after the voltage conversions, and they use an efficient charging scheme. In addition, replacing the 94%-efficient lead batteries with the 99.99%-efficient local UPS helps to lower the PUE. It’s a very efficient UPS system.

> 为整个 WSC 提供备份功率的柴油发电机在提供电源之前需要数十秒钟。Google 并没有将一个足够的电池填充足够的电池来为整个 WSC 供电几分钟（这是早期 WSC 的练习），而是将小电池放在每个机架的底部。由于 UPS 分配到每个机架上，因此仅在部署机架时就会产生成本，而不是预先支付完整 WSC 的 UPS 容量。这些电池也比传统电池更好，因为它们在电压转换后位于 DC 侧，并且使用有效的充电方案。此外，用 99.99％的本地 UPS 代替 94％效率的铅电池有助于降低 PUE。这是一个非常有效的 UPS 系统。

<img src="./media/image377.jpeg" style="width:1.84157in;height:3.4811in" />

![](./media/image378.png)

Figure 6.30 A Google rack for its WSC. Its dimensions are about 7 ft high, 4 ft wide, and 2 ft deep (2 m× 1.2 m× 0.5 m). The Top of Rack switches are indeed at the top of this rack. Next comes the power converter that converts from 240 V AC to 48 V DC for the servers in the rack using a bus bar at the back of the rack. Next is the 20 slots (depending on the height of the server) that can be configured for the various types of servers that can be placed in the rack. Up to four servers can be placed per tray. At the bottom of the rack are high-efficiency distributed modular DC uninterruptible power supply (UPS) batteries.

> 图 6.30 WSC 的 Google 架子。它的尺寸约为 7 英尺，宽 4 英尺，深 2 英尺（2 m×1.2 m×0.5 m）。机架开关的顶部确实位于此机架的顶部。接下来是使用架子后部的公交栏从机架中的服务器转换为 240 V AC 到 48 V DC 的动力转换器。接下来是可以为可以放置在机架中的各种服务器配置的 20 个插槽（取决于服务器的高度）。每个托盘最多可以放置四个服务器。机架的底部是高效分布的模块化直流电源（UPS）电池。

![](./media/image380.png)

It is comforting that the top of the rack in [Figure 6.30](#_bookmark307) does indeed contain the Top of Rack switch, which we describe next.

> 令人欣慰的是，[图 6.30]（#\_ bookmark307）中机架的顶部确实包含了机架开关的顶部，我们接下来描述了这一点。

### Networking in a Google WSC

> ###网络在 Google WSC 中

The Google WSC network uses a topology called _Clos_, which is named after the telecommunications expert who invented it ([Clos, 1953](#_bookmark938)). [Figure 6.31](#_bookmark308) shows the structure of the Google Clos network. It is a multistage network that uses low port-count (“low radix”) switches, offers fault tolerance, and increases both the network scale and its bisection bandwidth. Google increases the scale simply by adding stages to the multistage network. The fault tolerance is provided by its inherent redundancy, which means a failure of any link has only a small impact on the overall network capacity.

> Google WSC 网络使用称为 *CLOS* 的拓扑，该拓扑以发明它的电信专家命名（[CLOS，1953]（#_ bookmark938））。[图 6.31]（#_ bookmark308）显示了 Google 关闭网络的结构。它是一个多阶段网络，它使用低端口计数（“低辐射”）开关，提供容错性，并增加网络尺度及其一分配带宽。Google 仅通过将阶段添加到多阶段网络来增加规模。容错的固有冗余提供，这意味着任何链接的故障对整体网络容量的影响只有很小的影响。

![](./media/image380.png)

As [Section 6.6](#cross-cutting-issues-3) describes, Google builds customer switches from standard commodity switch chips and uses centralized control for network routing and man- agement. Every switch is given a consistent copy of the current topology of the network, which simplifies the more complex routing of a Clos network.

> 如[第 6.6 节]（＃交叉切割 - 发行-3）所述，Google 构建了从标准商品开关芯片的客户交换机，并使用集中式控制进行网络路由和管理。每个交换机都将获得网络当前拓扑的一致副本，这简化了关闭网络的更复杂的路由。

![](./media/image381.png)

Figure 6.31 A Clos network has three logical stages containing crossbar switches: ingress, middle, and egress. Each input to the ingress stage can go through any of the middle stages to be routed to any output of the egress stage. In this figure, the middle stages are the _M_ Spine Blocks, and the ingress and egress stages are in the _N_ Edge Activation Blocks. [Figure 6.22](#_bookmark298) shows the changes in the Spine Blocks and the Edge Aggregation Blocks over many generations of Clos networks in Google WSCs.

> 图 6.31 关闭网络具有三个逻辑阶段，其中包含横梁开关：入口，中间和出口。进入入口阶段的每个输入都可以穿过任何中间阶段，以路由到出口阶段的任何输出。在此图中，中间阶段是 *m* 脊柱块，入口和出口阶段位于 *n* 边缘激活块中。[图 6.22]（#\_ bookmark298）显示了 Google WSC 中许多世代关闭网络的脊柱块和边缘聚合块的变化。

Figure 6.32 Building blocks of the Jupiter Clos network.

> 图 6.32 木星关闭网络的构建块。

The latest Google switch is Jupiter, which is the switch’s sixth generation. [Figure 6.32](#_bookmark309) shows the building blocks of the switch, and [Figure 6.33](#_bookmark310) shows the wiring of the middle blocks housed in racks. All the cables use bundles of optical fibers.

> 最新的 Google Switch 是木星，这是 Switch 的第六代。[图 6.32]（#_ bookmark309）显示了开关的构建块，[图 6.33]（#_ bookmark310）显示了存放在机架中的中间块的接线。所有电缆都使用多种光纤束。

The commodity switch chip for Jupiter is a 16 16 crossbar using 40 Gbps links. The Top of Rack switch has four of these chips, which are configured with 48 40-Gbps links to the servers and 16 40-Gbps links to the network fabric, yielding an oversubscription of just 3:1, which is better than earlier generations. Moreover, this generation was the first time that servers were offered with 40-Gbps links.

> 木星的商品开关芯片是使用 40 Gbps 链接的 16 16 横杆。机架开关的顶部有四个芯片，它们配备了 48 个 40-GBPS 链接，指向服务器和 16 个 40-GBPS 链接到网络面料，仅获得 3：1 的超额订阅，这比早期几代更好。此外，这一代人是服务器第一次提供 40 Gbps 链接。

The middle blocks in [Figures 6.32](#_bookmark309) and [6.33](#_bookmark310) consist of 16 of the switch chips. They use two stages, with 256 10-Gbps links for the Top of Rack connectivity and 64 40-Gbps links to connect to the rest of the network fabric through the spine. Each of the chips in the Top of Rack switch connects to eight middle blocks using dual redundant 10-Gbps links.

> [图 6.32]（#_ bookmark309）和[6.33]（#_ bookmark310）中的中间块由 16 个开关芯片组成。他们使用两个阶段，带有 256 个 10 Gbps 的链接，用于机架连接的顶部，并通过脊柱连接 64 个 40-GBPS 链接，以连接到网络织物的其余部分。机架开关顶部的每个芯片都使用双冗余的 10-GBPS 链接连接到八个中间块。

Each aggregation block is connected to the spine block with 512 40-Gbps links. A spine block uses 24 switch chips to offer 128 40-Gbps ports to the aggre- gation blocks. At the largest scale, they use 64 aggregation blocks to provide dual redundant links. At this maximum size, the bisection bandwidth is an impressive

> 每个聚合块通过 512 40-Gbps 链接连接到脊柱块。一个脊柱块使用 24 个开关芯片，向凝块块提供 128 40 Gbps 端口。在最大规模上，他们使用 64 个聚合块来提供双冗余链接。在此最大尺寸下，双分配带宽是令人印象深刻的

![](./media/image382.jpeg)

Figure 6.33 Middle blocks of the Jupiter switches housed in racks. Four are packed in a rack. A rack can hold two spine blocks.

> 图 6.33 木星开关的中间块包含在机架中。四个被包装在架子上。架子可以容纳两个脊柱块。

### Servers in a Google WSC

> ###服务器在 Google WSC 中

Now that we have seen how to power, cool, and communicate, we are finally ready to see the computers that do the actual work of the WSC.

> 既然我们已经看到了如何提供动力，冷静和交流，那么我们终于准备好查看从事 WSC 实际工作的计算机。

The example server in [Figure 6.34](#_bookmark311) has two sockets, each containing an 18-core Intel Haswell processor running at 2.3 GHz (see Section 5.8). The photo shows 16 DIMMs, and these servers are typically deployed with 256 GB total of DDR3-1600 DRAM. The Haswell memory hierarchy has two 32 KiB L1 caches, a 256 KiB L2 cache, and 2.5 MiB of L3 cache per core, resulting in a 45 MiB L3 cache. The local memory bandwidth is 44 GB/s with a latency of 70 ns, and the intersocket bandwidth is 31 GB/s with a latency of 140 ns to remote memory. [Kanev et al. (2015)](#_bookmark967) highlighted the differences between the SPEC benchmark suite and a WSC workload. An L3 cache is barely needed for SPEC, but it is useful for a real WSC workload.

> [图 6.34]（#_ bookmark311）中的示例服务器具有两个插座，每个插座包含一个 18 核 Intel Haswell 处理器，运行在 2.3 GHz 时（请参阅第 5.8 节）。该照片显示了 16 个 DIMM，这些服务器通常用 256 GB 的 DDR3-1600 DRAM 部署。Haswell 存储器层次结构具有两个 32 KIB L1 缓存，一个 256 KIB L2 高速缓存和 2.5 MIB 的 L3 CACHE 每个核心，导致 45 MIB L3 CACHE。局部内存带宽为 44 GB/s，延迟为 70 ns，股际带宽为 31 GB/s，延迟为 140 ns，至远程存储器。[Kanev 等。（2015）]（#_ bookmark967）突出显示了规格基准套件和 WSC 工作负载之间的差异。SPEC 几乎不需要 L3 缓存，但对于真正的 WSC 工作负载很有用。

The baseline design has a single network interface card (NIC) for a 10 Gbit/s Ethernet link, although 40 Gbit/s NICs are available. (Other cloud providers moved to 25 Gbit/s or multiples thereof.) While the photo in [Figure 6.34](#_bookmark311) shows two SATA disk drives, each of which can contain up to 8 TB, the server also can be configured with SSD flash drives with 1 TB of storage. The peak power of the baseline is about 150 watts. Four of these servers can fit in a slot of the rack in [Figure 6.30](#_bookmark307).

> 基线设计具有 10 GBIT/S 以太网链接的单个网络接口卡（NIC），尽管有 40 个 GBIT/S NICS 可用。（其他云提供商移至 25 GBIT/s 或其倍数。）虽然[图 6.34]中的照片（#_ bookmark311）显示了两个 SATA 磁盘驱动器，每个驱动器最多可包含 8 tb，但也可以配置服务器 SSD 闪存驱动器带有 1 TB 的存储空间。基线的峰值功率约为 150 瓦。这些服务器中的四个可以适合[图 6.30]中的机架插槽（#_ Bookmark307）。

<img src="./media/image383.jpeg" style="width:3.54224in;height:2.805in" />

Figure 6.34 An example server from a Google WSC. The Haswell CPUs (2 sockets 18 cores 2 threads 72 “virtual cores” per machine) have 2.5 MiB last level cache per core or 45 MiB using DDR3-1600. They use the Wellsburg Platform Controller Hub and have a TFP of 150 W.

> 图 6.34 来自 Google WSC 的示例服务器。Haswell CPU（2 个插座 18 个核 2 螺纹 72“每台机器虚拟内核”）具有 2.5 MIB 的最后一个级别的缓存或使用 DDR3-1600 的 45 MIB。他们使用 Wellsburg 平台控制器中心，TFP 为 150W。

This baseline node is supplemented to offer a storage (or “diskfull”) node. The second unit contains 12 SATA disks and is connected to the server over PCIe. Peak power for a storage node is about 300 watts.

> 该基线节点补充以提供存储（或“ diskfull”）节点。第二个单元包含 12 个 SATA 磁盘，并通过 PCIE 连接到服务器。存储节点的峰值功率约为 300 瓦。

### Conclusion

> ＃＃＃ 结论

In the previous edition, the Google WSC we described had a PUE of 1.23 in 2011. As of 2017, the average PUE of the whole Google fleet of 16 sites dropped to 1.12, with the Belgium WSC leading the way with a 1.09 PUE. The energy-saving techniques include

> 在上一版中，我们描述的 Google WSC 在 2011 年的速度为 1.23。截至 2017 年，整个 Google Fleet 的平均 16 个网站的平均速度下降到 1.12，而比利时 WSC 以 1.09 pue 领先。节能技术包括

- Operating servers at higher temperatures means that air has to be chilled only to 80+°F (27°C) instead of the traditional 64–71°F (18–22°C).

> - 在较高温度下的操作服务器意味着空气必须仅冷却至 80+°F（27°C），而不是传统的 64-71°F（18-22°C）。

- A higher target for cold air temperature helps put the facility more often within the range that can be sustained by cooling towers, which are more energy- efficient than traditional chillers.

> - 更高的冷空气温度目标有助于将设施更频繁地置于通过冷却塔可以维持的范围内，而冷却塔比传统冷却器更节能。

- Deploying WSCs in temperate climates to allow use of evaporative cooling exclusively for large portions of the year.

> - 在温带气候中部署 WSC，以允许在一年中仅使用蒸发冷却。

- Adding large fans for entire rooms to work in concert with the small fans of the servers to reduce energy while satisfying worst-case scenarios.

> - 为整个房间添加大型风扇与服务器的小风扇合作，以减少能量，同时满足最坏的情况。

- Averaging the cooling per server to whole racks of servers by deploying the cooling coils per row to accommodate warmer and cooler racks.

> - 通过部署每行冷却线圈以容纳较温和的较冷架，将每台服务器的冷却到整个服务器平均。

- Deploying extensive monitoring hardware and software to measure actual PUE versus designed PUE improves operational efficiency.

> - 部署广泛的监视硬件和软件以测量实际 PUE 与设计的 PUE 可提高运营效率。

- Operating more servers than the worst-case scenario for the power distribution system would suggest. It is safe since it’s statistically improbable that thou- sands of servers would all be highly busy simultaneously as long as there is a monitoring system to off-load work in the unlikely case that they did ([Fan](#_bookmark946) [et al., 2007; Ranganathan et al., 2006](#_bookmark946)). PUE improves because the facility is operating closer to its fully designed capacity, where it is at its most efficient because the servers and cooling systems are not energy-proportional. Such increased utilization reduces demand for new servers and new WSCs.

> - 与发电机系统最坏的情况相比，操作更多的服务器。这是安全的，因为从统计学上讲，只要有一个监视系统可以在不太可能的情况下，您的服务器都会同时高度忙碌，这是安全的（[fan]（#_ bookmark946）[等，。，2007; Ranganathan 等，2006]（#_ bookmark946））。PUE 改善了，因为该设施的运行距离更接近其全面设计的容量，因为服务器和冷却系统不是能源累积的，因此它最有效。这种增加的利用率减少了对新服务器和新 WSC 的需求。

It will be interesting to see what innovations remain to further improve the WSC efficiency so that we are good guardians of our environment. It is hard to imagine now how engineers might halve the power and cooling overhead of a WSC prior to the next edition of this book, as they did between the previous edition and this one.

> 有趣的是，仍然有哪些创新能够进一步提高 WSC 效率，以便我们成为环境的好监护人。现在很难想象工程师在下一本书的下一本书之前如何将 WSC 的功率和冷却开销减半，就像上一版和本书之间一样。

Despite WSC being just 15 years old, WSC architects like those at Google have already uncovered many pitfalls and fallacies about WSCs, often the hard way. As we said in the introduction, WSC architects are today’s Seymour Crays.

> 尽管 WSC 只有 15 岁，但像 Google 一样的 WSC 建筑师已经发现了有关 WSC 的许多陷阱和谬论，这通常是困难的。正如我们在介绍中所说的那样，WSC 建筑师是今天的西摩·克雷（Seymour Crays）。

Fallacy _Cloud computing providers are losing money._

> 谬论\_Cloud 计算提供商正在亏损。

When AWS was announced, a popular question about cloud computing was whether it was profitable at the low prices at the time. Amazon Web Services has grown so large that it must be recorded separately in Amazon’s quarterly reports. To the surprise of some, AWS has proved to be the most profitable portion of the company. AWS had $12.2 billion in revenue for 2016, with an operating margin of 25%, whereas Amazon’s retail operations had an operating margin of less than 3%. AWS is consistently responsible for three-fourths of Amazon’s profits.

> 当 AWS 宣布时，关于云计算的一个流行问题是，当时它是否以低价盈利。亚马逊 Web 服务的发展如此之大，以至于必须在亚马逊的季度报告中分别记录下来。令人惊讶的是，AWS 被证明是公司中最有利可图的部分。AWS 的 2016 年收入为 122 亿美元，营业利润率为 25％，而亚马逊的零售业务的营业利润率不到 3％。AWS 一直负责亚马逊的四分之三的利润。

Pitfall _Focusing on average performance instead of 99th percentile performance._

> 陷阱\_平均表现而不是第 99％的性能。

As [Dean and Barroso (2013)](#_bookmark940) observed, developers of WSC services worry about the tail more than they care about the mean. If some customers get terrible perfor- mance, that experience can drive them away to a competitor, and they’ll never return.

> 正如[Dean and Barroso（2013）]（#\_ bookmark940）所观察到的那样，WSC 服务的开发人员更担心尾巴，而不是关心均值。如果某些顾客得到糟糕的表现，那么这种经验可以将他们带到竞争对手身上，而他们将永远不会回来。

Pitfall _Using too wimpy a processor when trying to improve WSC cost-performance._

> 陷阱\_尝试改善 WSC 成本绩效时的处理器太 w 弱。

Amdahl's Law still applies to WSC. There will be some serial work for each request and that can increase request latency if this work runs on a slow server ([H€olzle,](#_bookmark962) [2010; Lim et al., 2008](#_bookmark962)). If the serial work increases latency, then the cost of using a wimpy processor must include the software development costs to optimize the code to return it to the lower latency. The larger number of threads of many slow servers can also be more difficult to schedule and load balance, and thus the var- iability in thread performance can lead to longer latencies. When required to wait for the longest task, a 1-in-1000 chance of bad scheduling is probably not an issue with 10 tasks, but problematic with 1000 tasks.

> Amdahl 的定律仍然适用于 WSC。每个请求都会有一些串行工作，如果此工作在慢速服务器上运行（[H€olzle，]（#_ bookmark962）[2010; lim 等，2008]（#_ bookmark962）），则可以增加请求延迟。如果串行工作增加了延迟，那么使用 W 弱处理器的成本必须包括软件开发成本以优化代码以将其返回到较低的延迟。许多慢速服务器的较大线程也可能更难安排和负载平衡，因此线程性能的可变性可能会导致更长的潜伏期。当需要等待最长的任务时，不良调度的 1000 个机会可能不是 10 个任务的问题，而是 1000 个任务问题。

Many smaller servers can also lead to lower utilization because it’s clearly easier to schedule fewer things. Finally, even some parallel algorithms get less efficient when the problem is partitioned too finely. The Google rule of thumb is to use the low-end range of server class computers ([Barroso and](#_bookmark924) [H€olzle, 2009](#_bookmark924)).

> 许多较小的服务器也可以导致较低的利用率，因为显然更容易安排更少的东西。最后，即使某些平行算法过于细微地分配问题，也会效率降低。Google 经验法则是使用低端服务器类计算机（[Barroso and]（#_ bookmark924）[H€Olzle，2009]（#_ bookmark924））。

As a concrete example, [Reddi et al. (2010)](#_bookmark995) compared embedded microproces- sors (Atom) and server microprocessors (Nehalem Xeon) running the Bing search engine. They found that the latency of a query was about three times longer on Atom than on Xeon. Moreover, the Xeon was more robust. As load increases on Xeon, quality of service degrades gradually and modestly. The Atom design quickly violates its quality-of-service target as it tries to absorb additional load. Although the Atom design is more energy-efficient, the response time affects rev- enue, and the revenue loss is likely much greater than the cost savings of less energy. Energy-efficient designs that cannot match the response-time goals are unlikely to be deployed; we’ll see another version of this pitfall lesson in the next chapter (Section 7.9).

> 作为具体的例子，[Reddi 等。（2010）]（#\_ bookmark995）比较了嵌入式的微处理器（Atom）和服务器微处理器（Nehalem Xeon）运行 Bing 搜索引擎。他们发现，查询的延迟在原子上的延迟大约是 Xeon 的三倍。而且，Xeon 更强大。随着 Xeon 负载的增加，服务质量逐渐降低。原子设计很快违反了其服务质量目标，因为它试图吸收额外的负载。尽管原子设计更节能，但响应时间会影响转化，并且收入损失可能比能源减少的成本节省大得多。不太可能部署无法匹配响应时间目标的节能设计；我们将在下一章（第 7.9 节）中查看此陷阱课程的另一个版本。

This behavior translates directly into search quality. Given the importance of latency to the user, as [Figure 6.12](#_bookmark286) suggests, the Bing search engine uses multiple strategies to refine search results if the query latency has not yet exceeded a cutoff latency. The lower latency of the larger Xeon nodes means they can spend more time refining search results. Thus, even when the Atom had almost no load, it gave worse answers in 1% of the queries than Xeon. At normal loads, 2% of the answers were worse.

> 这种行为直接转化为搜索质量。鉴于[图 6.12]（#\_ bookmark286）所暗示的延迟对用户的重要性，如果查询延迟尚未超过截止延迟，则 Bing 搜索引擎会使用多种策略来完善搜索结果。较大的 Xeon 节点的较低延迟意味着他们可以花更多的时间来完善搜索结果。因此，即使原子几乎没有负载，也比 Xeon 给出了 1％的疑问的答案。在正常负载下，答案的 2％更糟。

[Kanev et al. (2015)](#_bookmark967) has more recent, yet consistent, results.

> [Kanev 等。（2015）]（#\_ bookmark967）有更多但一致的结果。

Pitfall _Inconsistent measure of PUE by different companies._

> 陷阱\_不同公司对 pue 的措施。

Google’s PUE measurements start from the power before it reaches the substa- tion. Some measure at the entrance to the WSC, which skips voltage step downs that represent a 6% loss. There will also be different results depending on the season of the year if the WSC relies on the atmosphere to help cool the system. Finally, some report the design goal of the WSC instead of measuring the result- ing system. The most conservative and best PUE measurement is a running average of the past 12 months of the measured PUE, starting from the feed of the utility.

> Google 的 PUE 测量始于功率，然后才能取代。WSC 入口处有些尺寸，跳过了损失 6％的电压下降。如果 WSC 依靠大气来帮助冷却系统，则还会取决于一年的季节。最后，一些人报告了 WSC 的设计目标，而不是测量结果系统。最保守和最佳的 PUE 测量是从实用程序的饲料开始的测量 pue 的过去 12 个月的运行平均值。

Fallacy _Capital costs of the WSC facility are higher than for the servers that it houses._

> WSC 设施的谬误\_级别的成本高于其所容纳的服务器。

Although a quick look at [Figure 6.13](#_bookmark287) on page 453 might lead one to that conclu- sion, that quick glimpse ignores the length of amortization for each part of the full WSC. However, the facility lasts 10–15 years, whereas the servers need to be repurchased every 3 or 4 years. Using the amortization times in [Figure 6.13](#_bookmark287) of 10 years and 3 years, respectively, the capital expenditures over a decade are

> 尽管第 453 页上的[图 6.13]（#\_ bookmark287）的快速查看可能会导致一个结论，但快速瞥见忽略了完整 WSC 的每个部分的摊销时间。但是，该设施持续 10 - 15 年，而服务器需要每 3 或 4 年回购。使用 10 年和 3 年的[图 6.13]（图 6.13]中的摊销时间，十年来的资本支出是

$72 million for the facility and 3.3 $67 million, or $221 million, for servers. Thus, the capital costs for servers in a WSC over a decade are a factor of three higher than for the WSC facility.

> 该设施为 7200 万美元，服务器的 6700 万美元或 2.21 亿美元。因此，十年来，WSC 中服务器的资本成本比 WSC 设施高三倍。

Pitfall _Trying to save power with inactive low power modes versus active low power modes._

> 陷阱\_旅行以通过不活动的低功率模式与主动低功率模式节省电力。

[Figure 6.3](#_bookmark275) on page 441 shows that the average utilization of servers is between 10% and 50%. Given the concern about operational costs of a WSC from [Section 6.4](#the-efficiency-and-cost-of-warehouse-scale-computers), one would think low power modes would be a huge help.

> [图 6.3]（#\_ bookmark275）第 441 页上显示服务器的平均利用率在 10％至 50％之间。考虑到[第 6.4 节]（＃效率和成本构成的房屋规模计算机）的 WSC 运营成本的关注，人们会认为低功率模式将是巨大的帮助。

As [Chapter 1](#_bookmark2) mentions, DRAMs or disks cannot be accessed in these _inactive low power modes_, so they must be returned to fully active mode to read or write, no matter how low the rate. The pitfall is that the time and energy required to return to fully active mode make inactive low power modes less attractive. [Figure 6.3](#_bookmark275) shows that almost all servers average at least 10% utilization, so long periods of low activity might be expected, but not long periods of inactivity (Lo et al., 2014).

> 正如[第 1 章]（#_ bookmark2）提到的那样，在这些_ Intactive 低功率模式中无法访问 DRAM 或磁盘，因此无论速率多低，都必须将它们返回到完全活跃的模式才能读取或写入。陷阱是，返回完全活跃模式所需的时间和能量使无效的低功率模式的吸引力降低了。[图 6.3]（#\_ bookmark275）表明，几乎所有服务器的平均水平至少使用 10％，因此可能会预期长时间的低活动时间，但不长期无活动（Lo 等，2014）。

In contrast, processors still run in lower power modes at a small multiple of the regular rate, so _active low power modes_ are much easier to use. Note that the time to move to fully active mode for processors is also measured in microsec- onds, so active low power modes also address the latency concerns about low power modes.

> 相比之下，处理器仍然以较低的电源模式运行，以固定速率的一小倍数，因此 *active 低功率模式*更容易使用。请注意，在 MicroSec-onds 中还测量了转移到完全活跃模式的时间，因此，主动低功率模式还解决了有关低功率模式的延迟问题。

Fallacy _Given improvements in DRAM dependability and the fault tolerance of WSC systems software, there is no need to spend extra for ECC memory in a WSC._

> 谬误\_ DRAM 可靠性的改进和 WSC Systems 软件的可容忍度，无需在 WSC 中花费额外的 ECC 内存。

Because ECC adds 8 bits to every 64 bits of DRAM, potentially a ninth of the DRAM costs could be saved by eliminating error-correcting code (ECC), especially since measurements of DRAM have claimed failure rates of 1000–5000 FIT (failures per billion hours of operation) per megabit ([Tezzaron Semiconductor, 2004](#_bookmark1012)).

> 由于 ECC 每 64 位 DRAM 增加了 8 位，因此可以通过消除错误校正代码（ECC）来节省 DRAM 成本的九分之一，尤其是因为 DRAM 的测量值声称失败率为 1000-5000 次（每十亿美元的失败率）每小时）每兆位（[Tezzaron Semiconductor，2004]（#\_ bookmark1012））。

[Schroeder et al. (2009)](#_bookmark999) studied measurements of the DRAMs with ECC pro- tection at the majority of Google’s WSCs, which was surely many hundreds of thousands of servers, over a 2.5-year period. They found 15–25 times higher FIT rates than had been published, or 25,000–70,000 failures per megabit. Failures affected more than 8% of DIMMs, and the average DIMM had 4000 correctable errors and 0.2 uncorrectable errors per year. Measured at the server, about a third experienced DRAM errors each year, with an average of 22,000 correctable errors and 1 uncorrectable error per year. That is, for one-third of the servers, one memory error was corrected every 2.5 h. Note that these systems used the more powerful Chipkill codes rather than the simpler SECDED codes. If the easier scheme had been used, the uncorrectable error rates would have been 4–10 times higher.

> [Schroeder 等。（2009）]（#\_ bookmark999）在大多数 Google 的 WSC 中研究了 DRAM 的测量值，该测量在 2。5 年的时间内肯定是数十万台服务器。他们发现拟合率是公布的 15-25 倍，或每兆比特的失败 25,000-70,000 倍。失败影响了 DIMM 的 8％以上，平均 DIMM 每年有 4000 个可更正错误和 0.2 个不可纠正的错误。在服务器上测量，每年大约有三分之一的 DRAM 错误，平均有 22,000 个可更正错误和每年 1 个不可纠正的错误。也就是说，对于三分之一的服务器，每 2.5 h 纠正一次内存误差。请注意，这些系统使用了更强大的 Chipkill 代码，而不是简单的 secd 代码。如果使用了更轻松的方案，则无法纠正的错误率将是高 4-10 倍。

In a WSC that had only parity error protection, the servers would have to reboot for each memory parity error. If the reboot time were 5 min, one-third of the machines would spend 20% of their time rebooting! Such behavior would lower the performance of the expensive facility by about 6%. Moreover, these systems would suffer many uncorrectable errors without operators being notified that they occurred.

> 在仅具有奇偶校验错误保护的 WSC 中，服务器必须为每个内存奇偶校验错误重新启动。如果重新启动时间为 5 分钟，那么三分之一的机器将花费 20％的时间重新启动！这种行为将使昂贵设施的性能降低约 6％。此外，这些系统将遭受许多不可纠正的错误，而不会通知操作员发生。

In the early years, Google used DRAM that did not even have parity protection. In 2000, during testing before shipping the next release of the search index, it started suggesting random documents in response to test queries ([Barroso and](#_bookmark924) [H€olzle, 2009](#_bookmark924)). The reason was a stuck-at-zero fault in some DRAMs, which cor- rupted the new index. Google added consistency checks to detect such errors in the future. As WSC grew in size and as ECC DIMMs became more affordable, ECC became the standard in Google WSCs. ECC has the added benefit of making it much easier to find broken DIMMs during repair.

> 在早期，Google 使用的 DRAM 甚至没有均等保护。在 2000 年，在运送下一个搜索索引的测试期间，它开始提出随机文档以响应测试查询（[barroso and]（#_ bookmark924）[h€olzle，2009]（#_ bookmark924））。原因是某些 DRAM 中的零断层陷入困境，这使新指数破裂。Google 添加了一致性检查以检测将来的错误。随着 WSC 的大小和 ECC DIMM 的价格越来越便宜，ECC 成为 Google WSC 的标准。ECC 具有额外的好处，使得在维修过程中更容易找到破坏的昏暗。

Such data suggest why the Fermi GPU ([Chapter 4](#_bookmark165)) adds ECC to its memory where its predecessors didn’t even have parity protection. Moreover, these FIT rates cast doubts on efforts to use the Intel Atom processor in a WSC—because of its improved power efficiency—since the chip set did not support ECC DRAM.

> 这样的数据表明了为什么 Fermi GPU（[[第 4 章]（#\_ Bookmark165））将 ECC 添加到其内存中，其中其前任甚至没有奇偶校验保护。此外，这些合适的速度对在 WSC 中使用 Intel Atom 处理器的努力（由于其功率提高的提高）引起了怀疑，因为芯片套件不支持 ECC DRAM。

Pitfall _Coping effectively with microsecond delays as opposed to nanosecond or millisecond delays._

> 陷阱\_与纳秒或毫秒延迟有效地有效地延迟。

[Barroso et al. (2017)](#_bookmark926) point out that modern computer systems make it easy for pro- grammers to mitigate latencies in the nanosecond and millisecond timescales (such as cache and DRAM accesses at tens of nanoseconds and disk accesses at a few milliseconds) but that such systems significantly lack support for microsecond- scale events. Programmers get a synchronous interface to the memory hierarchy, with hardware doing heroic work so that such accesses appear consistent and coherent ([Chapter 2](#_bookmark46)). Operating systems offer programmers a similar synchronous interface for a disk read, with many lines of OS code enabling the safe switching to another process while waiting for the disk and then returning again to the original process when the data is ready. We need new mechanisms to cope with the micro- second delays of memory technologies like Flash or the fast network interfaces like 100 Gbit/s Ethernet.

> [Barroso 等。（2017 年）]（#_ bookmark926）指出，现代计算机系统使程序可以轻松地减轻纳米秒和毫秒的时间标准（例如，在数十纳秒和磁盘上访问的纳米秒和毫秒时标），但在几毫秒内访问）这样的系统大大缺乏对微秒尺度事件的支持。程序员获得了与内存层次结构的同步接口，并且硬件执行英雄工作，以使此类访问看起来一致且连贯（[[第 2 章]（#_ bookmark46））。操作系统为程序员提供了一个类似的同步接口，用于磁盘读取，许多 OS 代码可以在等待磁盘时安全切换到另一个过程，然后在数据准备就绪时再次返回原始过程。我们需要新的机制来应对闪存或快速网络接口（例如 100 GBIT/S 以太网）的记忆技术的微观延迟。

Fallacy _Turning off hardware during periods of low activity improves cost-performance of a WSC._

> 谬误\_在低活动期间撤离硬件可改善 WSC 的成本绩效。

[Figure 6.14](#_bookmark288) on page 454 shows that the cost of amortizing the power distribution and cooling infrastructure is 50% higher than the entire monthly power bill. Thus, although it certainly would save some money to compact workloads and turn off idle machines, even if half the power were saved, the monthly operational bill would be reduced only by 7%. There would also be practical problems to overcome because the extensive WSC monitoring infrastructure depends on being able to poke equipment and see it respond. Another advantage of energy proportionality and active low power modes is that they are compatible with the WSC monitoring infrastructure, which allows a single operator to be responsible for more than 1000 servers. Note also that preventive maintenance is one of the important tasks that take place during idle time.

> [图 6.14]（#\_ bookmark288）第 454 页上表明，摊销电源分配和冷却基础设施的成本比整个每月电费账单高 50％。因此，尽管它肯定会节省一些钱来紧凑工作量并关闭空闲机器，但即使节省了一半的功率，但每月的运营账单也只会减少 7％。也将有实际问题要克服，因为广泛的 WSC 监控基础架构取决于能够戳戳设备并看到其响应。能源比例和主动低功率模式的另一个优点是它们与 WSC 监视基础架构兼容，该基础架构允许单个操作员负责 1000 多个服务器。还要注意，预防性维护是在空闲时间内发生的重要任务之一。

The conventional WSC wisdom is to run other valuable tasks during periods of little activity to recoup the investment in power distribution and cooling. A prime example is the batch MapReduce jobs that create indices for search. Another exam- ple of getting value from meager utilization is spot pricing on AWS, which the example in [Figure 6.17](#_bookmark292) on page 461 illustrates. AWS users who are flexible about when their tasks are run can save up to a factor of four for computation by letting AWS schedule the tasks more flexibly using spot instances, such as when the WSC would otherwise have low utilization.

> 传统的 WSC 智慧是在很少的活动期间运行其他有价值的任务，以弥补发电和冷却的投资。一个主要的示例是创建用于搜索索引的批处理 MapReduce 作业。从微薄利用率中获得价值的另一个检查是 AWS 上的斑点定价，该示例在第 461 页上的[图 6.17]（#\_ bookmark292）中进行了示例。通过让 AWS 更灵活地安排任务的 AWS 用户可以灵活地运行其任务可以节省四倍的计算，例如何时 WSC 何时使用较低的利用率。
