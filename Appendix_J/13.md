## Exercises

1. [12] <J.2>Using `n` bits, what is the largest and smallest integer that can be repre- sented in the two’s complement system?
2. [20/25] <J.2>In the subsection "Signed Numbers" (page J-7), it was stated that two’s complement overflows when the carry into the high-order bit position is dif-
   ferent from the carry-out from that position.
3. [20] <J.2>Give examples of pairs of integers for all four combinations of carry-in and carry-out. Verify the rule stated above.
4. [25] <J.2>Explain why the rule is always true.

   1. [12] <J.2>Using 4-bit binary numbers, multiply —8 ×— 8
      using Booth recoding.
   2. [15] <J.2>[Equations J.2.1](#_bookmark766) and [J.2.2](#_bookmark767) are for adding two `n`-bit numbers. Derive similar equations for subtraction, where there will be a borrow instead of a carry.
5. [25] <J.2>On a machine that doesn’t detect integer overflow in hardware, show how you would detect overflow on a signed addition operation in software.
6. [15/15/20] <J.3>Represent the following numbers as single-precision and double-precision IEEE floating-point numbers:

   1. [15] <J.3> 10.
   2. [15] <J.3> 10.5.
   3. [20] <J.3> 0.1.
7. [12/12/12/12/12] <J.3>Below is a list of floating-point numbers. In single preci- sion, write down each number in binary, in decimal, and give its representation in IEEE arithmetic.
8. [12] <J.3>The largest number less than 1.
9. [12] <J.3>The largest number.
10. [12] <J.3>The smallest positive normalized number.
11. [12] <J.3>The largest denormal number.
12. [12] <J.3>The smallest positive number.

<!-- -->

1. [15] <J.3>Is the ordering of nonnegative floating-point numbers the same as inte- gers when denormalized numbers are also considered?
2. [20] <J.3>Write a program that prints out the bit patterns used to represent floating-point numbers on your favorite computer. What bit pattern is used for NaN?
3. [15] <J.4>Using `p` = 4, show how the binary floating-point multiply algorithm computes the product of 1.875 × 1.875.
4. [12/10] <J.4>Concerning the addition of exponents in floating-point multiply:

   1. [12] <J.4>What would the hardware that implements the addition of expo- nents look like?
   2. [10] <J.4>If the bias in single precision were 129 instead of 127, would addi- tion be harder or easier to implement?
5. [15/12] <J.4>In the discussion of overflow detection for floating-point multipli- cation, it was stated that (for single precision) you can detect an overflowed expo- nent by performing exponent addition in a 9-bit adder.
6. [15] <J.4>Give the exact rule for detecting overflow.
7. [12] <J.4>Would overflow detection be any easier if you used a 10-bit adder instead?

<!-- -->

1. [15/10] <J.4>Floating-point multiplication:

   1. [15] <J.4>Construct two single-precision floating-point numbers whose prod- uct doesn’t overflow until the final rounding step.
   2. [10] <J.4>Is there any rounding mode where this phenomenon cannot occur?
2. [15] <J.4>Give an example of a product with a denormal operand but a normal- ized output. How large was the final shifting step? What is the maximum possible shift that can occur when the inputs are double-precision numbers?
3. [15] <J.5>Use the floating-point addition algorithm on page J-23 to compute 1.0102 —.10012 (in 4-bit precision).
4. [10/15/20/20/20] <J.5>In certain situations, you can be sure that `a` + `b` is exactly representable as a floating-point number, that is, no rounding is necessary.

   1. [10] <J.5>If `a`, `b` have the same exponent and different signs, explain why `a` > \+ `b` is exact. This was used in the subsection "Speeding Up Addition" on page J-25.
5. [15] <J.5>Give an example where the exponents differ by 1, `a` and `b` have dif- ferent signs, and `a` + `b` is not exact.
6. [20] <J.5>If `a b` 0, and the top two bits of `a` cancel when computing `a b`, explain why the result is exact (this fact is mentioned on page J-22).
7. [20] <J.5>If `a b` 0, and the exponents differ by 1, show that `a b` is exact unless the high order bit of `a b` is in the same position as that of `a` (mentioned
   in "Speeding Up Addition," page J-25).
8. [20] <J.5>If the result of `a b` or `a` + `b` is denormal, show that the result is exact (mentioned in the subsection "Underflow," on page J-36).

<!-- -->

1. [15/20] <J.5>Fast floating-point addition (using parallel adders) for `p` = 5.

   1. [15] <J.5>Step through the fast addition algorithm for `a` + `b,` where `a` = 1.01112 and `b` =.110112.
2. [20] <J.5>Suppose the rounding mode is toward + ∞. What complication arises in the above example for the adder that assumes a carry-out? Suggest a solution.
3. [12] <J.4, J.5>How would you use two parallel adders to avoid the final round-up addition in floating-point multiplication?
4. [30/10] <J.5>This problem presents a way to reduce the number of addition steps in floating-point addition from three to two using only a single adder.

   1. [30] <J.5>Let `A` and `B` be integers of opposite signs, with `a` and `b` their mag- nitudes. Show that the following rules for manipulating the unsigned numbers `a` and `b` gives `A` + `B.`
5. Complement one of the operands.
6. Use end-around carry to add the complemented operand and the other (uncomplemented) one.
7. If there was a carry-out, the sign of the result is the sign associated with the uncomplemented operand.
8. Otherwise, if there was no carry-out, complement the result, and give it the sign of the complemented operand.

   1. [10] <J.5>Use the above to show how steps 2 and 4 in the floating-point addi- tion algorithm on page J-23 can be performed using only a single addition.

   <!-- -->

   1. [20/15/20/15/20/15] <J.6>Iterative square root.

      1. [20] <J.6>Use Newton’s method to derive an iterative algorithm for square root. The formula will involve a division.
      2. [15] <J.6>What is the fastest way you can think of to divide a floating-point number by 2?
      3. [20] <J.6>If division is slow, then the iterative square root routine will also be slow. Use Newton’s method on `f`(_x_) 1/_x_2 \_a_ to derive a method that doesn’t use any divisions.
9. [15] <J.6>Assume that the ratio division by 2 : floating-point add : floating- point multiply is 1:2:4. What ratios of multiplication time to divide time makes each iteration step in the method of part (c) faster than each iteration in the method of part (a)?
10. [20] <J.6>When using the method of part (a), how many bits need to be in the initial guess in order to get double-precision accuracy after three iterations? (You may ignore rounding error.)
11. [15] <J.6>Suppose that when spice runs on the TI 8847, it spends 16.7% of its time in the square root routine (this percentage has been measured on other machines). Using the values in Figure J.36 and assuming three iterations, how much slower would spice run if square root were implemented in software using the method of part(a)?
12. [10/20/15/15/15] <J.6>Correctly rounded iterative division.

Let `a` and `b` be floating-point numbers with `p`-bit significands (_p_ 53 in double precision). Let `q` be the exact quotient _q a_/_b_,1 `q` < 2. Suppose that `q` is the result of an iteration process, that `q` has a few extra bits of precision, and that 0 < `q q` < 2—_p_. For the following, it is important that `q` < `q`, even when `q` can be exactly represented as a floating-point number.

1. [10] <J.6>If `x` is a floating-point number, and 1 `x` < 2, what is the next rep- resentable number after `x`?
2. [20] <J.6>Show how to compute `q`' from `q`, where `q`' has `p` + 1 bits of precision and \|_q_ — `q`'\|< 2—_p_.
3. [15] <J.6>Assuming round to nearest, show that the correctly rounded quo- tient is either `q`', `q`' — 2—_p_, or `q`' + 2—_p_.
4. [15] <J.6>Give rules for computing the correctly rounded quotient from `q`' based on the low-order bit of `q`' and the sign of `a` — `bq`'.
5. [15] <J.6>Solve part (c) for the other three rounding modes.

<!-- -->

1. [15] <J.6>Verify the formula on page J-30. (_Hint_: If `xn` = `x` 0(2 — \_x*0_b*)× Π h1+ (1 — `x b`)2*i* i, then 2 — `x b` = 2 — `x b`(2 — `x b`)Πh1+ (1 — `x b` 2*i* i 2 — h1 — (1 — `x b`)2iΠh1+ (1 — `x b`)2*i* i.)
2. [15] <J.7>Our example that showed that double rounding can give a different answer from rounding once used the round-to-even rule. If halfway cases are always rounded up, is double rounding still dangerous?
3. [10/10/20/20] <J.7>Some of the cases of the italicized statement in the "Preci- sions" subsection (page J-33) aren’t hard to demonstrate.

   1. [10] <J.7>What form must a binary number have if rounding to `q` bits followed by rounding to `p` bits gives a different answer than rounding directly to `p` bits?
   2. [10] <J.7>Show that for multiplication of `p`-bit numbers, rounding to `q` bits followed by rounding to `p` bits is the same as rounding immediately to `p` bits if `q` ≥ 2*p*.
   3. [20] <J.7>If `a` and `b` are `p`-bit numbers with the same sign, show that rounding `a` + `b` to `q` bits followed by rounding to `p` bits is the same as rounding immedi- ately to `p` bits if `q` ≥ 2*p* + 1.
   4. [20] <J.7>Do part (c) when `a` and `b` have opposite signs.
4. [Discussion] <J.7>In the MIPS approach to exception handling, you need a test for determining whether two floating-point operands could cause an exception. This should be fast and also not have too many false positives. Can you come up with a practical test? The performance cost of your design will depend on the distribution of floating-point numbers. This is discussed in [Knuth [1981]](#_bookmark821) and the Hamming paper in [Swartzlander [1990]](#_bookmark834).
5. [12/12/10] <J.8>Carry-skip adders.

   1. [12] <J.8>Assuming that time is proportional to logic levels, how long does it take an `n`-bit adder divided into (fixed) blocks of length `k` bits to perform an addition?
6. [12] <J.8>What value of `k` gives the fastest adder?
7. [10] <J.8>Explain why the carry-skip adder takes time 0(,_n_).

<!-- -->

1. [10/15/20] <J.8>Complete the details of the block diagrams for the following adders.

   1. [10] <J.8>In [Figure J.15](#_bookmark787), show how to

      > implement the "1" and "2" boxes in terms of AND and OR gates.
      >
   2. [15] <J.8>In [Figure J.19](#_bookmark791), what signals

      > need to flow from the adder cells in the top row into the "C"
      > cells? Write the logic equations for the "C" box.
      >
   3. [20] <J.8>Show how to extend the block diagram in J.17

      > so it will produce the carry-out bit \_c_8.
      >
2. [15] <J.9>For ordinary Booth recoding, the multiple of `b`
   used in the *i*th step is simply `ai`—1
   `ai`. Can you find a similar formula for radix-4 Booth
   recoding (overlapped triplets)?
3. [20] <J.9>Expand [Figure J.29](#_bookmark798) in the
   fashion of J.27, showing the individual adders.
4. [25] <J.9>Write out the analog of Figure J.25 for radix-8
   Booth recoding.
5. [18] <J.9>Suppose that `an`—1 …
   `a` 1*a_0 and \_bn*—1 …
   `b` 1*b_0 are being added in a signed-digit
   adder as illustrated in the example on page J-53. Write a formula
   for the *i*th bit of the sum, \_si*, in terms of
   `ai`, `ai`—1,
   `ai`—2, `bi`,
   `bi`—1, and `bi`—2.
6. [15] <J.9>The text discussed radix-4 SRT division with
   quotient digits of —2,
   —1, 0, 1, 2. Suppose that 3 and —3 are also allowed as quotient digits. What relation replaces \|_ri_\|≤ 2*b*/3?
7. [25/20/30] <J.9>Concerning the SRT division table, Figure
   J.34:

   1. [25] <J.9>Write a program to generate the results of
      Figure J.34.
   2. [20] <J.9>Note that Figure J.34 has a certain symmetry

      > with respect to pos- itive and negative values of P. Can you
      > find a way to exploit the symmetry and
      > only store the values for positive P?
      >
8. [30] <J.9>Suppose a carry-save adder is used instead of a
   propagate adder. The input to the quotient lookup table will be `k`
   bits of divisor and `l` bits of
   remainder, where the remainder bits are computed by summing the top `l` bits of the sum and carry registers. What are `k` and `l`? Write a program to generate the analog of Figure J.34.
9. [12/12/12] <J.9, J.12>The first several million Pentium
   chips produced had a flaw that caused division to sometimes return
   the wrong result. The Pentium uses
   a radix-4 SRT algorithm similar to the one illustrated in the example on page J-56 (but with the remainder stored in carry-save format; see Exercise J.33(c)). Accord- ing to Intel, the bug was due to five incorrect entries in the quotient lookup table.
10. [12] <J.9, J.12>The bad entries should have had a quotient of plus or minus 2, but instead had a quotient of 0. Because of redundancy, it’s conceivable that the
    algorithm could "recover" from a bad quotient digit on later iterations. Show that this is not possible for the Pentium flaw.
11. [12] <J.9, J.12>Since the operation is a floating-point divide rather than an integer divide, the SRT division algorithm on page J-45 must be modified in
    two ways. First, step 1 is no longer needed, since the divisor is already normal- ized. Second, the very first remainder may not satisfy the proper bound ( `r` 2*b*/3 for Pentium; see page J-55). Show that skipping the very first left shift in step 2(a) of the SRT algorithm will solve this problem.
12. [12] <J.9, J.12>If the faulty table entries were indexed by
    a remainder that could occur at the very first divide step (when the
    remainder is the divisor), ran-
    dom testing would quickly reveal the bug. This didn’t happen. What does that tell you about the remainder values that index the faulty entries?
13. [12] <J.6, J.9>The discussion of the remainder-step instruction assumed that division was done using a bit-at-a-time algorithm. What would have to change
    if division were implemented using a higher-radix method?
14. [25] <J.9>In the array of [Figure J.28](#_bookmark797), the fact that an array can be pipelined is not exploited. Can you come up with a design that feeds the output of the bottom CSA
    into the bottom CSAs instead of the top one, and that will run faster than the arrangement of [Figure J.28](#_bookmark797)?
