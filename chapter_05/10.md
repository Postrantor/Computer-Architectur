## Historical Perspectives and References

Section M.7 (available online) looks at the history of multiprocessors and parallel processing. Divided by both time period and architecture, the section features dis- cussions on early experimental multiprocessors and some of the great debates in parallel processing. Recent advances are also covered. References for further read- ing are included.

## Case Studies and Exercises by Amr Zaky and David A. Wood

### Case Study 1: Single Chip Multicore Multiprocessor

##### _Concepts illustrated by this case study_

- Snooping Coherence Protocol Transitions
- Coherence Protocol Performance
- Coherence Protocol Optimizations
- Synchronization
  A multicore SMT multiprocessor is illustrated in [Figure 5.37](#_bookmark264). Only the cache con- tents are shown. Each core has a single, private cache with coherence maintained using the snooping coherence protocol of [Figure 5.7](#_bookmark223). Each cache is direct-mapped, with four lines, each holding 2 bytes (to simplify diagram). For further simplifica- tion, the whole line addresses in memory are shown in the address fields in the caches, where the tag would normally exist. The coherence states are denoted M, S, and I for Modified, Shared, and Invalid.

1. \[10/10/10/10/10/10/10\] &lt;5.2&gt; For each part of this exercise,
   the initial cache and memory state are assumed to initially have the
   contents shown in [Figure 5.37](#_bookmark264). Each
   part of this exercise specifies a sequence of one or more CPU operations of the form

Figure 5.37 Multicore (point-to-point) multiprocessor.

Ccore#: R, &lt;address&gt; for reads and

Ccore#: W, &lt;address&gt; &lt;-- &lt;value written&gt; for writes. For example,

C3: R, AC10 & C0: W, AC18 &lt;-- 0018

Read and write operations are for 1 byte at a time. Show the resulting state (i.e., coherence state, tags, and data) of the caches and memory after the actions given below. Show only the cache lines that experience some state change; for example: C0.L0: (I, AC20, 0001) indicates that line 0 in core 0 assumes an “invalid” coherence state (I), stores AC20 from the memory, and has data con- tents 0001. Furthermore, represent any changes to the memory state as M:

1. \[20/20/20/20\] &lt;5.3&gt; The performance of a snooping cache-coherent multiprocessor depends on many detailed implementation issues that determine how quickly a cache
   responds with data in an exclusive or M state block. In some implementations, a pro- cessor read miss to a cache block that is exclusive in another processor’s cache is faster than a miss to a block in memory. This is because caches are smaller, and thus faster, than main memory. Conversely, in some implementations, misses satisfied by memory are faster than those satisfied by caches. This is because caches are generally optimized for “front side” or CPU references, rather than “back side” or snooping accesses. For the multiprocessor illustrated in [Figure 5.37](#_bookmark264), consider the execution of a sequence of operations on a single processor core where

- read and write hits generate no stall cycles;
- read and write misses generate N*<sub>memory</sub>* and N*<sub>cache</sub>* stall cycles if satisfied by memory and cache, respectively;
- write hits that generate an invalidate incur N*<sub>invalidate</sub>* stall cycles; and
- a write-back of a block, either due to a conflict or another processor’s request to an exclusive block, incurs an additional N*<sub>writeback</sub>* stall cycles.
  Consider two implementations with different performance characteristics summa- rized in [Figure 5.38](#_bookmark265).

Figure 5.38 Snooping coherence latencies.

To observe how these cycle values are used, we illustrate how the following sequence of operations, assuming the initial caches’ states in [Figure 5.37](#_bookmark264), behave under implementation 1.

C1: R, AC10 C3: R, AC10

For simplicity, assume that the second operation begins after the first com- pletes, even though they are on different processor cores.

For Implementation 1,

- the first read generates 50 stall cycles because the read is
  satisfied by C0’s cache: C1 stalls for 40 cycles while it waits for
  the block, and C0 stalls for 10 cycles while it writes the block
  back to memory in response to C1’s request; and
- the second read by C3 generates 100 stall cycles because its miss is
  satisfied by memory.
  Therefore this sequence generates a total of 150 stall cycles.

For the following sequences of operations, how many stall cycles are generated by each implementation?

1. \[20\] &lt;5.3&gt; C0: R, AC20
   C0: R, AC28

C0: R, AC30

1. \[20\] &lt;5.3&gt; C0: R, AC00
   C0: W, AC08 &lt;-- 48 C0: W, AC30 &lt;-- 78
2. \[20\] &lt;5.3&gt; C1: R, AC20
   C1: R, AC28

C1: R, AC30

1. \[20\] &lt;5.3&gt; C1: R, AC00
   C1: W, AC08 &lt;-- 48 C1: W, AC30 &lt;-- 78
2. \[20\] &lt;5.2&gt;
   Someapplicationsreadalargedatasetfirstandthenmodifymostorallofit.
   The base MSI coherence protocol will first fetch all of the cache
   blocks in the Shared
   state and then be forced to perform an invalidate operation to upgrade them to the

Modified state. The additional delay has a significant impact on some workloads. The MESI addition to the standard protocol (see [Section 5.2](#centralized-shared-memory-architectures)) provides some relief in these cases. Draw new protocol diagrams for a MESI protocol that adds the Exclusive state and transitions to the base MSI protocol’s Modified, Shared, and Invalidate states.

1. \[20/20/20/20/20\] &lt;5.2&gt; Assume the cache contents of [Figure 5.37](#_bookmark264) and the timing of Implementation 1 in [Figure 5.38](#_bookmark265). What are the total stall cycles for the following
   code sequences with both the base protocol and the new MESI protocol in Exercise

5.3? Assume state transitions that require zero interconnect transactions incur no additional stall cycles.

1. \[20\] &lt;5.2&gt; C0: R, AC00
   C0: W, AC00 &lt;-- 40
2. \[20\] &lt;5.2&gt; C0: R, AC20
   C0: W, AC20 &lt;-- 60
3. \[20\] &lt;5.2&gt; C0: R, AC00
   C0: R, AC20
4. \[20\] &lt;5.2&gt; C0: R, AC00
   C1: W, AC00 &lt;-- 60
5. \[20\] &lt;5.2&gt; C0: R, AC00
   C0: W, AC00 &lt;-- 60 C1: W, AC00 &lt;-- 40
6. Code running on a single core and not sharing any variables with other cores can suffer some performance degradation because of the snooping coherence protocol. Consider the two following iterative loops are <u>NOT</u> functionally equivalent but they seem similar in complexity. One could be led to conclude that they would spend a comparably close number of cycles when executed on the same processor core.
   Loop 1 Loop 2

Repeat i: 1 .. n Repeat i:1 .. n

A\[i\] &lt;-- A\[i-1\] +B\[i\]; A\[i\] &lt;-- A\[i\] +B\[i\];

Assume that

- every cache line can hold exactly one element of A or B;
- arrays A and B do not interfere in the cache; and
- all the elements of A or B are in the cache before either loop is executed.
  Compare their performance when run on a core whose cache uses the MESI coherence protocol. Use the stall cycles data for Implementation 1 in [Figure 5.38](#_bookmark265). Assume that a cache line can hold multiple elements of A and B (A and B go to separate cache lines). How will this affect the relative performances of Loop1

and Loop2?

Suggest hardware and/or software mechanisms that would improve the perfor- mance of Loop1 on a single core.

1. \[20\] &lt;5.2&gt; Many snooping coherence protocols have additional
   states, state tran- sitions, or bus transactions to reduce the
   overhead of maintaining cache coherency. In Implementation 1 of
   Exercise 5.2, misses are incurring fewer stall cycles when
   they are supplied by cache than when they are supplied by memory. The MOESI protocol extension (see [Section 5.2](#centralized-shared-memory-architectures)) addresses this need.

Draw new protocol diagrams with the additional state and transitions.

1. \[20/20/20/20\] &lt;5.2&gt; For the following code sequences and the
   timing parameters for the two implementations in [Figure
   5.36](#_bookmark260), compute the total stall cycles for the
   base MSI protocol and the optimized MESI protocol in Exercise 5.3. Assume state transitions that do not require bus transactions incur no additional stall cycles.
2. \[20\] &lt;5.2&gt; C1: R, AC10
   C3: R, AC10

C0: R, AC10

1. \[20\] &lt;5.2&gt; C1: R, AC20
   C3: R, AC20

C0: R, AC20

1. \[20\] &lt;5.2&gt; C0: W, AC20 &lt;-- 80
   C3: R, AC20

C0: R, AC20

1. \[20\] &lt;5.2&gt; C0: W, AC08 &lt;--88
   C3: R, AC08

C0: W, AC08 &lt;-- 98

1. \[20/20/20/20\] &lt;5.5&gt; The spin lock is the simplest
   synchronization mechanism possible on most commercial shared-memory
   machines. This spin lock relies on
   the exchange primitive to atomically load the old value and store a new value. The lock routine performs the exchange operation repeatedly until it finds the lock unlocked (i.e., the returned value is 0).

addi x2, x0, \#1 lockit: EXCH x2, 0(x1)

bnez x2, lockit

The lock is released simply by storing a 0 into x2.

As discussed in [Section 5.5](#synchronization-the-basics), the more optimized spin lock employs cache coherence and uses a load to check the lock, allowing it to spin with a shared var- iable in the cache.

lockit: ld x2, 0(x1)

bnez x2, lockit addi x2, x0, \#1 EXCH x2,0(x1)

bnez x2, lockit

Assume that processor cores C0, C1, and C3 are all trying to acquire a lock at address 0xAC00 (i.e., register R1 holds the value 0xAC00). Assume the cache con- tents from [Figure 5.37](#_bookmark264) and the timing parameters from Implementation 1 in [Figure 5.38](#_bookmark265). For simplicity, assume the critical sections are 1000 cycles long.

1. \[20\] &lt;5.5&gt; Using the simple spin lock, determine _approximately_ how many memory stall cycles each processor incurs before acquiring the lock.
2. \[20\] &lt;5.5&gt; Using the optimized spin lock, determine _approximately_ how many memory stall cycles each processor incurs before acquiring the lock.
3. \[20\] &lt;5.5&gt; Using the simple spin lock, _approximately_ how many memory accesses occur?
4. \[20\] &lt;5.5&gt; Using the optimized spin lock, _approximately_ how many memory accesses occur?

### Case Study 2: Simple Directory-Based Coherence

##### _Concepts illustrated by this case study_

- Directory Coherence Protocol Transitions
- Coherence Protocol Performance
- Coherence Protocol Optimizations
  Consider the distributed shared-memory system illustrated in [Figure 5.39](#_bookmark266). It con- sists of 8 nodes of processor cores organized as three-dimensional hypercube with point-to-point interconnections, as shown in the figure. For simplification, we assume the following scaled-down configuration:
- Every node has a _single processor core_ with a direct-mapped L1 data cache with its dedicated cache controller.
- The L1 data cache has a capacity of two cache lines with a line size of B bytes.
- The L1 cache states are denoted M, S, and I for Modified, Shared, and Invalid. An example cache entry in some would like
  1: S, M3, 0xabcd --&gt;

Cache line 1 is in the “Shared” state; it contains memory block M3 and the data value of the block is 0xabcd.

- The system memory comprises 8 memory blocks (i.e., one memory block per node) and is distributed among the eight nodes, with every node owning a memory block. Node Ci owns memory block Mi.
- Each memory block is B-bytes wide and is tracked by a coherency directory entry stored with the memory block.
  Figure 5.39 Multicore multiprocessor with DSM.
- The state of each memory directory entry is denoted DM, DS, and DI
  for Direc- tory Modified, Directory Shared, and Directory Invalid.
  Additionally, the directory entry lists the block sharers using a
  bit vector with 1 bit for every node. Here is an example memory
  block and associated directory entry:
  M3: 0XABCD, DS, 00000011 --&gt;

Memory block M3 (in node C3) contains the value 0xABCD and is shared by nodes 0 and 1 (corresponding to 1s in the bit vector).

### Read/Write Notation

To describe read/write transactions, we will use the notation

Ci#: R, &lt;Mi&gt; for reads and

Ci#: W, &lt;Mi&gt; &lt;-- &lt;value written&gt; for writes.

For example,

C3: R, M2 describes the core in node 3 issuing a read transaction from an address at memory block M2 (the address may possibly be cached in C3 already).

C0: W, M3 &lt;-- 0018 describes the core in node 0 issuing a write transaction (data is 0X0018) to an address at memory block M3 (the address may possibly be cached in C0 already).

### Messages

The directory coherency schemes depend on exchange of command and/or data messages as described by the directory protocol described in [Figure 5.20](#_bookmark238). An example of a command message is a read request. An example of a data message is a read response (with data included).

- Messages originating an ending in the same node do not cross any inter- node links.
- Message with distinct source/destination nodes travel through inter-node links. These messages may be destined from one cache controller to another, from a cache controller to a directory controller, or from a directory controller to a cache controller.
- Messages traveling from a source node to a distinct destination node are stat- ically routed.
  The static routing algorithm selects a shorted path between the source and destination nodes.

The short path is determined by considering the binary representations of the source and destination indices (e.g., 001 for node C1 and 100 for node C4), then by moving from one node to a neighboring node that was not already crossed by the message.

- For example, to go from node 6 to node 0 (110 --&gt; 000), the path is
  110--&gt; 100--&gt; 000.
- Because more than one shorted path may exist (110--&gt; 010--&gt; 000 is another path for the preceding example), we assume that the path is
  selected by inverting first the least significant bit that is different from the corresponding bit in destination index. For example, to travel from node 1 to node 6 (001--&gt; 110), the path is 001--&gt; 000--&gt;

010--&gt; 110.

The longest possible path traveled by any message has 3 links (equal to the number of bits in the binary representation of a node index).

- A node can simultaneously process up to three messages from/to distinct neighboring nodes’ links if no two of them are competing for the same link resource as clarified by the following examples of messages sent/received to/from/through node 000.
  Messages: from 001 --&gt; 010; 010 --&gt; 000 (to cache/directory controller);

100 --&gt; 001. OK (distinct destinations).

Message: from 001 --&gt; 010; 000 --&gt; 001 (from cache/directory control- ler); 100 --&gt; 001.

Not OK as two messages are destined to node 001

In case of destination contention, ties are broken assigning priority to

1. message destined to the node (000 in example) cache or directory controller; then
2. messages forwarded from one to another (through 000 in example); then
3. messages originating from the node (000 in example) cache or directory controller.

- Assume the transmission and service delays in the following table.

<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 29%" />
<col style="width: 33%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Message type</th>
<th><blockquote>
<p>Cache controller</p>
</blockquote></th>
<th><blockquote>
<p>Directory controller</p>
</blockquote></th>
<th><blockquote>
<p>Link</p>
</blockquote></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>No data</td>
<td><blockquote>
<p>2 cycles</p>
</blockquote></td>
<td><blockquote>
<p>5 cycles</p>
</blockquote></td>
<td><blockquote>
<p>10 cycles</p>
</blockquote></td>
</tr>
<tr class="even">
<td>With data</td>
<td><blockquote>
<p>(3 + )<em>B</em>/<sub>4</sub>e) cycles</p>
</blockquote></td>
<td><blockquote>
<p>(6 + 10 * <em>B</em>) cycles</p>
</blockquote></td>
<td><blockquote>
<p>(4 + <em>B</em>)</p>
</blockquote></td>
</tr>
</tbody>
</table>

- If a message is forwarded through a node, it is first completely
  received by the node before being sent to the next node on the path.
- Assume any cache controller; directory controller has unlimited
  capacity to enqueue messages and service them in FCFS order.

  1. \[10/10/10\] &lt;5.4&gt; For each part of this exercise, assume
     that initially all caches lines are invalid, and the data in
     memory Mi is the byte i (0X00 &lt;=i &lt;= 0x07) repeated as
     many times as the block size. Assume that successive requests
     are completely serialized. That is, no core will issue a coherency request until the previous request (by same or different core) is completed.

For each of the following parts,

- show the final state (i.e., coherence state, sharers/owners, tags,
  and data) of the caches and directory controller (including data
  values) after the given transac- tion sequence has completed; and
- show the messages transferred (choose a suitable format for message
  types).

  1. \[10\] &lt;5.4&gt; C3: R, M4
     C3: R, M2

C7: W, M4 &lt;--0xaaaa C1: W, M4 &lt;--0xbbbb

1. \[10\] &lt;5.4&gt; C3: R, M0
   C3: R, M2

C6: W, M4 &lt;--0xaaaa C3: W, M4 &lt;--0xbbbb

1. \[10\] &lt;5.4&gt; C0: R, M7
   C3: R, M4

C6: W, M2 &lt;--0xaaaa C2: W, M2 &lt;--0xbbbb

1. \[10/10/10\] &lt;5.4&gt; The directory protocol used in 5.9 (based on [Figure 5.20](#_bookmark238)) assumes that the directory controller receives requests, sends invalidates, receives
   modified data, sends modified data to requester if block was dirty, and so on. Assume now that the directory controller will delegate some work to the cores. For example, it will notify the exclusive owner of a modified block when some other core needs the block and will have the owner send the block to the new sharer. Specifically, consider the following optimizations and indicate what their benefits (if any) are. Also, specify how the messages will be modified (in compar- ison with [Figure 5.20](#_bookmark238) protocol) to support the new change.

Hint: Benefits might be reduction in number of messages, faster response time, and so on.

1. \[10\] &lt;5.4&gt; On a write miss to a shared memory block, the directory controller sends the data to the requester and instructs the sharers to send their invalidate acknowledgements directly to the requester.
2. \[10\] &lt;5.4&gt; On a read miss to a block modified in some other core, the directory controller instructs the owner of the modified copy to directly forward the data to the requester.
3. \[10\] &lt;5.4&gt; On a read miss to a block in shared (S) state in some other cores, the directory controller instructs one of the sharers (say, the one closest to the
   requester) to directly forward the data to the requester.
4. \[15/15/15\] &lt;5.4&gt; In problem 5.9, it was assumed that all transactions on the sys- tem were serially executed, which is both unrealistic and inefficient in a DSM mul-
   ticore. We now relax this condition. We will require only that all transactions originating in one core are serialized. However, different cores can independently issue their read/write transactions and even compete for the same memory block. The transactions of problem 5.9 are represented next to reflect the new, relaxed constraints. Redo problem 5.9 with the new, relaxed constraints.
5. \[15\] &lt;5.4&gt;
   C1: W, M4 &lt;--0xbbbb C3: R, M4 C7: R, M2

C3: W, M4 &lt;--0xaaaa

1. \[15\] &lt;5.4&gt;
   C3: R, M0 C6: W, M4 &lt;--0xaaaa

C3: R, M2

C3: W, M4 &lt;--0xbbbb

1. \[15\] &lt;5.4&gt;
   C0: R, M7 C2: W, M2 &lt;--0xbbbb C3: R, M4 C6: W, M2 &lt;--0xaaaa
2. \[10/10\] &lt;5.4&gt; Use the routing and delay information described earlier and trace how the following groups of transactions will progress in the system (assume that
   all accesses are misses).
3. C0: R, M7 C2: W, M2 &lt;--0xbbbb C3: R, M4 C6: W, M2 &lt;--0xaaaa
4. C0: R, M7 C3: R, M7
   C2: W, M7 &lt;--0xbbbb
5. \[20\] &lt;5.4&gt; What extra complexities may arise if the messages
   can be adaptively rerouted on the links? For example, a coherency
   message from core M1 directory controller to C2 (expressed in binary
   as M<sub>001</sub> --&gt; C<sub>010</sub>) will be routed either
   through the inter-node path C<sub>001</sub>--&gt;
   C<sub>000</sub>--&gt; C<sub>010</sub> or the inter-node path
   C<sub>001</sub>--&gt; C<sub>011</sub>--
   &gt; C<sub>010</sub>, depending on link availability.
6. \[20\] &lt;5.4&gt; In a read miss, a cache might overwrite a line in
   the shared (S) state without notifying the directory that owns the
   corresponding memory block. Alter-
   natively, it will notify the directory so that it deletes this cache from the list of sharers.

Show how the following transaction groups (performed one at a time in series) will proceed under both approaches.

C3: R, M4 C3: R, M2

C2: W, M4 &lt;--0xabcd

### Case Study 3: Memory Consistency

##### _Concepts Illustrated by This Case Study_

- Legitimate Program Behavior Under Sequential Consistency (SC) Models
- Hardware Optimization Allowed for SC Models
- Using Synchronization Primitives to Make a Consistency Model Emulate
  a More Restrictive Model

  1. \[10/10\] &lt;5.6&gt; Consider the following code segments
     running on two processors P1 and P2. Assume A and B are
     initially 0.

P1:
While (B == 0); A=1;

P2:

While (A==0); B= 1;

1. If the processors adhere to sequential consistency (SC) consistency model. What are the possible values of A and B at the end of the segments? Show the statement interleaving supporting your answer(s).
2. Repeat (a) if the processors adhere to the total store order (TSO) consistency model.

<!-- -->

1. \[5\] &lt;5.6&gt; Consider the following code segments running on
   two processors P1 and P2. Assume A, and B, are initially 0. Explain
   how an optimizing compiler might make it impossible for B to be ever
   set to 2 in a sequentially consistent execution model.

P1:
A=1; A=2;

While (B == 0);

P2:

B=1;

While (A &lt;&gt; 1); B= 2;

1. \[10\] &lt;5.4&gt;. In a processor implementing a SC consistency model, the data cache is augmented with a data prefetch unit. Will that alter the SC implementation exe-
   cution results? Why or why not?
2. \[10/10\] &lt;5.6&gt; Assume that the following code segment is executed on a processor that implements partial store order (PSO),
   A=1;

B=2;

If (C== 3) D=B;

1. Augment the code with synchronization primitives to make it emulate the behavior of a total store order (TSO) implementation.
2. Augment the code with synchronization primitives to make it emulate the behavior of a sequential consistency (SC) implementation.

<!-- -->

1. \[20/20/20\] &lt;5.6&gt; Sequential consistency (SC) requires that all reads and writes appear to have executed in some total order. This may require the processor to stall
   in certain cases before committing a read or write instruction. Consider the code sequence

write A read B

where the write A results in a cache miss and the read B results in a cache hit. Under SC, the processor must stall read B until after it can order (and thus per- form) write A. Simple implementations of SC will stall the processor until the cache

receives the data and can perform the write.

Release consistency (RC) consistency mode (see [Section 5.6](#models-of-memory-consistency-an-introduction)) relaxes these constraints: ordering—when desired—is enforced by judicious use of synchroni- zation operations. This allows, among other optimizations, processors to imple- ment write buffers, which hold committed writes that have not yet been ordered with respect to other processors’ writes. Reads can pass (and potentially bypass) the write buffer in RC (which they could not do in SC).

Assume that one memory operation can be performed per cycle and that oper- ations that hit in the cache or that can be satisfied by the write buffer introduce no stall cycles. Operations that miss incur the latencies listed in [Figure 5.38](#_bookmark265).

How many stall cycles occur _prior_ to each operation for both the SC and RC consistency models? (Write buffer can hold at most one write.)

1. \[20\] &lt;5.6&gt; P0: write 110 &lt;-- 80 //assume miss (no other
   cache has the line)

P0: read 108 //assume miss (no other cache has the line)

1. \[20\] &lt;5.6&gt; P0: read 110 //assume miss (no other
   cache has the line)

P0: write 100 &lt;-- 90 //assume hit

1. \[20\] &lt;5.6&gt; P0: write 100 &lt;-- 80 //assume miss P0: write
   110 &lt;-- 90 //assume hit

<!-- -->

1. \[20\] &lt;5.6&gt; Repeat part (a) of problem 5.19 under an SC model
   on a processor that has a read prefetch unit. Assume a read prefetch
   was triggered 20 cycles in advance
   of the write operation.

### Exercises

1. \[15\] &lt;5.1&gt; Assume that we have a function for an application
   of the form F(i, p), which gives the fraction of time that exactly i
   processors are usable given that a
   total of p processors are available. This means that

_p i_=1

_F_(_i_, _p_) = 1
Assume that when _i_ processors are in use, the applications run _i_ times faster.

1. Rewrite Amdahl’s Law so that it gives the speedup as a function of p for some application.
2. An application A runs on single processor for a time T seconds. Different por- tions of its running time can improve if a larger number of processors is used. [Figure 5.40](#_bookmark267) provides the details.
   How much speedup will A achieve when on 8 processors?
3. Repeat for 32 processors and an infinite number of processors.

<!-- -->

1. \[15/20/10\] &lt;5.1&gt; In this exercise, we examine the effect of
   the interconnection network topology on the CPI of programs running
   on a 64-processor
   distributed-memory multiprocessor. The processor clock rate is 2.0 GHz, and the base CPI of an application with all references hitting in the cache is 0.75. Assume that 0.2% of the instructions involve a remote communication reference. The cost of a remote communication reference is (100 + 10 h) ns, h being the num- ber of communication network hops that a remote reference has to make to the remote processor memory and back. Assume all communication links are bidirectional.
2. \[15\] &lt;5.1&gt; Calculate the worst-case remote communication
   cost when the 64
   processors are arranged as a ring, as an 8 ×*n* 8 processor grid, or as a hypercube

(hint: longest communication path on a 2 hypercube has _n_ links).

<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 10%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="header">
<th><span id="_bookmark267" class="anchor"></span>Fraction of T</th>
<th><blockquote>
<p>20%</p>
</blockquote></th>
<th><blockquote>
<p>20%</p>
</blockquote></th>
<th><blockquote>
<p>10%</p>
</blockquote></th>
<th><blockquote>
<p>5%</p>
</blockquote></th>
<th><blockquote>
<p>15%</p>
</blockquote></th>
<th><blockquote>
<p>20%</p>
</blockquote></th>
<th><blockquote>
<p>10%</p>
</blockquote></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Processors (P)</td>
<td><blockquote>
<p>1</p>
</blockquote></td>
<td><blockquote>
<p>2</p>
</blockquote></td>
<td><blockquote>
<p>4</p>
</blockquote></td>
<td><blockquote>
<p>6</p>
</blockquote></td>
<td><blockquote>
<p>8</p>
</blockquote></td>
<td><blockquote>
<p>16</p>
</blockquote></td>
<td><blockquote>
<p>128</p>
</blockquote></td>
</tr>
</tbody>
</table>
 Figure 5.40 Percentage of application’s A time that can use up to P processors.

1. \[20\] &lt;5.1&gt; Compare the base CPI of the application with no remote commu- nication to the CPI achieved with each of the three topologies in part (a).

<!-- -->

1. \[15\] &lt;5.2&gt; Show how the basic snooping protocol of [Figure 5.6](#_bookmark222) can be changed for a write-through cache. What is the major hardware functionality that is not needed with a write-through cache compared with a write-back cache?
2. \[20/20\] &lt;5.2&gt; Please answer the following problems:

   1. \[20\] &lt;5.2&gt; Add a clean exclusive state to the basic

      > snooping cache coherence protocol ([Figure
      > 5.6](#_bookmark222)). Show the protocol in the finite state
      > machine format used in the figure.
      >
   2. \[20\] &lt;5.2&gt; Add an “owned” state to the protocol of

      > part (a) and describe using the same finite state machine
      > format used in [Figure 5.6](#_bookmark222).
      >
3. \[15\] &lt;5.2&gt; One proposed solution for the problem of false sharing is to add a valid bit per word. This would allow the protocol to invalidate a word without removing the entire block, letting a processor keep a portion of a block in its cache while
   another processor writes a different portion of the block. What extra complications are introduced into the basic snooping cache coherence protocol ([Figure 5.6](#_bookmark222)) by this addition? Consider all possible protocol actions.
4. \[15/20\] &lt;5.3&gt; This exercise studies the impact of aggressive techniques to exploit instruction-level parallelism in the processor when used in the design of shared-
   memory multiprocessor systems. Consider two systems identical except for the processor. System A uses a processor with a simple single-issue, in-order pipeline, and system B uses a processor with four-way issue, out-of-order execution and a reorder buffer with 64 entries.
5. \[15\] &lt;5.3&gt; Following the convention of [Figure 5.11](#_bookmark228), let us divide the execution time into instruction execution, cache access, memory access, and other stalls. How would you expect each of these components to differ between system A
   and system B?
6. \[10\] &lt;5.3&gt; Based on the discussion of the behavior of OLTP workload in [Section 5.3](#performance-of-symmetric-shared-memory-multiprocessors), what is the important difference between the OLTP workload
   and other benchmarks that limit benefit from a more aggressive processor design?
7. \[15\] &lt;5.3&gt; How would you change the code of an application to avoid false shar- ing? What might be done by a compiler and what might require programmer
   directives?
8. \[15\] &lt;5.3&gt; An application is calculating the number of occurrences of a certain word in a very large number of documents. A very large number of processors
   divided the work, searching the different documents. They created a huge array—word_count—of 32-bit integers, every element of which is the number of times the word occurred in some document. In a second phase, the computation is moved to a small SMP server with four processors. Each processor sums up approximately ¼ of the array elements. Later, one processor calculates the total sum.

for (int p= 0; p&lt;=3; p++) // Each iteration of is executed on a

separate processor.

{

sum \[p\] = 0;

for (int i= 0; i&lt; n/4; i++) // n is size of word_count and

is divisible by 4

sum\[p\] = sum\[p\] + word_count\[p+4\*i\];

}

total_sum = sum\[0\] +sum\[1\]+sum\[2\]+sum\[3\] //executed only
on processor.

1. Assuming each processor has a 32-byte L1 data cache. Identify the cache line sharing (true or false) that the code exhibits.
2. Rewrite the code to reduce the number of misses to elements of the array word_count.
3. Identify a manual fix you can make to the code to rid it of any
   false sharing.

   1. \[15\] &lt;5.4&gt; Assume a directory-based cache coherence
      protocol. The directory currently has information that indicates
      that processor P1 has the data in “exclu-
      sive” mode. If the directory now gets a request for the same cache block from pro- cessor P1, what could this mean? What should the directory controller do? (Such cases are called “race conditions” and are the reason why coherence protocols are so hard to design and verify.)
4. \[20\] &lt;5.4&gt; A directory controller can send invalidates for
   lines that have been replaced by the local cache controller. To
   avoid such messages, and to keep the
   directory consistent, replacement hints are used. Such messages tell the controller that a block has been replaced. Modify the directory coherence protocol of [Section 5.4](#distributed-shared-memory-and-directory-based-coherence) to use such replacement hints.
5. \[20/15/20/15\] &lt;5.4&gt; One downside of a straightforward
   implementation of direc- tories using fully populated bit vectors is
   that the total size of the directory infor-
   mation scales as the product: processor count memory blocks. If memory grows linearly with processor count, the total size of the directory grows quadratically in the processor count. In practice, because the directory needs only 1 bit per memory block (which is typically 32–128 bytes), this problem is not serious for small-to- moderate processor counts. For example, assuming a 128-byte block, and P pro- cessors, the amount of directory storage compared to main memory is P/(128\*8) P/1024, which is 12.5% overhead for P 128 processors. We can avoid this prob- lem by observing that we need to keep only an amount of information that is pro- portional to the cache size of each processor. We explore some solutions in these exercises.
6. \[20\] &lt;5.4&gt; One method to obtain a scalable directory protocol is to organize the multiprocessor as a logical hierarchy with the processors as leaves of the
   hierarchy and directories positioned at the root of each subtree. The directory at each subtree records which descendants cache which memory blocks. It also

records the memory blocks—with a home in that subtree—that are cached out- side the subtree. Compute the amount of storage needed to record the proces- sor information for the directories, assuming that each directory is fully associative. Your answer should incorporate both the number of nodes at each level of the hierarchy as well as the total number of nodes.

1. \[15\] &lt;5.4&gt; Another approach to reducing the directory size is to allow only a limited number of the directory’s memory blocks to be shared at any given time.
   Implement the directory as a four-way set-associative cache storing full bit vec- tors. If a directory cache miss occurs, choose a directory entry and invalidate the entry. Describe how this organization will work elaborating what will happen as a is block read, written replaced and written back to memory. Modify the pro- tocol in [Figure 5.20](#_bookmark238) to reflect the new transitions required by this directory organization.
2. \[20\] &lt;5.4&gt; Rather than reducing the number of directory entries, we can imple- ment bit vectors that are not dense. For example, we can set every directory
   entry to 9 bits. If a block is cached in only one node outside its home, this field contains the node number. If the block is cached in more than one node outside its home, this field is a bit vector with each bit indicating a group of eight pro- cessors, at least one of which caches the block. Illustrate how this scheme would work for a 64-processor DSM machine that consists of eight 8-processors groups.
3. \[15\] An extreme approach to reducing the directory size is to implement an “empty” directory; that is, the directory in every processor does not store any memory states. It receives requests and forwards them as _appropriate_. What is the benefit of having such a directory over having no directory at all for a DSM system?

<!-- -->

1. \[10\] &lt;5.5&gt; Implement the classical compare-and-swap instruction using the _load linked/store conditional_ instruction pair.
2. \[15\] &lt;5.5&gt; One performance optimization commonly used is to pad synchroniza- tion variables so as not to have any other useful data in the same cache line. Con-
   struct an example demonstrating that this optimization can be extremely useful in some situations. Assume a snoopy write invalidate protocol.
3. \[30\] &lt;5.5&gt; One possible implementation of the _load linked/store conditional pair_ for multicore processors is to constrain these instructions to using uncached mem-
   ory operations. A monitor unit intercepts all reads and writes from any core to the memory. It keeps track of the source of the _load linked_ instructions and whether any intervening stores occur between the _load linked_ and its corresponding _store conditional_ instruction. The monitor can prevent any failing store conditional from writing any data and can use the interconnect signals to inform the processor that this store failed.

Design such a monitor for a memory system supporting a four-core SMP. Take into account that, generally, read and write requests can have different data sizes (4/ 8/16/32 bytes). Any memory location can be the target of a _load linked/store_

_conditional_ pair, and the memory monitor should assume that _load linked/store conditional_ references to any location can, possibly, be interleaved with regular accesses to the same location. The monitor complexity should be independent of the memory size.

1. \[25\] &lt;5.5&gt; Prove that, in a two-level cache hierarchy where
   L1 is closer to the processor, inclusion is maintained with no extra
   action if L2 has at least as much
   associativity as L1, both caches use LRU replacement, and both caches have the same block sizes.
2. \[Discussion\] &lt;5&gt; When trying to perform detailed performance
   evaluation of a multiprocessor system, system designers use one of
   three tools: analytical models,
   trace-driven simulation, and execution-driven simulation. Analytical models use mathematical expressions to model the behavior of programs. Trace-driven simu- lations run the applications on a real machine and generate a trace, typically of memory operations. These traces can be replayed through a cache simulator or a simulator with a simple processor model to predict the performance of the system when various parameters are changed. Execution-driven simulators simulate the entire execution maintaining an equivalent structure for the processor state and so on.
3. What are the accuracy/speed trade-offs between these approaches?
4. CPU traces, if not carefully collected, can exhibit artifacts of the system they are collected on. Discuss this issue while using branch-prediction and spin-wait synchronization as examples. (Hint: The program itself is not available to a pure CPU trace; just the trace is available.)

<!-- -->

1. \[40\] &lt;5.7, 5.9&gt; Multiprocessors and clusters usually show
   performance increases as you increase the number of the processors,
   with the ideal being n times speedup
   for n processors. The goal of this biased benchmark is to make a program that gets worse performance as you add processors. For example, this means that one pro- cessor on the multiprocessor or cluster runs the program fastest, two are slower, four are slower than two, and so on. What are the key performance characteristics for each organization that give inverse linear speedup?

This page intentionally left blank
