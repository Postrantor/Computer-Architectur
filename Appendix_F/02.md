## Interconnecting Two Devices

> ##互连两个设备

This section introduces the basic concepts required to understand how communi- cation between just two networked devices takes place. This includes concepts that deal with situations in which the receiver may not be ready to process incoming data from the sender and situations in which transport errors may occur. To ease understanding, the black box network at this point can be conceptualized as an ideal network that behaves as simple dedicated links between the two devices. [Figure F.3](#_bookmark600) illustrates this, where unidirectional wires run from device A to device B and _vice versa_, and each end node contains a buffer to hold the data. Regardless of the network complexity, whether dedicated link or not, a connection exists from each end node device to the network to inject and receive information to/from the network. We first describe the basic functions that must be performed at the end nodes to commence and complete communication, and then we discuss network media and the basic functions that must be performed by the network to carry out communication. Later, a simple performance model is given, along with sev- eral examples to highlight implications of key network parameters.

> 本节介绍了了解仅两个网络设备之间的通信所需的基本概念。这包括处理接收者可能无法准备好从发件人和可能发生运输错误的情况处理传入数据的概念。为了简化理解，此时的黑匣子网络可以被概念化为一个理想网络，该网络表现为两个设备之间的简单专用链接。[图 F.3](#_ bookmark600)说明了这一点，其中单向电线从设备 A 到设备 B 和_Vice versa_，每个末端节点都包含一个缓冲区来保存数据。无论网络复杂性如何，无论是专用链接，每个端节点设备与网络的连接都存在，以注入并从网络接收信息。我们首先描述必须在结尾节点开始和完整通信的基本功能，然后讨论网络媒体以及网络必须执行的基本功能才能进行通信。后来，给出了一个简单的性能模型，以及多个示例，以突出关键网络参数的含义。

### Network Interface Functions: Composing and Processing Messages

> ###网络接口函数：编写和处理消息

Suppose we want two networked devices to read a word from each other’s mem- ory. The unit of information sent or received is called a _message_. To acquire the desired data, the two devices must first compose and send a certain type of message in the form of a _request_ containing the address of the data within the other device. The address (i.e., memory or operand location) allows the receiver to identify where to find the information being requested. After processing the request, each device then composes and sends another type of message, a _reply_, containing the data. The address and data information is typically referred to as the message _payload._

> 假设我们希望两个网络设备能够从彼此的 mem-ory 中读取一个单词。发送或接收的信息单位称为_message_。为了获取所需的数据，这两个设备必须首先撰写并以_request_的形式发送某种类型的消息，该消息包含另一个设备内的数据地址。地址(即内存或操作数位置)允许接收者确定在何处找到所请求的信息。处理请求后，每个设备将组成并发送其他类型的消息，即包含数据的_reply_。地址和数据信息通常称为消息_PAYLOAD._

Figure F.3 A simple dedicated link network bidirectionally interconnecting two devices.

> 图 F.3 一个简单的专用链接网络双向互连两个设备。

In addition to payload, every message contains some control bits needed by the network to deliver the message and process it at the receiver. The most typical are bits to distinguish between different types of messages (e.g., request, reply, request acknowledge, reply acknowledge) and bits that allow the network to transport the information properly to the destination. These additional control bits are encoded in the _header_ and/or _trailer_ portions of the message, depending on their location relative to the message payload. As an example, [Figure F.4](#_bookmark601) shows the format of a message for the simple dedicated link network shown in [Figure F.3](#_bookmark600). This example shows a single-word payload, but messages in some interconnection networks can include several thousands of words.

> 除有效负载外，每个消息还包含网络所需的一些控制位来传递消息并在接收方处理。最典型的是将不同类型的消息区分(例如，请求，答复，请求确认，答复确认)和允许网络正确运输信息到目的地的位。这些额外的控制位编码在消息的_header_和/或_trailer_部分中，具体取决于其位置相对于消息有效负载。例如，[图 F.4](#_ bookmark601)显示了[图 F.3]中显示的简单专用链接网络的消息格式(#_ bookmark600)。此示例显示了一个单词有效负载，但是在某些互连网络中的消息可以包含数千个单词。

Before message transport over the network occurs, messages have to be com- posed. Likewise, upon receipt from the network, they must be processed. These and other functions described below are the role of the _network interface_ (also referred to as the _channel adapter_) residing at the end nodes. Together with some direct memory access (DMA) engine and link drivers to transmit/receive messages to/from the network, some dedicated memory or register(s) may be used to buffer outgoing and incoming messages. Depending on the network domain and design specifications for the network, the network interface hardware may consist of noth- ing more than the communicating device itself (i.e., for OCNs and some SANs) or a separate card that integrates several embedded processors and DMA engines with thousands of megabytes of RAM (i.e., for many SANs and most LANs and WANs).

> 在网络上发生消息传输之前，必须发出消息。同样，从网络收到后，必须对其进行处理。下面描述的这些功能和其他功能是_network Interface_(也称为_CHANNEL ADAPTER_)的角色，位于末端节点上。连同一些直接的内存访问(DMA)引擎和链接驱动程序以传输/接收消息向/从网络传输，可以使用一些专用的内存或寄存器来缓冲传出和传入的消息。根据网络的网络域和设计规范，网络接口硬件可能包括不超过通信设备本身(即 OCN 和一些 SANS)或一张单独的卡片或单独的卡，该卡将几个嵌入式处理器和 DMA 引擎与与 DMA 引擎集成在一起成千上万的 RAM 兆字节(即，对于许多 Sans 和大多数 Lans 和 Wans)。

In addition to hardware, network interfaces can include software or firmware to perform the needed operations. Even the simple example shown in [Figure F.3](#_bookmark600) may invoke messaging software to translate requests and replies into messages with the appropriate headers. This way, user applications need not worry about composing and processing messages as these tasks can be performed automatically at a lower level. An application program usually cooperates with the operating or runtime system to send and receive messages. As the network is likely to be shared by many processes running on each device, the operating system cannot allow messages intended for one process to be received by another. Thus, the messaging software must include protection mechanisms that distinguish between processes. This dis- tinction could be made by expanding the header with a _port_ number that is known by both the sender and intended receiver processes.

> 除硬件外，网络接口还可以包括软件或固件来执行所需的操作。即使是[图 F.3](#_ bookmark600)中显示的简单示例，也可以调用消息传递软件以将请求转换为带有适当标头的消息。这样，用户应用程序不必担心编写和处理消息，因为可以在较低级别上自动执行这些任务。申请程序通常与操作或运行时系统合作以发送和接收消息。由于网络可能会在每个设备上运行的许多流程共享，因此操作系统无法允许另一个过程被另一个过程接收。因此，消息传递软件必须包括区分流程的保护机制。可以通过使用_PORT_号码扩展标题，该编号是发件人和预期的接收器进程都知道的。

Figure F.4 An example packet format with header, payload, and checksum in the trailer.

> 图 F.4 预告片中的带有标头，有效负载和校验和校验和检查的示例数据包格式。

In addition to composing and processing messages, additional functions need to be performed by the end nodes to establish communication among the commu- nicating devices. Although hardware support can reduce the amount of work, some can be done by software. For example, most networks specify a maximum amount of information that can be transferred (i.e., _maximum transfer unit_) so that network buffers can be dimensioned appropriately. Messages longer than the maximum transfer unit are divided into smaller units, called _packets_ (or _datagrams_), that are transported over the network. Packets are reassembled into messages at the des- tination end node before delivery to the application. Packets belonging to the same message can be distinguished from others by including a _message ID_ field in the packet header. If packets arrive out of order at the destination, they are reordered when reassembled into a message. Another field in the packet header containing a _sequence number_ is usually used for this purpose.

> 除了编写和处理消息外，最终节点还需要执行其他功能，以在通信设备之间建立通信。尽管硬件支持可以减少工作量，但可以通过软件完成。例如，大多数网络指定可以传输的最大信息(即_maxmumimum 传输单位_)，以便可以适当地尺寸。较长的消息比最大传输单元分为较小的单位，称为_packets_(或_datagrams_)，这些单元通过网络传输。在传递到应用程序之前，将数据包重新组合到消息端节点上的消息中。属于同一消息的数据包可以通过在数据包标题中包含_Message ID_字段来区分其他消息。如果数据包在目的地停止订单，则将它们重新组装成消息时将重新排序。通常将包含_ sequence number_的数据包标头中的另一个字段用于此目的。

The sequence of steps the end node follows to commence and complete com- munication over the network is called a _communication protocol_. It generally has symmetric but reversed steps between sending and receiving information. Commu- nication protocols are implemented by a combination of software and hardware to accelerate execution. For instance, many network interface cards implement hard- ware timers as well as hardware support to split messages into packets and reas- semble them, compute the cyclic redundancy check (CRC) _checksum,_ handle virtual memory addresses, and so on.

> 步骤的顺序端节点开始开始并通过网络完整通讯称为_-Communication 协议_。它通常具有对称，但在发送和接收信息之间进行了逆转。通信协议是通过软件和硬件的组合来加速执行的。例如，许多网络接口卡实现了硬件计时器以及硬件支持，以将消息分为数据包并将其放置，并计算循环冗余检查(CRC)_CHECKSUM，_处理虚拟内存地址等。

Some network interfaces include extra hardware to offload protocol processing from the host computer, such as TCP _offload engines_ for LANs and WANs. But, for interconnection networks such as SANs that have low latency requirements, this may not be enough even when lighter-weight communication protocols are used such as message passing interface (MPI). Communication performance can be further improved by bypassing the operating system (OS). OS bypassing can be implemented by directly allocating message buffers in the network interface memory so that applications directly write into and read from those buffers. This avoids extra memory-to-memory copies. The corresponding protocols are referred to as _zero-copy_ protocols or _user-level communication_ protocols. Protection can still be maintained by calling the OS to allocate those buffers at initialization and preventing unauthorized memory accesses in hardware.

> 某些网络接口包括额外的硬件，可从主机计算机上卸载协议处理，例如 tcp _offload grom_ for lans and wans。但是，对于诸如延迟需求较低的 SAN 之类的互连网络，即使使用较轻的通信协议，例如消息传递接口(MPI)也是不够的。通过绕过操作系统(OS)，可以进一步提高沟通性能。可以通过直接在网络接口内存中分配消息缓冲区来实现 OS 旁路，以便将应用程序直接从这些缓冲区写入并读取。这避免了额外的内存到记忆副本。相应的协议称为_zero-copy_协议或_user 级通信_协议。仍然可以通过调用 OS 在初始化时分配这些缓冲区并防止硬件中未经授权的内存访问来维护保护。

In general, some or all of the following are the steps needed to send a message at end node devices over a network:

> 通常，以下一些或全部是通过网络在末端节点设备发送消息所需的步骤：

1. The application executes a system call, which copies data to be sent into an operating system or network interface buffer, divides the message into packets (if needed), and composes the header and trailer for packets.

> 1.该应用程序执行系统调用，该系统调用要将要发送到操作系统或网络接口缓冲区的数据将消息划分为数据包(如果需要的话)，并组成数据包的标题和预告片。

2. The checksum is calculated and included in the header or trailer of packets.

> 2.计算校验和包含在数据包的标题或拖车中。

3. The timer is started, and the network interface hardware sends the packets.

> 3.启动计时器，网络接口硬件发送数据包。

Message reception is in the reverse order:

> 消息接收是相反的顺序：

3\. The network interface hardware receives the packets and puts them into its buffer or the operating system buffer.

> 3 \。网络接口硬件接收数据包并将其放入其缓冲区或操作系统缓冲区中。

2\. The checksum is calculated for each packet. If the checksum matches the sender’s checksum, the receiver sends an acknowledgment back to the packet sender. If not, it deletes the packet, assuming that the sender will resend the packet when the associated timer expires.

> 2 \。计算每个数据包的校验和。如果校验和匹配发件人的校验和，则接收器将确认回到数据包发件人。如果没有，它会删除数据包，假设在关联的计时器到期时发件人将重新发送数据包。

1. Once all packets pass the test, the system reassembles the message, copies the data to the user’s address space, and signals the corresponding application.

> 1.所有数据包通过测试后，系统将重新组装消息，将数据复制到用户的地址空间，并向相应的应用程序发出信号。

The sender must still react to packet acknowledgments:

> 发件人仍然必须对数据包确认：

- When the sender gets an acknowledgment, it releases the copy of the corre- sponding packet from the buffer.

> - 当发件人获得确认时，它将从缓冲区中释放相应数据包的副本。

- If the sender reaches the time-out instead of receiving an acknowledgment, it resends the packet and restarts the timer.

> - 如果发件人到达超时而不是收到确认，则将重新发送数据包并重新启动计时器。

Just as a protocol is implemented at network end nodes to support communi- cation, protocols are also used across the network structure at the physical, data link, and network layers responsible primarily for packet transport, flow control, error handling, and other functions described next.

> 就像在网络端节点上实施协议以支持通信一样，在物理，数据链路和网络层的网络结构上也使用协议，主要负责数据包运输，流量控制，错误处理以及其他所描述的功能下一个。

### Basic Network Structure and Functions: Media and Form Factor, Packet Transport, Flow Control, and Error Handling

> ###基本网络结构和功能：媒体和外形，数据包传输，流量控制和错误处理

Once a packet is ready for transmission at its source, it is injected into the network using some dedicated hardware at the network interface. The hardware includes some transceiver circuits to drive the physical network media—either electrical or optical. The type of _media_ and _form factor_ depends largely on the interconnect distances over which certain signaling rates (e.g., transmission speed) should be sustainable. For centimeter or less distances on a chip or multichip module, typi- cally the middle to upper copper metal layers can be used for interconnects at multi- Gbps signaling rates per line. A dozen or more layers of copper traces or tracks imprinted on circuit boards, midplanes, and backplanes can be used for Gbps differential-pair signaling rates at distances of about a meter or so. Category 5E unshielded twisted-pair copper wiring allows 0.25 Gbps transmission speed over distances of 100 meters. Coaxial copper cables can deliver 10 Mbps over kilometer distances. In these conductor lines, distance can usually be traded off for higher transmission speed, up to a certain point. Optical media enable faster transmission speeds at distances of kilometers. Multimode fiber supports 100 Mbps transmis- sion rates over a few kilometers, and more expensive single-mode fiber supports Gbps transmission speeds over distances of several kilometers. Wavelength divi- sion multiplexing allows several times more bandwidth to be achieved in fiber (i.e., by a factor of the number of wavelengths used).

> 一旦数据包准备在其源中进行传输，就可以使用网络接口上的一些专用硬件注入网络。该硬件包括一些收发器电路，以驱动物理网络媒体(无论是电气还是光学)。_media_和_form factor _的类型在很大程度上取决于互连距离，在该距离上，某些信号传导速率(例如，传输速度)应具有可持续性。对于芯片或多芯片模块上的厘米或更少的距离，通常将中部至上铜金属层用于每行的多 GBPS 信号速率以互连。在电路板，中间平面和背板上印有十几层或更多层的铜痕迹或轨道可用于 GBPS 差速器信号传导速率，距离约一米左右。5E 类未屏蔽的扭曲铜线接线允许在 100 米的距离内进行 0.25 Gbps 的传输速度。同轴铜电缆可以在公里距离内输送 10 Mbps。在这些导体线路中，通常可以将距离以更高的传输速度进行交易，直到某个点。光学介质可以在公里距离处更快地传输速度。多模纤维在几公里上支持 100 Mbps 的传输速率，而更昂贵的单模光纤支持 GBPS 的传输速度在几公里的距离上。波长分区多路复用允许在纤维中获得更多的带宽(即，所使用的波长数量的因素)。

The hardware used to drive network links may also include some encoders to encode the signal in a format other than binary that is suitable for the given trans- port distance. Encoding techniques can use multiple voltage levels, redundancy, data and control rotation (e.g., 4b5b encoding), and/or a guaranteed minimum number of signal transitions per unit time to allow for clock recovery at the receiver. The signal is decoded at the receiver end, and the packet is stored in the corresponding buffer. All of these operations are performed at the network physical layer, the details of which are beyond the scope of this appendix. Fortu- nately, we do not need to worry about them. From the perspective of the data link and higher layers, the physical layer can be viewed as a long linear pipeline without staging in which signals propagate as waves through the network transmission medium. All of the above functions are generally referred to as _packet transport_. Besides packet transport, the network hardware and software are jointly responsible at the data link and network protocol layers for ensuring reliable delivery of packets. These responsibilities include: (1) preventing the sender from sending packets at a faster rate than they can be processed by the receiver, and (2) ensuring that the packet is neither garbled nor lost in transit. The first responsibility is met by either discarding packets at the receiver when its buffer is full and later notifying the sender to retransmit them, or by notifying the sender to stop sending packets when the buffer becomes full and to resume later once it has room for more packets. The latter strategy is generally known as _flow control_.

> 用于驱动网络链接的硬件还可能包括一些编码器，以使用适用于给定的转移距离的二进制格式编码信号。编码技术可以使用多个电压级别，冗余，数据和控制旋转(例如 4B5B 编码)和/或保证每单位时间的最小信号转换数量，以允许接收器处的时钟恢复。信号在接收器端解码，数据包存储在相应的缓冲区中。所有这些操作均在网络物理层执行，其详细信息超出了本附录的范围。事实上，我们不需要担心它们。从数据链路和较高层的角度来看，物理层可以视为长线性管道，而无需分阶段，即信号通过网络传输介质传播为波。以上所有功能通常称为_packet Transport_。除了数据包运输外，网络硬件和软件在数据链路和网络协议层上负责，以确保数据包的可靠交付。这些责任包括：(1)防止发件人以比接收器处理更快的速度发送数据包，以及(2)确保数据包在运输中既不乱乱，也不会丢失。当缓冲区满足时，通过在接收器的数据包上丢弃数据包，然后通知发件人重新启动它们，或者通过通知发件人在缓冲区填满时停止发送数据包，以后再恢复，一旦它有更多的空间，就可以履行第一个责任。数据包。后一种策略通常称为_Flow Control_。

There are several interesting techniques commonly used to implement flow control beyond simple _handshaking_ between the sender and receiver. The more popular techniques are _Xon/Xoff_ (also referred to as _Stop & Go_) and _credit-based_ flow control. Xon/Xoff consists of the receiver notifying the sender either to stop or to resume sending packets once high and low buffer occupancy levels are reached, respectively, with some hysteresis to reduce the number of notifications. Notifica- tions are sent as “stop” and “go” signals using additional control wires or encoded in control packets. Credit-based flow control typically uses a credit counter at the sender that initially contains a number of credits equal to the number of buffers at the receiver. Every time a packet is transmitted, the sender decrements the credit counter. When the receiver consumes a packet from its buffer, it returns a credit to the sender in the form of a control packet that notifies the sender to increment its counter upon receipt of the credit. These techniques essentially control the flow of packets into the network by _throttling_ packet injection at the sender when the receiver reaches a low watermark or when the sender runs out of credits.

> 在发送者和接收器之间，通常用于实现简单_HANDSHAKING 的流量控制，有几种有趣的技术。更流行的技术是_xon/xoff_(也称为_STOP＆GO_)和基于_credit 的_流量控制。XON/XOFF 由接收器组成，一旦达到高和低缓冲区占用水平，通知发送者要停止或恢复发送数据包，并进行了一些滞后以减少通知的数量。通知使用其他控制线或在控制数据包中编码为“停止”和“ GO”信号。基于信用的流量控制通常使用发件人的信用计数器，该信用计数器最初包含许多等于接收器的缓冲区数量的信用量。每次传输数据包时，发件人都会减少信用计数器。当接收器从其缓冲区中消耗数据包时，它将以控制数据包的形式将信用额度返回给发件人，以通知发件人在收到信用后递增其柜台。当接收器达到低水标记或发件人耗尽信用时，这些技术实质上通过_throttling_数据包注入将数据包流入网络。

Xon/Xoff usually generates much less control traffic than credit-based flow control because notifications are only sent when the high or low buffer occupancy levels are crossed. On the other hand, credit-based flow control requires less than half the buffer size required by Xon/Xoff. Buffers for Xon/Xoff must be large enough to prevent overflow before the “stop” control signal reaches the sender. Overflow cannot happen when using credit-based flow control because the sender will run out of credits, thus stopping transmission. For both schemes, full link bandwidth utilization is possible only if buffers are large enough for the distance over which communication takes place.

> XON/XOFF 通常比基于信用的流量控制的控制流量要少得多，因为仅在越过高或低缓冲占用水平时发送通知。另一方面，基于信用的流量控制需要 XON/XOFF 所需的缓冲区大小的一半。XON/XOFF 的缓冲区必须足够大，以防止在“停止”控制信号到达发件人之前溢出。当使用基于信用的流量控制时，溢出不会发生，因为发件人会用尽信用，从而停止传输。对于这两个方案，仅当缓冲区足够大以进行通信的距离时，才有可能进行完整的链接带宽利用。

Let’s compare the buffering requirements of the two flow control techniques in a simple example covering the various interconnection network domains.

> 让我们在一个涵盖各种互连网络域的简单示例中比较两种流控制技术的缓冲要求。

Example Suppose we have a dedicated-link network with a raw data bandwidth of 8 Gbps for each link in each direction interconnecting two devices. Packets of 100 bytes (including the header) are continuously transmitted from one device to the other to fully utilize network bandwidth. What is the minimum amount of credits and buffer space required by credit-based flow control assuming interconnect distances of 1 cm, 1 m, 100 m, and 10 km if only link propagation delay is taken into account? How does the minimum buffer space compare against Xon/Xoff?

> 示例假设我们有一个专用链接网络，每个链接的原始数据带宽为 8 GBP，每个方向互连两个设备。100 个字节的数据包(包括标头)连续地从一个设备传输到另一个设备，以充分利用网络带宽。如果仅考虑链接传播延迟，则基于信用的流量控制所需的最小信用和缓冲空间数量是多少？最小缓冲空间与 XON/XOFF 的比较如何？

_Answer_ At the start, the receiver buffer is initially empty and the sender contains a number of credits equal to buffer capacity. The sender will consume a credit every time a packet is transmitted. For the sender to continue transmitting packets at network speed, the first returned credit must reach the sender before the sender runs out of credits. After receiving the first credit, the sender will keep receiving credits at the same rate it transmits packets. As we are considering only propagation delay over the link and no other sources of delay or overhead, null processing time at the sender and receiver are assumed. The time required for the first credit to reach the sender since it started transmission of the first packet is equal to the round-trip propagation delay for the packet transmitted to the receiver and the return credit transmitted back to the sender. This time must be less than or equal to the packet transmission time multiplied by the initial credit count:

> _answer_在开始时，接收器缓冲区最初是空的，并且发件人包含许多等于缓冲区容量的信用。每次传输数据包时，发件人都会消耗信贷。为了使发件人继续以网络速度传输数据包，在发件人运行的信用额度之前，必须先返回的信用来触及发件人。在获得第一个信用额之后，发件人将继续以与传输数据包相同的速率收到信用。由于我们仅考虑链接上的传播延迟，也没有其他延迟或开销来源，因此假定发件人和接收器的无效处理时间。由于开始传输第一个数据包以来，第一个信贷到达发件人所需的时间等于传输到接收器的数据包的往返传播延迟，并将退货信贷传递给发件人。这段时间必须小于或等于数据包传输时间乘以初始信用计数：

As each credit represents one packet-sized buffer entry, the minimum amount of credits (and, likewise, buffer space) needed by each device is one for the 1 cm and 1 m distances, 10 for the 100 m distance, and 1000 packets for the 10 km distance. For Xon/Xoff, this minimum buffer size corresponds to the buffer fragment from the high occupancy level to the top of the buffer and from the low occupancy level to the bottom of the buffer. With the added hysteresis between both occupancy levels to reduce notifications, the minimum buffer space for Xon/Xoff turns out to be more than twice that for credit-based flow control.

> 由于每个信用代表一个数据包大小的缓冲区条目，因此每个设备所需的最低信用量(以及同样是缓冲空间)是 1 厘米，距离为 1 m，100 m 距离为 10，而 1000 个数据包用于 10 公里的距离。对于 XON/XOFF，此最小缓冲区大小对应于从高占用水平到缓冲区顶部以及从低占用水平到缓冲区底部的缓冲片段。随着两个占用水平之间的增加滞后以减少通知，XON/XOFF 的最小缓冲空间被证明是基于信用的流量控制的两倍以上。

Networks that implement flow control do not need to drop packets and are sometimes referred to as _lossless_ networks; networks that drop packets are some- times referred to as _lossy_ networks. This single difference in the way packets are handled by the network drastically constrains the kinds of solutions that can be implemented to address other related network problems, including packet routing, congestion, deadlock, and reliability, as we will see later in this appendix. This difference also affects performance significantly as dropped packets need to be retransmitted, thus consuming more link bandwidth and suffering extra delay. These behavioral and performance differences ultimately restrict the interconnec- tion network domains for which certain solutions are applicable. For instance, most networks delivering packets over relatively short distances (e.g., OCNs and SANs) tend to implement flow control; on the other hand, networks delivering packets over relatively long distances (e.g., LANs and WANs) tend to be designed to drop packets. For the shorter distances, the delay in propagating flow control informa- tion back to the sender can be negligible, but not so for longer distance scales. The kinds of applications that are usually run also influence the choice of lossless ver- sus lossy networks. For instance, dropping packets sent by an Internet client like a Web browser affects only the delay observed by the corresponding user. However, dropping a packet sent by a process from a parallel application may lead to a sig- nificant increase in the overall execution time of the application if that packet’s delay is on the critical path.

> 实现流控制的网络无需删除数据包，有时将其称为_lossless_网络；丢弃数据包的网络有时称为_lossy_网络。网络处理数据包处理方式的这种差异极大地限制了可以实现的解决方案的种类，以解决其他相关的网络问题，包括数据包路由，拥堵，僵局和可靠性，正如我们将在本附录后面看到的那样。这种差异也会显着影响性能，因为需要重新传输掉落的数据包，从而消耗了更多的链路带宽并遭受额外的延迟。这些行为和性能差异最终限制了适用某些解决方案的互连网络域。例如，大多数网络在相对较短的距离(例如 OCN 和 SANS)上传递数据包倾向于实现流量控制。另一方面，网络在相对较长的距离(例如 LAN 和 WANS)上输送数据包往往被设计为放置数据包。对于较短的距离，向发件人传播流量控制信息的延迟可能可以忽略不计，但对于更长的距离尺度而言，延迟不可忽略。通常运行的应用程序的种类也会影响无损 verss Lossy 网络的选择。例如，像 Web 浏览器一样发送由 Internet 客户端发送的数据包仅影响相应用户观察到的延迟。但是，如果该数据包的延迟在关键路径上，则从并行应用程序中删除流程发送的数据包可能会导致应用程序的整体执行时间大大增加。

The second responsibility of ensuring that packets are neither garbled nor lost in transit can be met by implementing some mechanisms to detect and recover from transport errors. Adding a checksum or some other error detection field to the packet format, as shown in [Figure F.4](#_bookmark601), allows the receiver to detect errors. This redundant information is calculated when the packet is sent and checked upon receipt. The receiver then sends an acknowledgment in the form of a control packet if the packet passes the test. Note that this acknowledgment control packet may simultaneously contain flow control information (e.g., a credit or stop signal), thus reducing control packet overhead. As described earlier, the most common way to recover from errors is to have a timer record the time each packet is sent and to presume the packet is lost or erroneously transported if the timer expires before an acknowledgment arrives. The packet is then resent.

> 通过实施某些机制来检测和从运输错误中恢复，可以确保既不弄乱或丢失的数据包的第二个责任。如[图 F.4](#_ bookmark601)所示，将校验和一些错误检测字段添加到数据包格式中，允许接收器检测错误。当收据时发送并检查数据包时，将计算此冗余信息。然后，如果数据包通过测试，则接收器以控制数据包的形式发送确认。请注意，此确认控制数据包可以同时包含流控制信息(例如，信用或停止信号)，从而减少了控制数据包的开销。如前所述，从错误中恢复的最常见方法是将一个计时器记录发送到每个数据包发送的时间，并假定如果计时器在确认之前到期，则将数据包丢失或错误地运输。然后，数据包是不满的。

The communication protocol across the network and network end nodes must handle many more issues other than packet transport, flow control, and reliability. For example, if two devices are from different manufacturers, they might order bytes differently within a word (Big Endian versus Little Endian byte ordering). The protocol must reverse the order of bytes in each word as part of the delivery system. It must also guard against the possibility of duplicate packets if a delayed packet were to become unstuck. Depending on the system requirements, the pro- tocol may have to implement _pipelining_ among operations to improve perfor- mance. Finally, the protocol may need to handle network congestion to prevent performance degradation when more than two devices are connected, as described later in [Section F.7](#practical-issues-for-commercial-interconnection-networks).

> 除数据包传输，流控制和可靠性外，整个网络和网络端节点上的通信协议必须处理更多问题。例如，如果两个设备来自不同的制造商，则它们可能会在一个单词中以不同的方式订购字节(Big Endian 和 Little Endian Byte Ordering)。该协议必须作为交付系统的一部分扭转每个单词中字节的顺序。如果延迟的数据包被拆除，它还必须防止重复包装的可能性。根据系统的要求，预托可能必须在操作中实现_PIPELINGIND，以改善穿孔。最后，该协议可能需要处理网络拥塞，以防止连接两个以上的设备时性能降解，如[F.7 节]后面所述(＃实用 - 商业交流连接网络网络)中所述。

### Characterizing Performance: Latency and Effective Bandwidth

> ###表征性能：延迟和有效带宽

Now that we have covered the basic steps for sending and receiving messages between two devices, we can discuss performance. We start by discussing the latency when transporting a single packet. Then we discuss the effective bandwidth (also known as throughput) that can be achieved when the transmission of multiple packets is pipelined over the network at the packet level.

> 现在，我们已经涵盖了两个设备之间发送和接收消息的基本步骤，我们可以讨论性能。我们首先讨论运输单个数据包时的延迟。然后，我们讨论有效的带宽(也称为吞吐量)，当在数据包级别上通过网络管道输送多个数据包的传输可以实现。

[Figure F.5](#_bookmark602) shows the basic components of latency for a single packet. Note that some latency components will be broken down further in later sections as the inter- nals of the “black box” network are revealed. The timing parameters in [Figure F.5](#_bookmark602) apply to many interconnection network domains: inside a chip, between chips on a board, between boards in a chassis, between chassis within a computer, between computers in a cluster, between clusters, and so on. The values may change, but the components of latency remain the same.

> [图 F.5](#_ bookmark602)显示了单个数据包的延迟的基本组成部分。请注意，由于揭示了“黑匣子”网络的间隔，因此某些延迟组件将进一步分解。[图 F.5](#_ bookmark602)中的定时参数适用于许多互连网络域：在芯片内部，木板上的芯片之间，底盘之间的木板之间，底盘之间的底盘，底盘之间，计算机之间的计算机之间的计算机之间的计算机，之间，之间集群，等等。值可能会改变，但是延迟的组成部分保持不变。

The following terms are often used loosely, leading to confusion, so we define them here more precisely:

> 以下术语通常被松散地使用，导致混乱，因此我们在这里更精确地定义它们：

- _Bandwidth_—Strictly speaking, the _bandwidth_ of a transmission medium refers to the range of frequencies for which the attenuation per unit length introduced by that medium is below a certain threshold. It must be distinguished from the _transmission speed_, which is the amount of information transmitted over a medium per unit time. For example, modems successfully increased transmis- sion speed in the late 1990s for a fixed bandwidth (i.e., the 3 KHz bandwidth provided by voice channels over telephone lines) by encoding more voltage levels and, hence, more bits per signal cycle. However, to be consistent with its more widely understood meaning, we use the term _band-width_ to refer to the maximum rate at which information can be transferred, where information includes packet header, payload, and trailer. The units are traditionally bits per second, although bytes per second is sometimes used. The term _bandwidth_ is also used to mean the measured speed of the medium (i.e., network links). _Aggregate bandwidth_ refers to the total data bandwidth supplied by the net- work, and _effective bandwidth_ or _throughput_ is the fraction of aggregate band- width delivered by the network to an application.

> - _bandWidth_  - 刻画，传输介质的_bandWidth_是指该介质引入的每单位长度衰减的频率范围低于一定阈值。必须将其与_transmission speed_区分开，即每单位时间中传输的信息量。例如，调制解调器通过编码更多的电压级别，因此，调制解调器在 1990 年代后期成功提高了固定带宽(即语音通道通过电话线提供的 3 kHz 带宽)的传输速度(即，每个信号循环更多的位。但是，要与其更广泛理解的含义一致，我们使用术语_band-width_来指代可传输信息的最大速率，其中信息包括数据包标头，有效载荷和拖车。传统上是每秒的单位，尽管有时会使用每秒字节。术语_bandWidth_也用于表示介质的测量速度(即网络链接)。_Aggregate 带宽_是指网络提供的总数据带宽，_有效的带宽_或_throughtup_是网络传递给应用程序的聚合频段宽度的一部分。

Figure F.5 Components of packet latency. Depending on whether it is an OCN, SAN, LAN, or WAN, the relative amounts of sending and receiving overhead, time of flight, and transmission time are usually quite different from those illustrated here.

> 图 F.5 数据包延迟的组件。根据是 OCN，SAN，LAN 还是 WAN，发送和接收开销，飞行时间和传输时间的相对量通常与此处所示的时间完全不同。

- _Time of flight_—This is the time for the first bit of the packet to arrive at the receiver, including the propagation delay over the links and delays due to other hardware in the network such as link repeaters and network switches. The unit of measure for time of flight can be in milliseconds for WANs, microseconds for LANs, nanoseconds for SANs, and picoseconds for OCNs.

> - _ flight _的时间_-这是数据包的第一位到达接收器的时候，包括由于网络中其他硬件(例如链接中继器和网络交换机)引起的链接和延迟的传播延迟。飞行时间的度量单位可以用于 WANS 的毫秒，LAN 的微秒，SANS 的纳米秒和 OCN 的 Picseconds。

- _Transmission time_—This is the time for the packet to pass through the network, not including time of flight. One way to measure it is the difference in time between when the first bit of the packet arrives at the receiver and when the last bit of that packet arrives at the receiver. By definition, transmission time is equal to the size of the packet divided by the data bandwidth of network links. This measure assumes there are no other packets contending for that bandwidth (i.e., a zero-load or no-load network).

> - _ transmission Time_-这是数据包通过网络而不是包括飞行时间的时候。测量它的一种方法是，何时数据包到达接收器的第一位与该数据包的最后位到达接收器之间的时间差。根据定义，传输时间等于数据包的大小除以网络链接的数据带宽。该措施假设没有其他数据包为该带宽(即零载或无负载网络)。

- _Transport latency_—This is the sum of time of flight and transmission time. Transport latency is the time that the packet spends in the interconnection net- work. Stated alternatively, it is the time between when the first bit of the packet is injected into the network and when the last bit of that packet arrives at the receiver. It does not include the overhead of preparing the packet at the sender or processing it when it arrives at the receiver.

> - _ Transport 延迟_-这是飞行和传输时间的时间之和。传输延迟是数据包在互连网络上花费的时间。另外，这是将数据包的第一位注入网络中的时间与该数据包的最后位到达接收器之间的时间。它不包括在发件人处准备数据包或处理接收器时处理的开销。

- _Sending overhead_—This is the time for the end node to prepare the packet (as opposed to the message) for injection into the network, including both hard- ware and software components. Note that the end node is busy for the entire time, hence the use of the term _overhead_. Once the end node is free, any sub- sequent delays are considered part of the transport latency. We assume that overhead consists of a constant term plus a variable term that depends on packet size. The constant term includes memory allocation, packet header preparation, setting up DMA devices, and so on. The variable term is mostly due to copies from buffer to buffer and is usually negligible for very short packets.

> - _运行开销_-这是最终节点准备数据包(与消息相反)以注入网络的时间，包括硬件和软件组件。请注意，末端节点在整个时间内都很忙，因此使用术语_overhead_。一旦末端节点是免费的，任何子节点延迟都被认为是传输延迟的一部分。我们假设开销由恒定项和一个取决于数据包大小的可变项组成。恒定项包括内存分配，数据包标头准备，设置 DMA 设备等等。该变量术语主要是由于从缓冲区到缓冲区的副本，通常可以忽略不计对于很短的数据包。

- _Receiving overhead_—This is the time for the end node to process an incoming packet, including both hardware and software components. We also assume here that overhead consists of a constant term plus a variable term that depends on packet size. In general, the receiving overhead is larger than the sending overhead. For example, the receiver may pay the cost of an interrupt or may have to reorder and reassemble packets into messages.

> - _receiving 开销_-这是最终节点处理传入数据包的时候，包括硬件和软件组件。我们在这里还假设，开销由恒定项和一个取决于数据包大小的变量项组成。通常，接收开销比发送开销大。例如，接收器可以支付中断费用，或者必须重新订购和重新组装数据包中的消息。

The total latency of a packet can be expressed algebraically by the following:

> 数据包的总延迟可以通过以下代数表示：

Let’s see how the various components of transport latency and the sending and receiving overheads change in importance as we go across the interconnection network domains: from OCNs to SANs to LANs to WANs.

> 让我们看看当我们跨越互连网络域时，运输延迟的各个组成部分以及发送和接收开销的重要性如何变化：从 OCN 到 SANS 到 SANS，再到 LAN 到 LAN 到 WANS。

Example Assume that we have a dedicated link network with a data bandwidth of 8 Gbps for each link in each direction interconnecting two devices within an OCN, SAN, LAN, or WAN, and we wish to transmit packets of 100 bytes (including the header) between the devices. The end nodes have a per-packet sending overhead of _x_ + 0.05 ns/byte and receiving overhead of 4/3(_x_) + 0.05 ns/byte, where _x_ is 0 μs for the OCN, 0.3 μs for the SAN, 3 μs for the LAN, and 30 μs for the WAN, which are typical for these network types. Calculate the total latency to send packets from one device to the other for interconnection distances of 0.5 cm, 5 m, 5000 m, and 5000 km assuming that time of flight consists only of link propagation delay (i.e., no switching or other sources of delay).

> 示例假设我们有一个专用的链接网络，每个方向上的每个链路的数据带宽为 8 Gbps，在 OCN，SAN，LAN 或 WAN 中互连两个设备，我们希望传输 100 个字节的数据包(包括标题)在设备之间。末端节点的每包发送开销为_x_ + 0.05 ns/byte，并接收 4/3(_x_) + 0.05 ns/byte 的开销，其中_x_为 OCN 为 0μs，SAN，3μs，3μs，3μs 对于 LAN，对于这些网络类型而言，WAN 是典型的 WAN。计算将数据包从一台设备发送到另一台设备的总延迟，以使互连距离为 0.5 cm，5 m，5000 m 和 5000 公里)。

_Answer_ Using the above expression and the calculation for propagation delay through a conductor given in the previous example, we can plug in the parameters for each of the networks to find their total packet latency. For the OCN:

> _answer_使用上述表达式和通过上一个示例中给出的导体进行传播延迟的计算，我们可以插入每个网络的参数，以找到其总数据包延迟。对于 OCN：

The increased fraction of the latency required by time of flight for the longer distances along with the greater likelihood of errors over the longer distances are among thereasons why WANs and LANs usemore sophisticated and time-consuming communication protocols, which increase sending and receiving overheads. The need for standardization is another reason. Complexity also increases due to the require- ments imposed on the protocol by the typical applications that run over the various interconnectionnetwork domains aswe go from tens to hundreds to thousandsto many thousands of devices. We will consider this in later sections when we discuss connect- ing more than two devices. The above example shows that the propagation delay com- ponent of time of flight for WANs and some LANs is so long that other latency components—including the sending and receiving overheads—can practically be ignored. This is not so for SANs and OCNs where the propagation delay pales in com- parison to the overheads and transmission delay. Remember that time-of-flight latency due to switches and other hardware in the network besides sheer propagation delay through the links is neglected in the above example. For noncongested networks, switch latency generally is small compared to the overheads and propagation delay through the links in WANs and LANs, but this is not necessarily so for multiprocessor SANs and multicore OCNs, as we will see in later sections.

> 在较长距离的情况下，飞行时间所需的延迟的比例增加，以及更长距离的错误可能性，这是为什么 WANS 和 LANS USEMORE 复杂且耗时的沟通协议的原因，这增加了发送和接收开销的开销。对标准化的需求是另一个原因。由于典型的应用程序在各种互连网络域上运行的典型应用程序，因此复杂性也会增加，因为我们从数十万到数百到数千个设备。当我们讨论连接两个以上的设备时，我们将在后面的部分中考虑这一点。上面的示例表明，WAN 和某些 LAN 的飞行时间的传播延迟延迟是如此长，以至于其他延迟组件(包括发送和接收开销)几乎可以被忽略。对于 SAN 和 OCN 而言，这并不是这样，在该 SAN 和 OCN 中，传播延迟在间接费用和传输延迟的范围内显得苍白。请记住，在上面的示例中忽略了由于开关和网络中的其他硬件而引起的飞行时间延迟。对于不昂贵的网络，与通过 WANS 和 LAN 的链接的开销和传播延迟相比，开关延迟通常很小，但是对于多处理器 SANS 和 Multicore OCN 而言，这并不一定如此，正如我们在后面的各节中所见。

So far, we have considered the transport of a single packet and computed the associated end-to-end total packet latency. In order to compute the effective band- width for two networked devices, we have to consider a continuous stream of packets transported between them. We must keep in mind that, in addition to min- imizing packet latency, the goal of any network optimized for a given cost and power consumption target is to transfer the maximum amount of available infor- mation in the shortest possible time, as measured by the effective bandwidth deliv- ered by the network. For applications that do not require a response before sending the next packet, the sender can overlap the sending overhead of later packets with the transport latency and receiver overhead of prior packets. This essentially pipe- lines the transmission of packets over the network, also known as _link pipelining_. Fortunately, as discussed in prior chapters of this book, there are many application areas where communication from either several applications or several threads from the same application can run concurrently (e.g., a Web server concurrently serving thousands of client requests or streaming media), thus allowing a device to send a stream of packets without having to wait for an acknowledgment or a reply. Also, as long messages are usually divided into packets of maximum size before transport, a number of packets are injected into the network in succession for such cases. If such overlap were not possible, packets would have to wait for prior packets to be acknowledged before being transmitted and thus suffer signif- icant performance degradation.

> 到目前为止，我们已经考虑了单个数据包的运输，并计算了相关的端到端总数据包延迟。为了计算两个网络设备的有效带宽，我们必须考虑它们之间运输的数据包的连续流。我们必须牢记，除了最小化数据包延迟外，针对给定的成本和功耗目标优化的任何网络的目标是在最短的时间内传输最大可用信息，如网络划定的有效带宽。对于在发送下一个数据包之前不需要响应的应用程序，发件人可以将以后数据包的发送开销与先前数据包的运输延迟和接收器开销重叠。本质上是将数据包传输到网络上的传输，也称为_link 管道_。幸运的是，正如本书的先前章节所讨论的那样，在许多应用领域中，来自几个应用程序的通信或同一应用程序中的几个线程可以同时运行(例如，Web 服务器同时服务数千个客户端请求或流媒体)，因此允许设备发送数据包流，而无需等待确认或答复。同样，由于长消息通常在运输前将长消息分为最大尺寸的数据包，因此在这种情况下，许多数据包被连续注入网络。如果无法重叠，则数据包将不得不等待先前的数据包在传输之前被确认，从而遭受显着的性能降级。

Packets transported in a pipelined fashion can be acknowledged quite straight- forwardly simply by keeping a copy at the source of all unacknowledged packets that have been sent and keeping track of the correspondence between returned acknowledgments and packets stored in the buffer. Packets will be removed from the buffer when the corresponding acknowledgment is received by the sender. This can be done by including the message ID and packet sequence number associated with the packet in the packet’s acknowledgment. Furthermore, a separate timer must be associated with each buffered packet, allowing the packet to be resent if the associated time-out expires.

> 只需将副本保留在已发送的所有未经认证的数据包的源头，并跟踪存储在缓冲区中的返回的确认和数据包之间的对应关系，就可以直接地以管道方式运输的数据包。当发件人收到相应的确认时，将从缓冲区中删除数据包。这可以通过在数据包确认中包含与数据包关联的消息 ID 和数据包序列编号来完成。此外，必须与每个缓冲数据包关联一个单独的计时器，如果关联的超时到期，则可以使数据包感到不满。

Pipelining packet transport over the network has many similarities with pipe- lining computation within a processor. However, among some differences are that it does not require any staging latches. Information is simply propagated through network links as a sequence of signal waves. Thus, the network can be considered as a logical pipeline consisting of as many stages as are required so that the time of flight does not affect the effective bandwidth that can be achieved. Transmission of a packet can start immediately after the transmission of the previous one, thus over- lapping the sending overhead of a packet with the transport and receiver latency of previous packets. If the sending overhead is smaller than the transmission time, packets follow each other back-to-back, and the effective bandwidth approaches the raw link bandwidth when continuously transmitting packets. On the other hand, if the sending overhead is greater than the transmission time, the effective band- width at the injection point will remain well below the raw link bandwidth. The resulting _link injection bandwidth_, BW<sub>LinkInjection</sub>, for each link injecting a contin- uous stream of packets into a network is calculated with the following expression:

> 网络上的管道数据包传输与处理器内的管道计算相似。但是，在某些区别中，它不需要任何登台闩锁。信息简单地通过网络链接作为一系列信号波传播。因此，该网络可以视为由尽可能多的阶段组成的逻辑管道，因此飞行时间不会影响可以实现的有效带宽。数据包的传输可以在上一个传输后立即启动，从而超过了数据包的发送开销，并使用上一数据包的传输和接收器延迟。如果发送开销的开销小于传输时间，则数据包互相跟随，并且在连续传输数据包时，有效的带宽接近原始链路带宽。另一方面，如果发送开销大于传输时间，则注入点的有效带宽度将保持在原始链路带宽之下。对每个链接注入连续数据包的每个链接的结果_link 注入带宽_，bw <ub>链接</sub> </sub> </sub>，使用以下表达式计算：

We must also consider what happens if the receiver is unable to consume packets at the same rate they arrive. This occurs if the receiving overhead is greater than the sending overhead and the receiver cannot process incoming packets fast enough. In this case, the _link reception bandwidth_, BW<sub>LinkReception</sub>, for each reception link of the network is less than the link injection bandwidth and is obtained with this expression:

> 我们还必须考虑如果接收器无法以与他们到达相同的速度消费数据包，会发生什么。如果接收开销大于发送开销，并且接收器无法足够快地处理传入数据包。在这种情况下，对于网络的每个接收链路，_link 接收带宽_，bw <sub> linkReception </sub> </sub> </sub> </sub>都小于链路注入带宽，并以此表达方式获得：

When communication takes place between two devices interconnected by ded- icated links, all the packets sent by one device will be received by the other. If the receiver cannot process packets fast enough, the receiver buffer will become full, and flow control will throttle transmission at the sender. As this situation is produced by causes external to the network, we will not consider it further here. Moreover, if the receiving overhead is greater than the sending overhead, the receiver buffer will fill up and flow control will, likewise, throttle transmission at the sender. In this case, the effect of flow control is, on average, the same as if we replace sending overhead with receiving overhead. Assuming an ideal network that behaves like two dedi- cated links running in opposite directions at the full link bandwidth between the two devices—which is consistent with our black box view of the network to this point—the resulting effective bandwidth is the smaller of twice the injection band- width (to account for the two injection links, one for each device) or twice the recep- tion bandwidth. This results in the following expression for effective bandwidth:

> 当通过推迟链接互连的两个设备之间进行通信时，另一个设备发送的所有数据包将由另一个设备接收。如果接收器无法足够快地处理数据包，则接收器缓冲区将变得完整，流量控制将在发件人处进行节气门传输。由于这种情况是由网络外部的原因产生的，因此我们在这里不会进一步考虑。此外，如果接收开销大于发送开销的开销，则接收器缓冲区将填充，流量控制也将在发件人处处于节气门传输。在这种情况下，平均而言，流量控制的效果与我们用接收开销替换开设开销的情况相同。假设一个理想的网络表现得像两个设备之间的完整链路带宽相反的两个专用链接，这与我们的网络的黑匣子视图一致 - 最终的有效带宽是两倍的较小的带宽注射带宽度(考虑两个注入链接，每个设备一个)或接收带宽的两倍。这将导致以下表达以进行有效带宽：

where Overhead max(Sending overhead, Receiving overhead). Taking into account the expression for the transmission time, it is obvious that the effective bandwidth delivered by the network is identical to the aggregate network band- width when the transmission time is greater than the overhead. Therefore, full network utilization is achieved regardless of the value for the time of flight and, thus, regardless of the distance traveled by packets, assuming ideal network behavior (i.e., enough credits and buffers are provided for credit-based and Xon/ Xoff flow control). This analysis assumes that the sender and receiver network interfaces can process only one packet at a time. If multiple packets can be pro- cessed in parallel (e.g., as is done in IBM’s Federation network interfaces), the overheads for those packets can be overlapped, which increases effective band- width by that overlap factor up to the amount bounded by the transmission time. Let’s use the equation on page F-17 to explore the impact of packet size, trans- mission time, and overhead on BW<sub>Link</sub> <sub>Injection</sub>, BW<sub>LinkReception</sub>, and effective bandwidth for the various network domains: OCNs, SANs, LANs, and WANs.

> 顶部最大的位置(发送开销，接收开销)。考虑到传输时间的表达式，很明显，当传输时间大于开销时，网络传递的有效带宽与聚集网络带宽相同。因此，无论飞行时间的价值如何，都可以实现完整的网络利用，因此，无论数据包的距离如何控制)。该分析假设发件人和接收器网络界面一次只能处理一个数据包。如果可以并行处理多个数据包(例如，就像在 IBM 的联邦网络界面中所做的那样)，则可以重叠这些数据包的开销，从而将有效的频段宽度增加到该重叠因子，最高为传输限制的量时间。让我们使用第 F-17 页的方程式来探索数据包大小，传输时间和开销对 BW <ub> link </sub> <sub> indection </sub>，bw <ub> linkRection </子 > 和各种网络领域的有效带宽：OCN，SANS，LAN 和 WANS。

Example As in the previous example, assume we have a dedicated link network with a data bandwidth of 8 Gbps for each link in each direction interconnecting the two devices within an OCN, SAN, LAN, or WAN. Plot effective bandwidth versus packet size for each type of network for packets ranging in size from 4 bytes (i.e., a single 32-bit word) to 1500 bytes (i.e., the maximum transfer unit for Ether- net), assuming that end nodes have the same per-packet sending and receiving overheads as before: _x_ + 0.05 ns/byte and 4/3(_x_) + 0.05 ns/byte, respectively, where _x_ is 0 μs for the OCN, 0.3 μs for the SAN, 3 μs for the LAN, and 30 μs for the WAN. What limits the effective bandwidth, and for what packet sizes is the effec- tive bandwidth within 10% of the aggregate network bandwidth?

> 示例与上一个示例一样，假设我们具有一个专用的链接网络，每个方向上的每个链接的数据带宽为 8 Gbps，将 OCN，SAN，LAN 或 WAN 中的两个设备互连。绘图有效的带宽与数据包大小的每种类型的网络大小的数据包，大小从 4 个字节(即单个 32 位单词)到 1500 字节(即，以太网的最大传输单元)的大小不等，假设结束节点具有最大的传输单元)与以前相同的每包发送和接收开销相同：_x_ + 0.05 ns/byte 和 4/3(_x_) + 0.05 ns/byte，其中_x_为 OCN 为 0μs，SAN，3μs，3μs 对于 LAN，WAN 和 30μs。是什么限制了有效带宽？在哪个数据包中，有效的带宽在骨料网络带宽的 10％之内？

Figure F.6 Effective bandwidth versus packet size plotted in semi-log form for the four network domains. Overhead can be amortized by increasing the packet size, but for too large of an overhead (e.g., for WANs and some LANs) scaling the packet size is of little help. Other considerations come into play that limit the maximum packet size.

> 图 F.6 有效带宽与四个网络域的半循环形式绘制的数据包大小。可以通过增加数据包大小来摊销头顶，但是对于太大的开销(例如，对于 WANS 和某些 LAN)，扩展数据包的大小几乎没有帮助。其他考虑因素限制了最大数据包大小。

_Answer_ [Figure F.6](#_bookmark603) plots effective bandwidth versus packet size for the four network domains using the simple equation and parameters given above. For all packet sizes in the OCN, transmission time is greater than overhead (sending or receiv- ing), allowing full utilization of the aggregate bandwidth, which is 16 Gbps—that is, injection link (alternatively, reception link) bandwidth times two to account for both devices. For the SAN, overhead—specifically, receiving overhead—is larger than transmission time for packets less than about 800 bytes; consequently, packets of 655 bytes and larger are needed to utilize 90% or more of the aggregate band- width. For LANs and WANs, most of the link bandwidth is not utilized since over- head in this example is many times larger than transmission time for all packet sizes.

> _answer_ [图 F.6](#_ bookmark603)使用上面给出的简单方程式和参数来绘制四个网络域的有效带宽与四个网络域的数据包大小。对于 OCN 中的所有数据包尺寸，传输时间大于开销(发送或收到)，允许完全利用聚集体带宽(即 16 Gbps)，也就是说，注射链接(替代，接收链接)带宽乘以 2 次到 2 次。解释两个设备。对于 SAN 的开销(特别是接收开销)，小于传输时间小于大约 800 个字节的传输时间；因此，需要 655 个字节和较大的数据包来利用 90％或更多的聚集体宽度。对于 LAN 和 WAN，大多数链路带宽都没有使用，因为在此示例中，对于所有数据包尺寸的尺寸比传输时间大很多倍。

This example highlights the importance of reducing the sending and receiving overheads relative to packet transmission time in order to maximize the effective bandwidth delivered by the network.

> 这个示例强调了减少与数据包传输时间的发送和接收开销的重要性，以最大程度地提高网络传递的有效带宽。

The analysis above suggests that it is possible to provide some upper bound for the effective bandwidth by analyzing the path followed by packets and determining where the bottleneck occurs. We can extend this idea beyond the network interfaces by defining a model that considers the entire network from end to end as a pipe and identifying the narrowest section of that pipe. There are three areas of interest in that pipe: the aggregate of all network injection links and the corresponding _network injection bandwidth_ (BW<sub>NetworkInjection</sub>), the aggregate of all network reception links and the corresponding _network reception bandwidth_ (BW<sub>NetworkReception</sub>), and the aggregate of all network links and the corresponding _network bandwidth_ (BW<sub>Network</sub>). Expressions for these will be given in later sections as various layers of the black box view of the network are peeled away.

> 上面的分析表明，可以通过分析路径后面的路径和确定瓶颈在哪里来为有效带宽提供一些上限。我们可以通过定义将整个网络从端到端视为管道并识别该管道的最狭窄部分的模型，从而将这个想法扩展到网络接口之外。该管道中有三个感兴趣的领域：所有网络注入链接的总体和相应的_netWork 注入带宽_(BW <sub>网络注射</sub>)，所有网络接收链接的聚合和相应的_network tangwidth_(bw)<sub> network reception </sub>)，以及所有网络链接的聚合和相应的_network bandwidth_(bw <sub> network </sub>)。这些表达式将在后面的部分中给出，因为网络的黑匣子视图的各个层被剥离。

To this point, we have assumed that for just two interconnected devices the black box network behaves ideally and the network bandwidth is equal to the aggregate raw network bandwidth. In reality, it can be much less than the aggre- gate bandwidth as we will see in the following sections. In general, the effective bandwidth delivered end-to-end by the network to an application is upper bounded by the minimum across all three potential bottleneck areas:

> 为此，我们假设仅对于两个互连设备，黑匣子网络的行为是理想的，并且网络带宽等于汇总的原始网络带宽。实际上，它可能比以下各节看到的那样远远远远远远少于大门带宽。通常，网络向应用程序端对端交付的有效带宽在所有三个潜在瓶颈区域的最低限度上都限制：

Effective bandwidth = min BWNetworkInjection, BWNetwork, BWNetworkReception

> 有效带宽= min bwnetworkIndoction，bwnetwork，bwnetwork reception

We will expand upon this expression further in the following sections as we reveal more about interconnection networks and consider the more general case of inter- connecting more than two devices.

> 在以下各节中，我们将进一步扩展该表达式，因为我们透露了有关互连网络的更多信息，并考虑了连接两个以上设备的更一般情况。

In some sections of this appendix, we show how the concepts introduced in the section take shape in example high-end commercial products. Figure F.7 lists several commercial computers that, at one point in time in their existence, were among the highest-performing systems in the world within their class. Although these systems are capable of interconnecting more than two devices, they implement the basic functions needed for interconnecting only two devices. In addition to being applicable to the SANs used in those systems, the issues discussed in this section also apply to other interconnect domains: from OCNs to WANs.

> 在本附录的某些部分中，我们展示了本节中引入的概念如何在示例高端商业产品中形成。图 F.7 列出了几台商业计算机，这些计算机在其存在的一个时间点上是全球表现最高的系统之一。尽管这些系统能够互连两个以上的设备，但它们仅实现了仅互连两个设备所需的基本功能。除了适用于这些系统中使用的 SAN 外，本节中讨论的问题还适用于其他互连域：从 OCN 到 WANS。
