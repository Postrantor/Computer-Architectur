## Crosscutting Issues for Interconnection Networks

This section describes five topics discussed in other chapters that are fundamen- tally impacted by interconnection networks, and `vice versa`.

### Density-Optimized Processors versus SPEC-Optimized Processors

Given that people all over the world are accessing Web sites, it doesn’t really mat- ter where servers are located. Hence, many servers are kept at `collocation sites`, which charge by network bandwidth reserved and used and by space occupied and power consumed. Desktop microprocessors in the past have been designed to be as fast as possible at whatever heat could be dissipated, with little regard for the size of the package and surrounding chips. In fact, some desktop micropro- cessors from Intel and AMD as recently as 2006 burned as much as 130 watts! Floor space efficiency was also largely ignored. As a result of these priorities, power is a major cost for collocation sites, and processor density is limited by the power consumed and dissipated, including within the interconnect!

> 鉴于世界各地的人们都在访问网站，因此并没有真正在服务器所在的位置。因此，许多服务器都保存在 _Collocation 站点_，这些服务器由网络带宽保留和使用，以及被占用的空间和电源所消耗的空间。过去，桌面微处理器被设计为尽可能快地消散，几乎没有考虑包装和周围芯片的大小。实际上，来自英特尔和 AMD 的一些台式机摄制者最近燃烧了多达 130 瓦！地板空间效率也在很大程度上被忽略了。由于这些优先级，功率是套在一起站点的主要成本，并且处理器密度受到消耗和消散的功率(包括互连)的限制！

With the proliferation of portable computers (notebook sales exceeded desktop sales for the first time in 2005) and their reduced power consumption and cooling demands, the opportunity exists for using this technology to create considerably denser computation. For instance, the power consumption for the Intel Pentium M in 2006 was 25 watts, yet it delivered performance close to that of a desktop microprocessor for a wide set of applications. It is therefore conceivable that per- formance per watt or performance per cubic foot could replace performance per microprocessor as the important figure of merit. The key is that many applications already make use of large clusters, so it is possible that replacing 64 power-hungry processors with, say, 256 power-efficient processors could be cheaper yet be soft- ware compatible. This places greater importance on power- and performance- efficient interconnection network design.

> 随着便携式计算机的扩散(笔记本销售额在 2005 年首次超过台式机的销售)及其减少的功耗和冷却需求，因此存在使用该技术来创建相当密集的计算的机会。例如，2006 年英特尔奔腾 M 的功耗为 25 瓦，但它的性能接近了台式微处理器，用于广泛的应用。因此，可以想象的是，每瓦或每立方英尺的性能可以代替每个微处理器作为功绩的重要数字。关键是许多应用程序已经使用了大型群集，因此可以用 256 个发电器的处理器替换 64 个渴望的处理器，但可以更便宜，但可以兼容。这对功率和性能高效的互连网络设计更为重要。

The Google cluster is a prime example of this migration to many "cooler" processors versus fewer "hotter" processors. It uses racks of up to 80 Intel Pen- tium III 1 GHz processors instead of more power-hungry high-end processors. Other examples include blade servers consisting of 1-inch-wide by 7-inch-high rack unit blades designed based on mobile processors. The HP ProLiant BL10e G2 blade server supports up to 20 1-GHz ultra-low-voltage Intel Pentium M processors with a 400-MHz front-side bus, 1-MB L2 cache, and up to 1 GB memory. The Fujitsu Primergy BX300 blade server supports up to 20 1.4- or 1.6-GHz Intel Pentium M processors, each with 512 MB of memory expandable to 4 GB.

> Google 群集是这种迁移到许多 "冷却器" 处理器而不是 "热" 处理器的典型示例。它使用多达 80 个 Intel Pentium III 1 GHz 处理器的机架，而不是更多的耗电高端处理器。其他示例包括由基于移动处理器设计的 1 英寸范围内 7 英寸高的机架单元刀片组成的刀片服务器。HP Proliant BL10E G2 刀片服务器最多支持 20 1 GHz 超低电压 Intel Pentium M 处理器，具有 400 MHz 前侧总线，1-MB L2 CACHE 和 1 GB 内存。Fujitsu Primergy BX300 Blade Server 最多支持 20 1.4 或 1.6 GHz Intel Pentium M 处理器，每个处理器都有 512 MB 的内存可扩展到 4 GB。

### Smart Switches versus Smart Interface Cards

[Figure F.39](#_bookmark636) shows a trade-off as to where intelligence can be located within a net- work. Generally, the question is whether to have either smarter network interfaces or smarter switches. Making one smarter generally makes the other simpler and less expensive. By having an inexpensive interface, it was possible for Ethernet to become standard as part of most desktop and server computers. Lower-cost switches were made available for people with small configurations, not needing sophisticated forwarding tables and spanning-tree protocols of larger Ethernet switches.

> [图 F.39](#_bookmark636) 显示了关于智能可以在净工作中位于智能的位置的权衡。通常，问题是要具有智能网络接口还是更智能的开关。使一个更聪明的人通常会使另一个更简单，更便宜。通过具有廉价的接口，以太网可以成为大多数台式机和服务器计算机的一部分。为具有小配置的人提供了低成本开关，不需要复杂的转发表和较大以太网开关的跨越树协议。

Myrinet followed the opposite approach. Its switches are dumb components that, other than implementing flow control and arbitration, simply extract the first byte from the packet header and use it to directly select the output port. No routing tables are implemented, so the intelligence is in the network interface cards (NICs). The NICs are responsible for providing support for efficient communication and for implementing a distributed protocol for network (re)configuration. InfiniBand takes a hybrid approach by offering lower-cost, less sophisticated interface cards called target channel adapters (or TCAs) for less demanding devices such as disks—in the hope that it can be included within some I/O devices—and by offer- ing more expensive, powerful interface cards for hosts called host channel adapters (or HCAs). The switches implement routing tables.

> Myrinet 遵循相反的方法。它的开关是愚蠢的组件，除了实现流量控制和仲裁之外，只需从数据包标头中提取第一个字节，然后使用它直接选择输出端口。没有实现路由表，因此智能在网络接口卡(NICS)中。NIC 负责为有效的通信提供支持，并实施网络配置的分布式协议。Infiniband 通过提供较低成本的，不太复杂的界面卡来采用混合方法，称为目标通道适配器(或 TCA)，以用于磁盘等要求较低的设备，希望它可以包含在某些 I/O 设备中，并通过 offer-offer-offor--为主机提供更昂贵，功能强大的接口卡，称为主机通道适配器(或 HCAS)。交换机实现路由表。

Figure F.39 Intelligence in a network: switch versus network interface card. Note that Ethernet switches come in two styles, depending on the size of the network, and that InfiniBand network interfaces come in two styles, depending on whether they are attached to a computer or to a storage device. Myrinet is a proprietary system area network.

> 图 F.39 网络中的智能：交换机与网络接口卡。请注意，以太网开关有两种样式，具体取决于网络的大小，而 Infiniband 网络接口有两种样式，具体取决于它们是连接到计算机还是存储设备上的。Myrinet 是一个专有的系统区域网络。

### Protection and User Access to the Network

> ###保护和用户访问网络

A challenge is to ensure safe communication across a network without invoking the operating system in the common case. The Cray Research T3D supercomputer offers an interesting case study. Like the more recent Cray X1E, the T3D supports a global address space, so loads and stores can access memory across the network. Protection is ensured because each access is checked by the TLB. To support trans- fer of larger objects, a block transfer engine (BLT) was added to the hardware. Pro- tection of access requires invoking the operating system before using the BLT to check the range of accesses to be sure there will be no protection violations.

> 一个挑战是在常见情况下确保在不调用操作系统的情况下确保在网络上进行安全通信。Cray Research T3D 超级计算机提供了一个有趣的案例研究。像最近的 Cray X1E 一样，T3D 支持全局地址空间，因此负载和商店可以在整个网络上访问内存。由于 TLB 检查了每个访问，因此可以确保保护。为了支持较大对象的传输，将块传输引擎(BLT)添加到硬件中。访问的访问需要在使用 BLT 检查访问范围之前调用操作系统，以确保不会违反保护。

[Figure F.40](#_bookmark637) compares the bandwidth delivered as the size of the object varies for reads and writes. For very large reads (e.g., 512 KB), the BLT achieves the highest performance: 140 MB/sec. But simple loads get higher performance for 8 KB or less. For the write case, both achieve a peak of 90 MB/sec, presumably because of the limitations of the memory bus. But, for writes, the BLT can only match the performance of simple stores for transfers of 2 MB; anything smaller and it’s faster to send stores. Clearly, a BLT that can avoid invoking the operating system in the common case would be more useful.

> [图 F.40](#_bookmark637) 比较了随着对象的大小而传递的带宽，以读取和写入。对于非常大的读数(例如 512 KB)，BLT 的性能最高：140 MB/sec。但是简单的负载可在 8 KB 或更少的情况下获得更高的性能。对于写入案例，两者都达到 90 MB/sec 的峰值，大概是由于内存总线的局限性。但是，对于写作而言，BLT 只能与 2 MB 转移的简单商店的性能相匹配。任何较小的东西都更快地发送商店。显然，可以避免在常见情况下调用操作系统的 BLT 将更有用。

### Efficient Interface to the Memory Hierarchy versus the Network

> ###有效接口到内存层次结构与网络

Traditional evaluations of processor performance, such as SPECint and SPECfp, encourage integration of the memory hierarchy with the processor as the efficiency of the memory hierarchy translates directly into processor performance. Hence,

> 对处理器性能的传统评估，例如 SpecInt 和 SpecFP，鼓励将内存层次结构与处理器集成，因为内存层次结构的效率直接转化为处理器性能。因此，

![](../media/image713.png)

Figure F.40 Bandwidth versus transfer size for simple memory access instructions versus a block transfer device on the Cray Research T3D. (From [Arpaci et al. [1995]](#_bookmark643).)

> 图 F.40 带宽与传输大小，用于简单的内存访问指令与 Cray Research T3D 上的块传输设备。(摘自 [Arpaci 等人。\ [1995 ]](#_bookmark643)。

microprocessors have multiple levels of caches on chip along with buffers for writes. Because benchmarks such as SPECint and SPECfp do not reward good interfaces to interconnection networks, many machines make the access time to the network delayed by the full memory hierarchy. Writes must lumber their way through full write buffers, and reads must go through the cycles of first-, second-, and often third-level cache misses before reaching the interconnection network. This hierarchy results in newer systems having higher latencies to the interconnect than older machines.

> 微处理器在芯片上有多个级别的缓存以及用于写入的缓冲液。由于诸如 SpecInt 和 SpecFP 之类的基准不能奖励良好的接口到互连网络，因此许多机器可以使整个内存层次结构延迟网络的访问时间。写入必须通过完整的写缓冲区伐木，并且阅读必须经过到达互连网络之前的第一，第二或第三级高速缓存的周期。该层次结构导致较新的系统比较旧的机器具有更高的互连潜伏期。

Let’s compare three machines from the past: a 40-MHz SPARCstation-2, a 50- MHz SPARCstation-20 without an external cache, and a 50-MHz SPARCstation- 20 with an external cache. According to SPECint95, this list is in order of increas- ing performance. The time to access the I/O bus (S-bus), however, increases in this sequence: 200 ns, 500 ns, and 1000 ns. The SPARCstation-2 is fastest because it has a single bus for memory and I/O, and there is only one level to the cache. The SPARCstation-20 memory access must first go over the memory bus (M-bus) and then to the I/O bus, adding 300 ns. Machines with a second-level cache pay an extra penalty of 500 ns before accessing the I/O bus.

> 让我们比较过去的三台机器：40 MHz SparcStation-2，无外部缓存的 50-MHz SparcStation-20，以及带有外部缓存的 50 MHz SparcStation-20。根据 Specint95，此列表按照较高的性能顺序。但是，访问 I/O 总线(S-BUS)的时间增加了此顺序：200 ns，500 ns 和 1000 ns。SPARCSTATION-2 是最快的，因为它有一个用于内存的总线和 I/O，并且缓存只有一个级别。SparcStation-20 内存访问必须首先越过内存总线(M-BUS)，然后访问 I/O 总线，并添加 300 ns。在访问 I/O 总线之前，具有第二级缓存的机器支付 500 ns 的额外罚款。

### Compute-Optimized Processors versus Receiver Overhead

> ###计算优化的处理器与接收器开销

The overhead to receive a message likely involves an interrupt, which bears the cost of flushing and then restarting the processor pipeline, if not offloaded. As mentioned earlier, reading network status and receiving data from the network interface likely operate at cache miss speeds. If microprocessors become more superscalar and go to even faster clock rates, the number of missed instruction issue opportunities per message reception will likely rise to unacceptable levels.

> 收到消息的开销可能涉及中断，该中断带有冲洗的成本，然后重新启动处理器管道(即使没有卸载)。如前所述，读取网络状态和从网络接口接收数据可能以缓存失误速度运行。如果微处理器变得越来越超级，并且时钟速度更快，则每个消息接收的错过的指令问题数量可能会提高到不可接受的水平。

Fallacies and Pitfalls

> 谬论和陷阱

Myths and hazards are widespread with interconnection networks. This section mentions several warnings, so proceed carefully.

> 神话和危害与互连网络普遍存在。本节提到了一些警告，因此请仔细进行。

Fallacy `The interconnection network is very fast and does not need to be improved`

> 谬误*互连网络非常快，不需要改进*

The interconnection network provides certain functionality to the system, very much like the memory and I/O subsystems. It should be designed to allow proces- sors to execute instructions at the maximum rate. The interconnection network sub- system should provide high enough bandwidth to keep from continuously entering saturation and becoming an overall system bottleneck.

> 互连网络为系统提供了某些功能，非常类似于内存和 I/O 子系统。它应旨在允许程序员以最大速率执行说明。互连网络子系统应提供足够高的带宽，以防止连续进入饱和并成为整体系统瓶颈。

In the 1980s, when wormhole switching was introduced, it became feasible to design large-diameter topologies with single-chip switches so that the band- width capacity of the network was not the limiting factor. This led to the flawed belief that interconnection networks need no further improvement.

> 在 1980 年代，当引入虫洞开关时，使用单芯片开关设计大直径拓扑变得可行，因此网络的带宽容量不是限制因素。这导致了一种有缺陷的信念，即互连网络无需进一步的改进。

Since the 1980s, much attention has been placed on improving processor per- formance, but comparatively less has been focused on interconnection net- works. As technology advances, the interconnection network tends to represent an increasing fraction of system resources, cost, power consumption, and various other attributes that impact functionality and performance. Scaling the bandwidth simply by overdimensioning certain network parameters is no longer a cost-viable option. Designers must carefully consider the end-to- end interconnection network design in concert with the processor, memory, and I/O subsystems in order to achieve the required cost, power, functionality, and performance objectives of the entire system. An obvious case in point is multicore processors with on-chip networks.

> 自 1980 年代以来，已经对改善处理器的能力引起了很多关注，但相对较少的集中在互连网络上。随着技术的进步，互连网络倾向于代表系统资源，成本，功耗以及影响功能和性能的各种其他属性的越来越多。简单地通过过度估计某些网络参数来扩展带宽不再是可行的选择。设计师必须仔细考虑与处理器，内存和 I/O 子系统一起端到端互连网络设计，以实现整个系统的所需成本，功率，功能和性能目标。一个明显的例子是具有片上网络的多核心处理器。

Fallacy `Bisection bandwidth is an accurate cost constraint of a network`

> 谬误一分配带宽是网络的准确成本限制

Despite being very popular, bisection bandwidth has never been a practical con- straint on the implementation of an interconnection network, although it may be one in future designs. It is more useful as a performance measure than as a cost measure. Chip pin-outs are the more realistic bandwidth constraint.

> 尽管非常受欢迎，但两分带宽从来都不是实施互连网络的实用性，尽管它可能是将来的设计中的一个。与成本度量相比，它作为绩效指标更有用。芯片引脚是更现实的带宽约束。

Pitfall `Using bandwidth (in particular, bisection bandwidth) as the only measure of network performance`

> 陷阱*使用带宽(尤其是一分配带宽)作为网络性能的唯一度量*

It seldom is the case that aggregate network bandwidth (likewise, network bisec- tion bandwidth) is the end-to-end bottlenecking point across the network. Even if it were the case, networks are almost never 100% efficient in transporting packets across the bisection (i.e., ρ&lt; 100%) nor at receiving them at network endpoints (i.e., σ&lt; 100%). The former is highly dependent upon routing, switching, arbitra- tion, and other such factors while both the former and the latter are highly dependent upon traffic characteristics. Ignoring these important factors and concentrating only on raw bandwidth can give very misleading performance pre- dictions. For example, it is perfectly conceivable that a network could have higher aggregate bandwidth and/or bisection bandwidth relative to another network but also have lower measured performance!

> 聚合网络带宽（同样，网络二分带宽）很少是整个网络的端到端瓶颈点。 即使是这种情况，网络在通过二等分线传输数据包（即 ρ< 100%）或在网络端点接收数据包（即 σ< 100%）时也几乎永远不会 100% 有效。 前者高度依赖于路由、交换、仲裁和其他此类因素，而前者和后者都高度依赖于流量特性。 忽略这些重要因素并只关注原始带宽可能会给出非常具有误导性的性能预测。 例如，完全可以想象，一个网络相对于另一个网络可能具有更高的聚合带宽和/或二分带宽，但也具有较低的测量性能！

Apparently, given sophisticated protocols like TCP/IP that maximize delivered bandwidth, many network companies believe that there is only one figure of merit for networks. This may be true for some applications, such as video streaming, where there is little interaction between the sender and the receiver. Many appli- cations, however, are of a request-response nature, and so for every large message there must be one or more small messages. One example is NFS.

> 显然，鉴于 TCP/IP 等复杂的协议最大化传递的带宽，许多网络公司认为，网络只有一个值得的数字。对于某些应用程序(例如视频流)，这可能是正确的，那里的发送者和接收器之间几乎没有相互作用。但是，许多应用具有请求响应性质，因此，对于每一个大信息，必须有一个或多个小消息。一个例子是 NFS。

Figure F.41 compares a shared 10-Mbit/sec Ethernet LAN to a switched 155- Mbit/sec ATM LAN for NFS traffic. Ethernet drivers were better tuned than the ATM drivers, such that 10-Mbit/sec Ethernet was faster than 155-Mbit/sec ATM for payloads of 512 bytes or less. Figure F.41 shows the overhead time, trans- mission time, and total time to send all the NFS messages over Ethernet and ATM. The peak link speed of ATM is 15 times faster, and the measured link speed for 8- KB messages is almost 9 times faster. Yet, the higher overheads offset the benefits so that ATM would transmit NFS traffic only 1.2 times faster.

> 图 F.41 将共享的 10-mbit/sec 以太网 LAN 与开关的 155-MBIT/SEC ATM LAN 用于 NFS 流量。以太网驱动程序比 ATM 驱动程序更好地调整了，因此 10 毫米/秒的以太网比 155-mbit/sec 的 ATM 快于 512 个字节或更少。图 F.41 显示了开销时间，转移时间和总时间，以通过以太网和 ATM 发送所有 NFS 消息。ATM 的峰值链路速度快 15 倍，并且 8 kb 消息的测量链路速度快几乎 9 倍。然而，较高的开销抵消了收益，因此 ATM 只能传输 NFS 流量的速度仅 1.2 倍。

Figure F.41 Total time on a 10-Mbit Ethernet and a 155-Mbit ATM, calculating the total overhead and transmis- sion time separately. Note that the size of the headers needs to be added to the data bytes to calculate transmission time. The higher overhead of the software driver for ATM offsets the higher bandwidth of the network. These mea- surements were performed in 1994 using SPARCstation 10s, the ForeSystems SBA-200 ATM interface card, and the Fore Systems ASX-200 switch. (NFS measurements taken by Mike Dahlin of the University of California–Berkeley.)

> 图 F.41 在 10 毫升以太网和 155 毫升 ATM 上的总时间，分别计算总开销和传输时间。请注意，需要将标题的大小添加到数据字节中以计算传输时间。用于 ATM 的软件驱动程序的较高开销取消网络的较高带宽。这些微调是在 1994 年使用 SparcStation 10s，Foresystems SBA-200 ATM 接口卡和 Fore Systems ASX-200 Switch 进行的。(加州大学伯克利分校的 Mike Dahlin 进行的 NFS 测量。)

Pitfall `Not providing sufficient reception link bandwidth, which causes the network end nodes to become even more of a bottleneck to performance`

> 陷阱*不提供足够的接收链路带宽，这会导致网络结束节点变得更像是瓶颈*

Unless the traffic pattern is a permutation, several packets will concurrently arrive at some destinations when most source devices inject traffic, thus pro- ducing contention. If this problem is not addressed, contention may turn into congestion that will spread across the network. This can be dealt with by ana- lyzing traffic patterns and providing extra reception bandwidth. For example, it is possible to implement more reception bandwidth than injection bandwidth. The IBM Blue Gene/L, for example, implements an on-chip switch with 7-bit injection and 12-bit reception links, where the reception BW equals the aggre- gate switch input link BW.

> 除非流量模式为排列，否则几个数据包将同时到达大多数源设备注入流量的目的地，从而引起争议。如果没有解决此问题，则可能会变成跨网络的拥塞。可以通过分流的交通方式来解决这一问题，并提供额外的接收带宽。例如，与注射带宽相比，可以实现更多的接收带宽。例如，IBM 蓝色基因/L 实现了带有 7 位注入和 12 位接收链路的片上开关，其中接收 BW 等于 Accre-Gate 开关输入链路 BW。

Pitfall `Using high-performance network interface cards but forgetting about the I/O sub- system that sits between the network interface and the host processor`

> 陷阱\_使用高性能网络接口卡，但忘记了位于网络接口和主机处理器之间的 I/O 子系统

This issue is related to the previous one. Messages are usually composed in user space buffers and later sent by calling a send function from the communications library. Alternatively, a cache controller implementing a cache coherence protocol may compose a message in some SANs and in OCNs. In both cases, messages have to be copied to the network interface memory before transmission. If the I/O band- width is lower than the link bandwidth or introduces significant overhead, this is going to affect communication performance significantly. As an example, the first 10-Gigabit Ethernet cards in the market had a PCI-X bus interface for the system with a significantly lower bandwidth than 10 Gbps.

> 这个问题与上一个问题有关。消息通常在用户空间缓冲区中组成，然后通过调用通信库的发送功能来发送。另外，实现缓存相干协议的缓存控制器可以在某些 SAN 和 OCN 中构成消息。在这两种情况下，都必须在传输之前将消息复制到网络接口内存。如果 I/O 频带宽度低于链接带宽或引入重要的开销，则将显着影响沟通性能。例如，市场上的第一张 10 gibit 以太网卡具有该系统的 PCI-X 总线接口，其带宽明显低于 10 Gbpps。

Fallacy `Zero-copy protocols do not require copying messages or fragments from one buffer to another`

> [!note]
> zero-copy 是真正的零拷贝吗？

> 谬误 _zero-copy 协议不需要将消息或片段从一个缓冲区复制到另一个缓冲区_

Traditional communication protocols for computer networks allow access to com- munication devices only through system calls in supervisor mode. As a conse- quence of this, communication routines need to copy the corresponding message from the user buffer to a kernel buffer when sending a message. Note that the communication protocol may need to keep a copy of the message for retrans- mission in case of error, and the application may modify the contents of the user buffer once the system call returns control to the application. This buffer-to-buffer copy is eliminated in zero-copy protocols because the communication routines are executed in user space and protocols are much simpler.

> 用于计算机网络的传统通信协议只能通过主管模式下的系统调用访问通信设备。为此，在发送消息时，通信例程需要将相应消息从用户缓冲区复制到内核缓冲区。请注意，通信协议可能需要保留消息的副本以进行重新传播，以防发生错误，并且一旦系统调用将控制权返回到应用程序，该应用程序可能会修改用户缓冲区的内容。由于在用户空间中执行通信例程，并且协议要简单得多，因此在零拷贝协议中消除了此缓冲区到屏障副本。

However, messages still need to be copied from the application buffer to the memory in the network interface card (NIC) so that the card hardware can transmit it from there through to the network. Although it is feasible to eliminate this copy by allocating application message buffers directly in the NIC memory (and, indeed, this is done in some protocols), this may not be convenient in current systems because access to the NIC memory is usually performed through the I/O subsystem, which usually is much slower than accessing main memory. Thus, it is generally more efficient to compose the message in main memory and let DMA devices take care of the transfer to the NIC memory.

> 但是，仍然需要将消息从应用程序缓冲区复制到网络接口卡(NIC)中的内存，以便卡硬件可以从那里传输到网络。尽管可以通过直接在 NIC 内存中分配应用程序消息缓冲区来消除此副本是可行的(实际上，这是在某些协议中完成的)，但这在当前系统中可能不方便 I/O 子系统，通常比访问主内存要慢得多。因此，通常在主内存中撰写消息并让 DMA 设备负责转移到 NIC 内存是更有效的。

Moreover, what few people count is the copy from where the message frag- ments are computed (usually the ALU, with results stored in some processor reg- ister) to main memory. Some systolic-like architectures in the 1980s, like the iWarp, were able to directly transmit message fragments from the processor to the network, effectively eliminating all the message copies. This is the approach taken in the Cray X1E shared-memory multiprocessor supercomputer.

> 此外，很少有人计算的是从该消息计算出片段的副本(通常是 ALU，结果存储在某些处理器 reg-iSter 中)到主内存。1980 年代的一些类似收缩期的体系结构，例如 IWARP，能够将消息片段从处理器直接传输到网络，从而有效地消除了所有消息副本。这是 Cray X1E 共享内存多处理器超级计算机中采用的方法。

Similar comments can be made regarding the reception side; however, this does not mean that zero-copy protocols are inefficient. These protocols represent the most efficient kind of implementation used in current systems.

> 关于接待方的类似评论；但是，这并不意味着零拷贝方案效率低下。这些协议代表了当前系统中使用的最有效的实现。

Pitfall `Ignoring software overhead when determining performance`

> 陷阱*签名软件在确定性能时开销*

Low software overhead requires cooperation with the operating system as well as with the communication libraries, but even with protocol offloading it con- tinues to dominate the hardware overhead and must not be ignored. [Figures F.32](#_bookmark628) and F.41 give two examples, one for a SAN standard and the other for a WAN standard. Other examples come from proprietary SANs for supercomputers. The Connection Machine CM-5 supercomputer in the early 1990s had a software overhead of 20 μs to send a message and a hardware overhead of only 0.5 μs. The first Intel Paragon supercomputer built in the early 1990s had a hardware overhead of just 0.2 μs, but the initial release of the software had an overhead of 250 μs. Later releases reduced this overhead down to 25 μs and, more recently, down to only a few microseconds, but this still dominates the hardware overhead. The IBM Blue Gene/L has an MPI sending/receiving overhead of approximately 3 μs, only a third of which (at most) is attributed to the hardware.

> 低软件开销需要与操作系统以及通信库的合作，但即使使用协议卸载，它仍继续主导硬件开销，不容忽视。 [图 F.32](#_bookmark628) 和 F.41 给出了两个示例，一个用于 SAN 标准，另一个用于 WAN 标准。 其他示例来自超级计算机的专有 SAN。 1990 年代初期的 Connection Machine CM-5 超级计算机发送消息的软件开销为 20 微秒，而硬件开销仅为 0.5 微秒。 1990 年代初建造的第一台英特尔 Paragon 超级计算机的硬件开销仅为 0.2 微秒，但软件的初始版本有 250 微秒的开销。 后来的版本将此开销减少到 25 微秒，最近减少到只有几微秒，但这仍然是硬件开销的主要部分。 IBM Blue Gene/L 的 MPI 发送/接收开销约为 3 μs，其中只有三分之一（最多）归因于硬件。

> [!note]
> 这里有提及到一个 SAN 标准，之前好像是听到 chenxiao.zhao 提及到

This pitfall is simply Amdahl’s law applied to networks: Faster network hardware is superfluous if there is not a corresponding decrease in software overhead. The software overhead is much reduced these days with OS bypass, lightweight protocols, and protocol offloading down to a few micro- seconds or less, typically, but it remains a significant factor in determining performance.

> 这个陷阱只是 AMDAHL 的定律应用于网络：如果软件开销没有相应减少，则更快的网络硬件是多余的。如今，使用 OS 旁路，轻量级协议和协议下载到几个或更短的时间(通常)，通常会大大降低该软件开销，但通常是确定性能的重要因素。

Fallacy `MINs are more cost-effective than direct networks`

> 谬误 _mins 比直接网络更具成本效益_

A MIN is usually implemented using significantly fewer switches than the number of devices that need to be connected. On the other hand, direct networks usually include a switch as an integral part of each node, thus requiring as many switches as nodes to interconnect. However, nothing prevents the implementation of nodes with multiple computing devices on it (e.g., a multicore processor with an on-chip switch) or with several devices attached to each switch (i.e., bristling). In these cases, a direct network may be as (or even more) cost-effective as a MIN. Note that, for a MIN, several network interfaces may be required at each node to match the bandwidth delivered by the multiple links per node provided by the direct network.

> 通常，通常使用要连接的设备数量的开关数量要少得多。另一方面，直接网络通常包括一个开关作为每个节点的整体部分，因此需要与节点互连的节点一样多。但是，没有什么可以阻止在其上具有多个计算设备的节点的实现(例如，具有片上开关的多层处理器)或每个开关上附加了几个设备(即螺母)。在这些情况下，直接网络可能会(甚至更多)成本效益。请注意，在一个分钟内，每个节点可能需要几个网络接口，以匹配由直接网络提供的每个节点的多个链接传递的带宽。

Fallacy `Low-dimensional direct networks achieve higher performance than high-dimensional networks such as hypercubes`

> 谬误 _low 维直接网络比高维网络(例如 HyperCubes_)获得更高的性能

This conclusion was drawn by several studies that analyzed the optimal number of dimensions under the main physical constraint of bisection bandwidth. However, most of those studies did not consider link pipelining, considered only very short links, and/or did not consider switch architecture design constraints. The misplaced assumption that bisection bandwidth serves as the main limit did not help matters. Nowadays, most researchers and designers believe that high-radix switches are more cost-effective than low-radix switches, including some who concluded the opposite before.

> 这一结论是通过几项研究得出的，这些研究分析了一分之一的带宽的主要物理约束下的最佳尺寸数量。但是，大多数研究都不考虑链接管道链接，而仅被视为非常短的链接和/或不考虑切换体系结构设计约束。分配带宽作为主要极限的放置假设没有帮助。如今，大多数研究人员和设计师认为，高 radix 开关比低调开关更具成本效益，其中包括一些以前相反的结论。

Fallacy `Wormhole switching achieves better performance than other switching techniques`

> 谬误*虫孔切换比其他切换技术更好的性能*

Wormhole switching delivers the same no-load latency as other pipelined switch- ing techniques, like virtual cut-through switching. The introduction of wormhole switches in the late 1980s coinciding with a dramatic increase in network band- width led many to believe that wormhole switching was the main reason for the performance boost. Instead, most of the performance increase came from a drastic increase in link bandwidth, which, in turn, was enabled by the ability of wormhole switching to buffer packet fragments using on-chip buffers, instead of using the node’s main memory or some other off-chip source for that task. More recently, much larger on-chip buffers have become feasible, and virtual cutthrough achieved the same no-load latency as wormhole while delivering much higher throughput. This did not mean that wormhole switching was dead. It continues to be the switch- ing technique of choice for applications in which only small buffers should be used (e.g., perhaps for on-chip networks).

> 虫洞开关提供的无负载延迟与其他管道开关技术相同，例如虚拟切换开关。1980 年代后期的引入虫洞开关与网络频带的急剧增加相吻合，这使许多人相信虫洞切换是性能提升的主要原因。取而代之的是，大多数性能提高来自链接带宽的急剧增加，这反过来又通过使用芯片缓冲区的蠕虫孔切换到缓冲数据包片段的能力，而不是使用节点的主内存或其他其他 OFF - 该任务的芯片源。最近，片上缓冲液的更大，并且虚拟切割达到了与虫洞相同的无负载延迟，同时又能提供更高的吞吐量。这并不意味着虫洞切换已经死亡。它仍然是仅使用小型缓冲区(例如，可能用于芯片网络)的应用程序的转换技术。

Fallacy `Implementing a few virtual channels always increases throughput by allowing packets to pass through blocked packets ahead`

> 谬误\_完成一些虚拟通道总是通过允许数据包通过前面的包数据包来增加吞吐量

In general, implementing a few virtual channels in a wormhole switch is a good idea because packets are likely to pass blocked packets ahead of them, thus reducing latency and significantly increasing throughput. However, the improvements are not as dramatic for virtual cut-through switches. In virtual cut-through, buffers should be large enough to store several packets. As a con- sequence, each virtual channel may introduce HOL blocking, possibly degrad- ing performance at high loads. Adding virtual channels increases cost, but it may deliver little additional performance unless there are as many virtual chan- nels as switch ports and packets are mapped to virtual channels according to their destination (i.e., virtual output queueing). It is certainly the case that vir- tual channels can be useful in virtual cut-through networks to segregate differ- ent traffic classes, which can be very beneficial. However, multiplexing the packets over a physical link on a flit-by-flit basis causes all the packets from different virtual channels to get delayed. The average packet delay is signifi- cantly shorter if multiplexing takes place on a packet-by-packet basis, but in this case packet size should be bounded to prevent any one packet from monopolizing the majority of link bandwidth.

> 通常，在虫洞开关中实现一些虚拟通道是一个好主意，因为数据包可能会通过它们前方的封锁数据包，从而减少延迟并大大增加吞吐量。但是，对于虚拟切割开关而言，这些改进并不是那么引人注目。在虚拟切割中，缓冲区应足够大，可以存储几包。顺便说一句，每个虚拟通道可能会引入 Hol 阻塞，可能会在高载荷下降低性能。添加虚拟通道会增加成本，但是除非有与开关端口和数据包一样多的虚拟频道，否则它可能几乎没有额外的性能。当然，纯通道可以在虚拟切割网络中有用，以分离不同的流量类别，这可能是非常有益的。但是，将数据包在物理链接上以 Flit-Flit 基础为基础，从而导致来自不同虚拟通道的所有数据包都会延迟。平均数据包延迟是否明显缩短，如果逐包进行多功能，但是在这种情况下，应将数据包大小限制，以防止任何一个数据包垄断大多数链接带宽。

Fallacy `Adaptive routing causes out-of-order packet delivery, thus introducing too much overhead needed to reorder packets at the destination device`

> 谬误 _ADAPTIVE 路由导致订购外数据包交付，因此在目标设备上重新排序数据包需要过多的开销_

Adaptive routing allows packets to follow alternative paths through the network depending on network traffic; therefore, adaptive routing usually introduces outof-order packet delivery. However, this does not necessarily imply that reorder- ing packets at the destination device is going to introduce a large overhead, making adaptive routing not useful. For example, the most efficient adaptive routing algo- rithms to date support fully adaptive routing in some virtual channels but required deterministic routing to be implemented in some other virtual channels in order to prevent deadlocks (à la the IBM Blue Gene/L). In this case, it is very easy to select between adaptive and deterministic routing for each individual packet. A single bit in the packet header can indicate to the switches whether all the virtual channels can be used or only those implementing deterministic routing. This hardware sup- port can be used as indicated below to eliminate packet reordering overhead at the destination.

> 自适应路由允许数据包根据网络流量遵循替代路径。因此，自适应路由通常会引入 Out-Order-rord-cacket thressive。但是，这并不一定意味着目标设备上的数据包将引入大开销，从而使自适应路由无用。例如，最有效的自适应路由算法将在某些虚拟通道中支持完全自适应路由，但需要在其他一些虚拟通道中实现确定性路由，以防止死锁(àlaibm blue Gene/L)。在这种情况下，对于每个单独的数据包，在自适应和确定性路由之间进行选择非常容易。数据包标头中的一个位可以指示开关是否可以使用所有虚拟通道，也只能仅实现确定性路由的那些。可以使用此硬件支持，如下所示，以消除目的地的数据包在开销上。

Most communication protocols for parallel computers and clusters implement two different protocols depending on message size. For short messages, an eager protocol is used in which messages are directly transmitted, and the receiving nodes use some preallocated buffer to temporarily store the incoming message. On the other hand, for long messages, a rendezvous protocol is used. In this case, a control message is sent first, requesting the destination node to allocate a buffer large enough to store the entire message. The destination node confirms buffer allocation by returning an acknowledgment, and the sender can proceed with frag- menting the message into bounded-size packets, transmitting them to the destination.

> 并行计算机和簇的大多数通信协议根据消息大小实现两个不同的协议。对于简短消息，使用了一个急切的协议，其中直接传输消息，并且接收节点使用一些预关注的缓冲区来临时存储传入的消息。另一方面，对于长消息，使用集合协议。在这种情况下，首先发送控制消息，要求目标节点分配足够大的缓冲区以存储整个消息。目标节点通过返回确认来确认缓冲区分配，并且发件人可以将消息分解为有限大小的数据包，从而将其传输到目的地。

If eager messages use only deterministic routing, it is obvious that they do not introduce any reordering overhead at the destination. On the other hand, packets belonging to a long message can be transmitted using adaptive routing. As every packet contains the sequence number within the message (or the off- set from the beginning of the message), the destination node can store every incoming packet directly in its correct location within the message buffer, thus incurring no overhead with respect to using deterministic routing. The only thing that differs is the completion condition. Instead of checking that the last packet in the message has arrived, it is now necessary to count the arrived packets, notifying the end of reception when the count equals the message size. Taking into account that long messages, even if not frequent, usually consume most of the network bandwidth, it is clear that most packets can benefit from adaptive routing without introducing reordering overhead when using the pro- tocol described above.

> 如果急切的消息仅使用确定性路由，很明显，他们不会在目的地引入任何开销。另一方面，可以使用自适应路由传输属于长消息的数据包。由于每个数据包都包含消息中的序列编号(或从消息开头的偏离设置)，目标节点可以将每个传入的数据包直接存储在消息缓冲区中的正确位置中，从而在使用方面无需使用与使用相对于使用确定性路由。唯一不同的是完成条件。现在有必要计算到达数据包，而不是检查消息中的最后一个数据包是否到达，当计数等于消息大小时，请通知接收到结束。考虑到长消息，即使不频繁，通常会消耗大多数网络带宽，很明显，大多数数据包可以从自适应路由中受益，而无需在使用上面描述的 Pro-Tocol 时引入开销。

Fallacy `Adaptive routing by itself always improves network fault tolerance because it allows packets to follow alternative paths`

> 谬误 _ADAPTIVE 路由本身总是改善网络容错，因为它允许数据包遵循替代路径_

Adaptive routing by itself is not enough to tolerate link and/or switch failures. Some mechanism is required to detect failures and notify them, so that the routing logic could exclude faulty paths and use the remaining ones. Moreover, while a given link or switch failure affects a certain number of paths when using determin- istic routing, many more source/destination pairs could be affected by the same failure when using adaptive routing. As a consequence of this, some switches implementing adaptive routing transition to deterministic routing in the presence of failures. In this case, failures are usually tolerated by sending messages through alternative paths from the source node. As an example, the Cray T3E implements direction-order routing to tolerate a few failures. This fault-tolerant routing technique avoids cycles in the use of resources by crossing directions in order (e.g., `X`+, `Y`+, `Z`+, `Z` , `Y` , then `X` ). At the same time, it provides an easy way to send packets through nonminimal paths, if necessary, to avoid crossing faulty com- ponents. For instance, a packet can be initially forwarded a few hops in the `X`+ direction even if it has to go in the `X`— direction at some point later.

> **自适应路由本身不足以容忍链路和/或交换机故障。 需要某种机制来检测故障并通知它们，以便路由逻辑可以排除故障路径并使用其余路径**。 此外，虽然给定的链路或交换机故障在使用确定性路由时会影响一定数量的路径，但在使用自适应路由时，更多的源/目标对可能会受到相同故障的影响。 因此，一些交换机在出现故障时将自适应路由转换为确定性路由。 在这种情况下，通常通过从源节点通过替代路径发送消息来容忍故障。 例如，Cray T3E 实现方向顺序路由以容忍一些故障。 这种容错路由技术通过按顺序交叉方向（例如，“X”+、“Y”+、“Z”+、“Z”、“Y”，然后是“X”）避免了资源使用循环。 同时，它提供了一种通过非最小路径发送数据包的简单方法，如果有必要，可以避免穿过故障组件。 例如，一个数据包最初可以在 `X`+ 方向转发几跳，即使它必须在稍后的某个时间点进入 `X`- 方向。

Pitfall `Trying to provide features only within the network versus end-to-end`

> 陷阱试图仅在网络与端到端提供功能

The concern is that of providing at a lower level the features that can only be accomplished at the highest level, thus only partially satisfying the communication demand. [Saltzer, Reed, and Clark [1984]](#_bookmark672) gave the end-to-end argument as follows:

> 关注的是，在较低级别提供只能在最高级别上完成的功能，因此只能部分满足沟通需求。[Saltzer，Reed 和 Clark \ [1984 ]](#_bookmark672)给出了端到端的论点，如下所示：

The function in question can completely and correctly be specified only with the knowledge and help of the application standing at the endpoints of the communication system. Therefore, providing that questioned function as a fea- ture of the communication system itself is not possible. [page 278]

> 只有在通信系统端点的应用程序的知识和帮助下，可以完全正确地指定所讨论的功能。因此，不可能将质疑的功能作为通信系统本身的效果。\ [第 278 页]

Their example of the pitfall was a network at MIT that used several gateways, each of which added a checksum from one gateway to the next. The programmers of the application assumed that the checksum guaranteed accuracy, incorrectly believing that the message was protected while stored in the memory of each gateway. One gateway developed a transient failure that swapped one pair of bytes per million bytes transferred. Over time, the source code of one operating system was repeat- edly passed through the gateway, thereby corrupting the code. The only solution was to correct infected source files by comparing them to paper listings and repair- ing code by hand! Had the checksums been calculated and checked by the appli- cation running on the end systems, safety would have been ensured.

> 他们的陷阱示例是 MIT 的一个网络，该网络使用了几个网关，每个网关都从一个网关添加了一个校验和下一个网关。应用程序的程序员假定校验和保证准确性，错误地认为该消息在存储在每个网关的内存中时受到保护。一个网关发生了瞬态故障，该故障交换了一对每百万个字节的传输。随着时间的流逝，一个操作系统的源代码被重复通过网关，从而破坏了代码。唯一的解决方案是通过将其与纸质清单进行比较和手工修复代码来纠正感染源文件！如果校验和通过在最终系统上运行的应用程序计算和检查，则将确保安全。

There is a useful role for intermediate checks at the link level, however, pro- vided that end-to-end checking is available. End-to-end checking may show that something is broken between two nodes, but it doesn’t point to where the problem is. Intermediate checks can discover the broken component.

> 在链接级别上的中间检查有一个有用的作用，但是，可以使用端到端检查。端到端检查可能表明两个节点之间有些破坏，但并不能指向问题所在。中间检查可以发现损坏的组件。

A second issue regards performance using intermediate checks. Although it is sufficient to retransmit the whole in case of failures from the end point, it can be much faster to retransmit a portion of the message at an intermediate point rather than wait for a time-out and a full message retransmit at the end point.

> 第二期涉及使用中级检查的性能。尽管在终点失败的情况下重新启动整体是足够的，但是在中间点重新传输一部分消息的速度可能会更快，而不是等待暂停，并且在终点上重新发送完整消息。

Pitfall `Relying on TCP/IP for all networks, regardless of latency, bandwidth, or software requirements`

> 陷阱*在所有网络上的 TCP/IP 上，无论延迟，带宽或软件要求*

The network designers on the first workstations decided it would be elegant to use a single protocol stack no matter where the destination of the message: Across a room or across an ocean, the TCP/IP overhead must be paid. This might have been a wise decision back then, especially given the unreliability of early Ethernet hard- ware, but it sets a high software overhead barrier for commercial systems of today. Such an obstacle lowers the enthusiasm for low-latency network interface hard- ware and low-latency interconnection networks if the software is just going to waste hundreds of microseconds when the message must travel only dozens of meters or less. It also can use significant processor resources. One rough rule of thumb is that each Mbit/sec of TCP/IP bandwidth needs about 1 MHz of processor speed, so a 1000-Mbit/sec link could saturate a processor with an 800- to 1000MHz clock.

> 第一批工作站上的网络设计人员认为，无论消息的目的地如何：跨过一个或跨过海洋，都必须支付 TCP/IP 开销。当时，这可能是一个明智的决定，特别是考虑到早期以太网硬件的不可靠性，但它为当今的商业系统设定了高的软件架空障碍。这样的障碍会降低对低延迟网络接口硬件和低延迟互连网络的热情，如果该软件只需浪费数百微秒即可到达消息，那么该消息必须仅传播数十米或更少。它还可以使用大量的处理器资源。一个粗略的经验法则是，TCP/IP 带宽的每个 MBIT/SEC 都需要约 1 MHz 的处理器速度，因此 1000 mbit/sec 的链接可以用 800-至 1000MHz 时钟饱和处理器。

The flip side is that, from a software perspective, TCP/IP is the most desirable target since it is the most connected and, hence, provides the largest number of opportunities. The downside of using software optimized to a particular LAN or SAN is that it is limited. For example, communication from a Java program depends on TCP/IP, so optimization for another protocol would require creation of glue software to interface Java to it.

> 另一方面，从软件的角度来看，TCP/IP 是最理想的目标，因为它是最连接的目标，因此提供了最多的机会。使用针对特定 LAN 或 SAN 进行优化的软件的缺点是它是有限的。例如，来自 Java 程序的通信取决于 TCP/IP，因此对另一个协议的优化将需要创建胶水软件以将 Java 连接到它。

TCP/IP advocates point out that the protocol itself is theoretically not as bur- densome as current implementations, but progress has been modest in commercial systems. There are also TCP/IP offloading engines in the market, with the hope of preserving the universal software model while reducing processor utilization and message latency. If processors continue to improve much faster than network speeds, or if multiple processors become ubiquitous, software TCP/IP may become less significant for processor utilization and message latency.

> TCP/IP 倡导者指出，该协议本身在理论上不像当前的实现那样盗用，但在商业系统中的进展是适度的。市场上也有 TCP/IP 卸载引擎，希望保留通用软件模型，同时降低处理器利用率和消息延迟。如果处理器的改进速度比网络速度更快，或者多个处理器无处不在，则软件 TCP/IP 对于处理器利用率和消息延迟而言可能会变得不那么重要。
