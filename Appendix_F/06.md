## Switch Microarchitecture

> ##开关微体系结构

Network switches implement the routing, arbitration, and switching functions of switched-media networks. Switches also implement buffer management mecha- nisms and, in the case of lossless networks, the associated flow control. For some networks, switches also implement part of the network management functions that explore, configure, and reconfigure the network topology in response to boot-up and failures. Here, we reveal the internal structure of network switches by describ- ing a basic switch microarchitecture and various alternatives suitable for different routing, arbitration, and switching techniques presented previously.

> 网络交换机实现了交换机网络的路由，仲裁和交换功能。交换机还实施缓冲管理机制，在无损网络的情况下，相关的流量控制。对于某些网络，交换机还实现了网络管理功能的一部分，该功能探索，配置和重新配置网络拓扑以响应启动和失败。在这里，我们通过描述基本的开关微体系结构和各种替代方案来揭示网络开关的内部结构，适合于先前介绍的不同路由，仲裁和开关技术。

### Basic Switch Microarchitecture

> ###基本开关微结构结构

The internal data path of a switch provides connectivity among the input and output ports. Although a shared bus or a multiported central memory could be used, these solutions are insufficient or too expensive, respectively, when the required aggre- gate switch bandwidth is high. Most high-performance switches implement an internal crossbar to provide nonblocking connectivity within the switch, thus allowing concurrent connections between multiple input-output port pairs. Buffer- ing of blocked packets can be done using first in, first out (FIFO) or circular queues, which can be implemented as _dynamically allocatable multi-queues_ (DAMQs) in static RAM to provide high capacity and flexibility. These queues can be placed at input ports (i.e., _input buffered switch_), output ports (i.e., _output buffered switch_), centrally within the switch (i.e., _centrally buffered switch_), or at both the input and output ports of the switch (i.e., _input-output-buffered switch_). [Figure F.21](#_bookmark618) shows a block diagram of an input-output-buffered switch.

> 开关的内部数据路径可提供输入和输出端口之间的连接。尽管可以使用共享的总线或多寄电的中央内存，但是当所需的聚集开关带宽较高时，这些解决方案分别不足或太昂贵。大多数高性能开关实现了一个内部横梁，以在交换机内提供非阻止连接，从而允许在多个输入输出端口对之间进行并发连接。可以使用首先，OUT（FIFO）或圆形队列进行阻止数据包的缓冲，可以在静态 RAM 中以_DYNAGNANDANE 上可分配的多标语_（DAMQ）实现，以提供高容量和灵活性。可以将这些队列放在输入端口（即_input 缓冲开关_），输出端口（即_Output 缓冲开关_），集中在交换机内（即_ Centrally Buffered Switch_），或在交换机的输入和输出端口（即，_input-Output-Buffered switch_）。[图 F.21]（#_ bookmark618）显示了输入输出缓冲开关的框图。

Routing can be implemented using a finite-state machine or forwarding table within the routing control unit of switches. In the former case, the routing infor- mation given in the packet header is processed by a finite-state machine that deter- mines the allowed switch output port (or ports if routing is adaptive), according to the routing algorithm. Portions of the routing information in the header are usually stripped off or modified by the routing control unit after use to simplify processing at the next switch along the path. When routing is implemented using forwarding tables, the routing information given in the packet header is used as an address to access a forwarding table entry that contains the allowed switch output port(s) pro- vided by the routing algorithm. Forwarding tables must be preloaded into the switches at the outset of network operation. Hybrid approaches also exist where the forwarding table is reduced to a small set of routing bits and combined with a small logic block. Those routing bits are used by the routing control unit to know what paths are allowed and decide the output ports the packets need to take. The goal with those approaches is to build flexible yet compact routing control units, eliminating the area and power wastage of a large forwarding table and thus being suitable for OCNs. The routing control unit is usually implemented as a centralized resource, although it could be replicated at every input port so as not to become a bottleneck. Routing is done only once for every packet, and packets typically are large enough to take several cycles to flow through the switch, so a centralized routing control unit rarely becomes a bottleneck. [Figure F.21](#_bookmark618) assumes a centralized routing control unit within the switch.

> 可以在开关的路由控制单元中使用有限状态机或转发表实现路由。在前一种情况下，根据路由算法，可以通过有限状态机器处理数据包标头中给出的路由信息，该机器会确定允许的开关输出端口（或端口（如果路由为自适应））。使用后，通常在路由控制单元中剥离或修改了标题中的部分路由信息，以简化沿着路径的下一个开关处的处理。当使用转发表实现路由时，数据包标头中给出的路由信息被用作访问包含路由算法所允许的开关输出端口的转发表条目的地址。在网络操作开始时，必须将转发表预加载到开关中。混合方法也存在，在将转发表缩小为一小部分路由位并与一个小逻辑块结合在一起的情况下也存在混合方法。路由控制单元使用这些路由位来知道允许哪些路径并确定数据包所需的输出端口。这些方法的目标是建立灵活但紧凑的路由控制单元，消除大型转发桌的区域和动力浪费，从而适合 OCN。路由控制单元通常作为集中资源实现，尽管可以在每个输入端口进行复制，以免成为瓶颈。每个数据包仅进行一次路由，并且数据包通常足够大，可以花几个周期流过开关，因此集中式路由控制单元很少成为瓶颈。[图 F.21]（#_ bookmark618）假设交换机内的集中式路由控制单元。

Figure F.21 Basic microarchitectural components of an input-output-buffered switch.

> 图 F.21 输入输出缓冲开关的基本微体系结构。

Arbitration is required when two or more packets concurrently request the same output port, as described in the previous section. Switch arbitration can be implemented in a centralized or distributed way. In the former case, all of the requests and status information are transmitted to the central switch arbitration unit; in the latter case, the arbiter is distributed across the switch, usually among the input and/or output ports. Arbitration may be performed multiple times on packets, and there may be multiple queues associated with each input port, increasing the number of arbitration requests that must be processed. Thus, many implementations use a hierarchical arbitration approach, where arbitration is first performed locally at every input port to select just one request among the corre- sponding packets and queues, and later arbitration is performed globally to process the requests made by each of the local input port arbiters. [Figure F.21](#_bookmark618) assumes a centralized arbitration unit within the switch.

> 当两个或多个数据包同时要求相同的输出端口时，需要仲裁。可以以集中式或分布式的方式实施开关仲裁。在前一种情况下，所有请求和状态信息都将传输到中央开关仲裁单位；在后一种情况下，仲裁商通常在交换机上分布，通常在输入和/或输出端口中。仲裁可以在数据包上多次执行，并且可能会有与每个输入端口关联的多个队列，从而增加了必须处理的仲裁请求数量。因此，许多实施方法都使用层次仲裁方法，在每个输入端口首先在本地执行仲裁，以在 corre-sponding 数据包和队列中仅选择一个请求，然后在全球范围内执行仲裁，以处理每个本地每个本地提出的请求输入端口仲裁者。[图 F.21]（#_ bookmark618）假设交换机内的集中仲裁单位。

The basic switch microarchitecture depicted in [Figure F.21](#_bookmark618) functions in the fol- lowing way. When a packet starts to arrive at a switch input port, the link controller decodes the incoming signal and generates a sequence of bits, possibly deserializ- ing data to adapt them to the width of the internal data path if different from the external link width. Information is also extracted from the packet header or link control signals to determine the queue to which the packet should be buffered. As the packet is being received and buffered (or after the entire packet has been buffered, depending on the switching technique), the header is sent to the routing unit. This unit supplies a request for one or more output ports to the arbitration unit. Arbitration for the requested output port succeeds if the port is free and has enough space to buffer the entire packet or flit, depending on the switching technique. If wormhole switching with virtual channels is implemented, additional arbitration and allocation steps may be required for the transmission of each individual flit. Once the resources are allocated, the packet is transferred across the internal cross- bar to the corresponding output buffer and link if no other packets are ahead of it and the link is free. Link-level flow control implemented by the link controller pre- vents input queue overflow at the neighboring switch on the other end of the link. If virtual channel switching is implemented, several packets may be time- multiplexed across the link on a flit-by-flit basis. As the various input and output ports operate independently, several incoming packets may be processed concur- rently in the absence of contention.

> [图 F.21]（#_ bookmark618）中描述的基本开关微体系结构以范围的方式函数。当数据包开始到达开关输入端口时，链接控制器解码传入信号并生成一系列位，如果与外部链路宽度不同，则可能对数据进行调整以使其适应内部数据路径的宽度。还从数据包标头或链接控制信号中提取信息，以确定应向数据包进行缓冲的队列。当收到数据包并进行缓冲时（或整个数据包被缓冲，具体取决于开关技术），将标题发送到路由单元。该单元向仲裁单位提供一个或多个输出端口的请求。如果端口是免费的，并且有足够的空间来缓冲整个数据包或 FLIT，则请求的输出端口的仲裁会成功。如果实现了使用虚拟通道的蠕虫孔切换，则可能需要进行其他仲裁和分配步骤以传输每个单独的 FLIT。一旦分配了资源，如果没有其他数据包在其前面，链接是免费的，则将数据包传输到相应的输出缓冲区和链接。链接控制器预测输入队列在链接另一端的相邻开关处实现的链接级流量控制。如果实现了虚拟通道切换，则可以在逐牌的基础上进行几个数据包在整个链接上多样地。由于各种输入和输出端口独立运行，因此在没有争议的情况下，可以同意几个传入数据包。

### Buffer Organizations

> ###缓冲组织

As mentioned above, queues can be located at the switch input, output, or both sides. Output-buffered switches have the advantage of completely eliminating _head-of-line blocking_. Head-of-line (HOL) blocking occurs when two or more packets are buffered in a queue, and a blocked packet at the head of the queue blocks other packets in the queue that would otherwise be able to advance if they were at the queue head. This cannot occur in output-buffered switches as all the packets in a given queue have the same status; they require the same output port. However, it may be the case that all the switch input ports simultaneously receive a packet for the same output port. As there are no buffers at the input side, output buffers must be able to store all those incoming packets at the same time. This requires implementing output queues with an internal switch _speedup_ of _k_. That is, output queues must have a write bandwidth _k_ times the link bandwidth, where _k_ is the number of switch ports. This oftentimes is too expensive. Hence, this solu- tion by itself has rarely been implemented in lossless networks. As the probability of concurrently receiving many packets for the same output port is usually small, commercial systems that use output-buffered switches typically implement only moderate switch speedup, dropping packets on rare buffer overflow.

> 如上所述，队列可以位于开关输入，输出或两侧。输出缓冲开关的优势是完全消除了线路阻止_。当两个或更多数据包在队列中缓冲时，会发生线路（HOL）阻止，而在队列的头部被阻止的数据包在队列中的其他数据包中被阻塞，如果它们在队列处，否则它们将能够前进头。这不能在输出缓冲开关中发生，因为给定队列中的所有数据包都具有相同的状态。他们需要相同的输出端口。但是，可能是所有交换输入端口同时接收同一输出端口的数据包。由于输入侧没有缓冲区，因此输出缓冲区必须能够同时存储所有这些传入数据包。这需要使用_K_的内部开关_speedup_实现输出队列。也就是说，输出队列必须具有写入带宽_k_乘以链接带宽，其中_k_是开关端口的数量。这种情况通常太贵了。因此，这种解决方案本身很少在无损网络中实现。由于同时接收相同输出端口的许多数据包的概率通常是很小的商业系统，因此使用输出缓冲开关通常仅实现中等开关加速，在稀有缓冲区溢出上删除数据包。

Switches with buffers on the input side are able to receive packets without hav- ing any switch speedup; however, HOL blocking can occur within input port queues, as illustrated in [Figure F.22(a)](#_bookmark619). This can reduce switch output port utiliza- tion to less than 60% even when packet destinations are uniformly distributed. As shown in [Figure F.22(b)](#_bookmark619), the use of virtual channels (two in this case) can mitigate HOL blocking but does not eliminate it. A more effective solution is to organize the input queues as _virtual output queues_ (VOQs), shown in [Figure F.22(c)](#_bookmark619). With this, each input port implements as many queues as there are output ports, thus provid- ing separate buffers for packets destined to different output ports. This is a popular technique widely used in ATM switches and IP routers. The main drawbacks of

> 输入侧带有缓冲区的开关能够接收数据包，而无需任何开关加速；但是，如[图 F.22（a）]（#_ bookmark619）中所示，可能会在输入端口队列中发生 Hol 阻塞。这可以将开关输出端口利用率降低到小于 60％，即使数据包目的地均匀分布。如[图 F.22（b）]（#_ bookmark619）所示，虚拟通道的使用（在这种情况下为两个）可以减轻 HOL 阻塞，但不会消除它。一个更有效的解决方案是将输入队列组织为_virtual 输出队列_（VOQS），如[图 F.22（c）]（#_ bookmark619）如图所示。这样，每个输入端口都会在有输出端口的情况下实现尽可能多的队列，从而为注定到不同输出端口的数据包提供单独的缓冲区。这是 ATM 开关和 IP 路由器中广泛使用的一种流行技术。主要缺点

![](./media/image657.png)
![](./media/image658.png)
![](./media/image659.png)
![](./media/image659.png)
![](./media/image659.png)
![](./media/image660.png)
![](./media/image660.png)
![](./media/image661.png)
![](./media/image661.png)
![](./media/image661.png)

Figure F.22 (a) Head-of-line blocking in an input buffer, (b) the use of two virtual channels to reduce HOL block- ing, and (c) the use of virtual output queuing to eliminate HOL blocking within a switch. The shaded input buffer is the one to which the crossbar is currently allocated. This assumes each input port has only one access port to the switch’s internal crossbar.

> 图 F.22（a）输入缓冲区中的线路阻塞，（b）使用两个虚拟通道来减少 HOL 阻塞，以及（c）使用虚拟输出排队以消除在一个内部的 hol 封锁转变。阴影输入缓冲区是当前分配的横杆的一种。假设每个输入端口只有一个访问交换机内部横梁的访问端口。

VOQs, however, are cost and lack of scalability: The number of VOQs grows qua- dratically with switch ports. Moreover, although VOQs eliminate HOL blocking within a switch, HOL blocking occurring at the network level end-to-end is not solved. Of course, it is possible to design a switch with VOQ support at the network level also—that is, to implement as many queues per switch input port as there are output ports across the entire network—but this is extremely expensive. An alter- native is to dynamically assign only a fraction of the queues to store (cache) sep- arately only those packets headed for congested destinations.

> 但是，VOQ 是成本且缺乏可扩展性的：VOQ 的数量随交换机端口而急剧增长。此外，尽管 VOQ 消除了开关中的 HOL 阻塞，但在网络级别端到端的 HOL 阻塞尚未解决。当然，还可以在网络级别上设计带有 VOQ 支持的开关，也就是说，随着整个网络中的输出端口，每个交换机输入端口都会实现尽可能多的队列 - 但这非常昂贵。一个改变的天然是动态分配一小部分排队存储（缓存），仅将这些数据包用于拥挤目的地。

Combined input-output-buffered switches minimize HOL blocking when there is sufficient buffer space at the output side to buffer packets, and they minimize the switch speedup required due to buffers being at the input side. This solution has the further benefit of decoupling packet transmission through the internal crossbar of the switch from transmission through the external links. This is especially useful for cut-through switching implementations that use virtual channels, where flit transmissions are time-multiplexed over the links. Many designs used in commer- cial systems implement input-output-buffered switches.

> 当输出侧有足够的缓冲空间到缓冲数据包时，组合输入输出缓冲开关可最大程度地减少 HOL 阻塞，并且由于缓冲区处于输入侧，因此它们最小化了所需的开关加速度。该解决方案具有通过开关的内部横杆从传输从外部链路传输的进一步好处。这对于使用虚拟通道的切换切换实现特别有用，在该实现中，将 Flit Transmissions 在链接上进行时间进行时间进行操作。商业系统中使用的许多设计实现了输入输出缓冲开关。

### Routing Algorithm Implementation

> ###路由算法实现

It is important to distinguish between the routing algorithm and its implementation. While the routing algorithm describes the rules to forward packets across the net- work and affects packet latency and network throughput, its implementation affects the delay suffered by packets when reaching a node, the required silicon area, and the power consumption associated with the routing computation. Several techniques have been proposed to pre-compute the routing algorithm and/or hide the routing computation delay. However, significantly less effort has been devoted to reduce silicon area and power consumption without significantly affecting routing flexibil- ity. Both issues have become very important, particularly for OCNs. Many existing designs address these issues by implementing relatively simple routing algorithms, but more sophisticated routing algorithms will likely be needed in the future to deal with increasing manufacturing defects, process variability, and other complications arising from continued technology scaling, as discussed briefly below.

> 区分路由算法及其实现非常重要。路由算法描述了在整个网络上转发数据包并影响数据包延迟和网络吞吐量的规则，但其实现会影响数据包到达节点时数据包所遭受的延迟，所需的硅区域以及与路由计算相关的功耗。已经提出了几种技术来预先计算路由算法和/或隐藏路由计算延迟。然而，努力大大减少了减少硅面积和功耗，而不会显着影响路由屈曲。这两个问题都变得非常重要，尤其是对于 OCN。许多现有的设计通过实施相对简单的路由算法来解决这些问题，但是将来可能需要更复杂的路由算法来处理不断增加的制造缺陷，过程可变性以及持续技术扩展引起的其他并发症，如下所述。

As mentioned in a previous section, depending on where the routing algorithm is computed, two basic forms of routing exist: source and distributed routing. In source routing, the complexity of implementation is moved to the end nodes where paths need to be stored in tables, and the path for a given packet is selected based on the destination end node identifier. In distributed routing, however, the complexity is moved to the switches where, at each hop along the path of a packet, a selection of the output port to take is performed. In distributed routing, two basic implemen- tations exist. The first one consists of using a logic block that implements a fixed routing algorithm for a particular topology. The most common example of such an implementation is dimension-order routing, where dimensions are offset in an established order. Alternatively, distributed routing can be implemented with for- warding tables, where each entry encodes the output port to be used for a particular destination. Therefore, in the worst case, as many entries as destination nodes are required.

> 如前一节所述，取决于计算路由算法的位置，存在两种基本的路由形式：源和分布式路由。在源路由中，实现的复杂性被移至需要存储在表中的路径的末端节点，并根据目标末端节点标识符选择给定数据包的路径。但是，在分布式路由中，复杂性移至开关，在该开关中，在每个跳动的路径上，每个跳跃的路径都可以选择要采用的输出端口。在分布式路由中，存在两个基本的实现。第一个是使用逻辑块，该逻辑块用于特定拓扑的固定路由算法。这种实现的最常见示例是维订单路由，其中尺寸以既定顺序的抵消。另外，可以使用填写表来实现分布式路由，其中每个条目编码要用于特定目的地的输出端口。因此，在最坏的情况下，需要许多目的地节点的条目。

Both methods for implementing distributed routing have their benefits and drawbacks. Logic-based routing features a very short computation delay, usually requires a small silicon area, and has low power consumption. However, logic- based routing needs to be designed with a specific topology in mind and, therefore, is restricted to that topology. Table-based distributed routing is quite flexible and supports any topology and routing algorithm. Simply, tables need to be filled with the proper contents based on the applied routing algorithm (e.g., the up\*/down\* routing algorithm can be defined for any irregular topology). However, the down side of table-based distributed routing is its non-negligible area and power cost. Also, scalability is problematic in table-based solutions as, in the worst case, a sys- tem with N end nodes (and switches) requires as many as N tables each with N entries, thus having quadratic cost.

> 实施分布式路由的两种方法都有其好处和缺点。基于逻辑的路由具有非常短的计算延迟，通常需要一个小的硅区域，并且功耗低。但是，基于逻辑的路由需要考虑特定的拓扑，因此仅限于该拓扑。基于表的分布式路由非常灵活，并支持任何拓扑和路由算法。简而言之，需要根据应用的路由算法填充适当的内容（例如，可以针对任何不规则拓扑来定义向上\*/down \*路由算法）。但是，基于桌子的分布式路由的下降是其不可忽略的区域和电源成本。同样，在基于表的解决方案中，可伸缩性是有问题的，因为在最坏的情况下，具有 n 端节点（和开关）的系统需要每个 n 台上的 n 个桌子，因此具有二次成本。

Depending on the network domain, one solution is more suitable than the other. For instance, in SANs, it is usual to find table-based solutions as is the case with InfiniBand. In other environments, like OCNs, table-based implementations are avoided due to the aforementioned costs in power and silicon area. In such envi- ronments, it is more advisable to rely on logic-based implementations. Herein lies some of the challenges OCN designers face: ever continuing technology scaling through device miniaturization leads to increases in the number of manufacturing defects, higher failure rates (either transient or permanent), significant process var- iations (transistors behaving differently from design specs), the need for different clock frequency and voltage domains, and tight power and energy budgets. All of these challenges translate to the network needing support for heterogeneity. Dif- ferent—possibly irregular—regions of the network will be created owing to failed components, powered down switches and links, disabled components (due to unacceptable variations in performance) and so on. Hence, heterogeneous systems may emerge from a homogeneous design. In this framework, it is important to effi- ciently implement routing algorithms designed to provide enough flexibility to address these new challenges.

> 根据网络域，一个解决方案比另一个解决方案更合适。例如，在 SANS 中，通常可以像 Infiniband 一样找到基于表的解决方案。在其他环境（例如 OCN）中，由于上述电力和硅区域的成本，因此可以避免基于桌子的实现。在这样的环境中，建议依靠基于逻辑的实现。OCN 设计师面临的一些挑战所在：通过设备微型化的持续技术扩展会导致制造缺陷的数量增加，更高的故障率（瞬态或永久性），重要的过程量（晶体管与设计规格的行为不同），需要不同的时钟频率和电压域以及紧密的功率和能量预算。所有这些挑战都转化为需要支持异质性的网络。由于组件失败，开关和链接电源，禁用组件（由于性能的不可接受的变化）等，将创建网络区域的区别。因此，异质系统可能会从同质设计中出现。在此框架中，重要的是有效地实施旨在提供足够灵活性来应对这些新挑战的路由算法。

A well-known solution for providing a certain degree of flexibility while being much more compact than traditional table-based approaches is interval routing \[[Leeuwen 1987](#_bookmark682)\], where a range of destinations is defined for each output port. Although this approach is not flexible enough, it provides a clue on how to address emerging challenges. A more recent approach provides a plausible implementation design point that lies between logic-based implementation (efficiency) and table- based implementation (flexibility). Logic-Based Distributed Routing (LBDR) is a hybrid approach that takes as a reference a regular 2D mesh but allows an irregular network to be derived from it due to changes in topology induced by manufactur- ing defects, failures, and other anomalies. Due to the faulty, disabled, and powered- down components, regularity is compromised and the dimension-order routing algorithm can no longer be used. To support such topologies, LBDR defines a set of configuration bits at each switch. Four connectivity bits are used at each switch to indicate the connectivity of the switch to the neighbor switches in the topology. Thus, one connectivity bit per port is used. Those connectivity bits are used, for instance, to disable an output port leading to a faulty component. Addi- tionally, eight routing bits are used, two per output port, to define the available routing options. The value of the routing bits is set at power-on and is computed from the routing algorithm to be implemented in the network. Basically, when a routing bit is set, it indicates that a packet can leave the switch through the asso- ciated output port and is allowed to perform a certain turn at the next switch. In this respect, LBDR is similar to interval routing, but it defines geographical areas instead of ranges of destinations. [Figure F.23](#_bookmark620) shows an example where a topology-agnostic routing algorithm is implemented with LBDR on an irregular topology. The figure shows the computed configuration bits.

> 提供一定程度的灵活性，同时比传统的基于桌子的方法更紧凑的一个众所周知的解决方案是间隔路由\ [[[Leeuwen 1987]（#_ bookmark682）\]，其中为每个输出端口定义了一系列目的地。尽管这种方法不够灵活，但它提供了有关如何应对新兴挑战的线索。一种最新的方法提供了一个合理的实现设计点，该设计位于基于逻辑的实现（效率）和基于表格的实现（灵活性）之间。基于逻辑的分布式路由（LBDR）是一种混合方法，它作为常规 2D 网格的参考，但由于制造缺陷，故障和其他异常引起的拓扑变化，可以从其衍生出不规则的网络。由于有缺陷，禁用和电动的组件，规律性受到损害，并且不再使用尺寸订购路由算法。为了支持此类拓扑，LBDR 在每个开关处定义了一组配置位。每个开关都使用四个连接位，以指示开关与拓扑中邻居开关的连接性。因此，使用每个端口一个连接位。例如，使用这些连接位来禁用导致故障组件的输出端口。另外，使用八个路由位，每个输出端口两个，以定义可用的路由选项。路由位的值设置为电源，并根据要在网络中实现的路由算法进行计算。基本上，当设置路由位时，它表明数据包可以使开关通过协会的输出端口，并允许在下一个开关处执行一定的转弯。在这方面，LBDR 类似于间隔路由，但它定义了地理区域而不是目的地范围。[图 F.23]（#_ bookmark620）显示了一个示例，其中用 LBDR 在不规则的拓扑结构上实现了拓扑 - 不合时宜的路由算法。该图显示了计算的配置位。

The connectivity and routing bits are used to implement the routing algorithm. For that purpose, a small set of logic gates are used in combination with the con- figuration bits. Basically, the LBDR approach takes as a reference the initial topol- ogy (a 2D mesh), and makes a decision based on the current coordinates of the router, the coordinates of the destination router, and the configuration bits. [Figure F.24](#_bookmark621) shows the required logic, and [Figure F.25](#_bookmark622) shows an example of where a packet is forwarded from its source to its destination with the use of the config- uration bits. As can be noticed, routing restrictions are enforced by preventing the use of the west port at switch 10.

> 连接和路由位用于实现路由算法。为此，将一小部分逻辑门与构造位结合使用。基本上，LBDR 方法作为参考作为初始 topol-ogy（一个 2D 网格），并根据路由器的当前坐标，目标路由器的坐标和配置位做出决定。[图 F.24]（#_ bookmark621）显示了所需的逻辑，[图 F.25]（#_ bookmark622）显示了一个示例，说明了将数据包从其源到目的地转发到目的地的示例。可以注意到，通过防止在开关 10 处使用西端口，可以实现路由限制。

LBDR represents a method for efficient routing implementation in OCNs. This mechanism has been recently extended to support non-minimal paths, collective communication operations, and traffic isolation. All of these improve- ments have been made while maintaining a compact and efficient implementation with the use of a small set of configuration bits. A detailed description of LBDR and its extensions, and the current research on OCNs can be found in [Flich \[2010\]](#_bookmark655).

> LBDR 代表了在 OCN 中有效路由实现的方法。最近已扩展了该机制，以支持非最小程度的路径，集体通信操作和交通隔离。所有这些改进都是在使用少量配置位的情况下保持紧凑而有效的实现的同时进行的。可以在[flich \ [2010 \]]（#_ bookmark655）中找到对 LBDR 及其扩展的详细描述以及当前对 OCN 的研究。

Figure F.23 Shown is an example of an irregular network that uses LBDR to implement the routing algorithm.

> 图 F.23 是使用 LBDR 实现路由算法的不规则网络的示例。

![](./media/image662.png)
<img src="./media/image669.png" style="width:1.22966in;height:1.71354in" />
![](./media/image670.png)

Figure F.24 LBDR logic at each input port of the router.

> 图 F.24 在路由器的每个输入端口处的 LBDR 逻辑。

![](./media/image671.png)

Figure F.25 Example of routing a message from Router 14 to Router 5 using LBDR at each router.

> 图 F.25 在每个路由器处使用 LBDR 将消息从路由器 14 路由路由器 5 路由 5 路由。

### Pipelining the Switch Microarchitecture

> ###管道开关微体系结构

Performance can be enhanced by pipelining the switch microarchitecture. Pipe- lined processing of packets in a switch has similarities with pipelined execution of instructions in a vector processor. In a vector pipeline, a single instruction indi- cates what operation to apply to all the vector elements executed in a pipelined way. Similarly, in a switch pipeline, a single packet header indicates how to pro- cess all of the internal data path physical transfer units (or _phits_) of a packet, which are processed in a pipelined fashion. Also, as packets at different input ports are independent of each other, they can be processed in parallel similar to the way mul- tiple independent instructions or threads of pipelined instructions can be executed in parallel.

> 可以通过管道开关微体系结构来增强性能。开关中数据包的管道处理与矢量处理器中的指令执行相似。在矢量管道中，单个指令指示要以管道方式执行的所有矢量元素应用哪些操作。同样，在开关管道中，单个数据包标头指示如何以包装方式处理数据包的所有内部数据路径物理传输单位（或_PHITS_）。同样，由于不同输入端口的数据包彼此独立，因此可以并行处理它们，类似于可以并行执行管道指令的构图独立指令或线程。

The switch microarchitecture can be pipelined by analyzing the basic functions performed within the switch and organizing them into several stages. [Figure F.26](#_bookmark623) shows a block diagram of a five-stage pipelined organization for the basic switch microarchitecture given in [Figure F.21](#_bookmark618), assuming cut-through switching and the use of a forwarding table to implement routing. After receiving the header portion of the packet in the first stage, the routing information (i.e., destination address) is used in the second stage to look up the allowed routing option(s) in the forwarding table. Concurrent with this, other portions of the packet are received and buffered in the input port queue at the first stage. Arbitration is performed in the third stage. The crossbar is configured to allocate the granted output port for the packet in the fourth stage, and the packet header is buffered in the switch output port and ready for transmission over the external link in the fifth stage. Note that the second and third stages are used only by the packet header; the payload and trailer portions of the packet use only three of the stages—those used for data flow-thru once the internal data path of the switch is set up.

> 可以通过分析交换机内执行的基本功能并将其整理成几个阶段来管道上的开关微构造结构。[图 F.26]（#_ bookmark623）显示了[图 F.21]中给出的基本开关微体系结构的五阶段管道组织的框图表实现路由。在第一阶段接收数据包的标题部分后，在第二阶段使用路由信息（即目标地址），以查找转发表中的允许路由选项。同时，第一阶段的输入端口队列中收到数据包的其他部分。仲裁是在第三阶段进行的。横梁配置为在第四阶段分配给数据包的授予的输出端口，并且数据包标头在开关输出端口中缓冲，并准备在第五阶段通过外部链接传输。请注意，第二阶段和第三阶段仅由数据包标头使用；数据包的有效负载和拖车部分仅使用三个阶段 - 一旦设置了开关的内部数据路径，用于数据流的阶段。

Figure F.26 Pipelined version of the basic input-output-buffered switch. The notation in the figure is as follows: IB is the input link control and buffer stage, RC is the route computation stage, SA is the crossbar switch arbitration stage, ST is the crossbar switch traversal stage, and OB is the output buffer and link control stage. Packet fragments (flits) coming after the header remain in the IB stage until the header is processed and the crossbar switch resources are provided.

> 图 F.26 基本输入输出缓冲开关的管道版本。图中的符号如下：IB 是输入链路控制和缓冲区阶段，RC 是路线计算阶段，SA 是横梁开关仲裁阶段，ST 是横梁开关遍历阶段，OB 是输出缓冲区和链接控制阶段。在标头保持在 IB 阶段之后的数据包片段（FLITS），直到处理标头并提供横杆开关资源为止。

A virtual channel switch usually requires an additional stage for virtual channel allocation. Moreover, arbitration is required for every flit before transmission through the crossbar. Finally, depending on the complexity of the routing and arbi- tration algorithms, several clock cycles may be required for these operations.

> 虚拟通道开关通常需要用于虚拟通道分配的额外阶段。此外，在通过横杆传输之前，每次 FLIT 都需要仲裁。最后，根据路由和弧算法的复杂性，这些操作可能需要几个时钟周期。

### Other Switch Microarchitecture Enhancements

> ###其他开关微结构的增强功能

As mentioned earlier, internal switch speedup is sometimes implemented to increase switch output port utilization. This speedup is usually implemented by increasing the clock frequency and/or the internal data path width (i.e., phit size) of the switch. An alternative solution consists of implementing several parallel data paths from each input port’s set of queues to the output ports. One way of doing this is by increasing the number of crossbar input ports. When implementing several physical queues per input port, this can be achieved by devoting a separate crossbar port to each input queue. For example, the IBM Blue Gene/L implements two crossbar access ports and two read ports per switch input port.

> 如前所述，有时会实现内部开关速度以增加开关输出端口利用率。通常通过增加开关的时钟频率和/或内部数据路径宽度（即 phit 大小）来实现此加速。另一种解决方案包括从每个输入端口的一组队列到输出端口实现多个并行数据路径。这样做的一种方法是增加横梁输入端口的数量。在每个输入端口实现几个物理队列时，可以通过将单独的横杆端口用于每个输入队列来实现。例如，IBM 蓝色基因/L 实现了两个横梁访问端口和每个交换机输入端口的两个读取端口。

Another way of implementing parallel data paths between input and output ports is to move the buffers to the crossbar crosspoints. This switch architecture is usually referred to as a _buffered crossbar switch_. A buffered crossbar provides independent data paths from each input port to the different output ports, thus mak- ing it possible to send up to _k_ packets at a time from a given input port to _k_ different output ports. By implementing independent crosspoint memories for each input- output port pair, HOL blocking is eliminated at the switch level. Moreover, arbi- tration is significantly simpler than in other switch architectures. Effectively, each output port can receive packets from only a disjoint subset of the crosspoint mem- ories. Thus, a completely independent arbiter can be implemented at each switch output port, each of those arbiters being very simple.

> 在输入和输出端口之间实现并行数据路径的另一种方法是将缓冲区移至横杆跨点。此开关体系结构通常称为_掩盖的横梁开关_。缓冲横梁提供从每个输入端口到不同输出端口的独立数据路径，从而使其一次从给定的输入端口到_K_不同的输出端口，一次将最多发送_K_数据包发送。通过为每个输入输出端口对实现独立的跨点记忆，在开关级别消除了 HOL 阻塞。此外，与其他交换机体系结构相比，弧形明显简单。有效地，每个输出端口只能从交叉点内存的不相交子集接收数据包。因此，可以在每个交换机输出端口上实现一个完全独立的仲裁者，每个仲裁者都非常简单。

A buffered crossbar would be the ideal switch architecture if it were not so expensive. The number of crosspoint memories increases quadratically with the number of switch ports, dramatically increasing its cost and reducing its scalability with respect to the basic switch architecture. In addition, each crosspoint memory must be large enough to efficiently implement link-level flow control. To reduce cost, most designers prefer input-buffered or combined input-output-buffered switches enhanced with some of the mechanisms described previously.

> 如果它不是那么昂贵，则缓冲横带将是理想的开关体系结构。交叉点记忆的数量随着开关端口的数量而增加，大大增加了其成本，并降低了相对于基本开关体系结构的可扩展性。此外，每个跨点存储器必须足够大以有效地实现链接级流量控制。为了降低成本，大多数设计师更喜欢通过前面描述的一些机制来增强输入缓冲或组合输入输出缓冲开关。
