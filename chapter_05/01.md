## Introduction

As the quotations that open this chapter show, the view that advances in uniprocessor architecture were nearing an end has been held by some researchers for many years. Clearly, these views were premature; in fact, during the period of 1986–2003, uniprocessor performance growth, driven by the microprocessor, was at its highest rate since the first transistorized computers in the late 1950s and early 1960s.

> 正如本章打开的引文所表明的那样，一些研究人员已经持续了多年，人们对单层架构的进步已经接近结束。显然，这些观点还为时过早。实际上，在 1986 - 2003 年期间，自 1950 年代末和 1960 年代初的第一台晶体管计算机以来，由微处理器驱动的 UniProcessor 性能增长是最高的。

Nonetheless, the importance of multiprocessors was growing throughout the 1990s as designers sought a way to build servers and supercomputers that achieved higher performance than a single microprocessor, while exploiting the tremendous cost-performance advantages of commodity microprocessors. As we discussed in Chapters [1](#_bookmark2) and [3](#_bookmark93), the slowdown in uniprocessor performance arising from diminishing returns in exploiting instruction-level parallelism (ILP) combined with growing concern over power has led to a new era in computer architec- ture—an era where multiprocessors play a major role from the low end to the high end. The second quotation captures this clear inflection point.

> 尽管如此，在整个 1990 年代，多处理器的重要性在整个 1990 年代都在增长，因为设计师寻求一种方法来构建与单个微处理器相比，具有更高性能的服务器和超级计算机，同时利用了商品微处理器的巨大成本效果优势。正如我们在章节 [1](#_bookmark2) 和 [3](#_bookmark93) 中讨论的那样，单层处理器性能的放缓是由于利用指令级别的平行性(ILP)的回报递减而引起的，与对权力的越来越关注导致了新的关注计算机架构的时代 - 多处理器从低端到高端发挥重要作用的时代。第二引号捕获了这个明确的拐点。

This increased importance of multiprocessing reflects several major factors:

- The dramatically lower efficiencies in silicon and energy use that were encoun- tered between 2000 and 2005 as designers attempted to find and exploit more ILP, which turned out to be inefficient, since power and silicon costs grew faster than performance. Other than ILP, the only scalable and general-purpose way we know to increase performance faster than the basic technology allows (from a switching perspective) is through multiprocessing.
- A growing interest in high-end servers as cloud computing and software-as-a- service become more important.
- A growth in data-intensive applications driven by the availability of massive amounts of data on the Internet.
- The insight that increasing performance on the desktop is less important (outside of graphics, at least), either because current performance is acceptable or because highly compute- and data-intensive applications are being done on the cloud.
- An improved understanding of how to use multiprocessors effectively, especially in server environments where there is significant inherent parallel- ism, arising from large datasets (usually in the form of data parallelism), "natural-world" parallelism (which occurs in scientific and engineering codes), or parallelism among large numbers of independent requests (request-level parallelism).
- The advantages of leveraging a design investment by replication rather than unique design; all multiprocessor designs provide such leverage.

> 多处理的这种越来越重要的重要性反映了几个主要因素：
>
> - 由于设计师试图找到和利用更多的 ILP，因此在 2000 年至 2005 年之间遇到的硅和能源利用效率较大，事实证明其效率低下，因为 Power 和 Silicon 的成本比性能增长快。除 ILP 外，我们知道的唯一可扩展和通用的方式比基本技术更快地提高性能(从开关角度)是通过多处理。
> - 随着云计算和软件的服务变得越来越重要，人们对高端服务器的兴趣越来越大。
> - 由 Internet 上大量数据的可用性驱动的数据密集型应用程序的增长。
> - 桌面上的性能提高不那么重要(至少在图形之外)的见解要么是因为当前的性能是可以接受的，要么是因为在云上正在完成高度计算和数据密集型应用程序。
> - 对如何有效地使用多处理器的了解，尤其是在服务器环境中存在着重要固有的平行 - ISM，这是由大数据集引起的(通常是以数据并行性的形式)，"自然世界" 并行性(在科学和科学和工程代码)或大量独立请求(请求级并行性)之间的并行性。
> - 通过复制而不是独特的设计利用设计投资的优势；所有多处理器设计都提供了这样的杠杆作用。

The third quotation reminds us that multicore may provide only limited possi- bilities for scaling performance. The combination of Amdahl’s Law effects and the end of Dennard scaling mean that the future of multicore may be limited, at least as a method of scaling up the performance of single applications. We return to this topic late in the chapter.

> 第三篇引号提醒我们，多核心可能仅提供有限的扩展性能可能性。Amdahl 的法律效应和 Dennard 缩放结束的结合意味着**多核心的未来可能受到限制**，至少是扩大单个应用程序性能的方法。我们在本章后期返回此主题。

In this chapter, we focus on exploiting thread-level parallelism (TLP). TLP implies the existence of multiple program counters and thus is exploited primarily through MIMDs. Although MIMDs have been around for decades, the movement of thread-level parallelism to the forefront across the range of computing from embed- ded applications to high-end severs is relatively recent. Likewise, the extensive use of thread-level parallelism for a wide-range of general-purpose applications, versus either transaction processing or scientific applications, is relatively new.

> 在本章中，我们专注于利用**线程级并行性(TLP)**。TLP 意味着存在多个程序计数器，因此主要是通过 MIMDS 利用的。尽管 MIMD 已经存在数十年了，但线程级并行性转移到从嵌入式应用到高端级别的计算范围的最前沿的运动是相对较新的。同样，在广泛的通用应用中，与交易处理或科学应用相对较新，将线程级并行的广泛使用是相对较新的。

Our focus in this chapter is on _multiprocessors_, which we define as computers consisting of tightly coupled processors whose coordination and usage are typi- cally controlled by a single operating system and that share memory through a shared address space. Such systems exploit thread-level parallelism through two different software models. The first is the execution of a tightly coupled set of threads collaborating on a single task, which is typically called _parallel processing_. The second is the execution of multiple, relatively independent processes that may originate from one or more users, which is a form of _request-level parallelism_, although at a much smaller scale than what we explore in the next chapter. Request-level parallelism may be exploited by a single application running on multiple processors, such as a database responding to queries, or multiple applica- tions running independently, often called _multiprogramming_.

> 我们在本章中的重点是 *multiprocessors*，我们将其定义为由紧密耦合的处理器组成的计算机，其协调和用法通常由单个操作系统控制，并通过共享地址空间共享内存。这样的系统通过两个不同的软件模型利用线程级并行性。首先是执行在单个任务上协作的紧密耦合的线程集，该线程通常称为 *Paralleal Processing*。第二个是执行多个，相对独立的过程，这些过程可能源自一个或多个用户，这是 *request 级并行主义的一种形式，尽管比我们在下一章中探索的规模要小得多。请求级并行性可以通过在多个处理器上运行的单个应用程序(例如对查询响应的数据库或独立运行的多个应用程序)来利用，通常称为\_multiprogramming*。

The multiprocessors we examine in this chapter typically range in size from a dual processor to dozens and sometimes hundreds of processors and communicate and coordinate through the sharing of memory. Although sharing through memory implies a shared address space, it does not necessarily mean there is a single physical memory. Such multiprocessors include both single-chip systems with multiple cores, known as _multicore_, and computers consisting of multiple chips, each of which is typically a multicore. Many companies make such multiproces- sors, including HP, Dell, Cisco, IBM, SGI, Lenovo, Oracle, Fujitsu, and many others.

> 我们在本章中检查的多处理器通常范围从双处理器到数十个处理器，有时甚至数百个处理器，并通过共享内存进行通信和协调。尽管通过内存共享意味着共享地址空间，但这并不一定意味着有一个物理内存。这样的多处理器包括具有多个内核的单芯片系统，称为 *multicore*，以及由多个芯片组成的计算机，每个芯片通常都是多核心。许多公司都会制作此类多种多样，包括 HP，Dell，Cisco，IBM，SGI，Lenovo，Oracle，Fujitsu 等。

In addition to true multiprocessors, we will return to the topic of multithread- ing, a technique that supports multiple threads executing in an interleaved fashion on a single multiple-issue processor. Many multicore processors also include support for multithreading.

> 除了真正的多处理器外，我们还将返回多线程的主题，该技术支持多个线程以交错方式在单个多发处理器上执行的多个线程。许多多层处理器还包括对多线程的支持。

In the next chapter, we consider ultrascale computers built from very large numbers of processors, connected with networking technology (not necessarily the same networking technology used to connect computers to the Internet) and often called _clusters_; these large-scale systems are used for cloud computing primarily with massive numbers of independent tasks being executed in parallel. More recently, computationally intensive tasks that can be easily made parallel, such as Search and certain machine learning algorithms have also made use of clusters. When these clusters grow to tens of thousands of servers and beyond, we call them _warehouse-scale computers_. Amazon, Google, Microsoft, and Facebook all make warehouse-scale computers.

> 在下一章中，我们考虑了由大量处理器构建的 Ultrascale 计算机，这些计算机与网络技术相连(不一定是用于将计算机连接到 Internet 的相同的网络技术)，并且通常称为 *CLUSTERS*；这些大规模系统用于云计算，主要是并行执行大量独立任务。最近，可以轻松地平行的计算密集型任务，例如搜索和某些机器学习算法，也使用了集群。当这些群集生长到数万次及以后的服务器时，我们称它们为*warehouse-Scale-Scale Computers *。亚马逊，Google，Microsoft 和 Facebook 都制作仓库规模计算机。

In addition to the multiprocessors we study here and the warehouse-scaled systems of the next chapter, there are a range of special large-scale multiprocessor systems, sometimes called _multicomputers_, which are less tightly coupled than the multiprocessors examined in this chapter but usually more tightly coupled than the warehouse-scale systems of the next chapter. The primary use for such multicom- puters is in high-end scientific computation, although they are sometimes used for commercial applications filling the niche between multiprocessors and warehouse- scale computers. The Cray X series and IBM BlueGene are typical examples of these multicomputers.

> 除了我们在此处研究的多处理器和下一章的仓库尺度系统外，还有一系列特殊的大型多处理器系统，有时称为 *multicomputers* 与下一章的仓库规模系统相比紧密结合。此类多组件的主要用途是在高端科学计算中，尽管有时用于填充多处理器和仓库比例计算机之间的利基市场的商业应用。Cray X 系列和 IBM Bluegene 是这些多机构的典型示例。

Many other books, such as [Culler et al. (1999)](#_bookmark939), cover such systems in detail. Because of the large and changing nature of the field of multiprocessing (the just- mentioned Culler et al. reference is over 1000 pages and discusses only multipro- cessing!), we have chosen to focus our attention on what we believe is the most important and general-purpose portions of the computing space. Appendix I discusses some of the issues that arise in building such computers in the context of large-scale scientific applications.

> 许多其他书籍，例如 [Culler 等人。(1999)](#_bookmark939)，详细介绍此类系统。由于多处理领域的巨大和不断变化的性质(刚提到的 Culler 等人。参考是超过 1000 页，仅讨论多发性！)，我们选择将注意力集中在我们认为是最重要的是最重要的事情上和计算空间的通用部分。附录 I 讨论了在大规模科学应用程序中构建此类计算机时出现的一些问题。

Our focus will be on multiprocessors with roughly 4–256 processor cores, which might occupy anywhere from 4 to 16 separate chips. Such designs vastly dominate in terms of both units and dollars. In large-scale multiprocessors, the interconnection networks are a critical part of the design; Appendix F focuses on that topic.

> 我们的重点将放在大约 4–256 个处理器核心的多处理器上，这些处理器可能占据 4 到 16 个单独的芯片。这样的设计在单位和美元方面都在很大程度上占主导地位。在大规模的多处理器中，互连网络是设计的关键部分。附录 F 专注于该主题。

### Multiprocessor Architecture: Issues and Approach

To take advantage of an MIMD multiprocessor with _n_ processors, we must usually have at least _n_ threads or processes to execute; with multithreading, which is present in most multicore chips today, that number is 2–4 times higher. The inde- pendent threads within a single process are typically identified by the programmer or created by the operating system (from multiple independent requests). At the other extreme, a thread may consist of a few tens of iterations of a loop, generated by a parallel compiler exploiting data parallelism in the loop. Although the amount of computation assigned to a thread, called the _grain size_, is important in consid- ering how to exploit thread-level parallelism efficiently, the important qualitative distinction from instruction-level parallelism is that thread-level parallelism is identified at a high level by the software system or programmer and that the threads consist of hundreds to millions of instructions that may be executed in parallel.

> 要利用带有 *n* 处理器的 MIMD 多处理器，我们通常必须至少有 *n* 线程或进程要执行；随着当今大多数多芯片芯片中存在的多线程，该数字高 2-4 倍。单个过程中的独立线程通常由程序员识别或由操作系统创建(来自多个独立请求)。在另一个极端情况下，线程可能由一个循环的几十迭代组成，该循环由平行编译器生成，**该编译器利用数据并行性数据并行。尽管分配给线程的计算量(称为 *grain size*)对于如何有效地利用线程级并行性，但与指令级别的并行性相关的重要定性区别在于螺纹级别的平行性是在高的高度上确定的**。按软件系统或程序员进行级别，并且线程由数以千计的指令组成，可以并行执行。

Threads can also be used to exploit data-level parallelism, although the over- head is usually higher than would be seen with an SIMD processor or with a GPU (see [Chapter 4](#_bookmark165)). This overhead means that grain size must be sufficiently large to exploit the parallelism efficiently. For example, although a vector processor or GPU may be able to efficiently parallelize operations on short vectors, the resulting grain size when the parallelism is split among many threads may be so small that the overhead makes the exploitation of the parallelism prohibitively expensive in an MIMD.

> 线程也可以用来利用数据级并行性，尽管高头通常比 SIMD 处理器或 GPU 所见(请参阅[[第 4 章 4](#_bookmark165))。该开销意味着晶粒尺寸必须足够大以有效利用并行性。例如，尽管矢量处理器或 GPU 可能能够有效地在短矢量上平行操作，但是当平行性分配到许多线程中时，由此产生的晶粒大小可能很小，以至于开销使平行性在 MIMD 中的剥削高昂地昂贵。

Existing shared-memory multiprocessors fall into two classes, depending on the number of processors involved, which in turn dictates a memory organization and interconnect strategy. We refer to the multiprocessors by their memory orga- nization because what constitutes a small or large number of processors continues to change over time.

> 现有的共享内存多处理器分为两个类，具体取决于所涉及的处理器数量，这反过来又决定了内存组织和互连策略。我们通过其内存态度来指代多处理器，因为构成少数或大量处理器的内容会随着时间的推移而继续变化。

The first group, which we call _symmetric (shared-memory) multiprocessors_ (SMPs), or _centralized shared-memory multiprocessors_, features small to moder- ate numbers of cores, typically 32 or fewer. For multiprocessors with such small processor counts, it is possible for the processors to share a single centralized mem- ory that all processors have equal access to, thus the term _symmetric_. In multicore chips, the memory is often shared in a centralized fashion among the cores; most existing multicores are SMPs, but not all. (Note that some literature mistakenly appears to use SMP to stand for Shared Memory Processor, but this usage is erroneous.)

> 我们称之为 *symmetric(共享 - 内存)多处理器*(SMP)或 *centralized 共享 - 内存多处理器*的第一组具有小型到模型的内核数，通常为 32 个或更少。对于具有如此小的处理器计数的多处理器，处理器有可能共享一个单一的集中式内存，所有处理器都可以同等地访问，因此术语 *symmetric*。在多层芯片中，记忆通常以集中式的方式共享。大多数现有的多周是 SMP，但不是全部。(请注意，某些文献似乎错误地使用 SMP 代表共享内存处理器，但是这种用法是错误的。)

Some multicores have nonuniform access to the outermost cache, a structure called _NUCA_ for _Nonuniform Cache Access_, and are thus are not truly SMPs, even if they have a single main memory. The IBM Power8 has distributed L3 caches with nonuniform access time to different addresses in L3.

> 某些多大盘对最外面的缓存具有非均匀访问权限，该结构称为 *nuniform 缓存访问* nuca\_\_，因此即使它们具有单个主内存，也不是真正的 SMP。IBM Power8 已分布了 L3 缓存，并在 L3 中的不同地址分布了不均匀的访问时间。

In multiprocessors consisting of multiple multicore chips, there are often sep- arate memories for each multicore chip. Thus the memory is distributed rather than centralized. As we will see later in the chapter, many designs with distrib- uted memory have fast access to a local memory and much slower access to remote memory; often the differences in access time to various remote memories are small in comparison to the difference between the access times to the local memory and to a remote memory. In such designs, the programmer and software system need to be aware of whether accesses are to local or remote memory, but may be able to ignore the distribution of accesses among remote memories. Because an SMP approach becomes less attractive with a growing number of pro- cessors, most of the very largest multiprocessors use some form of distributed memory.

> 在由多个多层芯片组成的多处理器中，每个多层芯片通常都有分隔记忆。因此，记忆是分布而不是集中的。正如我们将在本章后面看到的那样，许多具有分布式内存的设计都可以快速访问本地内存，并且可以访问远程内存的速度较慢；与访问本地内存和远程内存之间的差异相比，对各种远程记忆的访问时间的差异通常很小。在这样的设计中，程序员和软件系统需要意识到访问是对本地或远程内存的访问，但可能能够忽略远程记忆中访问的分布。由于 SMP 方法在越来越多的过程中变得不吸引人，因此大多数最大的多处理器都使用某种形式的分布式内存。

SMP architectures are also sometimes called _uniform memory access_ (UMA) multiprocessors, arising from the fact that all processors have a uniform latency from memory, even if the memory is organized into multiple banks. [Figure 5.1](#_bookmark216) shows what these multiprocessors look like. The architecture of SMPs is the topic of [Section 5.2](#centralized-shared-memory-architectures), and we explain the approach in the context of a multicore.

> **SMP 体系结构有时也称为* Uniform Memory Access*(UMA)多处理器**，这是由于所有处理器都具有内存均匀延迟的事实，即使存储器被组织成多个银行。[图 5.1](#_bookmark216) 显示了这些多处理器的外观。SMP 的体系结构是[第 5.2 节](＃集中式共享 - 内存 - 构造)的主题，我们在多核心的上下文中解释了该方法。

The alternative design approach consists of multiprocessors with physically distributed memory, called _distributed shared memory_ (DSM). [Figure 5.2](#_bookmark217) shows what these multiprocessors look like. To support larger processor counts, memory must be distributed among the processors rather than centralized; oth- erwise, the memory system would not be able to support the bandwidth demands of a larger number of processors without incurring excessively long access latency.

> 替代设计方法由具有物理分布的内存的多处理器组成，称为* distribed 共享存储器*(DSM)。[图 5.2](#_bookmark217) 显示了这些多处理器的外观。为了支持较大的处理器计数，必须在处理器中分布内存，而不是集中式。视而，内存系统将无法支持大量处理器的带宽需求，而不会产生过长的访问延迟。

Figure 5.1 Basic structure of a centralized shared-memory multiprocessor based on a multicore chip. Multiple processor-cache subsystems share the same physical mem- ory, typically with one level of shared cache on the multicore, and one or more levels of private per-core cache. The key architectural property is the uniform access time to all of the memory from all of the processors. In a multichip design, an interconnection net- work links the processors and the memory, which may be one or more banks. In a single- chip multicore, the interconnection network is simply the memory bus.

> 图 5.1 基于多核芯片的集中式共享内存多处理器的基本结构。多个处理器调查子系统共享相同的物理内存，通常在多项式上具有一个共享的缓存以及一个或多个级别的私有每核缓存。关键的体系结构属性是从所有处理器中所有内存的统一访问时间。在多芯片设计中，互连网络工作将处理器和内存链接，这可能是一个或多个银行。在单个芯片多核中，互连网络仅仅是内存总线。

With the rapid increase in processor performance and the associated increase in a processor’s memory bandwidth requirements, the size of a multiprocessor for which distributed memory is preferred continues to shrink. The introduction of multicore processors has meant that even some 2-chip multiprocessors, which might have 16–64 processor cores, use distributed memory. The larger number of processors also raises the need for a high-bandwidth interconnect, of which we will see examples in Appendix F. Both directed networks (i.e., switches) and indirect networks (typically multidimensional meshes) are used.

> 随着处理器性能的快速增加以及处理器内存带宽要求的相关增加，首选分布式内存的多处理器的大小继续缩小。引入多层处理器意味着即使是一些可能具有 16-64 个处理器内核的 2 芯片多处理器，也使用分布式内存。较大数量的处理器还增加了对高带宽互连的需求，我们将在附录 F 中看到示例。有向网络(即开关)和间接网络(通常是多维网格)。

Distributing the memory among the nodes both increases the bandwidth and reduces the latency to local memory. A DSM multiprocessor is also called a _NUMA_ (nonuniform memory access) because the access time depends on the location of a data word in memory. The key disadvantages for a DSM are that communicating data among processors becomes somewhat more complex and a DSM requires more effort in the software to take advantage of the increased memory bandwidth afforded by distributed memories. Because most multicore-based multiprocessors with more than a few processor chips use distributed memory, we will explain the operation of distributed memory multiprocessors from this viewpoint.

> 在节点之间分配内存既会增加带宽并减少局部内存的延迟。DSM 多处理器也称为 *numa*(不一致的内存访问)，因为访问时间取决于数据字在内存中的位置。DSM 的关键缺点是，处理器之间的数据变得更加复杂，并且 DSM 需要在软件中进行更多的努力，以利用分布式记忆提供的增加的内存带宽。由于大多数具有多个处理器芯片的基于多核算的多处理器都使用分布式内存，因此我们将从此角度解释分布式内存多处理器的操作。

Figure 5.2 The basic architecture of a distributed-memory multiprocessor in 2017 typically consists of a multi- core multiprocessor chip with memory and possibly I/O attached and an interface to an interconnection network that connects all the nodes. Each processor core shares the entire memory, although the access time to the local memory attached to the core’s chip will be much faster than the access time to remote memories.

> 图 5.2 2017 年分布式内存多处理器的基本体系结构通常由带有内存的多核多处理器芯片组成，可能是 I/O 附件以及连接所有节点的互连网络的接口。每个处理器核心都共享整个内存，尽管访问附加到核心芯片的本地内存的访问时间将比访问远程内存的访问时间要快得多。

In both SMP and DSM architectures, communication among threads occurs through a shared address space, meaning that a memory reference can be made by any processor to any memory location, assuming it has the correct access rights. The term _shared memory_ associated with both SMP and DSM refers to the fact that the _address space_ is shared.

> 在 SMP 和 DSM 架构中，线程之间的通信是通过共享地址空间进行的，这意味着任何处理器都可以对任何内存位置进行内存参考，假设它具有正确的访问权限。与 SMP 和 DSM 关联的术语 *SHARED MOMEN* 是指共享 *ADDRESS SPACE* 共享的事实。

In contrast, the clusters and warehouse-scale computers in the next chapter look like individual computers connected by a network, and the memory of one pro- cessor cannot be accessed by another processor without the assistance of software protocols running on both processors. In such designs, message-passing protocols are used to communicate data among processors.

> 相比之下，下一章中的集群和仓库尺度计算机看起来像是由网络连接的单个计算机，并且如果没有在两个处理器上运行的软件协议的帮助，另一个处理器的内存将无法访问另一个处理器。在这样的设计中，通信协议用于在处理器之间传达数据。

### Challenges of Parallel Processing

The application of multiprocessors ranges from running independent tasks with essentially no communication to running parallel programs where threads must communicate to complete the task. Two important hurdles, both explainable with Amdahl’s Law, make parallel processing challenging. To overcome these hurdles typically requires a comprehensive approach that addresses the choice of algorithm and its implementation, the underlying programming language and sys- tem, the operating system and its support functions, and the architecture and hard- ware implementation. Although in many instances, one of these is a key bottleneck, when scaling to a larger processor counts (approaching 100 or more), often _all_ aspects of the software and hardware need attention.

> 多处理器的应用范围从实质上没有通信的独立任务到运行的并行程序，线程必须进行通信以完成任务。两个重要的障碍，都可以用 Amdahl 的定律来解释，这使平行处理具有挑战性。为了克服这些障碍，通常需要一种全面的方法，以解决算法及其实施，基础编程语言和系统，操作系统及其支持功能以及体系结构和硬件实施。尽管在许多情况下，其中之一是一个关键的瓶颈，但是在扩展到较大的处理器计数(接近 100 或更多)时，通常是软件和硬件的方面需要注意。

The first hurdle has to do with the limited parallelism available in programs, and the second arises from the relatively high cost of communications. Limitations in available parallelism make it difficult to achieve good speedups in any parallel processor, as our first example shows.

> 第一个障碍与计划中可用的有限平行性有关，第二个障碍是由于沟通成本较高而引起的。正如我们的第一个示例所示，可用并行性的局限性使得在任何并行处理器中都难以实现良好的加速。

Example Suppose you want to achieve a speedup of 80 with 100 processors. What fraction of the original computation can be sequential?

> 示例假设您想通过 100 个处理器实现 80 的加速。原始计算的哪一部分可以是顺序的？

For simplicity in this example, assume that the program operates in only two modes: parallel with all processors fully used, which is the enhanced mode, or serial with only one processor in use. With this simplification, the speedup in enhanced mode is simply the number of processors, whereas the fraction of enhanced mode is the time spent in parallel mode. Substituting into the previous equation:

> 为了简单起见，在此示例中，假设程序仅以两种模式运行：与完全使用的所有处理器并行，即增强模式或仅使用一个处理器的串行。通过这种简化，增强模式的加速仅仅是处理器的数量，而增强模式的比例是在并行模式下花费的时间。替换为先前的等式：

Thus, to achieve a speedup of 80 with 100 processors, only 0.25% of the original computation can be sequential! Of course, to achieve linear speedup (speedup of _n_ with _n_ processors), the entire program must usually be parallel with no serial portions. In practice, programs do not just operate in fully parallel or sequential mode, but often use less than the full complement of the processors when running in parallel mode. Amdahl’s Law can be used to analyze applications with varying amounts of speedup, as the next example shows.

> 因此，为了通过 100 个处理器实现 80 的加速，只有 0.25％的原始计算可以是顺序的！当然，要实现线性加速(使用 *n* 处理器的 *n* 加速)，整个程序通常必须平行，没有串行部分。实际上，程序不仅在完全并行或顺序模式下运行，而且通常在并行模式下运行时使用的处理器的完整补充。如下一个示例所示，AMDAHL 的定律可用于分析具有不同速度的应用程序。

Example Suppose we have an application running on a 100-processor multiprocessor, and assume that application can use 1, 50, or 100 processors. If we assume that 95% of the time we can use all 100 processors, how much of the remain- ing 5% of the execution time must employ 50 processors if we want a speedup of 80?

> 示例假设我们有一个在 100 处理器多处理器上运行的应用程序，并假设应用程序可以使用 1、50 或 100 处理器。如果我们假设 95％的时间可以使用所有 100 个处理器，那么如果我们想要 80 的加速，则其余 5％的执行时间必须使用 50 个处理器？

If 95% of an application can use 100 processors perfectly, to get a speedup of 80, 4.8% of the remaining time must be spent using 50 processors and only 0.2% can be serial!

> 如果 95％的应用程序可以完美使用 100 个处理器，则要获得 80 的加速度，剩余时间的 4.8％必须使用 50 个处理器，只有 0.2％可以是序列号！

The second major challenge in parallel processing involves the large latency of remote access in a parallel processor. In existing shared-memory multipro- cessors, communication of data between separate cores may cost 35–50 clock cycles and among cores on separate chips anywhere from 100 clock cycles to as much as 300 or more clock cycles (for large-scale multiprocessors), depending on the communication mechanism, the type of interconnection network, and the scale of the multiprocessor. The effect of long communication delays is clearly substantial. Let’s consider a simple example.

> **并行处理中的第二个主要挑战涉及并行处理器中远程访问的较大延迟**。在现有的共享内存的多侦察员中，单独核心之间的数据的通信可能成本为 35-50 个时钟周期，而在 100 个时钟周期到达 300 个或更多时钟周期(大型多处理器)的单独芯片上的内核之间，在任何地方的芯子之间的核心之间的核心(对于大型多处理器)，取决于通信机制，互连网络的类型和多处理器的规模。长期沟通延迟的影响显然是很大的。让我们考虑一个简单的例子。

Example Suppose we have an application running on a 32-processor multiprocessor that has a 100 ns delay to handle a reference to a remote memory. For this application, assume that all the references except those involving communication hit in the local memory hierarchy, which is obviously optimistic. Processors are stalled on a remote request, and the processor clock rate is 4 GHz. If the base CPI (assum- ing that all references hit in the cache) is 0.5, how much faster is the multiprocessor if there is no communication versus if 0.2% of the instructions involve a remote communication reference?

> 示例假设我们有一个在 32 处理器多处理器上运行的应用程序，该应用程序具有 100 ns 延迟以处理对远程内存的引用。对于此应用程序，假设除了涉及通信命中的所有引用外，这显然是乐观的。处理器在远程请求下停滞不前，处理器时钟速率为 4 GHz。如果基本 CPI(假设所有引用命中在缓存中)为 0.5，那么如果没有通信与 0.2％的指令涉及远程通信参考，则多处理器的速度要快多少？

The multiprocessor with all local references is 1.3/0.5 2.6 times faster. In prac- tice, the performance analysis is much more complex because some fraction of the noncommunication references will miss in the local hierarchy and the remote access time does not have a single constant value. For example, the cost of a remote reference could be worse because contention caused by many references trying to use the global interconnect can lead to increased delays, or the access time might be better if memory were distributed and the access was to the local memory.

> 所有本地参考的多处理器的速度为 1.3/0.5 2.6 倍。在实践中，性能分析要复杂得多，因为非通信引用的一部分将在本地层次结构中错过，并且远程访问时间没有单个常数值。例如，远程参考的成本可能会更糟，因为许多试图使用全局互连的参考引起的争论可能会导致延迟增加，或者如果分配内存并且访问权限是对本地内存的访问，则访问时间可能会更好。

This problem could have also been analyzed using Amdahl’s Law, an exercise we leave to the reader.

> 也可以使用 Amdahl 的定律对这个问题进行分析，这是我们留给读者的练习。

These problems—insufficient parallelism and long-latency remote communi- cation—are the two biggest performance challenges in using multiprocessors. The problem of inadequate application parallelism must be attacked primarily in software with new algorithms that offer better parallel performance, as well as by software systems that maximize the amount of time spent executing with the full complement of processors. Reducing the impact of long remote latency can be attacked both by the architecture and by the programmer. For example, we can reduce the frequency of remote accesses with either hardware mechanisms, such as caching shared data, or software mechanisms, such as restructuring the data to make more accesses local. We can try to tolerate the latency by using multi- threading (discussed later in this chapter) or by using prefetching (a topic we cover extensively in [Chapter 2](#_bookmark46)).

> 这些问题 - 不足的并行性和长期远程交流 - 是使用多处理器的两个最大绩效挑战。不足的应用并行性问题必须主要用新算法的软件来攻击，这些算法提供了更好的并行性能，以及通过软件系统最大限度地使用处理器的完整补充来执行时间的时间。减少长远程延迟的影响可以受到架构和程序员的攻击。例如，我们可以**使用硬件机制(例如共享数据或软件机制)来减少远程访问的频率**，例如重组数据以使更多访问权限在本地。我们可以尝试通过使用多线程(在本章后面讨论)或使用预摘要(我们在[第 2 章](#_bookmark46)中广泛介绍的主题)来忍受延迟。

Much of this chapter focuses on techniques for reducing the impact of long remote communication latency. For example, [Sections 5.2 through 5.4](#centralized-shared-memory-architectures) discuss how caching can be used to reduce remote access frequency, while maintaining a coherent view of memory. [Section 5.5](#synchronization-the-basics) discusses synchronization, which, because it inherently involves interprocessor communication and also can limit parallelism, is a major potential bottleneck. [Section 5.6](#models-of-memory-consistency-an-introduction) covers latency-hiding techniques and memory consistency models for shared memory. In Appendix I, we focus primar- ily on larger-scale multiprocessors that are used predominantly for scientific work.

> 本章的大部分都集中在减少长期远程通信延迟的影响的技术上。例如，[第 5.2 至 5.4 节](＃集中式共享 - 内存架构)讨论如何使用缓存来减少远程访问频率，同时保持连贯的内存视图。[第 5.5 节](＃同步 - 基础)讨论了同步，因为它固有地涉及分解器通信并可能限制并行性，所以它是主要的潜在瓶颈。[第 5.6 节](＃内存 - 符合性模型 - 引言)涵盖了共享内存的延迟隐藏技术和内存一致性模型。在附录 I 中，我们集中于主要用于科学工作的大型多处理器。

In that appendix, we examine the nature of such applications and the challenges of achieving speedup with dozens to hundreds of processors.

> 在该附录中，我们研究了此类应用的性质以及数十个处理器的加速挑战。
