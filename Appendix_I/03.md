## Characteristics of Scientific Applications

> ##科学应用的特征

The primary use of scalable shared-memory multiprocessors is for true parallel programming, as opposed to multiprogramming or transaction-oriented comput- ing. The primary target of parallel computing is scientific and technical applica- tions. Thus, understanding the design issues requires some insight into the behavior of such applications. This section provides such an introduction.

> 可扩展的共享内存多处理器的主要用途是真正的并行编程，而不是以多编程或面向交易为导向的计算。并行计算的主要目标是科学和技术应用。因此，了解设计问题需要对此类应用程序的行为有所了解。本节提供了这样的介绍。

### Characteristics of Scientific Applications

> ###科学应用的特征

Our scientific/technical parallel workload consists of two applications and two computational kernels. The kernels are fast Fourier transformation (FFT) and an LU decomposition, which were chosen because they represent commonly used techniques in a wide variety of applications and have performance characteristics typical of many parallel scientific applications. In addition, the kernels have small code segments whose behavior we can understand and directly track to specific architectural characteristics. Like many scientific applications, I/O is essentially nonexistent in this workload.

> 我们的科学/技术并行工作负载包括两个应用和两个计算内核。核是快速傅立叶变换（FFT）和 LU 分解，之所以选择，是因为它们代表了各种应用中常用技术，并且具有许多平行科学应用的典型性能特征。此外，内核具有小的代码段，我们可以理解其行为并直接跟踪特定的体系结构特征。像许多科学应用一样，I/O 在此工作量中本质上不存在。

The two applications that we use in this appendix are Barnes and Ocean, which represent two important but very different types of parallel computation. We briefly describe each of these applications and kernels and characterize their basic behavior in terms of parallelism and communication. We describe how the prob- lem is decomposed for a distributed shared-memory multiprocessor; certain data decompositions that we describe are not necessary on multiprocessors that have a single, centralized memory.

> 我们在本附录中使用的两个应用程序是 Barnes 和 Ocean，代表了两种重要但非常不同类型的并行计算。我们简要描述了这些应用程序和内核，并根据并行性和沟通来表征它们的基本行为。我们描述了如何针对分布式共享内存多处理器分解概率；我们描述的某些数据分解对于具有单个集中存储器的多处理器不需要。

##### _The FFT Kernel_

> ##### _ fft 内核_

The FFT is the key kernel in applications that use spectral methods, which arise in fields rangingfrom signalprocessingtofluid flowtoclimate modeling. The FFT appli- cation we study here is a one-dimensional version of a parallel algorithm for a complex number FFT. It has a sequential execution time for _n_ data points of _n_ log _n_. The algo- rithm uses a high radix (equal to p*n*) that minimizes communication. The measurements shown in this appendix are collected for a million-point input data set.

> FFT 是使用光谱方法的应用中的关键内核，该应用在范围内出现的频谱方法从信号普罗加德流体流动式气候建模中出现。我们在这里研究的 FFT 应用是用于复杂数字 FFT 的平行算法的一维版本。它具有_n_ log _n_的_n_数据点的顺序执行时间。算法使用高辐射（等于 P*n*），以最大程度地减少通信。本附录中显示的测量值是为百万点输入数据集收集的。

There are three primary data structures: the input and output arrays of the data being transformed and the roots of unity matrix, which is precomputed and only read during the execution. All arrays are organized as square matrices. The six steps in the algorithm are as follows:

> 有三个主要数据结构：要转换的数据的输入和输出阵列以及 Unity 矩阵的根，它是预先计算的，仅在执行过程中读取。所有阵列均以方形矩阵组织。算法中的六个步骤如下：

1. Transpose data matrix.

> 1.转置数据矩阵。

2. Perform 1D FFT on each row of data matrix.

> 2.对每行数据矩阵执行 1D FFT。

3. Multiply the roots of unity matrix by the data matrix and write the result in the data matrix.

> 3.将 Unity 矩阵的根乘以数据矩阵，然后在数据矩阵中写入结果。

4. Transpose data matrix.

> 4.转置数据矩阵。

5. Perform 1D FFT on each row of data matrix.

> 5.对每行数据矩阵执行 1D FFT。

6. Transpose data matrix.

> 6.转置数据矩阵。

The data matrices and the roots of unity matrix are partitioned among processors in contiguous chunks of rows, so that each processor’s partition falls in its own local memory. The first row of the roots of unity matrix is accessed heavily by all pro- cessors and is often replicated, as we do, during the first step of the algorithm just shown. The data transposes ensure good locality during the individual FFT steps, which would otherwise access nonlocal data.

> 数据矩阵和 Unity 矩阵的根部在连续的行中分配在处理器之间，因此每个处理器的分区都属于其本地内存。统一矩阵的根的第一行均由所有专业人员大量访问，并且在刚刚显示的算法的第一步中，经常像我们一样复制。数据转置确保在单个 FFT 步骤中确保良好的位置，否则将访问非本地数据。

The only communication is in the transpose phases, which require all-to-all communication of large amounts of data. Contiguous subcolumns in the rows assigned to a processor are grouped into blocks, which are transposed and placed into the proper location of the destination matrix. Every processor transposes one block locally and sends one block to each of the other processors in the system. Although there is no reuse of individual words in the transpose, with long cache blocks it makes sense to block the transpose to take advantage of the spatial locality afforded by long blocks in the source matrix.

> 唯一的通信是在转置阶段，需要全面交流大量数据。分配给处理器的行中的连续子列分组为块，这些块被转置并将其放置在目标矩阵的正确位置中。每个处理器在本地转换一个块，并将一个块发送到系统中的每个其他处理器。尽管在转置中没有单个单词的重复使用，但在长缓存块中，阻止转置以利用源矩阵中长块提供的空间位置是有意义的。

##### _The LU Kernel_

> ##### _ lu kernel_

LU is an LU factorization of a dense matrix and is representative of many dense linear algebra computations, such as QR factorization, Cholesky factorization, and eigenvalue methods. For a matrix of size _n n_ the running time is _n_<sup>3</sup> and the parallelism is proportional to _n_<sup>2</sup>. Dense LU factorization can be performed efficiently by blocking the algorithm, using the techniques in [Chapter 2](#_bookmark46), which leads to highly efficient cache behavior and low communication. After blocking the algorithm, the dominant computation is a dense matrix multiply that occurs in the innermost loop. The block size is chosen to be small enough to keep the cache miss rate low and large enough to reduce the time spent in the less parallel parts of the computation. Relatively small block sizes (8 8 or 16 16) tend to satisfy both criteria.

> lu 是密集矩阵的 LU 分解，代表了许多密集的线性代数计算，例如 QR 分解，Cholesky 分解和特征值方法。对于大小_n n_的矩阵，运行时间为_n_ <sup> 3 </sup>，并且并行性与_n_ <sup> 2 </sup>成正比。通过[第 2 章]（#_ bookmark46）中的技术阻止算法，可以通过阻止算法来有效地进行密集的 lu 分解，从而导致高效的高速缓存行为和较低的通信。阻止算法后，主要计算是在最内向环中发生的密集矩阵倍数。选择块大小足够小，以使高速缓存率低率低和足够大，以减少计算不平行部分所花费的时间。相对较小的块大小（8 8 或 16 16）倾向于满足这两个标准。

Two details are important for reducing interprocessor communication. First, the blocks of the matrix are assigned to processors using a 2D tiling: The _<sup><u>n</u></sup>_ × _<u>n</u>_

> 两个细节对于减少处理器交流很重要。首先，矩阵的块使用 2D 瓷砖分配给处理器：_ <sup> <> <u> n </u> </u> </sup> _×_ <u> n </u> _

(where each block is _B B_) matrix of blocks is allocated by laying a grid of size _p p_ over the matrix of blocks in a cookie-cutter fashion until all the blocks are allocated to a processor. Second, the dense matrix multiplication is performed by the processor that owns the _destination_ block. With this blocking and allocation scheme, communication during the reduction is both regular and predictable. For the measurements in this appendix, the input is a 512 512 matrix and a block of 16 16 is used.

> （每个块为_b b_）块的矩阵通过以 cookie-cutter 方式将大小_p p_的网格分配给块的矩阵，直到将所有块分配给处理器。其次，密集矩阵乘法由拥有_destination_块的处理器执行。通过这种阻止和分配方案，减少期间的通信既是规则的，也是可预测的。对于本附录中的测量值，输入是 512 512 矩阵，使用了 16 16 的块。

A natural way to code the blocked LU factorization of a 2D matrix in a shared address space is to use a 2D array to represent the matrix. Because blocks are allo- cated in a tiled decomposition, and a block is not contiguous in the address space in a 2D array, it is very difficult to allocate blocks in the local memories of the processors that own them. The solution is to ensure that blocks assigned to a processor are allocated locally and contiguously by using a 4D array (with the first two dimensions specifying the block number in the 2D grid of blocks, and the next two specifying the element in the block).

> 在共享地址空间中 2D 矩阵的阻塞 LU 分解的一种自然方法是使用 2D 数组来表示矩阵。由于块是在瓷砖分解中分配的，并且在 2D 数组中的地址空间中不连续，因此很难在拥有它们的处理器的本地记忆中分配块。该解决方案是确保通过使用 4D 数组在本地和连续分配给处理器的块（前两个维度在块的 2D 网格中指定块号，而下两个在块中指定了元素）。

##### _The Barnes Application_

> ##### _ barnes 应用程序_

Barnes is an implementation of the Barnes-Hut _n_-body algorithm solving a problem in galaxy evolution. _N-body algorithms_ simulate the interaction among a large number of bodies that have forces interacting among them. In this instance, the bodies represent collections of stars and the force is gravity. To reduce the computational time required to model completely all the individual interactions among the bodies, which grow as _n_<sup>2</sup>, _n_-body algorithms take advan- tage of the fact that the forces drop off with distance. (Gravity, for example, drops off as 1/_d_<sup>2</sup>, where _d_ is the distance between the two bodies.) The Barnes-Hut algorithm takes advantage of this property by treating a collection of bodies that are “far away” from another body as a single point at the center of mass of the collection and with mass equal to the collection. If the body is far enough from any body in the collection, then the error introduced will be negligible. The collections are structured in a hierarchical fashion, which can be represented in a tree. This algorithm yields an _n_ log _n_ running time with par- allelism proportional to _n_.

> Barnes 是 Barnes-Hut _N_-Body 算法的实现，该算法解决了银河发展中的问题。_n 体算法_模拟了许多在它们之间相互作用的物体之间的相互作用。在这种情况下，身体代表恒星的集合，力是重力。为了减少完全建模物体之间所有单个相互作用所需的计算时间，它们成长为_n_ <sup> 2 </sup>，_n_-body 算法使力量的事实是，力量随距离下降。（例如，重力以 1/_d_ <sup> 2 </sup>下降，其中_d_是两个身体之间的距离。）Barnes-Hut 算法通过处理该属性来利用这一属性，通过治疗一组物体从另一个身体处以“远距离”，作为集合质量中心的一个点，质量等于集合。如果身体与收藏中的任何身体都足够远，那么引入的错误将可以忽略不计。这些集合以层次结构的方式结构，可以在树上表示。该算法产生一个_n_ log _n_运行时间，与_n_成正比的 parlelism。

The Barnes-Hut algorithm uses an octree (each node has up to eight children) to represent the eight cubes in a portion of space. Each node then represents the collection of bodies in the subtree rooted at that node, which we call a _cell_. Because the density of space varies and the leaves represent individual bodies, the depth of the tree varies. The tree is traversed once per body to compute the net force acting on that body. The force calculation algorithm for a body starts at the root of the tree. For every node in the tree it visits, the algorithm determines if the center of mass of the cell represented by the subtree rooted at the node is “far enough away” from the body. If so, the entire subtree under that node is approximated by a single point at the center of mass of the cell, and the force that this center of mass exerts on the body is computed. On the other hand, if the center of mass is not far enough away, the cell must be “opened” and each of its subtrees visited. The distance between the body and the cell, together with the error tolerances, determines which cells must be opened. This force calcula- tion phase dominates the execution time. This appendix takes measurements using 16K bodies; the criterion for determining whether a cell needs to be opened is set to the middle of the range typically used in practice.

> Barnes-Hut 算法使用 OCTREE（每个节点有多达八个孩子）代表一部分空间中的八个立方体。然后，每个节点代表植根于该节点的子树中的身体集合，我们称之为_cell_。由于空间的密度变化，叶子代表各个物体，因此树的深度有所不同。该树每只身体一次穿越一次，以计算作用于该身体的净力。物体的力计算算法从树的根部开始。对于它访问的树中的每个节点，算法确定植根于节点的子树表示的细胞的质量中心是否“足够远”远离身体。如果是这样，则该节点下的整个子树由单个细胞中心的一个点近似，并且该质量中心在体内施加的力是计算的。另一方面，如果质量中心不够远，则必须“打开”牢房，并且其每个子树都访问了。身体和细胞之间的距离以及误差公差确定必须打开哪些单元格。该力计算阶段主导执行时间。该附录使用 16K 身体进行测量；确定是否需要打开单元格的标准设置为通常在实践中使用的范围的中间。

Obtaining effective parallel performance on Barnes-Hut is challenging because the distribution of bodies is nonuniform and changes over time, making partition- ing the work among the processors and maintenance of good locality of reference difficult. We are helped by two properties: (1) the system evolves slowly, and (2) because gravitational forces fall off quickly, with high probability, each cell requires touching a small number of other cells, most of which were used on the last time step. The tree can be partitioned by allocating each processor a subtree. Many of the accesses needed to compute the force on a body in the subtree will be to other bodies in the subtree. Since the amount of work associated with a subtree varies (cells in dense portions of space will need to access more cells), the size of the subtree allocated to a processor is based on some measure of the work it has to do (e.g., how many other cells it needs to visit), rather than just on the number of nodes in the subtree. By partitioning the octree representation, we can obtain good load balance and good locality of reference, while keeping the partitioning cost low. Although this partitioning scheme results in good locality of reference, the resulting data references tend to be for small amounts of data and are unstructured. Thus, this scheme requires an efficient implementation of shared-memory communication.

> 在 Barnes-Hut 上获得有效的并行性能是具有挑战性的，因为身体的分布是不均匀的，并且会随着时间的流逝而变化，从而使处理器之间的工作分配并维持良好的参考局部性困难。我们得到了两种属性的帮助：（1）系统缓慢发展，（2）由于引力迅速掉落，概率很高，每个单元都需要触摸少数其他细胞，其中大多数是在上次使用的。步。可以通过分配每个处理器一个子树来分区树。计算子树身体上的力所需的许多访问将是向子树中的其他尸体。由于与子树相关的工作量各不相同（空间密集部分需要访问更多单元格），因此分配给处理器的子树的大小基于其必须做的工作的某种度量（例如，如何它需要访问的许多其他单元格），而不仅仅是子树中的节点数量。通过对 OCTREE 表示形式进行分区，我们可以获得良好的负载平衡和良好的参考位置，同时保持分区成本较低。尽管这种分区方案导致了良好的参考局部性，但所得的数据参考往往是少量数据，并且是非结构化的。因此，该方案需要有效地实施共享记忆通信。

##### _The Ocean Application_

> ##### _海洋应用程序_

Ocean simulates the influence of eddy and boundary currents on large-scale flow in the ocean. It uses a restricted red-black Gauss-Seidel multigrid technique to solve a set of elliptical partial differential equations. _Red-black Gauss-Seidel_ is an iteration technique that colors the points in the grid so as to consistently update each point based on previous values of the adjacent neighbors. _Multigrid methods_ solve finite difference equations by iteration using hierarchical grids. Each grid in the hierarchy has fewer points than the grid below and is an approximation to the lower grid. A finer grid increases accuracy and thus the rate of convergence, while requiring more execution time, since it has more data points. Whether to move up or down in the hierarchy of grids used for the next iteration is deter- mined by the rate of change of the data values. The estimate of the error at every time step is used to decide whether to stay at the same grid, move to a coarser grid, or move to a finer grid. When the iteration converges at the finest level, a solution has been reached. Each iteration has _n_<sup>2</sup> work for an _n n_ grid and the same amount of parallelism.

> 海洋模拟了涡流和边界电流对海洋大规模流动的影响。它使用受限的红黑色高斯 - 赛义德多机技术来求解一组椭圆形的部分微分方程。_red-Black Gauss-Seidel_是一种迭代技术，它为网格中的点着色，以便基于相邻邻居的先前值始终更新每个点。_multigrid 方法_使用层次网格通过迭代解决有限差方程。层次结构中的每个网格的点比下面的网格少，并且是较低网格的近似值。较细的网格会提高准确性，从而提高收敛速度，同时需要更多的执行时间，因为它具有更多的数据点。无论是在用于下一次迭代的网格的层次结构中向上还是向下移动，都取决于数据值的变化率。每个时间步骤的错误估计都用于决定是否停留在同一网格上，移至更粗的网格或移至更细的网格。当迭代在最佳级别收敛时，已经达到了解决方案。每次迭代都有_n_ <sup> 2 </sup>用于_n n_网格和相同数量的并行性。

The arrays representing each grid are dynamically allocated and sized to the particular problem. The entire ocean basin is partitioned into square subgrids (as close as possible) that are allocated in the portion of the address space corre- sponding to the local memory of the individual processors, which are assigned responsibility for the subgrid. For the measurements in this appendix we use an input that has 130 130 grid points. There are five steps in a time iteration. Since data are exchanged between the steps, all the processors present synchronize at the end of each step before proceeding to the next. Communication occurs when the boundary points of a subgrid are accessed by the adjacent subgrid in nearest- neighbor fashion.

> 代表每个网格的阵列是动态分配的，并大小适用于特定问题。整个海盆都分配给平方子网格（尽可能接近），这些盆地分配在地址空间的部分中，这些空间空间范围内，这些空间将分配给单个处理器的本地内存，这些记忆被分配给子网格的责任。对于本附录中的测量值，我们使用具有 130 个 130 个网格点的输入。时间迭代有五个步骤。由于在步骤之间交换了数据，因此所有处理器都会在每个步骤的末尾同步，然后再继续进行下一个。当相邻子格里德以最近的邻居方式访问子网格的边界点时，就会发生通信。

##### _Computation/Communication for the Parallel Programs_

> ##### _ computation/for Parallel 程序的通信_

A key characteristic in determining the performance of parallel programs is the ratio of computation to communication. If the ratio is high, it means the application has lots of computation for each datum communicated. As we saw in [Section I.2](#interprocessor-communication-the-critical-performance-issue), communication is the costly part of parallel computing; therefore, high computation-to-communication ratios are very beneficial. In a parallel processing environment, we are concerned with how the ratio of computation to communica- tion changes as we increase either the number of processors, the size of the prob- lem, or both. Knowing how the ratio changes as we increase the processor count sheds light on how well the application can be sped up. Because we are often inter- ested in running larger problems, it is vital to understand how changing the data set size affects this ratio.

> 确定并行程序的性能的关键特征是计算与通信的比率。如果比率很高，则意味着该应用程序对所传达的每个基准都有很多计算。正如我们在[I.2]（＃contracsers-communication-the-Comminital-pormentrance-Issue）中看到的那样，通信是并行计算的昂贵一部分。因此，高计算与通信比率非常有益。在平行处理环境中，我们关注的是，随着我们增加处理器的数量，概率的大小或两者，计算与通信的比率如何变化。知道随着我们增加处理器计数的比率如何变化，这阐明了应用程序的加速程度。因为我们通常会涉及到较大的问题，所以了解更改数据集大小如何影响该比率是至关重要的。

To understand what happens quantitatively to the computation-to- communication ratio as we add processors, consider what happens separately to computation and to communication as we either add processors or increase prob- lem size. Figure I.1 shows that as we add processors, for these applications, the amount of computation per processor falls proportionately and the amount of com- munication per processor falls more slowly. As we increase the problem size, the computation scales as the _O_( ) complexity of the algorithm dictates. Communica- tion scaling is more complex and depends on details of the algorithm; we describe the basic phenomena for each application in the caption of Figure I.1.

> 为了了解在添加处理器时定量地对计算与通信比率进行定量发生的事情，请考虑在添加处理器或增加概率大小时分别对计算和通信发生的情况。图 I.1 表明，当我们添加处理器时，对于这些应用，每个处理器的计算量会按比例下降，并且每个处理器的连接量下降速度较慢。随着我们增加问题的大小，计算量表将算法的复杂性缩放为_o_（）的复杂性。通信缩放更为复杂，取决于算法的细节；我们描述了图 I.1 的标题中的每个应用程序的基本现象。

Application Scaling of computation Scaling of communication

> 通信计算缩放的应用程序缩放

Figure I.1 Scaling of computation, of communication, and of the ratio are critical factors in determining perfor- mance on parallel multiprocessors. In this table, _p_ is the increased processor count and _n_ is the increased dataset size. Scaling is on a per-processor basis. The computation scales up with _n_ at the rate given by _O_( ) analysis and scales down linearly as _p_ is increased. Communication scaling is more complex. In FFT, all data points must interact, so communication increases with _n_ and decreases with _p_. In LU and Ocean, communication is proportional to the boundary of a block, so it scales with dataset size at a rate proportional to the side of a square with _n_ points, namely, pn; for the same reason communication in these two applications scales inversely to pp. Barnes has the most complex scaling properties. Because of the fall-off of interaction between bodies, the basic number of interactions among bodies that require communication scales as pn. An additional factor of log _n_ is needed to maintain the relationships among the bodies. As processor count is increased, communication scales inversely to pp.

> 图 I.1 计算，通信和比率的缩放是确定并行多处理器上的性能的关键因素。在此表中，_p_是增加的处理器计数，_n_是增加的数据集大小。缩放是按处理的。计算以_n_的比率按_o_（）分析给出的速率扩展，并随着_p_的增加而线性地缩放。沟通缩放更为复杂。在 FFT 中，所有数据点都必须相互作用，因此通信随_n_的增加而增加，并用_p_减少。在 Lu 和 Ocean 中，通信与块的边界成正比，因此它以数据集大小的比例缩放，与_n_点的正方形的侧面成正比，即 PN；出于相同的原因，这两个应用程序中的通信成反比 pp。Barnes 具有最复杂的缩放属性。由于身体之间的相互作用下降，因此需要通信量表为 PN 的身体之间的基本相互作用数。还需要对数_n_的另一个因素来维持身体之间的关系。随着处理器计数的增加，通信量表成反比 PP。

The overall computation-to-communication ratio is computed from the indi- vidual growth rate in computation and communication. In general, this ratio rises slowly with an increase in dataset size and decreases as we add processors. This reminds us that performing a fixed-size problem with more processors leads to increasing inefficiencies because the amount of communication among processors grows. It also tells us how quickly we must scale dataset size as we add processors to keep the fraction of time in communication fixed. The following example illus- trates these trade-offs.

> 总体计算与通信比率是根据计算和通信中的独立增长率计算得出的。通常，该比率随着数据集大小的增加而缓慢上升，并且随着我们添加处理器而减小。这提醒我们，执行固定尺寸的问题使用更多的处理器导致效率不高，因为处理器之间的通信量增加。它还告诉我们，当我们添加处理器时，我们必须如何缩放数据集大小，以使固定的通信时间保持时间。以下示例幻觉是这些权衡。

Example Suppose we know that for a given multiprocessor the Ocean application spends 20% of its execution time waiting for communication when run on four processors. Assume that the cost of each communication event is independent of processor count, which is not true in general, since communication costs rise with processor count. How much faster might we expect Ocean to run on a 32-processor machine with the same problem size? What fraction of the execution time is spent on com- munication in this case? How much larger a problem should we run if we want the fraction of time spent communicating to be the same?

> 示例假设我们知道，对于给定的多处理器，海洋应用程序的执行时间为 20％，等待四个处理器运行时的通信。假设每个通信事件的成本独立于处理器计数，这通常是不正确的，因为沟通成本随处理器计数而上升。我们期望海洋在具有相同问题大小的 32 处理器上运行多少速度？在这种情况下，在通讯上花费了哪一部分执行时间？如果我们想要花费的时间相同的时间相同，我们应该遇到多少问题？

_Answer_ The computation-to-communication ratio for Ocean is p*n*/p*p*, so if the problem size is the same, the communicatipon frequency scales by _p_. This means that recognizing that the computation is decreased but the communication time is increased. If _T_<sub>4</sub> is the total execution time for four processors, then the execution time for 32 processors is

> _ANSWER_海洋的计算与通信比为 p*n*/p*p*，因此，如果问题大小相同，则可以通过_p_进行通信频率缩放。这意味着认识到计算减少，但通信时间增加。如果_t_ <sub> 4 </sub>是四个处理器的总执行时间，则 32 处理器的执行时间为

For the fraction of the communication time to remain the same, we must keep the computation-to-communication ratio the same, so the problem size must scale at the same rate as the processor count. Notice that, because we have changed the problem size, we cannot fairly compare the speedup of the original problem and the scaled problem. We will return to the critical issue of scaling applications for mul- tiprocessors in [Section I.6](#performance-measurement-of-parallel-processors-with-scientific-applications).

> 对于保持相同时间的通信时间的比例，我们必须保持计算与通信比率相同，因此问题大小必须以与处理器计数相同的速率扩展。请注意，由于我们已经改变了问题大小，因此我们无法公平地比较原始问题的加速和缩放问题。我们将返回[I.6 节]中的缩放应用程序的关键问题（＃与 - 科学应用程序的＃performance-Measurement-seperument-seperement-seperement-secientific-applications）。
