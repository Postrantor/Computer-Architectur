## The Major Hurdle of Pipelining—Pipeline Hazards

> ##管道上的主要障碍-Pipeline 危险

There are situations, called _hazards_, that prevent the next instruction in the instruc- tion stream from executing during its designated clock cycle. Hazards reduce the performance from the ideal speedup gained by pipelining. There are three classes of hazards:

> 有一些称为_hazards_的情况，可以防止在指定的时钟周期中执行指定流中的下一个指令。危险从管道上获得的理想加速降低了性能。有三类危害：

1. _Structural hazards_ arise from resource conflicts when the hardware cannot sup- port all possible combinations of instructions simultaneously in overlapped exe- cution. In modern processors, structural hazards occur primarily in special purpose functional units that are less frequently used (such as floating point divide or other complex long running instructions). They are not a major per- formance factor, assuming programmers and compiler writers are aware of the lower throughput of these instructions. Instead of spending more time on this infrequent case, we focus on the two other hazards that are much more frequent.

> 1. _结构危害_是资源冲突引起的，当硬件无法同时使用重叠的所有指令组合时。在现代处理器中，结构性危害主要发生在特殊用途的功能单元中，这些功能单位较少使用（例如浮点鸿沟或其他复杂的长期运行说明）。假设程序员和编译器作家知道这些说明的较低吞吐量，它们并不是主要的表现因素。我们没有花更多的时间在这种情况下，而是专注于其他两种危险，这些危害更加频繁。

2. _Data hazards_ arise when an instruction depends on the results of a previous instruction in a way that is exposed by the overlapping of instructions in the pipeline.

> 2.当指令取决于先前指令的结果时，就会出现_data 危害_，以管道中指令重叠而暴露的方式。

3. _Control hazards_ arise from the pipelining of branches and other instructions that change the PC.

> 3. _控制危险_是由分支的管道和其他更改 PC 的说明引起的。

Hazards in pipelines can make it necessary to _stall_ the pipeline. Avoiding a haz- ard often requires that some instructions in the pipeline be allowed to proceed while others are delayed. For the pipelines we discuss in this appendix, when an instruction is stalled, all instructions issued _later_ than the stalled instruction—and hence not as far along in the pipeline—are also stalled. Instructions issued _earlier_ than the stalled instruction—and hence farther along in the pipeline—must continue, because oth- erwise the hazard will never clear. As a result, no new instructions are fetched during the stall. We will see several examples of how pipeline stalls operate in this section— don’t worry, they aren’t as complex as they might sound!

> 管道中的危害可以使管道有必要。避免危险通常要求允许管道中的某些说明进行，而其他说明则延迟。对于我们在本附录中讨论的管道时，当指令停滞时，发出的所有指令都比停滞的指令（因此在管道中不远）也停滞不前。发出的说明比停滞的指令（因此在管道中更远）继续存在，因为危害永远不会清除，因此必须继续进行。结果，在摊位期间没有获取新的说明。我们将看到几个有关管道摊位在本节中运行的示例 - 不用担心，它们并不像听起来那样复杂！

### Performance of Pipelines With Stalls

> ###摊位的管道性能

A stall causes the pipeline performance to degrade from the ideal performance. Let’s look at a simple equation for finding the actual speedup from pipelining, starting with the formula from the previous section:

> 失速导致管道性能从理想性能中降低。让我们看一个简单的方程式，以从上一节的公式开始，从管道上找到实际加速。

Pipelining can be thought of as decreasing the CPI or the clock cycle time. Because it is traditional to use the CPI to compare pipelines, let’s start with that assumption. The ideal CPI on a pipelined processor is almost always 1. Hence, we can compute the pipelined CPI:

> 可以认为管道上的 CPI 或时钟周期时间。因为使用 CPI 比较管道是传统的，所以让我们从该假设开始。管道上的处理器上的理想 CPI 几乎总是 1。因此，我们可以计算管道的 CPI：

If we ignore the cycle time overhead of pipelining and assume that the stages are perfectly balanced, then the cycle time of the two processors can be equal, leading to Speedup CPI unpiplined 1 + Pipeline stall cycles per instruction

> 如果我们忽略管道上的周期时间开销，并假设阶段是完美平衡的，那么两个处理器的周期时间可以相等，从而导致 CPI 脱离了 1 + 管道的 1 + 管道停滞周期

One important simple case is where all instructions take the same number of cycles, which must also equal the number of pipeline stages (also called the _depth of the pipeline_). In this case, the unpipelined CPI is equal to the depth of the pipeline, leading to Speedup Pipeline depth 1 + Pipeline stall cycles per instruction

> 一个重要的简单情况是所有指令都采用相同数量的周期，这也必须等于管道阶段的数量（也称为管道的_ depth）。在这种情况下，脱落的 CPI 等于管道的深度，导致速度管道深度为 1 + 管道停滞周期，每个说明

If there are no pipeline stalls, this leads to the intuitive result that pipelining can improve performance by the depth of the pipeline.

> 如果没有管道摊位，这将导致直观的结果，即管道上可以通过管道深度提高性能。

### Data Hazards

> ###数据危害

A major effect of pipelining is to change the relative timing of instructions by over- lapping their execution. This overlap introduces data and control hazards. Data hazards occur when the pipeline changes the order of read/write accesses to operands so that the order differs from the order seen by sequentially executing instructions on an unpipelined processor. Assume instruction _i_ occurs in program order before instruction _j_ and both instructions use register _x_, then there are three different types of hazards that can occur between _i_ and _j_:

> 管道的主要影响是通过削减执行方式来改变指令的相对时间。这重叠会引入数据和控制危害。当管道更改对操作数的读取/写入访问的顺序时，就会发生数据危害，以使该顺序与通过在不封装处理器上依次执行指令所看到的顺序不同。假设指令_i_在指令_j_之前按程序顺序出现，并且两个指令使用寄存器_x_，然后在_i_和_j_之间可能发生三种不同类型的危险：

1. Read After Write (RAW) hazard: the most common, these occur when a read of register _x_ by instruction _j_ occurs before the write of register _x_ by instruc- tion _i_. If this hazard were not prevented instruction j would use the wrong value of _x_.

> 1.在写（原始）危险后读取：最常见的是，当通过指令读取寄存器_x_的读数时，就会发生这些发生。如果没有阻止这种危险，请使用_x_的错误值。

2. Write After Read (WAR) hazard: this hazard occurs when read of register _x_ by instruction _i_ occurs after a write of register _x_ by instruction _j_. In this case, instruction _i_ would use the wrong value of _x_. WAR hazards are impossible in the simple five stage, integrer pipeline, but they occur when instructions are reordered, as we will see when we discuss dynamically scheduled pipelines beginning on page C.65.

> 2.在阅读后写入（战争）危害：当读取寄存器_x_读取_ _i_时，发生这种危害，发生在写入寄存器_x_ _ _j_后。在这种情况下，指令_i_将使用_x_的错误值。在简单的五个阶段，整合器管道中，战争危害是不可能的，但是当我们重新排序说明时，就会发生这种危害，正如我们将在第 C.65 页上开始动态安排的管道时所看到的那样。

3. Write After Write (WAW) hazard: this hazard occurs when write of register _x_ by instruction _i_ occurs after a write of register _x_ by instruction _j_. When this occurs, register _x_ will have the wrong value going forward. WAR hazards are also impossible in the simple five stage, integrer pipeline, but they occur when instructions are reordered or when running times vary, as we will see later.

> 3.写入后写（waw）危害：当寄存器写入_x_的写入_i_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _j_时，就会发生这种危害。发生这种情况时，寄存器_x_将具有错误的值。在简单的五个阶段（整合器管道）中，战争危害也是不可能的，但是当重新排序或运行时间变化时，它们会发生，正如我们稍后会看到的。

[Chapter 3](#_bookmark93) explores the issues of data dependence and hazards in much more detail. For now, we focus only on RAW hazards.

> [第 3 章]（#_ bookmark93）更详细地探讨了数据依赖和危害的问题。目前，我们只关注原始危害。

![](./media/image501.png)
![](./media/image517.png)

All the instructions after the add use the result of the add instruction. As shown in [Figure C.4](#_bookmark488), the add instruction writes the value of x1 in the WB pipe stage, but the sub instruction reads the value during its ID stage, which results in a RAW hazard. Unless precautions are taken to prevent it, the sub instruction will read the wrong value and try to use it. In fact, the value used by the sub instruction is not even deterministic: though we might think it logical to assume that sub would always use the value of x1 that was assigned by an instruction prior to add, this is not always the case. If an interrupt should occur between the add and sub instructions, the WB stage of the add will complete, and the value of x1 at that point will be the result of the add. This unpredictable behavior is obviously unacceptable.

> 添加后的所有说明都使用添加指令的结果。如[图 C.4]（#_ bookmark488）所示，添加指令在 WB 管阶段写下 x1 的值，但是子指令在其 ID 阶段读取值，从而导致原始危害。除非采取预防措施来防止它，否则子指令将读取错误的价值并尝试使用它。实际上，子指令使用的值甚至都不是确定性的：尽管我们可能认为逻辑假设 SUB 会始终使用在添加之前由指令分配的 X1 值，但并非总是如此。如果应在添加指令和子指令之间发生中断，则添加的 WB 阶段将完成，并且在此点 X1 的值将是添加的结果。这种不可预测的行为显然是不可接受的。

![](./media/image518.png)

Figure C.4 The use of the result of the add instruction in the next three instructions causes a hazard, because the register is not written until after those instructions read it.

> 图 C.4 在接下来的三个说明中使用添加指令的结果的使用会导致危险，因为直到这些说明读取后才写寄存器。

The and instruction also creates a possible RAW hazard. As we can see from [Figure C.4](#_bookmark488), the write of x1 does not complete until the end of clock cycle 5. Thus, the and instruction that reads the registers during clock cycle 4 will receive the wrong results.

> 和指示还会产生可能的原始危害。从[图 C.4]（#_ bookmark488）中可以看到，X1 的写入直到时钟周期的结束。因此，在时钟周期 4 期间读取寄存器的读取寄存器的指令和指令将获得错误的结果。

The xor instruction operates properly because its register read occurs in clock cycle 6, after the register write. The or instruction also operates without incurring a hazard because we perform the register file reads in the second half of the cycle and the writes in the first half. Note that the xor instruction still depends on the add, but it no longer creates a hazard; a topic we explore in more detail in [Chapter 3](#_bookmark93).

> XOR 指令正常运行，因为其寄存器读取发生在寄存器写入后的时钟周期 6 中。或指令还可以在没有危害的情况下进行操作，因为我们执行寄存器文件在周期的下半年进行读取，并且上半年写作。请注意，XOR 指令仍然取决于添加，但不再造成危险。我们在[第 3 章]（#_ bookmark93）中更详细地探讨了一个主题。

The next subsection discusses a technique to eliminate the stalls for the hazard involving the sub and and instructions.

> 下一个小节将讨论一种消除涉及潜艇和说明危险的摊位的技术。

##### _Minimizing Data Hazard Stalls by Forwarding_

> ##### _ throcking _miniminimine thate 危害失速_

The problem posed in [Figure C.4](#_bookmark488) can be solved with a simple hardware technique called _forwarding_ (also called _bypassing_ and sometimes _short-circuiting_). The key insight in forwarding is that the result is not really needed by the sub until after the add actually produces it. If the result can be moved from the pipeline register where the add stores it to where the sub needs it, then the need for a stall can be avoided. Using this observation, forwarding works as follows:

> 可以使用称为_forwarding_（也称为_bypassing_，有时_short-circuiting_）的简单硬件技术来解决[图 C.4]（#_ bookmark488）中提出的问题。转发的关键见解是，在添加后直到添加后才产生后，该结果并不是真正需要的结果。如果可以从添加的管道寄存器将其存储到子需要的位置，则可以避免对摊位的需求。使用此观察结果，转发工作如下：

1. The ALU result from both the EX/MEM and MEM/WB pipeline registers is always fed back to the ALU inputs.

> 1.来自 EX/MEM/MEM/WB 管道寄存器的 ALU 始终回到 ALU 输入中。

2. If the forwarding hardware detects that the previous ALU operation has written the register corresponding to a source for the current ALU operation, control logic selects the forwarded result as the ALU input rather than the value read from the register file.

> 2.如果转发硬件检测到上一个 ALU 操作是否编写了与当前 ALU 操作源相对应的寄存器，则控制逻辑将转发的结果选择为 ALU 输入，而不是从寄存器文件中读取的值。

Notice that with forwarding, if the sub is stalled, the add will be completed and the bypass will not be activated. This relationship is also true for the case of an interrupt between the two instructions.

> 请注意，如果转发，如果子停滞，则添加程序将完成，并且旁路将不会被激活。对于两个说明之间中断的情况也是如此。

As the example in [Figure C.4](#_bookmark488) shows, we need to forward results not only from the immediately previous instruction but also possibly from an instruction that started two cycles earlier. [Figure C.5](#_bookmark489) shows our example with the bypass paths in place and highlighting the timing of the register read and writes. This code sequence can be executed without stalls.

> 正如[图 C.4]（#_ bookmark488）所示的示例所示，我们不仅需要从以前的指令中转发结果，而且还需要从早些时候开始两个周期的指令中转发结果。[图 C.5]（#_ bookmark489）显示了我们的示例，其中包含旁路路径，并突出显示寄存器的时机读取和写入。可以在没有失速的情况下执行此代码序列。

Forwarding can be generalized to include passing a result directly to the func- tional unit that requires it: a result is forwarded from the pipeline register corre- sponding to the output of one unit to the input of another, rather than just from

> 可以推广转发以包括将结果直接传递给需要它的函数单元：结果是从管道寄存器调节到一个单元的输出到另一个单元的输入的，而不是从另一个单元的输入中转发

![](./media/image519.png)
![](./media/image536.png)
![](./media/image538.png)
![](./media/image539.png)

Figure C.5 A set of instructions that depends on the add result uses forwarding paths to avoid the data hazard. The inputs for the sub and and instructions forward from the pipeline registers to the first ALU input. The or receives its result by forwarding through the register file, which is easily accomplished by reading the registers in the second half of the cycle and writing in the first half, as the dashed lines on the registers indicate. Notice that the forwarded result can go to either ALU input; in fact, both ALU inputs could use forwarded inputs from either the same pipeline register or from different pipeline registers. This would occur, for example, if the and instruction was and x6,x1,x4.

> 图 C.5 一组取决于添加结果的指令使用转发路径来避免数据危险。从管道寄存器到第一个 ALU 输入的子和指令的输入。OR 通过转发寄存器文件而收到的结果，该文件可以通过在周期的下半部分阅读寄存器并在上半场写入来完成，如寄存器上的虚线所示。请注意，转发结果可以转到任何一个 ALU 输入；实际上，两个 ALU 输入都可以使用来自同一管道寄存器或不同管道寄存器的转发输入。例如，如果是和 x6，x1，x4，则会发生这种情况。

To prevent a stall in this sequence, we would need to forward the values of the ALU output and memory unit output from the pipeline registers to the ALU and data memory inputs. [Figure C.6](#_bookmark490) shows all the forwarding paths for this example.

> 为了防止此序列中的失速，我们需要将 Alu 输出和内存单元输出的值从管道寄存器转发到 ALU 和数据存储器输入。[图 C.6]（#_ bookmark490）显示了此示例的所有转发路径。

![](./media/image540.png)
![](./media/image541.png)
![](./media/image556.png)

Figure C.6 Forwarding of operand required by stores during MEM. The result of the load is forwarded from the memory output to the memory input to be stored. In addition, the ALU output is forwarded to the ALU input for the address calculation of both the load and the store (this is no different than forwarding to another ALU operation). If the store depended on an immediately preceding ALU operation (not shown herein), the result would need to be forwarded to prevent a stall.

> 图 C.6 MEM 期间商店所需的操作数转发。负载的结果从内存输出转发到要存储的内存输入。此外，将 ALU 输出转发到 ALU 输入以进行负载和商店的地址计算（这与转发到另一个 ALU 操作没有什么不同）。如果商店取决于在 ALU 操作之前的立即（未显示），则需要转发结果以防止失速。

##### _Data Hazards Requiring Stalls_

> ##### _data 危害需要失速_

Unfortunately, not all potential data hazards can be handled by bypassing. Con- sider the following sequence of instructions:

> 不幸的是，并非所有潜在的数据危害都可以通过绕过来处理。考虑以下指令序列：

The pipelined data path with the bypass paths for this example is shown in [Figure C.7](#_bookmark491). This case is different from the situation with back-to-back ALU oper- ations. The ld instruction does not have the data until the end of clock cycle 4 (its MEM cycle), while the sub instruction needs to have the data by the beginning of that clock cycle. Thus, the data hazard from using the result of a load instruction cannot be completely eliminated with simple hardware. As [Figure C.7](#_bookmark491) shows, such a forwarding path would have to operate backward in time—a capability not yet available to computer designers! We _can_ forward the result immediately to the ALU from the pipeline registers for use in the and operation, which begins 2 clock cycles after the load. Likewise, the or instruction has no problem, because it receives the value through the register file. For the sub instruction, the forwarded result arrives too late—at the end of a clock cycle, when it is needed at the beginning.

> [图 C.7]（#_ bookmark491）中显示了带有旁路路径的管道数据路径。这种情况与背靠背 ALU 运营的情况不同。直到时钟周期 4（其 MEM 周期）结束时，LD 指令才具有数据，而子指令需要在该时钟周期的开始之前具有数据。因此，使用简单硬件无法完全消除使用负载指令结果的数据危害。如[图 C.7]（#_ bookmark491）所示，这样的转发路径必须在及时向后操作 - 计算机设计人员尚不可用的功能！我们_CAN_将结果立即从管道寄存器中转发到 ALU，以便在加载后开始 2 个时钟周期。同样，或说明没有问题，因为它通过寄存器文件接收值。对于子指令，转发结果到达太晚了 - 在开始时时钟周期结束时，在开始时需要。

![](./media/image557.png)
![](./media/image572.png)
![](./media/image573.png)

Figure C.7 The load instruction can bypass its results to the and and or instructions, but not to the sub, because that would mean forwarding the result in “negative time.”

> 图 C.7 负载指令可以将其结果绕过和和 /或指示，但不能向潜艇绕过，因为这意味着在“负时间”中转发结果。

The load instruction has a delay or latency that cannot be eliminated by for- warding alone. Instead, we need to add hardware, called a _pipeline interlock_, to preserve the correct execution pattern. In general, a pipeline interlock detects a haz- ard and stalls the pipeline until the hazard is cleared. In this case, the interlock stalls the pipeline, beginning with the instruction that wants to use the data until the source instruction produces it. This pipeline interlock introduces a stall or bubble, just as it did for the structural hazard. The CPI for the stalled instruction increases by the length of the stall (1 clock cycle in this case).

> 负载指令具有延迟或延迟，不能单独使用延迟。相反，我们需要添加称为_PIPLINE Interlock_的硬件，以保留正确的执行模式。通常，管道互锁会检测到一个危险，并使管道失速，直到清除危险为止。在这种情况下，互锁将管道失速，从想要使用数据的指令开始，直到源指令产生它为止。这条管道互锁会引入摊位或气泡，就像对结构危险一样。停滞指令的 CPI 增加了失速的长度（在这种情况下为 1 个时钟周期）。

[Figure C.8](#_bookmark492) shows the pipeline before and after the stall using the names of the pipeline stages. Because the stall causes the instructions starting with the sub to move one cycle later in time, the forwarding to the and instruction now goes through the register file, and no forwarding at all is needed for the or instruction. The insertion of the bubble causes the number of cycles to complete this sequence to grow by one. No instruction is started during clock cycle 4 (and none finishes during cycle 6).

> [图 C.8]（#_ bookmark492）使用管道阶段的名称显示失速前后的管道。由于摊位从子开始后稍后移动一个周期，因此转发到该指令，因此转发到现在通过寄存器文件，并且根本不需要转发或指令。气泡的插入导致循环的数量完成该序列的生长。在时钟周期 4 期间没有开始指导（在第 6 周期期间没有任何指示）。

Figure C.8 In the top half, we can see why a stall is needed: the MEM cycle of the load produces a value that is needed in the EX cycle of the sub, which occurs at the same time. This problem is solved by inserting a stall, as shown in the bottom half.

> 图 C.8 在上半部分，我们可以看到为什么需要一个档位：负载的 MEM 周期会产生一个同时发生的子循环中所需的值。如下半部分所示，通过插入摊位来解决此问题。

### Branch Hazards

> ###分支危害

_Control hazards_ can cause a greater performance loss for our RISC V pipeline than do data hazards. When a branch is executed, it may or may not change the PC to something other than its current value plus 4. Recall that if a branch changes the PC to its target address, it is a _taken_ branch; if it falls through, it is _not taken_, or _untaken_. If instruction _i_ is a taken branch, then the PC is usually not chan- ged until the end of ID, after the completion of the address calculation and comparison.

> _控制危险_可能会使我们的 RISC V 管道造成更大的性能损失，而不是数据危害。执行分支时，它可能会或可能不会将 PC 更改为其当前值以外的其他功能。请回想一下，如果分支将 PC 更改为其目标地址，则是_taken_分支；如果它掉落了，则是_不摄取的_或_untaken_。如果指令_i_是一个分支，则在完成地址计算和比较之后，PC 通常直到 ID 结束之前才能进行。

[Figure C.9](#_bookmark493) shows that the simplest method of dealing with branches is to redo the fetch of the instruction following a branch, once we detect the branch during ID (when instructions are decoded). The first IF cycle is essentially a stall, because it never performs useful work. You may have noticed that if the branch is untaken, then the repetition of the IF stage is unnecessary because the correct instruction was indeed fetched. We will develop several schemes to take advantage of this fact shortly.

> [图 C.9]（#_ bookmark493）表明，一旦我们在 ID 期间检测到分支（当指令解码时），处理分支的最简单方法是重做分支后的指令的获取。第一个如果周期本质上是一个摊位，因为它从未执行有用的工作。您可能已经注意到，如果分支未被遗忘，则不必重复 if 阶段，因为确实得到了正确的指示。我们将制定几个方案以不久利用这一事实。

One stall cycle for every branch will yield a performance loss of 10% to 30% depending on the branch frequency, so we will examine some techniques to deal with this loss.

> 每个分支机构的一个失速周期将根据分支频率产生 10％至 30％的性能损失，因此我们将检查一些处理此损失的技术。

Figure C.9 A branch causes a one-cycle stall in the five-stage pipeline. The instruction after the branch is fetched, but the instruction is ignored, and the fetch is restarted once the branch target is known. It is probably obvious that if the branch is not taken, the second IF for branch successor is redundant. This will be addressed shortly.

> 图 C.9 A 分支在五阶段管道中导致单周期失速。提取分支后的指令，但忽略了指令，一旦已知分支目标，就可以重新启动提取。很明显，如果不采用分支，则第二个分支是继任者是多余的。这将很快解决。

##### _Reducing Pipeline Branch Penalties_

> ##### _reducing 管道分支机构罚款_

There are many methods for dealing with the pipeline stalls caused by branch delay; we discuss four simple compile time schemes in this subsection. In these four schemes the actions for a branch are static—they are fixed for each branch during the entire execution. The software can try to minimize the branch penalty using knowledge of the hardware scheme and of branch behavior. We will then look at hardware-based schemes that dynamically predict branch behavior, and [Chapter 3](#_bookmark93) looks at more powerful hardware techniques for dynamic branch prediction.

> 有许多方法可以处理由分支延迟引起的管道摊位；我们在本小节中讨论了四个简单的编译时间方案。在这四个方案中，分支的动作是静态的，它们在整个执行过程中为每个分支固定。该软件可以尝试使用硬件方案和分支行为的知识来最大程度地减少分支机构的惩罚。然后，我们将研究动态预测分支行为的基于硬件的方案，[第 3 章]（#_ bookmark93）着眼于动态分支预测的更强大的硬件技术。

The simplest scheme to handle branches is to _freeze_ or _flush_ the pipeline, holding or deleting any instructions after the branch until the branch destination is known. The attractiveness of this solution lies primarily in its simplicity both for hardware and software. It is the solution used earlier in the pipeline shown in [Figure C.9](#_bookmark493). In this case, the branch penalty is fixed and cannot be reduced by software.

> 处理分支的最简单方案是_freeze_或_flush_管道，保留或删除分支之后的任何指令，直到已知分支目标为止。该解决方案的吸引力主要在于硬件和软件的简单性。这是[图 C.9]（#_ bookmark493）所示的管道中使用的解决方案。在这种情况下，分支机构是固定的，无法通过软件减少。

A higher-performance, and only slightly more complex, scheme is to treat every branch as not taken, simply allowing the hardware to continue as if the branch were not executed. Here, care must be taken not to change the processor state until the branch outcome is definitely known. The complexity of this scheme arises from having to know when the state might be changed by an instruction and how to “back out” such a change.

> 较高性能，只有稍微复杂的方案是将每个分支视为未采用的每个分支，只是允许硬件继续，就好像没有执行分支一样。在这里，必须注意不要更改处理器状态，直到肯定已知分支结果为止。该方案的复杂性是由于必须知道何时可以通过指令改变状态以及如何“退出”这种变化。

In the simple five-stage pipeline, this _predicted-not-taken_ or _predicted-untaken_ scheme is implemented by continuing to fetch instructions as if the branch were a normal instruction. The pipeline looks as if nothing out of the ordinary is happen- ing. If the branch is taken, however, we need to turn the fetched instruction into a no-op and restart the fetch at the target address. [Figure C.10](#_bookmark494) shows both situations. An alternative scheme is to treat every branch as taken. As soon as the branch is decoded and the target address is computed, we assume the branch to be taken and begin fetching and executing at the target. This buys us a one-cycle improvement when the branch is actually taken, because we know the target address at the end of ID, one cycle before we know whether the branch condition is satisfied in the ALU stage. In either a predicted-taken or predicted-not-taken scheme, the compiler can improve performance by organizing the code so that the most frequent path matches the hardware’s choice.

> 在简单的五阶段管道中，通过继续获取指令，就好像分支是正常的指令，可以实现此_ predifted-not-taken_或_predistic-untaken_方案。该管道看起来好像没有什么与众不同。但是，如果采取分支，我们需要将获取的指令转换为 no-op，并在目标地址重新启动获取。[图 C.10]（#_ bookmark494）显示了这两种情况。另一种方案是将每个分支视为所采用。分支被解码并计算目标地址后，我们假设要采用该分支并开始在目标上获取和执行。当我们实际采用分支机构时，这为我们提供了一个单周的改进，因为我们知道 ID 末尾的目标地址，一个周期，然后我们知道分支条件是否在 ALU 阶段满足。在预测的或预测的未捕获的方案中，编译器可以通过组织代码来提高性能，从而使最频繁的路径与硬件的选择相匹配。

Figure C.10 The predicted-not-taken scheme and the pipeline sequence when the branch is untaken (top) and taken (bottom). When the branch is untaken, determined during ID, we fetch the fall-through and just continue. If the branch is taken during ID, we restart the fetch at the branch target. This causes all instructions following the branch to stall 1 clock cycle.

> 图 C.10 当分支未被遗忘（顶部）和（底部）时，预测未捕获的方案和管道序列。当分支未被遗忘时，在 ID 期间确定，我们将获取秋季的临时，然后继续。如果在 ID 期间拍摄分支，我们将重新启动分支目标的提取。这会导致分支之后的所有说明到失速 1 时钟周期。

A fourth scheme, which was heavily used in early RISC processors is called _delayed branch_. In a delayed branch, the execution cycle with a branch delay of one is branch instruction sequential successor<sub>1</sub> branch target if taken

> 在早期 RISC 处理器中大量使用的第四个方案称为_删除分支_。在延迟的分支中，具有一个分支延迟的执行周期是分支指令顺序连续<sub> 1 </sub>分支目标如果采用

The sequential successor is in the _branch delay slot_. This instruction is executed whether or not the branch is taken. The pipeline behavior of the five-stage pipeline with a branch delay is shown in [Figure C.11](#_bookmark495). Although it is possible to have a branch delay longer than one, in practice almost all processors with delayed branch have a single instruction delay; other techniques are used if the pipeline has a lon- ger potential branch penalty.The job of the compiler is to make the successor instructions valid and useful.

> 顺序继任者在_ranth 延迟插槽中。是否采用分支，执行此指令。[图 C.11]（#_ bookmark495）显示了具有分支延迟的五阶段管道的管道行为。尽管可能的分支延迟时间长于一个，但实际上，几乎所有具有延迟分支的处理器都有一个指令延迟。如果管道有潜在的分支机构罚款，则使用其他技术。编译器的工作是使后继指令有效且有用。

Although the delayed branch was useful for short simple pipelines at a time when hardware prediction was too expensive, the technique complicates imple- mentation when there is dynamic branch prediction. For this reason, RISC V appropriately omitted delayed branches.

> 尽管在硬件预测过于昂贵的时候，延迟分支对于简单的管道很有用，但是当有动态分支预测时，该技术会使技术变得复杂。因此，RISC V 适当省略了延迟分支机构。

Figure C.11 The behavior of a delayed branch is the same whether or not the branch is taken. The instructions in the delay slot (there was only one delay slot for most RISC architectures that incorporated them) are executed. If the branch is untaken, execution continues with the instruction after the branch delay instruction; if the branch is taken, execution continues at the branch target. When the instruction in the branch delay slot is also a branch, the meaning is unclear: if the branch is not taken, what should happen to the branch in the branch delay slot? Because of this confusion, architectures with delay branches often disallow putting a branch in the delay slot.

> 图 C.11 延迟分支的行为是相同的，无论是否采用分支。执行了延迟插槽中的说明（大多数 RISC 架构都只有一个延迟插槽）。如果分支未被遗忘，则在分支延迟指令后继续执行指令；如果采取分支，则在分支目标处继续执行。当分支延迟插槽中的指令也是一个分支时，含义不清楚：如果未采取分支，分支延迟插槽中的分支应该会发生什么？由于这种混乱，带有延迟分支的体系结构通常禁止将分支放在延迟插槽中。

##### _Performance of Branch Schemes_

> ##### _分支方案的表现

What is the effective performance of each of these schemes? The effective pipeline speedup with branch penalties, assuming an ideal CPI of 1, is

> 这些方案中的每一个的有效性能是什么？假设理想的 CPI 为 1，则有效的分支机构加速管道加速度为

The branch frequency and branch penalty can have a component from both uncon- ditional and conditional branches. However, the latter dominate because they are more frequent.

> 分支频率和分支惩罚可以具有来自无条件和条件分支的组成部分。但是，后者占主导地位，因为它们更频繁。

Example For a deeper pipeline, such as that in a MIPS R4000 and later RISC processors, it takes at least three pipeline stages before the branch-target address is known and an additional cycle before the branch condition is evaluated, assuming no stalls on the registers in the conditional comparison. A three-stage delay leads to the branch penalties for the three simplest prediction schemes listed in [Figure C.12](#_bookmark496).

> 较深的管道的示例，例如在 MIPS R4000 和后来的 RISC 处理器中，它至少需要三个管道阶段，然后才知道分支目标地址，并且在评估分支条件之前是一个额外的循环，假设在寄存器上没有失速在条件比较中。三阶段的延迟导致[图 C.12]中列出的三个最简单的预测方案（#_ bookmark496）中列出分支机构罚款。

Find the effective addition to the CPI arising from branches for this pipeline, assuming the following frequencies:

> 假设以下频率，请找到该管道分支的 CPI 的有效添加：

Figure C.12 Branch penalties for the three simplest prediction schemes for a deeper pipeline.

> 图 C.12 针对更深层管道的三种最简单预测方案的分支处罚。

Figure C.13 CPI penalties for three branch-prediction schemes and a deeper pipeline.

> 图 C.13 CPI 对三个分支预测方案和更深层管道的 CPI 处罚。

The differences among the schemes are substantially increased with this longer delay. If the base CPI were 1 and branches were the only source of stalls, the ideal pipeline would be 1.56 times faster than a pipeline that used the stall-pipeline scheme. The predicted-untaken scheme would be 1.13 times better than the stall-pipeline scheme under the same assumptions.

> 随着较长的延迟，方案之间的差异大大增加。如果基本 CPI 为 1，并且分支是摊位的唯一来源，则理想管道的速度将比使用 Stall-Pipeline 方案的管道快 1.56 倍。在相同假设下，预测的未公开方案将是摊位置脚步方案的 1.13 倍。

### Reducing the Cost of Branches Through Prediction

> ###通过预测降低分支机构的成本

As pipelines get deeper and the potential penalty of branches increases, using delayed branches and similar schemes becomes insufficient. Instead, we need to turn to more aggressive means for predicting branches. Such schemes fall into two classes: low-cost static schemes that rely on information available at compile time and strategies that predict branches dynamically based on program behavior. We discuss both approaches here.

> 随着管道变得更深，分支的潜在惩罚增加，使用延迟的分支和类似方案变得不足。相反，我们需要转向预测分支机构的更积极的手段。这些方案分为两类：低成本静态方案，这些方案依赖于在编译时间和基于程序行为动态预测分支的策略上可用的信息。我们在这里讨论两种方法。

### Static Branch Prediction

> ###静态分支预测

A key way to improve compile-time branch prediction is to use profile information collected from earlier runs. The key observation that makes this worthwhile is that the behavior of branches is often bimodally distributed; that is, an individual branch is often highly biased toward taken or untaken. [Figure C.14](#_bookmark498) shows the suc- cess of branch prediction using this strategy. The same input data were used for runs and for collecting the profile; other studies have shown that changing the input so that the profile is for a different run leads to only a small change in the accuracy of profile-based prediction.

> 改善编译时间分支预测的关键方法是使用从早期运行中收集的配置文件信息。使这一值得的关键观察是，分支的行为通常是双峰分布的。也就是说，单个分支通常会高度偏向于采取或未被取消。[图 C.14]（#_ bookmark498）使用此策略显示了分支预测的成功。相同的输入数据用于运行和收集配置文件。其他研究表明，更改输入，以使轮廓适用于不同的运行，这仅导致基于配置文件预测的准确性的微小变化。

The effectiveness of any branch prediction scheme depends both on the accu- racy of the scheme and the frequency of conditional branches, which vary in SPEC from 3% to 24%. The fact that the misprediction rate for the integer programs is higher and such programs typically have a higher branch frequency is a major lim- itation for static branch prediction. In the next section, we consider dynamic branch predictors, which most recent processors have employed.

> 任何分支预测方案的有效性都取决于该方案的准确性和条件分支的频率，该规格从 3％到 24％不等。整数程序的错误预测率更高，并且此类程序通常具有较高的分支频率，这是静态分支预测的主要限制。在下一节中，我们考虑了最新处理器采用的动态分支预测因子。

Figure C.14 Misprediction rate on SPEC92 for a profile-based predictor varies widely but is generally better for the floating-point programs, which have an average mis- prediction rate of 9% with a standard deviation of 4%, than for the integer programs, which have an average misprediction rate of 15% with a standard deviation of 5%. The actual performance depends on both the prediction accuracy and the branch fre- quency, which vary from 3% to 24%.

> 图 C.14 基于配置文件的预测指标的 Spec92 的错误预测率差异很大，但通常对于浮点程序，其平均错误预测率为 9％，标准偏差为 4％，而不是整数计划的平均错误预测率为 15％，标准偏差为 5％。实际绩效取决于预测准确性和分支频率，从 3％到 24％不等。

### Dynamic Branch Prediction and Branch-Prediction Buffers

> ###动态分支预测和分支预测缓冲区

The simplest dynamic branch-prediction scheme is a _branch-prediction buffer_ or _branch history table_. A branch-prediction buffer is a small memory indexed by the lower portion of the address of the branch instruction. The memory contains a bit that says whether the branch was recently taken or not. This scheme is the simplest sort of buffer; it has no tags and is useful only to reduce the branch delay when it is longer than the time to compute the possible target PCs.

> 最简单的动态分支预测方案是_Branch 预测缓冲区或_Branch 历史记录 table_。分支预测缓冲区是由分支指令地址的下部索引的小内存。记忆中包含一些说明该分支最近是否已采用的记忆。该方案是最简单的缓冲区。它没有标签，仅在比计算可能的目标 PC 的时间更长的时间时减少分支延迟。

With such a buffer, we don’t know, in fact, if the prediction is correct—it may have been put there by another branch that has the same low-order address bits. But this doesn’t matter. The prediction is a hint that is assumed to be correct, and fetch- ing begins in the predicted direction. If the hint turns out to be wrong, the predic- tion bit is inverted and stored back.

> 实际上，有了这样的缓冲区，我们不知道预测是正确的，它可能是由另一个具有相同低阶地址位的分支机构放置的。但这没关系。预测是一个假定正确的提示，而获取始于预测的方向。如果提示被证明是错误的，则谓词位会倒置并存储回去。

This buffer is effectively a cache where every access is a hit, and, as we will see, the performance of the buffer depends on both how often the prediction is for the branch of interest and how accurate the prediction is when it matches. Before we analyze the performance, it is useful to make a small, but important, improvement in the accuracy of the branch-prediction scheme.

> 该缓冲区实际上是一个缓存，每个访问都是命中率，正如我们将看到的，缓冲区的性能取决于预测的频率以及预测匹配时的预测程度。在分析性能之前，对分支预测方案的准确性进行小但重要但重要的改善很有用。

This simple 1-bit prediction scheme has a performance shortcoming: even if a branch is almost always taken, we will likely predict incorrectly twice, rather than once, when it is not taken, because the misprediction causes the prediction bit to be flipped.

> 这个简单的 1 位预测方案具有性能缺点：即使几乎总是在分支上占用分支，我们可能会预测错误的两次，而不是一次不进行时，因为错误预测会导致预测位的额定值。

To remedy this weakness, 2-bit prediction schemes are often used. In a 2-bit scheme, a prediction must miss twice before it is changed. [Figure C.15](#_bookmark499) shows the finite-state processor for a 2-bit prediction scheme.

> 为了解决这种弱点，经常使用 2 位预测方案。在 2 位方案中，预测在更改之前必须错过两次。[图 C.15]（#_ bookmark499）显示了 2 位预测方案的有限状态处理器。

A branch-prediction buffer can be implemented as a small, special “cache” accessed with the instruction address during the IF pipe stage, or as a pair of bits attached to each block in the instruction cache and fetched with the instruction. If the instruction is decoded as a branch and if the branch is predicted as taken, fetch- ing begins from the target as soon as the PC is known. Otherwise, sequential fetch- ing and executing continue. As [Figure C.15](#_bookmark499) shows, if the prediction turns out to be wrong, the prediction bits are changed.

> 分支预测缓冲区可以作为在 IF 管道阶段的指令地址访问的小型，特殊的“缓存”，或者作为指令缓存中每个块附加到每个块上的一对位，并随指令获取。如果将指令解码为分支，并且如果将分支预测为所采用的分支，则一旦已知 PC，就可以从目标开始。否则，将继续进行顺序提取和执行。如[图 C.15]（#_ bookmark499）所示，如果预测错误，则预测位会更改。

What kind of accuracy can be expected from a branch-prediction buffer using 2 bits per entry on real applications? [Figure C.16](#_bookmark500) shows that for the SPEC89 bench- marks a branch-prediction buffer with 4096 entries results in a prediction accuracy ranging from over 99% to 82%, or a _misprediction rate_ of 1%–18%. A 4K entry buffer, like that used for these results, is considered small in 2017, and a larger buffer could produce somewhat better results.

> 在实际应用程序上，使用每个条目 2 位可以从分支预测缓冲区中获得什么样的精度？[图 C.16]（#_ bookmark500）表明，对于 Spec89，标记具有 4096 条条目的分支预测缓冲区的预测准确性从 99％以上到 82％，或_misprediction redication rative Rative _> 1％–18％。像这些结果一样使用的 4K 进入缓冲液在 2017 年被认为很小，更大的缓冲区可能会产生更好的结果。

![](./media/image574.png)

Figure C.15 The states in a 2-bit prediction scheme. By using 2 bits rather than 1, a branch that strongly favors taken or not taken—as many branches do—will be mispre- dicted less often than with a 1-bit predictor. The 2 bits are used to encode the four states in the system. The 2-bit scheme is actually a specialization of a more general scheme that has an _n_-bit saturating counter for each entry in the prediction buffer. With an _n_-bit counter, the counter can take on values between 0 and 2*<sup>n</sup>* 1: when the counter is greater than or equal to one-half of its maximum value (2*<sup>n</sup>* 1), the branch is pre- dicted as taken; otherwise, it is predicted as untaken. Studies of _n_-bit predictors have shown that the 2-bit predictors do almost as well, thus most systems rely on 2-bit branch predictors rather than the more general _n_-bit predictors.

> 图 C.15 2 位预测方案中的状态。通过使用 2 位而不是 1 位，与 1 位预测指标相比，一个强烈偏爱或不采用的分支（许多分支机构确实如此）的频率要少。这两个位用于编码系统中的四个状态。2 位方案实际上是一个更通用的方案的专业化，该方案在预测缓冲区中的每个条目中都具有_n_-tatusing Counter。使用_n_-bit 计数器，计数器可以在 0 和 2*<sup> n </sup>*1 之间进行值，当计数器大于或等于其最大值的一半时（2*<sup> n </sup>* 1），该分支被预先定为采用；否则，它被预测为未被遗忘。_n_-bit 预测因子的研究表明，2 位预测变量也几乎也是如此，因此大多数系统都依赖于 2 位分支预测指标，而不是更通用的_n_-bit 预测指标。

Figure C.16 Prediction accuracy of a 4096-entry 2-bit prediction buffer for the SPEC89 benchmarks. The misprediction rate for the integer benchmarks (gcc, espresso, eqntott, and li) is substantially higher (average of 11%) than that for the floating-point programs (average of 4%). Omitting the floating-point kernels (nasa7, matrix300, and tomcatv) still yields a higher accuracy for the FP benchmarks than for the integer bench- marks. These data, as well as the rest of the data in this section, are taken from a branch- prediction study done using the IBM Power architecture and optimized code for that system. See [Pan et al. (1992)](#_bookmark987). Although these data are for an older version of a subset of the SPEC benchmarks, the newer benchmarks are larger and would show slightly worse behavior, especially for the integer benchmarks.

> 图 C.16 SPEC89 基准的 4096-Entry 2 位预测缓冲液的预测精度。整数基准（GCC，Espresso，Eqntott 和 Li）的错误预测率明显高于浮点计划（平均 4％）的错误（平均 11％）。省略浮点核（NASA7，Matrix300 和 Tomcatv）仍然比整数基准标记具有更高的 FP 基准测试精度。这些数据以及本节中的其余数据均取自使用 IBM 功率体系结构和该系统优化代码进行的分支预测研究。参见[Pan 等。（1992）]（#_ bookmark987）。尽管这些数据适用于 SPEC 基准的较旧版本，但较新的基准测试较大，并且表现出较差的行为，尤其是对于整数基准测试。

As we try to exploit more ILP, the accuracy of our branch prediction becomes critical. As we can see in [Figure C.16](#_bookmark500), the accuracy of the predictors for integer programs, which typically also have higher branch frequencies, is lower than for the loop-intensive scientific programs. We can attack this problem in two ways: by increasing the size of the buffer and by increasing the accuracy of the scheme we use for each prediction. A buffer with 4K entries, however, as [Figure C.17](#_bookmark501) shows, performs quite comparably to an infinite buffer, at least for benchmarks like those in SPEC. The data in [Figure C.17](#_bookmark501) make it clear that the hit rate of the buffer is not the major limiting factor. As we mentioned, simply increasing the number of bits per predictor without changing the predictor structure also has little impact. Instead, we need to look at how we might increase the accuracy of each predictor, as we will in [Chapter 3](#_bookmark93).

> 当我们尝试利用更多 ILP 时，分支预测的准确性变得至关重要。正如我们在[图 C.16]（#_ bookmark500）中可以看到的那样，整数程序的预测因子的准确性（通常也具有较高的分支频率）低于循环密集型科学程序。我们可以通过两种方式攻击这个问题：通过增加缓冲区的大小，并提高我们为每个预测使用的方案的准确性。但是，具有 4K 条目的缓冲区，如[图 C.17]（#_ Bookmark501）所示，至少对于 Spec 中的基准，至少对于像标准的基准相当可观。[图 C.17]（#_ Bookmark501）中的数据清楚地表明，缓冲区的命中率不是主要限制因素。正如我们提到的那样，简单地增加每个预测变量的位数而不更改预测变量结构也没有影响。取而代之的是，我们需要研究如何提高每个预测变量的准确性，就像[第 3 章]（#_ bookmark93）中一样。

Figure C.17 Prediction accuracy of a 4096-entry 2-bit prediction buffer versus an infinite buffer for the SPEC89 benchmarks. Although these data are for an older version of a subset of the SPEC benchmarks, the results would be comparable for newerversions with perhaps as many as 8K entries needed to match aninfinite 2-bitpredictor.

> 图 C.17 4096-Entry 2 位预测缓冲液的预测准确性与 SPEC89 基准的无限缓冲区的预测精度。尽管这些数据适用于规格基准的子集的较旧版本，但结果对于匹配 Aninfinite 2-BiteDictor 所需的 8K 条目的新 versions 将是可比的。
