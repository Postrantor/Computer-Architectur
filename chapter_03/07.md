## Exploiting ILP Using Multiple Issue and Static Scheduling

> ##使用多个问题和静态调度来利用 ILP

The techniques of the preceding sections can be used to eliminate data, control stalls, and achieve an ideal CPI of one. To improve performance further, we want to decrease the CPI to less than one, but the CPI cannot be reduced below one if we issue only one instruction every clock cycle.

> 前面部分的技术可用于消除数据，控制失速并获得一个理想的 CPI。为了进一步提高性能，我们希望将 CPI 降低到不到一个，但是如果我们在每个时钟周期中仅发布一份指令，则 CPI 不能低于一个。

The goal of the _multiple-issue processors_, discussed in the next few sections, is to allow multiple instructions to issue in a clock cycle. Multiple-issue processors come in three major flavors:

> 在接下来的几节中讨论的\_multipy-Issue 处理器的目标是允许在时钟周期中发出多个指令。多发处理器有三种主要口味：

1. Statically scheduled superscalar processors

> 1.静态安排的超量表处理器

2. VLIW (very long instruction word) processors

> 2. VLIW（非常长的指令词）处理器

3. Dynamically scheduled superscalar processors

> 3.动态安排的超量表处理器

The two types of superscalar processors issue varying numbers of instructions per clock and use in-order execution if they are statically scheduled or out-of-order execution if they are dynamically scheduled.

> SuperScalar 处理器的两种类型的指令数量不同，如果他们是静态计划的，或者如果它们是动态的，则使用固定执行。

VLIW processors, in contrast, issue a fixed number of instructions formatted either as one large instruction or as a fixed instruction packet with the parallelism among instructions explicitly indicated by the instruction. VLIW processors are inherently statically scheduled by the compiler. When Intel and HP created the IA-64 architecture, described in Appendix H, they also introduced the name EPIC (explicitly parallel instruction computer) for this architectural style.

> 相比之下，VLIW 处理器发布了固定数量的指令，将其格式化为一个大型说明，或作为固定的指令数据包，并在指令中明确指示的指令中具有并行性。VLIW 处理器本质上是由编译器静态安排的。当英特尔和 HP 创建了附录 H 中描述的 IA-64 体系结构时，他们还为此架构样式介绍了 Epic（明确平行指令计算机）的名称。

Although statically scheduled superscalars issue a varying rather than a fixed number of instructions per clock, they are actually closer in concept to VLIWs because both approaches rely on the compiler to schedule code for the processor. Because of the diminishing advantages of a statically scheduled superscalar as the issue width grows, statically scheduled superscalars are used primarily for narrow issue widths, normally just two instructions. Beyond that width, most designers choose to implement either a VLIW or a dynamically scheduled superscalar. Because of the similarities in hardware and required compiler technology, we focus on VLIWs in this section, and we will see them again in [Chapter 7](#_bookmark322). The insights of this section are easily extrapolated to a statically scheduled superscalar.

> 尽管按静态计划的超级标准发布了不同的，而不是每个时钟固定数量的指令，但实际上它们在概念上更接近 VLIW，因为两种方法都依赖编译器来为处理器安排代码。由于随着发行宽度的增长，静态计划的超级标准的优势降低，因此静态计划的超量表主要用于狭窄的发行宽度，通常仅为两个说明。除了该宽度之外，大多数设计师选择实现 VLIW 或动态计划的超级标准。由于硬件和所需编译器技术的相似之处，我们将重点放在本节中的 VLIW 上，我们将在[第 7 章]（#\_ bookmark322）中再次看到它们。本节的见解很容易被外推到静态计划的超尺度。

[Figure 3.19](#_bookmark120) summarizes the basic approaches to multiple issue and their distinguishing characteristics and shows processors that use each approach.

> [图 3.19]（#\_ bookmark120）总结了多个问题及其区分特征的基本方法，并显示了使用每种方法的处理器。

### The Basic VLIW Approach

> ###基本 VLIW 方法

VLIWs use multiple, independent functional units. Rather than attempting to issue multiple, independent instructions to the units, a VLIW packages the multiple operations into one very long instruction or requires that the instructions in the issue packet satisfy the same constraints. Because there is no fundamental difference in the two approaches, we will just assume that multiple operations are placed in one instruction, as in the original VLIW approach.

> VLIWS 使用多个独立的功能单元。VLIW 将多个操作包装成一个很长的指令，或者要求问题包中的指令满足相同的约束。由于这两种方法没有根本的差异，因此我们只是假设像原始 VLIW 方法一样，将多个操作放在一个指令中。

> Figure 3.19 The five primary approaches in use for multiple-issue processors and the primary characteristics that distinguish them. This chapter has focused on the hardware-intensive techniques, which are all some form of superscalar. Appendix H focuses on compiler-based approaches. The EPIC approach, as embodied in the IA-64 architecture, extends many of the concepts of the early VLIW approaches, providing a blend of static and dynamic approaches.

>> 图 3.19 用于多发处理器的五种主要方法以及区分它们的主要特征。本章的重点是硬件密集型技术，这些技术都是超级标准的某种形式。附录 H 专注于基于编译器的方法。IA-64 体系结构中体现的史诗方法扩展了早期 VLIW 方法的许多概念，提供了静态和动态方法的融合。
>>

Because the advantage of a VLIW increases as the maximum issue rate grows, we focus on a wider issue processor. Indeed, for simple two-issue processors, the overhead of a superscalar is probably minimal. Many designers would probably argue that a four-issue processor has manageable overhead, but as we will see later in this chapter, the growth in overhead is a major factor limiting wider issue processors.

> 由于 VLIW 的优势随着最大发行率的增长而增加，因此我们专注于更广泛的问题处理器。确实，对于简单的两问题处理器，超级标准的开销可能很小。许多设计师可能会争辩说，一个四问题的处理器具有可管理的开销，但是正如我们将在本章稍后看到的那样，开销的增长是限制更广泛问题处理器的主要因素。

Let’s consider a VLIW processor with instructions that contain five operations, including one integer operation (which could also be a branch), two floating-point operations, and two memory references. The instruction would have a set of fields for each functional unit—perhaps 16–24 bits per unit, yielding an instruction length of between 80 and 120 bits. By comparison, the Intel Itanium 1 and 2 contain six operations per instruction packet (i.e., they allow concurrent issue of two three-instruction bundles, as Appendix H describes).

> 让我们考虑一个带有五个操作的指令的 VLIW 处理器，包括一个整数操作（也可以是分支机构），两个浮点操作和两个内存引用。该指令将为每个功能单元提供一组字段 - 每单位可能 16-24 位，指令长度在 80 到 120 位之间。相比之下，Intel Itanium 1 和 2 每个说明包包含六个操作（即，如附录 H 所描述的那样，它们允许并发两个三个指令捆绑包）。

To keep the functional units busy, there must be enough parallelism in a code sequence to fill the available operation slots. This parallelism is uncovered by unrolling loops and scheduling the code within the single larger loop body. If the unrolling generates straight-line code, then _local scheduling_ techniques, which operate on a single basic block, can be used. If finding and exploiting the parallelism require scheduling code across branches, a substantially more complex _global_

> 为了使功能单元繁忙，代码序列中必须有足够的并行性才能填充可用的操作插槽。通过展开循环并将代码安排在单个较大的循环主体中，可以发现这种并行性。如果展开生成直线代码，则可以使用在单个基本块上操作的 *local Schedul* 技术。如果查找和利用并行性需要跨分支的调度代码，那么一个更复杂的 *global*

_scheduling_ algorithm must be used. Global scheduling algorithms are not only more complex in structure, but they also must deal with significantly more complicated trade-offs in optimization, because moving code across branches is expensive.

> *scheduling* 必须使用算法。全球调度算法不仅在结构上更加复杂，而且还必须处理优化方面的复杂权衡，因为跨分支机构移动代码很昂贵。

In Appendix H, we discuss _trace scheduling_, one of these global scheduling techniques developed specifically for VLIWs; we will also explore special hardware support that allows some conditional branches to be eliminated, extending the usefulness of local scheduling and enhancing the performance of global scheduling.

> 在附录 H 中，我们讨论了\_trace Schedul\_\_，这些全局调度技术之一是专门为 VLIWS 开发的。我们还将探索特殊的硬件支持，以消除某些条件分支，从而扩展了本地调度的有用性，并增强了全球调度的性能。

For now, we will rely on loop unrolling to generate long, straight-line code sequences so that we can use local scheduling to build up VLIW instructions and focus on how well these processors operate.

> 目前，我们将依靠循环展开来生成长，直线代码序列，以便我们可以使用本地调度来构建 VLIW 指令，并专注于这些处理器的运行效果。

Example Suppose we have a VLIW that could issue two memory references, two FP operations, and one integer operation or branch in every clock cycle. Show an unrolled version of the loop x\[i\] = x\[i\] + s (see page 158 for the RISC-V code) for such a processor. Unroll as many times as necessary to eliminate any stalls.

> 示例假设我们有一个 VLIW，可以在每个时钟周期内发出两个内存引用，两个 FP 操作以及一个整数操作或分支。为该处理器显示循环 x \ [i \] = x \ [i \] + s 的展开版本（请参阅第 158 页）。取消任何摊位的尽可能多次。

_Answer_ [Figure 3.20](#_bookmark121) shows the code. The loop has been unrolled to make seven copies of the body, which eliminates all stalls (i.e., completely empty issue cycles), and runs in 9 cycles for the unrolled and scheduled loop. This code yields a running rate of seven results in 9 cycles, or 1.29 cycles per result, nearly twice as fast as the two-issue superscalar of [Section 3.2](#basic-compiler-techniques-for-exposing-ilp) that used unrolled and scheduled code.

> _answer_ [图 3.20]（#\_ bookmark121）显示代码。循环已经展开，以制作七个身体的副本，从而消除了所有摊位（即完全空的发行周期），并在 9 个周期中以对展开和安排的循环运行。该代码在 9 个周期中产生 7 个结果的运行率，或每个结果 1.29 个周期，几乎是[3.2]（＃Basic-Compiler-techniques-for-exparping-ilp）的两次评估超量表的两倍展开和计划的代码。

> Figure 3.20 VLIW instructions that occupy the inner loop and replace the unrolled sequence. This code takes 9 cycles assuming correct branch prediction. The issue rate is 23 operations in 9 clock cycles, or 2.5 operations per cycle. The efficiency, the percentage of available slots that contained an operation, is about 60%. To achieve this issue rate requires a larger number of registers than RISC-V would normally use in this loop. The preceding VLIW code sequence requires at least eight FP registers, whereas the same code sequence for the base RISC-V processor can use as few as two FP registers or as many as five when unrolled and scheduled.

>> 图 3.20 VLIW 指令占据内部循环并替换展开的序列。该代码采用正确的分支预测，进行 9 个周期。发行率是 9 个时钟周期的 23 个操作，或每个周期 2.5 操作。效率是包含操作的可用插槽的百分比，约为 60％。为了达到此问题，需要比 RISC-V 通常在此循环中使用的寄存器数量。前面的 VLIW 代码序列至少需要八个 FP 寄存器，而基本 RISC-V 处理器的相同代码顺序可以使用少于两个 FP 寄存器，或者在展开和计划时多达五个。
>>

For the original VLIW model, there were both technical and logistical problems that made the approach less efficient. The technical problems were the increase in code size and the limitations of the lockstep operation. Two different elements combine to increase code size substantially for a VLIW. First, generating enough operations in a straight-line code fragment requires ambitiously unrolling loops (as in earlier examples), thereby increasing code size. Second, whenever instructions are not full, the unused functional units translate to wasted bits in the instruction encoding. In Appendix H, we examine software scheduling approaches, such as software pipelining, thatcanachievethebenefitsofunrollingwithoutasmuchcodeexpansion. To combat this code size increase, clever encodings are sometimes used. For example, there may be only one large immediate field for use by any functional unit. Another technique is to compress the instructions in main memory and expand them when they are read into the cache or are decoded. In Appendix H, we show other techniques, as well as document the significant code expansion seen in IA-64.

> 对于原始的 VLIW 模型，存在技术和后勤问题，使该方法效率降低了。技术问题是代码大小的增加和锁定操作的局限性。两个不同的元素结合在一起，以大大增加 VLIW 的代码大小。首先，在直线代码片段中生成足够的操作需要雄心勃勃的循环循环（如先前的示例），从而增加代码大小。其次，每当说明不满时，未使用的功能单元就会转化为指令编码中的浪费部分。在附录 H 中，我们检查了软件调度方法，例如软件管道，The Canachievethebenefitsofunrollingwithoutasmuchcodeexpansion。为了打击这种代码尺寸的增加，有时会使用巧妙的编码。例如，任何功能单元可能只有一个大型直接字段。另一种技术是压缩主内存中的说明并在将其读取到缓存或解码时将其扩展。在附录 H 中，我们显示了其他技术，并记录了 IA-64 中看到的重要代码扩展。

Early VLIWs operated in lockstep; there was no hazard-detection hardware at all. This structure dictated that a stall in any functional unit pipeline must cause the entire processor to stall because all the functional units had to be kept synchronized. Although a compiler might have been able to schedule the deterministic functional units to prevent stalls, predicting which data accesses would encounter a cache stall and scheduling them were very difficult to do. Thus caches needed to be blocking and causing _all_ the functional units to stall. As the issue rate and number of memory references became large, this synchronization restriction became unacceptable. In more recent processors, the functional units operate more independently, and the compiler is used to avoid hazards at issue time, while hardware checks allow for unsynchronized execution once instructions are issued.

> 早期的 vliws 以锁定状态操作；根本没有危险检测硬件。该结构规定，任何功能单位管道中的失速都必须导致整个处理器失速，因为所有功能单元必须保持同步。尽管编译器可能能够安排确定性功能单元以防止摊位，但预测哪些数据访问会遇到缓存摊位并安排它们非常困难。因此，缓存需要阻止并导致*all *功能单元失速。随着发行率和内存参考的数量变得大，这种同步限制变得不可接受。在最近的处理器中，功能单元更加独立地运行，并且编译器用于避免在发行时危害，而硬件检查允许发出指令后进行不同步的执行。

Binary code compatibility has also been a major logistical problem for generalpurpose VLIWs or those that run third-party software. In a strict VLIW approach, the code sequence makes use of both the instruction set definition and the detailed pipeline structure, including both functional units and their latencies. Thus different numbers of functional units and unit latencies require different versions of the code. This requirement makes migrating between successive implementations, or between implementations with different issue widths, more difficult than it is for a superscalar design. Of course, obtaining improved performance from a new superscalar design may require recompilation. Nonetheless, the ability to run old binary files is a practical advantage for the superscalar approach. In the domain-specific architectures, which we examine in [Chapter 7](#_bookmark322), this problem is not serious because applications are written specifically for an architectural configuration.

> 对于通用 Purpose VLIW 或运行第三方软件的二进制代码兼容性也是一个主要的后勤问题。在严格的 VLIW 方法中，代码顺序同时使用了指令集定义和详细的管道结构，包括功能单元及其潜伏期。因此，不同数量的功能单元和单位潜伏期需要代码的不同版本。这一要求使得在连续实现之间或在具有不同问题宽度的实现之间迁移，比超级量表设计更加困难。当然，从新的超级设计中获得改进的性能可能需要重新编译。尽管如此，运行旧二进制文件的能力对于超级标准方法来说是一个实践优势。在我们在[第 7 章]（#\_ bookmark322）中检查的特定领域架构中，此问题并不严重，因为应用程序是专门为架构配置编写的。

The EPIC approach, of which the IA-64 architecture is the primary example, provides solutions to many of the problems encountered in early general-purpose VLIW designs, including extensions for more aggressive software speculation and methods to overcome the limitation of hardware dependence while preserving binary compatibility.

> IA-64 体系结构是主要示例的史诗方法，它为早期通用使用 VLIW 设计中遇到的许多问题提供了解决方案，包括扩展的扩展，以实现更具侵略性的软件推测和克服硬件依赖的限制的方法二进制兼容性。

The major challenge for all multiple-issue processors is to try to exploit large amounts of ILP. When the parallelism comes from unrolling simple loops in FP programs, the original loop probably could have been run efficiently on a vector processor (described in the next chapter). It is not clear that a multiple-issue processor is preferred over a vector processor for such applications; the costs are similar, and the vector processor is typically the same speed or faster. The potential advantages of a multiple-issue processor versus a vector processor are the former’s ability to extract some parallelism from less structured code and to easily cache all forms of data. For these reasons, multiple-issue approaches have become the primary method for taking advantage of instruction-level parallelism, and vectors have become primarily an extension to these processors.

> 所有多发处理器的主要挑战是尝试利用大量 ILP。当并行性来自 FP 程序中的简单循环时，原始循环可能是在矢量处理器上有效运行的（下一章中描述）。目前尚不清楚多发性处理器比向量处理器首选此类应用程序。成本相似，矢量处理器通常相同的速度或更快。多发处理器与矢量处理器的潜在优势是前者从结构化较少的代码中提取一些并行性并轻松缓存所有形式的数据的能力。由于这些原因，多发方法已成为利用教学级并行性的主要方法，并且向量已成为这些处理器的扩展。
