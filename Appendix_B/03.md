## Six Basic Cache Optimizations

> ##六个基本缓存优化

The average memory access time formula gave us a framework to present cache optimizations for improving cache performance:

> 平均内存访问时间公式为我们提供了一个框架，以提高缓存优化以改善高速缓存性能：

Average memory access time = Hit time + Miss rate ×Miss penalty

> 平均内存访问时间=命中时间 + 错率 × 错过罚款

Hence, we organize six cache optimizations into three categories:

> 因此，我们将六个缓存优化分为三类：

- _Reducing the miss rate_—larger block size, larger cache size, and igher associativity

> - _REDUCS MISS RATE_  -  LARGER 块大小，较大的缓存大小和更高的关联性

- _Reducing the miss penalty_—multilevel caches and giving reads riority over writes

> - 减少罚款小姐 - 多重的缓存并给予阅读优先级与写作

- _Reducing the time to hit in the cache_—avoiding address translation hen indexing the cache

> - _还记录在缓存中击中的时间 - 避免地址转换母鸡索引缓存

[Figure B.18](#_bookmark457) on page B-40 concludes this section with a summary of the implemen- tation complexity and the performance benefits of these six techniques.

> [图 B.18](#_ bookmark457)在 B-40 上以启动复杂性和这六种技术的性能好处的摘要结束了本节。

The classical approach to improving cache behavior is to reduce miss rates, and we present three techniques to do so. To gain better insights into the causes of mis- ses, we first start with a model that sorts all misses into three simple categories:

> 改善缓存行为的经典方法是降低错过费率，我们提出了三种技术。为了更好地了解误解的原因，我们首先从一个模型开始，该模型分为三个简单类别：

- _Compulsory_—The very first access to a block _cannot_ be in the cache, so the block must be brought into the cache. These are also called _cold-start misses_ or _first-reference misses._

> - _compulsory_-首次访问块_cannot_位于缓存中，因此必须将块带入缓存中。这些也称为_ cold-start Misses_或_first-Reference Misses._

- _Capacity_—If the cache cannot contain all the blocks needed during execution of a program, capacity misses (in addition to compulsory misses) will occur because of blocks being discarded and later retrieved.

> - _capacity_-如果缓存无法包含程序执行过程中所需的所有块，则由于丢弃并将其检索到块而发生容量失误(除强制性失误外)。

- _Conflict_—If the block placement strategy is set associative or direct mapped, conflict misses (in addition to compulsory and capacity misses) will occur because a block may be discarded and later retrieved if too many blocks map to its set. These misses are also called _collision misses_. The idea is that hits in a fully associative cache that become misses in an _n_-way set-associative cache are due to more than _n_ requests on some popular sets.

> -_conflict_  - 如果设置块放置策略是关联的或直接映射的，则冲突错过(除强制性和容量失误外)，因为可能会丢弃一个块并以后将其检索到太多块映射到其集合。这些错过也称为_Collision Misses_。这个想法是，在_n_-way set-sassociative 高速缓存中成为完全关联的缓存中的命中率是由于某些流行的集合上的_n_请求超过_n_引起的。

([Chapter 5](#_bookmark213) adds a fourth C, for _coherency_ misses due to cache flushes to keep mul- tiple caches coherent in a multiprocessor; we won’t consider those here.)

> ([[第 5 章](#_ bookmark213)添加了第四个 C，因为由于缓存冲洗而导致的_coherency_ inside toss trounds fiste tose flushes，以使 multiple caches 保持在多处理器中相干；我们不会在这里考虑这些。)

[Figure B.8](#_bookmark448) shows the relative frequency of cache misses, broken down by the three C’s. Compulsory misses are those that occur in an infinite cache. Capacity misses are those that occur in a fully associative cache. Conflict misses are those that occur going from fully associative to eight-way associative, four-way associa- tive, and so on. [Figure B.9](#_bookmark449) presents the same data graphically. The top graph shows absolute miss rates; the bottom graph plots the percentage of all the misses by type of miss as a function of cache size.

> [图 B.8](#_ bookmark448)显示了被三个 C 分解的高速缓存失误的相对频率。强制性错过是在无限缓存中发生的。容量失误是在完全关联的缓存中发生的。冲突错过的是从完全关联到八方协会，四方协会等发生的情况。[图 B.9](#_ bookmark449)以图形方式显示相同的数据。顶部图显示了绝对的错率；底部图绘制了所有失误类型的所有失误的百分比，该函数的函数是缓存大小的函数。

To show the benefit of associativity, conflict misses are divided into misses caused by each decrease in associativity. Here are the four divisions of conflict misses and how they are calculated:

> 为了显示关联的好处，冲突错过被分为由于关联性下降而导致的失误。这是冲突错过的四个分区及其计算方式：

- _Eight-way_—Conflict misses due to going from fully associative (no conflicts) to eight-way associative

> - _eight-way _-联合因从完全关联(无冲突)到八路关联而错过了

- _Four-way_—Conflict misses due to going from eight-way associative to four- way associative

> - _four-way _-联合因从八方关联到四向关联而错过了

- _Two-way_—Conflict misses due to going from four-way associative to two-way associative

> - _two-way _-联合会因从四向关联到双向关联而错过

- _One-way_—Conflict misses due to going from two-way associative to one-way associative (direct mapped)

> - _ONE-WAY_  - 联合因从双向关联到单向关联而错过(直接映射)

As we can see from the figures, the compulsory miss rate of the SPEC2000 programs is very small, as it is for many long-running programs.

> 从数字中我们可以看到，Spec2000 程序的强制性失误非常小，因为这是许多长期运行的程序。

Having identified the three C’s, what can a computer designer do about them? Conceptually, conflicts are the easiest: Fully associative placement avoids all

> 确定了这三个 C，计算机设计师可以对它们做什么？从概念上讲，冲突是最简单的：完全关联的安置避免了所有人

Figure B.8 Total miss rate for each size cache and percentage of each according to the three C’s. Compulsory misses are independent of cache size, while capacity misses decrease as capacity increases, and conflict misses decrease as associativity increases. [Figure B.9](#_bookmark449) shows the same information graphically. Note that a direct-mapped cache of size _N_ has about the same miss rate as a two-way set-associative cache of size _N_/2 up through 128 K. Caches larger than 128 KiB do not prove that rule. Note that the Capacity column is also the fully associative miss rate. Data were collected as in [Figure B.4](#_bookmark442) using LRU replacement.

> 图 B.8 每个尺寸缓存的总数率和每个 C 的百分比根据三个 C 的百分比。强制性失误独立于缓存大小，而随着容量的增加，容量误差会减少，而冲突的失误随着关联性的增加而减少。[图 B.9](#_ bookmark449)以图形方式显示相同的信息。请注意，大小_n_的直接映射缓存与尺寸_n_/2 的双向设置相关缓存大致相同。请注意，容量列也是完全关联的失误率。如[图 B.4](#_ bookmark442)使用 LRU 替换中收集数据。

Figure B.9 Total miss rate (top) and distribution of miss rate (bottom) for each size cache according to the three C’s for the data in [Figure B.8](#_bookmark448). The top diagram shows the actual data cache miss rates, while the bottom diagram shows the percentage in each category. (_Space allows_ the graphs to show one extra cache size than can fit in [Figure B.8](#_bookmark448).)

> 图 B.9 根据[图 B.8]中的数据，根据三个 C 的数据缓存的总数率(顶部)和遗漏率的分布(底部)(#_ bookmark448)。顶图显示了实际数据缓存率率，而底部图显示了每个类别中的百分比。(_ space 允许_图表显示一个额外的缓存大小，而不是[图 b.8](#_ bookmark448)。

conflict misses. Full associativity is expensive in hardware, however, and may slow the processor clock rate (see the example on page B-29), leading to lower overall performance.

> 冲突错过了。但是，完全关联性在硬件中很昂贵，并且可能会减慢处理器时钟速率(请参见第 B-29 页的示例)，从而降低总体性能。

There is little to be done about capacity except to enlarge the cache. If the upper-level memory is much smaller than what is needed for a program, and a sig- nificant percentage of the time is spent moving data between two levels in the hierarchy, the memory hierarchy is said to _thrash_. Because so many replacements are required, thrashing means the computer runs close to the speed of the lower- level memory, or maybe even slower because of the miss overhead.

> 除了放大缓存外，容量几乎没有什么可做的。如果上层内存远小于程序所需的内存，并且在层次结构中的两个级别之间移动数据的时间很大，则表示内存层次结构向_thrash_表示。由于需要如此多的替换，因此敲击意味着计算机接近较低级别的内存速度，甚至由于错过的头顶而慢。

Another approach to improving the three C’s is to make blocks larger to reduce the number of compulsory misses, but, as we will see shortly, large blocks can increase other kinds of misses.

> 改进三个 C 的另一种方法是使块更大以减少强制性失误的数量，但是，正如我们将不久的将来，大型块可以增加其他类型的错过。

The three C’s give insight into the cause of misses, but this simple model has its limits; it gives you insight into average behavior but may not explain an individual miss. For example, changing cache size changes conflict misses as well as capacity misses, because a larger cache spreads out references to more blocks. Thus, a miss might move from a capacity miss to a conflict miss as cache size changes. Simi- larly, changing the block size can sometimes reduce capacity misses (in addition to the expected reduction in compusolory misses), as Gupta et al. (2013) show.

> 这三个 C 洞悉了失踪原因，但是这个简单的模型有其局限性。它使您深入了解平均行为，但可能无法解释个人错过。例如，更改缓存大小的变化冲突错过和容量失误，因为较大的缓存会传播到更多块的引用。因此，随着缓存尺寸的变化，错过可能会从容量失误转变为冲突。同样，改变块大小有时可以减少容量遗漏(除了预期的复杂性失误之外)，如 Gupta 等人所致。(2013)节目。

Note also that the three C’s also ignore replacement policy, because it is dif- ficult to model and because, in general, it is less significant. In specific circum- stances the replacement policy can actually lead to anomalous behavior, such as poorer miss rates for larger associativity, which contradicts the three C’s model. (Some have proposed using an address trace to determine optimal placement in memory to avoid placement misses from the three C’s model; we’ve not followed that advice here.)

> 还要注意，这三个 C 也忽略了替代政策，因为它很难模型，而且通常它不太重要。在特定的情况下，替代政策实际上可能导致异常行为，例如，较大的关联性较差的失误率较差，这与三个 C 的模型相矛盾。(有些人建议使用地址跟踪来确定在内存中的最佳位置，以避免三个 C 模型的放置遗漏；我们在这里没有遵循该建议。)

Alas, many of the techniques that reduce miss rates also increase hit time or miss penalty. The desirability of reducing miss rates using the three optimizations must be balanced against the goal of making the whole system fast. This first exam- ple shows the importance of a balanced perspective.

> las，许多降低失误率的技术也增加了命中时间或罚款。使用三个优化的降低失误率的可取性必须与使整个系统快速的目标保持平衡。第一次检查显示了平衡观点的重要性。

### First Optimization: Larger Block Size to Reduce Miss Rate

> ###首次优化：较大的块大小以降低错过费率

The simplest way to reduce miss rate is to increase the block size. [Figure B.10](#_bookmark450) shows the trade-off of block size versus miss rate for a set of programs and cache sizes. Larger block sizes will reduce also compulsory misses. This reduction occurs because the principle of locality has two components: temporal locality and spatial locality. Larger blocks take advantage of spatial locality.

> 降低错过率的最简单方法是增加块大小。[图 B.10](#_ bookmark450)显示了一组程序和缓存尺寸的块大小与错过率的权衡。较大的块尺寸也会减少强制性的失误。这种减少之所以发生，是因为局部原理具有两个组成部分：时间位置和空间位置。较大的块利用空间位置。

At the same time, larger blocks increase the miss penalty. Because they reduce the number of blocks in the cache, larger blocks may increase conflict misses and even capacity misses if the cache is small. Clearly, there is little reason to increase the block size to such a size that it _increases_ the miss rate. There is also no benefit to reducing miss rate if it increases the average memory access time. The increase in miss penalty may outweigh the decrease in miss rate.

> 同时，较大的块增加了罚款。由于它们减少了缓存中的块数量，因此较大的块可能会增加冲突的失误，甚至如果缓存很小，则容易失误。显然，没有理由将块大小提高到如此之大的尺寸，以至于_increases _误率。如果增加平均内存访问时间，减少错率也没有好处。罚款的增加可能超过了错率的降低。

Example [Figure B.11](#_bookmark451) shows the actual miss rates plotted in [Figure B.10](#_bookmark450). Assume the mem- ory system takes 80 clock cycles of overhead and then delivers 16 bytes every 2 clock cycles. Thus, it can supply 16 bytes in 82 clock cycles, 32 bytes in 84 clock cycles, and so on. Which block size has the smallest average memory access time for each cache size in [Figure B.11](#_bookmark451)?

> 示例[图 B.11](#_ bookmark451)显示了[图 B.10](#_ bookmark450)中绘制的实际失误费率。假设 MEM-ORY 系统采用 80 个开销的时钟周期，然后每 2 个时钟周期每 2 个时钟循环提供 16 个字节。因此，它可以在 82 个时钟周期中提供 16 个字节，84 个时钟周期中的 32 个字节等。[图 B.11](#_ bookmark451)中，哪个块大小的每个缓存大小的平均内存访问时间最小？

Figure B.11 Actual miss rate versus block size for the five different-sized caches in [Figure B.10](#_bookmark450). Note that for a 4 KiB cache, 256-byte blocks have a higher miss rate than 32- byte blocks. In this example, the cache would have to be 256 KiB in order for a 256-byte block to decrease misses.

> 图 B.11 [图 B.10]中的五个不同大小的缓存(#_ bookmark450)中的五个不同尺寸的缓存的实际错率与块大小。请注意，对于 4 KIB 缓存，256 个字节块的错率高于 32 个字节块。在此示例中，缓存必须为 256 KIB 才能使 256 个字节块减少失误。

Figure B.12 Average memory access time versus block size for five different-sized caches in [Figure B.10](#_bookmark450). Block sizes of 32 and 64 bytes dominate. The smallest average time per cache size is boldfaced.

> 图 B.12 [图 B.10]中的五个不同大小的缓存的平均内存访问时间与块大小(#_ bookmark450)。块大小为 32 和 64 个字节占主导地位。每个缓存尺寸的最小平均时间是大胆的。

[Figure B.12](#_bookmark452) shows the average memory access time for all block and cache sizes between those two extremes. The boldfaced entries show the fastest block size for a given cache size: 32 bytes for 4 KiB and 64 bytes for the larger caches. These sizes are, in fact, popular block sizes for processor caches today.

> [图 B.12](#_ bookmark452)显示了这两个极端之间所有块和缓存大小的平均内存访问时间。粗体的条目显示给定的缓存大小的最快块大小：32 个字节，用于 4 KIB 和 64 个字节，用于较大的缓存。实际上，这些尺寸是当今处理器缓存的流行块尺寸。

As in all of these techniques, the cache designer is trying to minimize both the miss rate and the miss penalty. The selection of block size depends on both the latency and bandwidth of the lower-level memory. High latency and high bandwidth encourage large block size because the cache gets many more bytes per miss for a small increase in miss penalty. Conversely, low latency and low bandwidth encour- age smaller block sizes because there is little time saved from a larger block. For example, twice the miss penalty of a small block may be close to the penalty of a block twice the size. The larger number of small blocks may also reduce conflict mis- ses. Note that [Figures B.10](#_bookmark450) and [B.12](#_bookmark452) show the difference between selecting a block size based on minimizing miss rate versus minimizing average memory access time. After seeing the positive and negative impact of larger block size on compul- sory and capacity misses, the next two subsections look at the potential of higher capacity and higher associativity.

> 与所有这些技术一样，缓存设计师正试图最大程度地减少错过率和罚款。块大小的选择取决于低级内存的延迟和带宽。高潜伏期和高带宽会鼓励大型块大小，因为缓存每次失误会增加更多的字节，从而减轻了罚款。相反，低潜伏期和低带宽鼓励较小的块尺寸，因为从较大的块中节省了很少的时间。例如，小块的错过的两倍可能接近块的罚款两倍。较大的小块也可能减少冲突错误。请注意，[图 B.10](#_ bookmark450)和[b.12](#_ bookmark452)显示了基于最小化误差率与最小化平均内存访问时间的块大小之间的区别。在看到较大的块大小对富度和容量失误的正面和负面影响之后，接下来的两个小节探讨了更高容量和更高关联的潜力。

### Second Optimization: Larger Caches to Reduce Miss Rate

> ###第二次优化：较大的缓存以降低错过费率

The obvious way to reduce capacity misses in [Figures B.8](#_bookmark448) and [B.9](#_bookmark449) is to increase capacity of the cache. The obvious drawback is potentially longer hit time and higher cost and power. This technique has been especially popular in off-chip caches.

> 减少[图 B.8](#_ bookmark448)和[b.9](#_ bookmark449)中容量失误的明显方法是增加缓存的容量。明显的缺点可能是较长的命中时间和更高的成本和力量。该技术在外片缓存中特别受欢迎。

### Third Optimization: Higher Associativity to Reduce Miss Rate

> ###第三优化：降低错过率的更高关联性

[Figures B.8](#_bookmark448) and [B.9](#_bookmark449) show how miss rates improve with higher associativity. There are two general rules of thumb that can be gleaned from these figures. The first is that eight-way set associative is for practical purposes as effective in reducing mis- ses for these sized caches as fully associative. You can see the difference by com- paring the eight-way entries to the capacity miss column in [Figure B.8](#_bookmark448), because capacity misses are calculated using fully associative caches.

> [图 b.8](#_ bookmark448)和[b.9](#_ bookmark449)显示了失误费率如何通过更高的关联性提高。从这些数字中可以收集两种一般的经验法则。首先是，八方集的关联是出于实际目的，可以有效地减少这些大小的缓存作为完全关联的误差。您可以通过在[图 b.8](#_ bookmark448)中的八方条目(＃bookmark448)中查看差异，因为使用完全关联的 caches 计算容量误差。

The second observation, called the _2:1 cache rule of thumb_, is that a direct- mapped cache of size _N_ has about the same miss rate as a two-way set associative cache of size _N_/2. This held in three C’s figures for cache sizes less than 128 KiB. Like many of these examples, improving one aspect of the average memory access time comes at the expense of another. Increasing block size reduces miss rate while increasing miss penalty, and greater associativity can come at the cost of increased hit time. Hence, the pressure of a fast processor clock cycle encour- ages simple cache designs, but the increasing miss penalty rewards associativity, as the following example suggests.

> 第二个观察结果称为_2：1 thumb_的缓存规则，是一个大小_n_的直接映射缓存与大小_n_/2 的双向套件的关联缓存大致相同。这以三个 C 的高速公路数字少于 128 KIB。像许多这些示例一样，改善平均内存访问时间的一个方面是以牺牲另一个方面为代价的。增加的块大小会降低错过率，同时罚款增加，而更大的关联性可能以增加命中时间为代价。因此，快速处理器时钟周期的压力鼓励了简单的缓存设计，但是如下示例所表明的那样，越来越多的罚款奖励关联。

Example Assume that higher associativity would increase the clock cycle time as listed as follows:

> 示例假设更高的关联将增加时钟周期时间如下：

Assume that the hit time is 1 clock cycle, that the miss penalty for the direct- mapped case is 25 clock cycles to a level 2 cache (see next subsection) that never misses, and that the miss penalty need not be rounded to an integral number of clock cycles. Using [Figure B.8](#_bookmark448) for miss rates, for which cache sizes are each of these three statements true?

> 假设命中时间是 1 个时钟周期，直接映射案例的罚款是 25 个时钟周期至 2 级缓存(请参阅下一个小节)，从不错过时钟周期数。使用[图 b.8](#_ bookmark448)以获取错率，这三个语句中的每个陈述都为哪个？

Figure B.13 Average memory access time using miss rates in [Figure B.8](#_bookmark448) for param- eters in the example. _Boldface_ type means that this time is higher than the number to the left, that is, higher associativity _increases_ average memory access time.

> 图 B.13 使用[图 B.8](#_ bookmark448)中的 param-eTers 中的遗漏率的平均内存访问时间。_boldface_类型意味着此时间高于左侧的数字，即更高的关联_increases_平均内存访问时间。

that the formulas in this example hold for caches less than or equal to 8 KiB for up to four-way associativity. Starting with 16 KiB, the greater hit time of larger asso- ciativity outweighs the time saved due to the reduction in misses.

> 此示例中的公式适用于小于或等于 8 KIB 的缓存，最多可容纳四向关联。从 16 KIB 开始，较大的较大协会的命中时间大于减少失误的时间。

Note that we did not account for the slower clock rate on the rest of the program in this example, thereby understating the advantage of direct-mapped cache.

> 请注意，在此示例中，我们没有考虑程序的其余部分的时钟速率，从而低估了直接映射缓存的优势。

### Fourth Optimization: Multilevel Caches to Reduce Miss Penalty

> ###第四优化：多级缓存以减少罚款

Reducing cache misses had been the traditional focus of cache research, but the cache performance formula assures us that improvements in miss penalty can be just as beneficial as improvements in miss rate. Moreover, Figure 2.2 on page 80 shows that technology trends have improved the speed of processors faster than DRAMs, making the relative cost of miss penalties increase over time.

> 减少缓存失误一直是缓存研究的传统重点，但是缓存性能公式向我们保证，改善罚款的改善可能与提高未命中率一样有益。此外，第 80 页的图 2.2 显示，技术趋势比 DRAM 更快地提高了处理器的速度，从而使罚款的相对成本随着时间而增加。

This performance gap between processors and memory leads the architect to this question: Should I make the cache faster to keep pace with the speed of pro- cessors, or make the cache larger to overcome the widening gap between the pro- cessor and main memory?

> 处理器和内存之间的这种性能差距使工程师解决了这个问题：我是否应该更快地使缓存保持步伐，或者使高速缓存更大以克服调查器和主内存之间的扩大差距？

One answer is, do both. Adding another level of cache between the original cache and memory simplifies the decision. The first-level cache can be small enough to match the clock cycle time of the fast processor. Yet, the second-level cache can be large enough to capture many accesses that would go to main mem- ory, thereby lessening the effective miss penalty.

> 一个答案是，两者都做。在原始缓存和内存之间添加另一个级别的缓存层，简化了决策。第一级缓存可能足够小，可以匹配快速处理器的时钟周期时间。然而，第二级缓存可能足够大，可以捕获许多可以进入主成员的访问，从而减少了有效的罚款。

Although the concept of adding another level in the hierarchy is straightfor- ward, it complicates performance analysis. Definitions for a second level of cache are not always straightforward. Let’s start with the definition of _average memory access time_ for a two-level cache. Using the subscripts L1 and L2 to refer, respec- tively, to a first-level and a second-level cache, the original formula is and so

> 尽管在层次结构中添加另一个级别的概念是直接的，但它使性能分析变得复杂。第二级缓存的定义并不总是直接的。让我们从_aaverage 内存访问时间_的定义开始，用于两级缓存。使用下标 L1 和 L2，将其分配给第一级和第二级缓存，原始公式为等等

In this formula, the second-level miss rate is measured on the leftovers from the first-level cache. To avoid ambiguity, these terms are adopted here for a two-level cache system:

> 在此公式中，在第一级缓存的剩余时间上测量了第二级失误率。为了避免歧义，这些术语在这里采用了两级缓存系统：

- _Local miss rate_—This rate is simply the number of misses in a cache divided by the total number of memory accesses to this cache. As you would expect, for the first-level cache it is equal to Miss rate<sub>L1</sub>, and for the second-level cache it is Miss rate<sub>L2</sub>.

> - _ local Miss Rate _-此速率仅仅是缓存中的错过数量除以此缓存的内存访问总数。如您所料，对于第一级缓存，它等于 MISS RATE <ub> L1 </sub>，对于第二级缓存，它是 MISS RATE <Sub> L2 </sub>。

- _Global miss rate_—The number of misses in the cache divided by the total num- ber of memory accesses generated by the processor. Using the terms above, the global miss rate for the first-level cache is still just Miss rate<sub>L1</sub>, but for the

> - _global Miss Rate _-缓存中的错过数除以处理器生成的内存访问的总数。使用上面的术语，第一级缓存的全局损失率仍然只是错率<ub> l1 </sub>，但对于

second-level cache it is Miss rate<sub>L1</sub> ×Miss rate<sub>L2</sub>.

> 二级缓存是 MISS 率<ub> L1 </sub>×MISS RATE <sub> L2 </sub>。

This local miss rate is large for second-level caches because the first-level cache skims the cream of the memory accesses. This is why the global miss rate is the more useful measure: It indicates what fraction of the memory accesses that leave the processor go all the way to memory.

> 对于二级缓存，这种本地的错率很高，因为第一级高速缓存略微撇开了内存访问的奶油。这就是为什么全局错过率是更有用的措施：它指示使处理器一直延伸到内存的内存访问的哪一部分。

Here is a place where the misses per instruction metric shines. Instead of con- fusion about local or global miss rates, we just expand memory stalls per instruc- tion to add the impact of a second-level cache.

> 这是每个指导度量的错过的地方。我们只是在每次指导中扩展内存摊位，而不是融合本地或全球的失误费率，以增加二级缓存的影响。

Example Suppose that in 1000 memory references there are 40 misses in the first-level cache and 20 misses in the second-level cache. What are the various miss rates? Assume the miss penalty from the L2 cache to memory is 200 clock cycles, the hit time of the L2 cache is 10 clock cycles, the hit time of L1 is 1 clock cycle, and there are 1.5 memory references per instruction. What is the average memory access time and average stall cycles per instruction? Ignore the impact of writes.

> 示例假设在 1000 个内存引用中，第一级缓存中有 40 个错过，第二级缓存中有 20 个错过。各种错率是多少？假设从 L2 缓存到内存的罚款是 200 个时钟周期，L2 缓存的命中时间为 10 个时钟周期，L1 的命中时间为 1 个时钟周期，并且每个指令有 1.5 个内存引用。每个说明的平均内存访问时间和平均失速周期是多少？忽略写作的影响。

_Answer_ The miss rate (either local or global) for the first-level cache is 40/1000 or 4%. The local miss rate for the second-level cache is 20/40 or 50%. The global miss rate of the second-level cache is 20/1000 or 2%. Then

> _answer_第一级缓存的 MISS 率(本地或全局)为 40/1000 或 4％。第二级缓存的本地错率为 20/40 或 50％。第二级缓存的全球错率为 20/1000 或 2％。然后

To see how many misses we get per instruction, we divide 1000 memory refer- ences by 1.5 memory references per instruction, which yields 667 instructions. Thus, we need to multiply the misses by 1.5 to get the number of misses per 1000 instructions. We have 40 1.5 or 60 L1 misses, and 20 1.5 or 30 L2 mis- ses, per 1000 instructions. For average memory stalls per instruction, assuming the misses are distributed uniformly between instructions and data:

> 要查看我们通过指令获得多少遗漏，我们将 1000 个内存引用除以 1.5 内存参考，该记忆参考可以产生 667 个说明。因此，我们需要将失误乘以 1.5，以获取每 1000 个说明的错过数量。我们有 40 个 1.5 或 60 L1 遗漏，每 1000 条说明，20 1.5 或 30 l2 miss。对于每个指令的平均内存摊位，假设错过在说明和数据之间均匀分布：

If we subtract the L1 hit time from the average memory access time (AMAT) and then multiply by the average number of memory references per instruction, we get the same average memory stalls per instruction:

> 如果我们从平均内存访问时间(AMAT)中减去 L1 命中时间，然后乘以每个指令的平均内存参考数量，我们每次指令获得相同的平均内存失速：

As this example shows, there may be less confusion with multilevel caches when calculating using misses per instruction versus miss rates.

> 如本示例所示，使用指令与错过率的错过计算时，多级缓存的混乱可能会减少。

Note that these formulas are for combined reads and writes, assuming a write- back first-level cache. Obviously, a write-through first-level cache will send _all_ writes to the second level, not just the misses, and a write buffer might be used. [Figures B.14](#_bookmark453) and [B.15](#_bookmark454) show how miss rates and relative execution time change with the size of a second-level cache for one design. From these figures we can gain two insights. The first is that the global cache miss rate is very similar to the single cache miss rate of the second-level cache, provided that the second-level cache is much larger than the first-level cache. Hence, our intuition and knowledge about the first-level caches apply. The second insight is that the local cache miss rate is _not_ a good measure of secondary caches; it is a function of the miss rate of the first- level cache, and hence can vary by changing the first-level cache. Thus, the global cache miss rate should be used when evaluating second-level caches.

> 请注意，这些公式用于合并的读取和写入，假设书面回头级缓存。显然，写入第一级缓存将发送_ALL_写入第二级，而不仅仅是错过，并且可以使用写缓冲区。[图 B.14](#_ bookmark453)和[b.15](#_ bookmark454)展示了率和相对执行时间如何随着一个设计的第二级高速缓存的大小而变化。从这些数字中，我们可以获得两个见解。第一个是，只要第二级高速缓存比第一级高速缓存大得多，全局高速缓存率与第二级高速缓存的单个缓存率非常相似。因此，我们对第一级缓存的直觉和知识适用。第二个见解是，本地缓存率是_not_的一个辅助缓存的良好度量；它是第一级缓存率的函数，因此可以通过更改第一级高速缓存而有所不同。因此，在评估二级缓存时应使用全局缓存率。

With these definitions in place, we can consider the parameters of second-level caches. The foremost difference between the two levels is that the speed of the first- level cache affects the clock rate of the processor, while the speed of the second- level cache only affects the miss penalty of the first-level cache. Thus, we can con- sider many alternatives in the second-level cache that would be ill chosen for the first-level cache. There are two major questions for the design of the second-level cache: Will it lower the average memory access time portion of the CPI, and how much does it cost?

> 有了这些定义，我们可以考虑第二级缓存的参数。两个级别之间的最大区别是，第一级缓存的速度会影响处理器的时钟速率，而第二级缓存的速度仅影响第一级缓存的罚款。因此，我们可以考虑第二级缓存中的许多替代方案，这些替代方法将被选为第一级缓存。对于第二级缓存的设计，有两个主要问题：它会降低 CPI 的平均内存访问时间部分吗？

The initial decision is the size of a second-level cache. Since everything in the first-level cache is likely to be in the second-level cache, the second-level cache should be much bigger than the first. If second-level caches are just a little bigger, the local miss rate will be high. This observation inspires the design of huge second-level caches—the size of main memory in older computers!

> 最初的决定是第二级缓存的大小。由于第一级缓存中的所有内容都可能位于第二级缓存中，因此第二级缓存应该比第一个级别大得多。如果二级缓存略大一些，那么当地的错率将很高。该观察结果激发了巨大的二级缓存的设计 - 旧计算机中的主要内存大小！

Figure B.14 Miss rates versus cache size for multilevel caches. Second-level caches _smaller_ than the sum of the two 64 KiB first-level caches make little sense, as reflected in the high miss rates. After 256 KiB the single cache is within 10% of the global miss rates. The miss rate of a single-level cache versus size is plotted against the local miss rate and global miss rate of a second-level cache using a 32 KiB first-level cache. The L2 caches (unified) were two-way set associative with replacement. Each had split L1 instruction and data caches that were 64 KiB two-way set associative with LRU replace- ment. The block size for both L1 and L2 caches was 64 bytes. Data were collected as in [Figure B.4](#_bookmark442).

> 图 B.14 多级缓存的错率与缓存大小。二级缓存_smaller _比两个 64 KIB 一级缓存的总和没有意义，正如高度遗漏率所反映的那样。256 KIB 之后，单个缓存在全球损失率的 10％以内。使用 32 KIB 的第一级高速缓存，绘制了单级高速缓存与大小的尺寸的错率。L2 缓存(统一)与更换的双向套件关联。每个都有与 LRU 替换为 64 KIB 双向套件关联的拆分 L1 指令和数据缓存。L1 和 L2 缓存的块大小为 64 个字节。如[图 B.4](#_ bookmark442)中收集数据。

One question is whether set associativity makes more sense for second-level caches.

> 一个问题是集合关联对于二级缓存是否更有意义。

Example Given the following data, what is the impact of second-level cache associativity on its miss penalty?

> 示例给定以下数据，第二级缓存关联对罚款的影响是什么？

Figure B.15 Relative execution time by second-level cache size. The two bars are for different clock cycles for an L2 cache hit. The reference execution time of 1.00 is for an 8192 KiB second-level cache with a 1-clock-cycle latency on a second-level hit. These data were collected the same way as in [Figure B.14](#_bookmark453), using a simulator to imitate the Alpha 21264.

> 图 B.15 相对执行时间按第二级缓存大小。这两个条适用于不同的时钟周期，用于 L2 高速缓存。参考执行时间为 1.00，适用于 8192 KIB 二级缓存，并在第二级命中率上带有 1-Clock-Cycle 延迟。这些数据的收集方式与[图 B.14](#_ bookmark453)中的方式相同，使用模拟器模仿 Alpha 21264。

Adding the cost of associativity increases the hit cost only 0.1 clock cycle, making the new first-level cache miss penalty:

> 添加关联成本增加了命中成本仅为 0.1 时钟周期，这使新的第一级缓存失误：

Miss penalty2-way L2 = 10.1+ 20% × 200 = 50.1 clock cycles

> 小姐 nointy2-way l2 = 10.1+ 20％×200 = 50.1 时钟周期

In reality, second-level caches are almost always synchronized with the first-level cache and processor. Accordingly, the second-level hit time must be an integral number of clock cycles. If we are lucky, we shave the second-level hit time to 10 cycles; if not, we round up to 11 cycles. Either choice is an improvement over the direct-mapped second-level cache:

> 实际上，二级缓存几乎总是与第一级缓存和处理器同步。因此，第二级命中时间必须是时钟周期的整数数量。如果幸运的话，我们将剃期的二级点击时间降到了 10 个周期。如果没有，我们最多汇总 11 个周期。两种选择都是对直接映射的第二级缓存的改进：

Miss penalty2-way L2 = 10 + 20% × 200 = 50.0 clock cycles Miss penalty2-way L2 = 11 + 20% × 200 = 51.0 clock cycles

> 小姐 nointy2-way l2 = 10 + 20％×200 = 50.0 时钟周期小姐 nistry2-way l2 = 11 + 20％×200 = 51.0 时钟周期

Now we can reduce the miss penalty by reducing the _miss rate_ of the second-level caches.

> 现在，我们可以通过降低第二级缓存的无限速度来减少罚款。

Another consideration concerns whether data in the first-level cache are in the second-level cache. _Multilevel inclusion_ is the natural policy for memory hierar- chies: L1 data are always present in L2. Inclusion is desirable because consistency between I/O and caches (or among caches in a multiprocessor) can be determined just by checking the second-level cache.

> 另一个考虑因素涉及第一级缓存中的数据是否在第二级缓存中。_multilevel 包含_是记忆层的自然政策：L1 数据始终存在于 L2 中。纳入是可取的，因为仅通过检查第二级缓存即可确定 I/O 和缓存之间的一致性(或多处理器中的缓存之间)。

One drawback to inclusion is that measurements can suggest smaller blocks for the smaller first-level cache and larger blocks for the larger second-level cache. For example, the Pentium 4 has 64-byte blocks in its L1 caches and 128-byte blocks in its L2 cache. Inclusion can still be maintained with more work on a second-level miss. The second-level cache must invalidate all first-level blocks that map onto the second-level block to be replaced, causing a slightly higher first-level miss rate. To avoid such problems, many cache designers keep the block size the same in all levels of caches.

> 包含包含的一个缺点是，测量值可以暗示较小的第一级缓存和较大较大的二级缓存的较小块。例如，奔驰 4 的 L1 缓存中有 64 个字节块，其 L2 缓存中有 128 字节块。在第二级错过的更多工作中，仍然可以维持包容性。第二级缓存必须使所有映射到第二级块的第一级块无效以更换，从而导致稍高的第一级失误率。为了避免此类问题，许多缓存设计人员在所有级别的缓存中保持块大小相同。

However, what if the designer can only afford an L2 cache that is slightly big- ger than the L1 cache? Should a significant portion of its space be used as a redun- dant copy of the L1 cache? In such cases a sensible opposite policy is _multilevel exclusion:_ L1 data are _never_ found in an L2 cache. Typically, with exclusion a cache miss in L1 results in a swap of blocks between L1 and L2 instead of a replacement of an L1 block with an L2 block. This policy prevents wasting space in the L2 cache. For example, the AMD Opteron chip obeys the exclusion property using two 64 KiB L1 caches and 1 MiB L2 cache.

> 但是，如果设计师只能负担比 L1 缓存略大的 L2 缓存该怎么办？是否应该将大部分空间用作 L1 缓存的重新副本？在这种情况下，明智的相反策略是_multilevel 排除：_ l1 数据是_never_在 L2 缓存中找到的。通常，在排除 L1 中的缓存失误会导致 L1 和 L2 之间的块互换，而不是用 L2 块替换 L1 块。该策略可防止 L2 缓存中的浪费空间。例如，AMD Opteron 芯片使用两个 64 KIB L1 缓存和 1 个 MIB L2 缓存来遵守排除属性。

As these issues illustrate, although a novice might design the first- and second- level caches independently, the designer of the first-level cache has a simpler job given a compatible second-level cache. It is less of a gamble to use a write through, for example, if there is a write-back cache at the next level to act as a backstop for repeated writes and it uses multilevel inclusion.

> 正如这些问题所说明的那样，尽管新手可能会独立设计第一级和二级缓存，但考虑到兼容的第二级缓存，第一级高速缓存的设计师的作业更简单。使用写入，例如，如果下一个级别的写下缓存可以充当重复写入的后备，并且使用多级包含在内，则不太赌博。

The essence of all cache designs is balancing fast hits and few misses. For second-level caches, there are far fewer hits than in the first-level cache, so the emphasis shifts to fewer misses. This insight leads to much larger caches and tech- niques to lower the miss rate, such as higher associativity and larger blocks.

> 所有缓存设计的本质都是平衡快速命中和几次失误。对于二级缓存，命中率要比第一级缓存少得多，因此重点转移到更少的错过。这种洞察力导致更大的缓存和技术降低了错过率，例如更高的关联性和较大的块。

### Fifth Optimization: Giving Priority to Read Misses over Writes to Reduce Miss Penalty

> ###第五优化：优先考虑阅读遗漏以减少罚款的写作

This optimization serves reads before writes have been completed. We start with looking at the complexities of a write buffer.

> 在写作完成之前，此优化功能读取。我们从查看写缓冲区的复杂性开始。

With a write-through cache the most important improvement is a write buffer of the proper size. Write buffers, however, do complicate memory accesses because they might hold the updated value of a location needed on a read miss.

> 通过写入缓存，最重要的改进是适当尺寸的写缓冲区。但是，写缓冲区确实使内存访问变得复杂，因为它们可能会保留读取失误所需的位置的更新值。

Example Look at this code sequence:

> 示例查看此代码序列：

Assume a direct-mapped, write-through cache that maps 512 and 1024 to the same block, and a four-word write buffer that is not checked on a read miss. Will the value in x2 always be equal to the value in x3?

> 假设直接映射的直通缓存将 512 和 1024 映射到同一块，以及一个未在读取失误上检查的四个字写缓冲区。x2 中的值始终等于 x3 中的值吗？

_Answer_ Using the terminology from [Chapter 2](#_bookmark46), this is a read-after-write data hazard in memory. Let’s follow a cache access to see the danger. The data in x3 are placed into the write buffer after the store. The following load uses the same cache index and is therefore a miss. The second load instruction tries to put the value in location 512 into register x2; this also results in a miss. If the write buffer hasn’t completed writing to location 512 in memory, the read of location 512 will put the old, wrong value into the cache block, and then into x2. Without proper precautions, x3x1 would not be equal to x2!

> _answer_使用[第 2 章](#_ bookmark46)中的术语，这是记忆中的读取过程中的读取数据危害。让我们遵循缓存访问以查看危险。X3 中的数据放在商店后的写缓冲区中。以下负载使用相同的缓存索引，因此是一个错过的。第二个负载指令试图将值放入位置 512 中的寄存器 x2；这也导致错过。如果写缓冲区尚未完成记忆中的位置 512 的写作，则位置 512 的读取将使旧的错误值放入缓存块中，然后将其放入 x2 中。没有适当的预防措施，x3x1 将不等于 x2！

The simplest way out of this dilemma is for the read miss to wait until the write buffer is empty. The alternative is to check the contents of the write buffer on a read miss, and if there are no conflicts and the memory system is available, let the read miss continue. Virtually all desktop and server processors use the latter approach, giving reads priority over writes.

> 解决这一困境的最简单方法是让阅读小姐等到写缓冲区为空。替代方法是检查读取失误上的写缓冲区的内容，如果没有冲突并且可用内存系统，请继续阅读 Miss。实际上，所有桌面和服务器处理器都使用后一种方法，从而优先于写入。

The cost of writes by the processor in a write-back cache can also be reduced. Suppose a read miss will replace a dirty memory block. Instead of writing the dirty block to memory, and then reading memory, we could copy the dirty block to a buffer, then read memory, and _then_ write memory. This way the processor read, for which the processor is probably waiting, will finish sooner. Similar to the pre- vious situation, if a read miss occurs, the processor can either stall until the buffer is empty or check the addresses of the words in the buffer for conflicts.

> 处理器在写下缓存中的写入成本也可以降低。假设读取小姐将替换一个肮脏的内存块。我们可以将肮脏的块复制到缓冲区，然后读取内存，然后读取内存，而不是将肮脏的块写入内存，然后读取内存。这样，处理器可能会等待的处理器读取将更早完成。与预先的情况类似，如果发生读数，则处理器可以停滞直到缓冲区空，或者检查缓冲区中单词的地址是否存在冲突。

Now that we have five optimizations that reduce cache miss penalties or miss rates, it is time to look at reducing the final component of average memory access time. Hit time is critical because it can affect the clock rate of the processor; in many processors today the cache access time limits the clock cycle rate, even for processors that take multiple clock cycles to access the cache. Hence, a fast hit time is multiplied in importance beyond the average memory access time for- mula because it helps everything.

> 现在，我们已经有五个优化来减少缓存失误或错过费率，现在是时候减少平均内存访问时间的最终组成部分了。命中时间至关重要，因为它会影响处理器的时钟速率；在当今的许多处理器中，缓存访问时间限制了时钟周期速率，即使对于处理多个时钟周期访问缓存的处理器也是如此。因此，快速命中时间的重要性超出了平均内存访问时间，因为它可以帮助所有事物。

### Sixth Optimization: Avoiding Address Translation During Indexing of the Cache to Reduce Hit Time

> ###第六个优化：避免在缓存索引期间的地址翻译以减少命中时间

Even a small and simple cache must cope with the translation of a virtual address from the processor to a physical address to access memory. As described in [Section B.4](#virtual-memory), processors treat main memory as just another level of the memory hierarchy, and thus the address of the virtual memory that exists on disk must be mapped onto the main memory.

> 即使是一个小而简单的缓存也必须应对从处理器到物理地址的虚拟地址转换到访问内存。如[B.4](＃Virtual-Memory)中所述，处理器将主内存视为内存层次结构的另一个级别，因此必须将存在于磁盘上的虚拟内存的地址映射到主内存上。

The guideline of making the common case fast suggests that we use virtual addresses for the cache, because hits are much more common than misses. Such caches are termed _virtual caches_, with _physical cache_ used to identify the tradi- tional cache that uses physical addresses. As we will shortly see, it is important to distinguish two tasks: indexing the cache and comparing addresses. Thus, the issues are whether a virtual or physical address is used to index the cache and whether a virtual or physical address is used in the tag comparison. Full virtual addressing for both indices and tags eliminates address translation time from a cache hit. Then why doesn’t everyone build virtually addressed caches?

> 使常见案例快速的指南表明，我们为缓存使用虚拟地址，因为命中比错过更为普遍。此类缓存称为_virtual caches_，用_ Physical Cache_用于识别使用物理地址的传统缓存。正如我们不久将看到的，区分两个任务很重要：索引缓存和比较地址。因此，问题是使用虚拟地址或物理地址来索引缓存以及标签比较中是否使用虚拟地址或物理地址。索引和标签的完整虚拟地址消除了缓存命中的地址翻译时间。那么，为什么每个人都不构建虚拟地解决缓存呢？

One reason is protection. Page-level protection is checked as part of the virtual to physical address translation, and it must be enforced no matter what. One solu- tion is to copy the protection information from the TLB on a miss, add a field to hold it, and check it on every access to the virtually addressed cache.

> 原因之一是保护。在虚拟地址翻译的一部分中检查页面级保护，无论如何都必须执行。一种解决方案是从 TLB 上复制 TLB 的保护信息，添加一个字段以持有它，并在对虚拟寻址的缓存中进行检查。

Another reason is that every time a process is switched, the virtual addresses refer to different physical addresses, requiring the cache to be flushed. [Figure B.16](#_bookmark455) shows the impact on miss rates of this flushing. One solution is to increase the width of the cache address tag with a _process-identifier tag_ (PID). If the operating system assigns these tags to processes, it only need flush the cache when a PID is recycled; that is, the PID distinguishes whether or not the data in the cache are for this program. [Figure B.16](#_bookmark455) shows the improvement in miss rates by using PIDs to avoid cache flushes.

> 另一个原因是，每次切换过程时，虚拟地址都会引用不同的物理地址，要求缓存刷新。[图 B.16](#_ bookmark455)显示了对这种冲洗率的错过的影响。一种解决方案是使用_process-stientifier tag_(PID)增加缓存地址标签的宽度。如果操作系统将这些标签分配给流程，则只需回收 PID 时即可冲洗缓存；也就是说，PID 区分了缓存中的数据是否用于此程序。[图 B.16](#_ bookmark455)通过使用 PID 避免缓存冲洗，显示了错过费率的改善。

Figure B.16 Miss rate versus virtually addressed cache size of a program measured three ways: without process switches (uniprocess), with process switches using a process-identifier tag (PID), and with process switches but without PIDs (purge). PIDs increase the uniprocess absolute miss rate by 0.3%–0.6% and save 0.6%–4.3% over purging. [Agarwal (1987)](#_bookmark918) collected these statistics for the Ultrix operating system run- ning on a VAX, assuming direct-mapped caches with a block size of 16 bytes. Note that the miss rate goes up from 128 to 256 K. Such nonintuitive behavior can occur in caches because changing size changes the mapping of memory blocks onto cache blocks, which can change the conflict miss rate.

> 图 B.16 误率与虚拟解决程序的高速缓存大小，测量了三种方法：没有过程开关(UniProcess)，使用过程识别符标签(PID)以及过程开关，但没有 PID(清除)。PIDS 将独立技术的绝对损失率提高了 0.3％–0.6％，并节省了 0.6％–4.3％。[Agarwal(1987)](#_ bookmark918)假设块大小为 16 个字节，则在 VAX 上收集了 Ultrix 操作系统运行的这些统计数据。请注意，错过率从 128 K 上升到 256K。这种非直觉行为可能发生在缓存中，因为更改尺寸会更改内存块的映射到缓存块上，这可能会改变冲突率失误率。

A third reason why virtual caches are not more popular is that operating sys- tems and user programs may use two different virtual addresses for the same phys- ical address. These duplicate addresses, called _synonyms_ or _aliases_, could result in two copies of the same data in a virtual cache; if one is modified, the other will have the wrong value. With a physical cache this wouldn’t happen, because the accesses would first be translated to the same physical cache block.

> 虚拟缓存不太受欢迎的第三个原因是操作系统和用户程序可能会使用两个不同的虚拟地址用于同一物理地址。这些重复的地址，称为_synonyms_或_aliases_，可能会在虚拟缓存中产生两个相同数据的副本。如果一个修改，另一个将具有错误的值。使用物理缓存不会发生，因为访问将首先转换为同一物理缓存块。

Hardware solutions to the synonym problem, called _antialiasing_, guarantee every cache block a unique physical address. For example, the AMD Opteron uses a 64 KiB instruction cache with a 4 KiB page and two-way set associativity; hence, the hardware must handle aliases involved with the three virtual address bits in the set index. It avoids aliases by simply checking all eight possible locations on a miss—two blocks in each of four sets—to be sure that none matches the phys- ical address of the data being fetched. If one is found, it is invalidated, so when the new data are loaded into the cache their physical address is guaranteed to be unique.

> 同义词问题的硬件解决方案，称为_antialiasing_，保证每个缓存块块一个唯一的物理地址。例如，AMD Opteron 使用 64 KIB 指令缓存，带有 4 KIB 页面和双向集合的关联性；因此，硬件必须处理与集合索引中三个虚拟地址位有关的别名。它可以通过简单地检查错过的所有八个可能位置(两组中的两组中的两个块)来避免别名，以确保没有一个匹配所获取数据的物理地址。如果找到一个，则将其无效，因此，当将新数据加载到缓存中时，他们的物理地址将是唯一的。

Software can make this problem much easier by forcing aliases to share some address bits. An older version of UNIX from Sun Microsystems, for example, required all aliases to be identical in the last 18 bits of their addresses; this restric- tion is called _page coloring_. Note that page coloring is simply set associative map- ping applied to virtual memory: the 4 KiB (2<sup>12</sup>) pages are mapped using 64 (2<sup>6</sup>) sets to ensure that the physical and virtual addresses match in the last 18 bits. This restriction means a direct-mapped cache that is 2<sup>18</sup> (256 K) bytes or smaller can never have duplicate physical addresses for blocks. From the perspective of the cache, page coloring effectively increases the page offset, as software guarantees that the last few bits of the virtual and physical page address are identical.

> 软件可以通过强迫别名共享某些地址位来使这个问题更容易。例如，来自 Sun Microsystems 的 UNIX 的较旧版本要求所有别名在其地址的最后 18 位相同；此修复称为_ Page Coloring_。请注意，Page Coloring 仅设置为“关联映射”，应用于虚拟内存：使用 64(2 <sup> 6 </sup>)设置 4 KIB(2 <sup> 12 </sup>)页面，以确保物理和虚拟地址匹配最近 18 位。该限制意味着直接映射的缓存为 2 <sup> 18 </sup>(256 K)字节或较小的字节永远无法具有块的重复物理地址。从缓存的角度来看，页面着色有效地增加了页面偏移，因为软件保证了虚拟和物理页面地址的最后几个位相同。

The final area of concern with virtual addresses is I/O. I/O typically uses phys- ical addresses and thus would require mapping to virtual addresses to interact with a virtual cache. (The impact of I/O on caches is further discussed in Appendix D.)

> 虚拟地址关注的最终区域是 I/O。I/O 通常使用物理地址，因此需要映射到虚拟地址以与虚拟缓存进行交互。(在附录 D 中进一步讨论了 I/O 对缓存的影响。)

One alternative to get the best of both virtual and physical caches is to use part of the page offset—the part that is identical in both virtual and physical addresses—to index the cache. At the same time as the cache is being read using that index, the virtual part of the address is translated, and the tag match uses phys- ical addresses.

> 获得最好的虚拟和物理缓存中最好的替代方法是使用页面偏移的一部分(在虚拟地址和物理地址中相同的部分)来索引缓存。同时，使用该索引读取缓存，地址的虚拟部分被翻译，标签匹配使用了物理地址。

This alternative allows the cache read to begin immediately, and yet the tag comparison is still with physical addresses. The limitation of this _virtually indexed, physically tagged_ alternative is that a direct-mapped cache can be no bigger than the page size. For example, in the data cache in [Figure B.5](#_bookmark443) on page B-13, the index is 9 bits and the cache block offset is 6 bits. To use this trick, the virtual page size would have to be at least 2<sup>(9+6)</sup> bytes or 32 KiB. If not, a portion of the index must be translated from virtual to physical address. [Figure B.17](#_bookmark456) shows the organization of the caches, translation lookaside buffers (TLBs), and virtual memory when this technique is used.

> 这种替代方法允许缓存读取，但标签比较仍与物理地址有关。此_置索引的限制，物理上标记的_替代方案是直接映射的高速缓存不得大于页面大小。例如，在[图 B.5](#_ bookmark443)中的数据缓存中，索引为 9 位，缓存块偏移量为 6 位。要使用此技巧，虚拟页面大小必须至少为 2 <sup>(9+6)</sup> bytes 或 32 KIB。如果没有，则必须将索引的一部分从虚拟地址转换为物理地址。[图 B.17](#_ bookmark456)显示了使用此技术时，显示了缓存，翻译 LookAside Buffers(TLB)和虚拟内存的组织。

![](./media/image484.png)

![](./media/image485.png)

Figure B.17 The overall picture of a hypothetical memory hierarchy going from virtual address to L2 cache access. The page size is 16 KiB. The TLB is two-way set associative with 256 entries. The L1 cache is a direct-mapped 16 KiB, and the L2 cache is a four-way set associative with a total of 4 MiB. Both use 64-byte blocks. The virtual address is 64 bits and the physical address is 40 bits.

> 图 B.17 从虚拟地址到 L2 缓存访问的假设内存层次结构的总体图。页面大小为 16 KIB。TLB 是带有 256 个条目的双向套件联合。L1 缓存是直接映射的 16 KIB，L2 高速缓存是一个四向套件的关联，总计 4 MIB。两者都使用 64 个字节块。虚拟地址为 64 位，物理地址为 40 位。

Associativity can keep the index in the physical part of the address and yet still support a large cache. Recall that the size of the index is controlled by this formula:

> 关联性可以将索引保留在地址的物理部分中，但仍支持大量缓存。回想一下该索引的大小由此公式控制：

For example, doubling associativity and doubling the cache size does not change the size of the index. The IBM 3033 cache, as an extreme example, is 16-way set associative, even though studies show there is little benefit to miss rates above 8-way set associativity. This high associativity allows a 64 KiB cache to be addressed with a physical index, despite the handicap of 4 KiB pages in the IBM architecture.

> 例如，加倍关联和加倍缓存大小不会改变索引的大小。作为一个极端的例子，IBM 3033 缓存是 16 向相关的，尽管研究表明，在 8 向设置关联以上的错率几乎没有好处。尽管 IBM 体系结构中有 4 个 KIB 页面，但这种高关联性允许使用物理索引解决 64 KIB 缓存。

Figure B.18 Summary of basic cache optimizations showing impact on cache performance and complexity for the techniques in this appendix. Generally a technique helps only one factor. + means that the technique improves the factor, – means it hurts that factor, and blank means it has no impact. The complexity measure is subjective, with 0 being the easiest and 3 being a challenge.

> 图 B.18 基本缓存优化的摘要，显示了此附录中该技术对缓存性能和复杂性的影响。通常，一种技术仅有助于一个因素。+ 意味着该技术改善了因素，这意味着它会伤害该因素，而空白意味着它没有影响。复杂性度量是主观的，其中 0 是最简单的，3 是一个挑战。

### Summary of Basic Cache Optimization

> ###基本缓存优化摘要

The techniques in this section to improve miss rate, miss penalty, and hit time gen- erally impact the other components of the average memory access equation as well as the complexity of the memory hierarchy. [Figure B.18](#_bookmark457) summarizes these tech- niques and estimates the impact on complexity, with + meaning that the technique improves the factor, – meaning it hurts that factor, and blank meaning it has no impact. No optimization in this figure helps more than one category.

> 在本节中提高错过率，错过罚款和达到时间的技术中的技术在通常影响平均内存访问方程式的其他组成部分以及内存层次结构的复杂性。[图 B.18](#_ bookmark457)总结了这些技术，并估算了对复杂性的影响， + 意味着该技术改善了因素，这意味着它会损害该因素，并且空白意味着它没有影响。该图中没有优化有助于多个类别。
