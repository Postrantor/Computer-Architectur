> [!note]
> CPU 的指令设计时候会考虑预测
> 那在线程调度的时候是否也可以参考这样的设计逻辑
> CPU 有缓存命中，线程是否也可以有线程命中？
> 另外，CPU 在考虑使用预测来增加执行速度的时候，也要考虑没有预测对的时候该怎么做
> 同样的线程也是，结合前面两篇论文的中断线程化，是不是这样的构想具有一定的可行性？！

> ![note]
> 对于指令预测这样的动作，可以用机器人技术中的运动学、动力学的相关技术来解释吗？

## Advanced Techniques for Instruction Delivery and Speculation

In a high-performance pipeline, especially one with multiple issues, predicting branches well is not enough; we actually have to be able to deliver a high-bandwidth instruction stream. In recent multiple-issue processors, this has meant delivering 4–8 instructions every clock cycle. We look at methods for increasing instruction delivery bandwidth first. We then turn to a set of key issues in implementing advanced speculation techniques, including the use of register renaming versus reorder buffers, the aggressiveness of speculation, and a technique called _value prediction_, which attempts to predict the result of a computation and which could further enhance ILP.

> 在高性能的管道中，尤其是一个有多个问题的管道，很好地预测分支是不够的。实际上，我们必须能够提供高带宽指令流。在最近的多发处理器中，这意味着每个时钟周期提供 4-8 个说明。我们首先研究了提高指令输送带宽的方法。然后，我们求助于实施高级推测技术的一组关键问题，包括使用寄存器重命名与重新排序缓冲区，投机的侵略性以及一种称为 _Value Prediction_ 的技术，该技术试图预测计算的结果，并可以进一步进行进一步的计算结果增强 ILP。

### Increasing Instruction Fetch Bandwidth

A multiple-issue processor will require that the average number of instructions fetched every clock cycle be at least as large as the average throughput. Of course, fetching these instructions requires wide enough paths to the instruction cache, but the most difficult aspect is handling branches. In this section, we look at two methods for dealing with branches and then discuss how modern processors integrate the instruction prediction and prefetch functions.

> 多发处理器将要求每个时钟周期的平均指令数量至少与平均吞吐量一样大。当然，获取这些说明需要足够的路径到达指令缓存，但最困难的方面是处理分支。在本节中，我们研究了处理分支机构的两种方法，然后讨论现代处理器如何整合说明预测和预取功能。

##### _Branch-Target Buffers_

To reduce the branch penalty for our simple five-stage pipeline, as well as for deeper pipelines, we must know whether the as-yet-undecoded instruction is a branch and, if so, what the next program counter (PC) should be. If the instruction is a branch and we know what the next PC should be, we can have a branch penalty of zero. A branch-prediction cache that stores the predicted address for the next instruction after a branch is called a _branch-target buffer_ or _branch-target cache._ [Figure 3.25](#_bookmark128) shows a branch-target buffer.

> 为了减少我们简单的五阶段管道以及更深层管道的分支机构罚款，我们必须知道尚未编码的指令是否是分支机构，如果是的，则下一个程序计数器(PC)应该是什么。如果指令是分支机构，我们知道下一个 PC 应该是什么，我们可以将分支罚款为零。一个分支预测缓存，该缓存存储在分支后将下一个指令的预测地址称为 _Branch-target-target buffer_ 或 _ranch-target-target cache._ [图 3.25](#_bookmark128) 显示了分支机构 - 塔吉式缓冲区。

Because a branch-target buffer predicts the next instruction address and will send it out _before_ decoding the instruction, we _must_ know whether the fetched instruction is predicted as a taken branch. If the PC of the fetched instruction matches an address in the prediction buffer, then the corresponding predicted PC is used as the next PC. The hardware for this branch-target buffer is essentially identical to the hardware for a cache.

> 由于分支机构 - 目标缓冲区可以预测下一个指令地址，并将其发送给 _before_ 解码，因此我们* must*知道是否将获取的指令预测为删除分支。如果获取指令的 PC 与预测缓冲区中的地址匹配，则将相应的预测 PC 用作下一个 PC。该分支靶标缓冲区的硬件与缓存的硬件基本相同。

If a matching entry is found in the branch-target buffer, fetching begins immediately at the predicted PC. Note that unlike a branch-prediction buffer, the predictive entry must be matched to this instruction because the predicted PC will be sent

> 如果在分支目标缓冲区中发现了匹配条目，则立即从预测的 PC 开始获取。请注意，与分支预测缓冲区不同，预测条目必须与此指令匹配，因为预测 PC 将发送

Yes: then instruction is taken branch and predicted PC should be used as the next PC

> 是：然后将指令分支分支，预测 PC 应作为下一个 PC

Figure 3.25 A branch-target buffer. The PC of the instruction being fetched is matched against a set of instruction addresses stored in the first column; these represent the addresses of known branches. If the PC matches one of these entries, then the instruction being fetched is a taken branch, and the second field, predicted PC, contains the prediction for the next PC after the branch. Fetching begins immediately at that address. The third field, which is optional, may be used for extra prediction state bits.

> 图 3.25 分支目标缓冲区。要获取的指令的 PC 与第一列中存储的一组指令地址匹配；这些代表已知分支的地址。如果 PC 与这些条目之一匹配，则获取的指令是一个分支，而第二个字段预测 PC 将包含分支之后的下一个 PC 的预测。从该地址立即开始获取。可选的第三个字段可用于额外的预测状态位。

out before it is known whether this instruction is even a branch. If the processor did not check whether the entry matched this PC, then the wrong PC would be sent out for instructions that were not branches, resulting in worse performance. We need to store only the predicted-taken branches in the branch-target buffer because an untaken branch should simply fetch the next sequential instruction, as if it were not a branch.

> 在知道此指令是否甚至是分支之前。如果处理器不检查条目是否匹配该 PC，则将发送错误的 PC 以获取不是分支的说明，从而导致性能较差。我们需要仅存储预测的分支在分支目标缓冲区中，因为未归咎的分支应该简单地获取下一个顺序指令，就好像它不是分支一样。

[Figure 3.26](#_bookmark129) shows the steps when using a branch-target buffer for a simple fivestage pipeline. As we can see in this figure, there will be no branch delay if a branch-prediction entry is found in the buffer and the prediction is correct. Otherwise, there will be a penalty of at least two clock cycles. Dealing with the mispredictions and misses is a significant challenge because we typically will have to halt instruction fetch while we rewrite the buffer entry. Thus we want to make this process fast to minimize the penalty.

> [图 3.26](#_bookmark129) 显示了使用分支靶向缓冲区进行简单的 Fivestage 管道时的步骤。正如我们在此图中看到的那样，如果在缓冲区中找到分支预测条目，并且预测是正确的，则不会有分支延迟。否则，至少将罚款两个时钟周期。处理错误的预测和错过是一个重大挑战，因为我们通常必须在重写缓冲区输入时停止指导获取。因此，我们希望快速地进行此过程，以最大程度地减少罚款。

Figure 3.26 The steps involved in handling an instruction with a branch-target buffer.

> 图 3.26 使用分支目标缓冲区处理指令所涉及的步骤。

To evaluate how well a branch-target buffer works, we first must determine the penalties in all possible cases. [Figure 3.27](#_bookmark130) contains this information for a simple five-stage pipeline.

> 为了评估分支机构缓冲区的效果，我们首先必须在所有可能的情况下确定处罚。[图 3.27](#_bookmark130) 包含此信息的简单五阶段管道。

Example Determine the total branch penalty for a branch-target buffer assuming the penalty cycles for individual mispredictions in [Figure 3.27](#_bookmark130). Make the following assumptions about the prediction accuracy and hit rate:

> 示例确定分支机构缓冲区的总分支机构罚款，假设[图 3.27]中的单个错误预测的罚款周期(#\_bookmark130)。对预测准确性和命中率做出以下假设：

Prediction accuracy is 90% (for instructions in the buffer).

> 预测准确性为 90％(用于缓冲区中的说明)。

Hit rate in the buffer is 90% (for branches predicted taken).

> 缓冲液中的命中率为 90％(对于预测的分支机构)。

_Answer_ We compute the penalty by looking at the probability of two events: the branch is predicted taken but ends up being not taken, and the branch is taken but is not found in the buffer. Both carry a penalty of two cycles.

> _answer_ 我们通过查看两个事件的概率来计算罚款：预测分支已被捕获但最终未拿走，并且将分支拿走，但在缓冲区中找不到。两者都受到两个周期的罚款。

The improvement from dynamic branch prediction will grow as the pipeline length, and thus the branch delay grows; in addition, better predictors will yield a greater performance advantage. Modern high-performance processors have branch misprediction delays on the order of 15 clock cycles; clearly, accurate prediction is critical!

> 动态分支预测的改进将随着管道长度而增长，因此分支延迟的增长。此外，更好的预测因子将产生更大的性能优势。现代高性能处理器在 15 个时钟周期的顺序上有分支机构的错误预测延迟；显然，准确的预测至关重要！

One variation on the branch-target buffer is to store one or more _target instructions_ instead of, or in addition to, the predicted _target address_. This variation has two potential advantages. First, it allows the branch-target buffer access to take longer than the time between successive instruction fetches, possibly allowing a larger branch-target buffer. Second, buffering the actual target instructions allows us to perform an optimization called _branch folding_. Branch folding can be used to obtain 0-cycle unconditional branches and sometimes 0-cycle conditional branches. As we will see, the Cortex A-53 uses a single-entry branch target cache that stores the predicted target instructions.

> 分支目标缓冲区上的一种变体是存储一个或多个 *target 指令*而不是或除了预测的 _target 地址_。这种变化具有两个潜在的优势。首先，它允许分支目标缓冲区的访问时间比连续的指令提取之间的时间更长，这可能允许更大的分支目标缓冲区。其次，对实际目标指令进行缓冲使我们可以执行称为 *Branch 折叠*的优化。分支折叠可用于获得 0 周期的无条件分支，有时是 0 周期条件分支。正如我们将看到的那样，Cortex A-53 使用单个分支目标缓存，该目标缓存存储了预测的目标指令。

Consider a branch-target buffer that buffers instructions from the predicted path and is being accessed with the address of an unconditional branch. The only function of the unconditional branch is to change the PC. Thus, when the branch-target buffer signals a hit and indicates that the branch is unconditional, the pipeline can simply substitute the instruction from the branch-target buffer in place of the instruction that is returned from the cache (which is the unconditional branch). If the processor is issuing multiple instructions per cycle, then the buffer will need to supply multiple instructions to obtain the maximum benefit. In some cases, it may be possible to eliminate the cost of a conditional branch.

> 考虑一个分支机构缓冲区，该缓冲区从预测路径中缓冲指令，并通过无条件分支的地址访问。无条件分支的唯一功能是更改 PC。因此，当分支 - 靶标缓冲区信号并指示分支是无条件的时，管道可以简单地从分支靶标缓冲区代替从缓存返回的指令(这是无条件分支)的指令。如果处理器每个周期发出多个说明，则缓冲区将需要提供多个说明以获得最大收益。在某些情况下，可能有可能消除条件分支的成本。

Figure 3.27 Penalties for all possible combinations of whether the branch is in the buffer and what it actually does, assuming we store only taken branches in the buffer. There is no branch penalty if everything is correctly predicted and the branch is found in the target buffer. If the branch is not correctly predicted, the penalty is equal to 1 clock cycle to update the buffer with the correct information (during which an instruction cannot be fetched) and 1 clock cycle, if needed, to restart fetching the next correct instruction for the branch. If the branch is not found and taken, a 2-cycle penalty is encountered, during which time the buffer is updated.

> 图 3.27 对分支是否在缓冲区中及其实际功能的所有可能组合的惩罚，假设我们仅存储在缓冲区中的分支。如果正确预测所有内容并且在目标缓冲区中找到分支，则没有分支机构。如果分支未正确预测，则罚款等于 1 个时钟周期，以使用正确的信息(在此期间无法获取指令)和 1 个时钟周期(如果需要)，以便重新启动以获取下一个正确的指令分支。如果找不到分支，则会遇到 2 周期的罚款，在此期间更新缓冲区。

### Specialized Branch Predictors: Predicting Procedure Returns, Indirect Jumps, and Loop Branches

As we try to increase the opportunity and accuracy of speculation, we face the challenge of predicting indirect jumps, that is, jumps whose destination address varies at runtime. High-level language programs will generate such jumps for indirect procedure calls, select or case statements, and FORTRAN-computed gotos, although many indirect jumps simply come from procedure returns. For example, for the SPEC95 benchmarks, procedure returns account for more than 15% of the branches and the vast majority of the indirect jumps on average. For objectoriented languages such as C++ and Java, procedure returns are even more frequent. Thus focusing on procedure returns seems appropriate.

> 当我们试图提高投机的机会和准确性时，我们面临着预测间接跳跃的挑战，即，在运行时目标地址会变化的跳跃。高级语言程序将为间接过程调用，选择或案例语句以及 FORTRAN 计算的 GOTO 生成此类跳跃，尽管许多间接跳跃仅来自过程返回。例如，对于 SPEC95 基准，过程返回的帐户占分支机构的 15％以上，并且间接大部分的平均跳跃平均跳跃。对于诸如 C ++ 和 Java 之类的反对语言，过程返回更加频繁。因此，专注于程序返回似乎是合适的。

Though procedure returns can be predicted with a branch-target buffer, the accuracy of such a prediction technique can be low if the procedure is called from multiple sites and the calls from one site are not clustered in time. For example, in SPEC CPU95, an aggressive branch predictor achieves an accuracy of less than 60% for such return branches. To overcome this problem, some designs use a small buffer of return addresses operating as a stack. This structure caches the most recent return addresses, pushing a return address on the stack at a call and popping one off at a return. If the cache is sufficiently large (i.e., as large as the maximum call depth), it will predict the returns perfectly. [Figure 3.28](#_bookmark131) shows the performance of such a return buffer with 0–16 elements for a number of the SPEC CPU95 benchmarks. We will use a similar return predictor when we examine the studies of ILP in [Section 3.10](#cross-cutting-issues). Both the Intel Core processors and the AMD Phenom processors have return address predictors.

> 尽管可以使用分支目标缓冲区来预测过程返回，但是如果从多个站点调用该过程并且来自一个站点的调用没有及时聚集，则这种预测技术的准确性可能会很低。 例如，在 SPEC CPU95 中，激进的分支预测器对于此类返回分支的准确率低于 60%。 为了克服这个问题，一些设计使用一个小的返回地址缓冲区作为堆栈运行。 此结构缓存最近的返回地址，在调用时将返回地址压入堆栈，并在返回时弹出一个。 如果缓存足够大（即与最大调用深度一样大），它将完美地预测返回。 [图 3.28](#_bookmark131) 显示了具有 0-16 个元素的此类返回缓冲区在许多 SPEC CPU95 基准测试中的性能。 在 [第 3.10 节](#cross-cutting-issues) 中检查 ILP 研究时，我们将使用类似的回报预测器。 Intel Core 处理器和 AMD Phenom 处理器都有返回地址预测器。

In large server applications, indirect jumps also occur for various function calls and control transfers. Predicting the targets of such branches is not as simple as in a procedure return. Some processors have opted to add specialized predictors for all indirect jumps, whereas others rely on a branch target buffer.

> 在大型服务器应用程序中，各种功能调用和控制传输也会发生间接跳跃。预测此类分支的目标并不像过程返回那样简单。一些处理器选择为所有间接跳跃添加专门的预测指标，而另一些则依靠分支目标缓冲区。

Although a simple predictor like gshare does a good job of predicting many conditional branches, it is not tailored to predicting loop branches, especially for long running loops. As we observed earlier, the Intel Core i7 920 used a specialized loop branch predictor. With the emergence of tagged hybrid predictors, which are as good at predicting loop branches, some recent designers have opted to put the resources into larger tagged hybrid predictors rather than a separate loop branch predictor.

> 尽管像 Gshare 这样的简单预测指标在预测许多条件分支方面做得很好，但它并不是针对预测循环分支的量身定制的，尤其是对于长期运行循环。正如我们之前观察到的那样，英特尔核心 i7 920 使用了专门的循环分支预测指标。随着标记的混合预测变量的出现，这些预测因子擅长预测循环分支，一些最近的设计师选择将资源放入较大的标记混合预测指标，而不是单独的环路分支预测器中。

![](../media/image209.png)

Figure 3.28 Prediction accuracy for a return address buffer operated as a stack on a number of SPEC CPU95 benchmarks. The accuracy is the fraction of return addresses predicted correctly. A buffer of 0 entries implies that the standard branch prediction is used. Because call depths are typically not large, with some exceptions, a modest buffer works well. These data come from [Skadron et al. (1999)](#_bookmark1005) and use a fix-up mechanism to prevent corruption of the cached return addresses.

> 图 3.28 返回地址缓冲区的预测准确性作为堆栈在许多 SPEC CPU95 基准测试中运行。精度是正确预测的返回地址的比例。0 个条目的缓冲区意味着使用了标准分支预测。由于呼叫深度通常不大，但有些例外，适度的缓冲区效果很好。这些数据来自 [Skadron 等人。(1999)](#_bookmark1005)，并使用固定机制来防止缓存的返回地址的损坏。

##### _Integrated Instruction Fetch Units_

To meet the demands of multiple-issue processors, many recent designers have chosen to implement an integrated instruction fetch unit as a separate autonomous unit that feeds instructions to the rest of the pipeline. Essentially, this amounts to recognizing that characterizing instruction fetch as a simple single pipe stage given the complexities of multiple issue is no longer valid.

> 为了满足多发处理器的需求，许多最近的设计师选择将集成指令提供作为单独的自主单元，将指令馈送到管道的其余部分。从本质上讲，这相当于认识到将指令表征为简单的单管阶段，鉴于多个问题的复杂性不再有效。

Instead, recent designs have used an integrated instruction fetch unit that integrates several functions:

> 取而代之的是，最近的设计使用了一个集成的指令获取单元，该单元集成了几个功能：

1. _Integrated branch prediction_—The branch predictor becomes part of the instruction fetch unit and is constantly predicting branches, so as to drive the fetch pipeline.

> 1. _集成分支预测_-分支预测器成为指令提取单元的一部分，并不断预测分支，以驱动提取管道。

2. _Instruction prefetch_—To deliver multiple instructions per clock, the instruction fetch unit will likely need to fetch ahead. The unit autonomously manages the prefetching of instructions (see [Chapter 2](#_bookmark46) for a discussion of techniques for doing this), integrating it with branch prediction.

> 2. _ Instruction prefetch_-要为每个时钟提供多个指令，指令获取单元可能需要提前获取。该单元自主管理指令的预摘要(请参阅[第 2 章](#_bookmark46)，以讨论这样做的技术)，将其与分支预测集成在一起。

3. _Instruction memory access and buffering_—When fetching multiple instructions per cycle, a variety of complexities are encountered, including the difficulty that fetching multiple instructions may require accessing multiple cache lines. The instruction fetch unit encapsulates this complexity, using prefetch to try to hide the cost of crossing cache blocks. The instruction fetch unit also provides buffering, essentially acting as an on-demand unit to provide instructions to the issue stage as needed and in the quantity needed.

> 3. _ Instruction Memory 访问和缓冲_-每个周期获取多个指令时，会遇到各种复杂性，包括很难获取多个指令可能需要访问多个缓存线。指令获取单元封装了这种复杂性，使用预取尝试隐藏越过缓存块的成本。指令提取单元还提供缓冲，实质上是按需单元，可根据需要和需要的数量为问题阶段提供指令。

Virtually all high-end processors now use a separate instruction fetch unit connected to the rest of the pipeline by a buffer containing pending instructions.

> 实际上，所有高端处理器现在都使用单独的指令获取单元，该指令通过包含已待处理指令的缓冲区连接到管道的其余部分。

### Speculation: Implementation Issues and Extensions

In this section, we explore five issues that involve the design trade-offs and challenges in multiple-issue and speculation, starting with the use of register renaming, the approach that is sometimes used instead of a reorder buffer. We then discuss one important possible extension to speculation on control flow: an idea called value prediction.

> 在本节中，我们探讨了五个问题，这些问题涉及多个问题和投机中的设计权衡和挑战，从使用寄存器重命名开始，该方法有时使用的方法代替了重新订购缓冲区。然后，我们讨论对控制流的投机的一个重要扩展：一种称为价值预测的想法。

##### _Speculation Support: Register Renaming Versus Reorder Buffers_

One alternative to the use of a reorder buffer (ROB) is the explicit use of a larger physical set of registers combined with register renaming. This approach builds on the concept of renaming used in Tomasulo’s algorithm and extends it. In Tomasulo’s algorithm, the values of the _architecturally visible registers_ (x0, . . . r31 and f0, . . . f31) are contained, at any point in execution, in some combination of the register set and the reservation stations. With the addition of speculation, register values may also temporarily reside in the ROB. In either case, if the processor does not issue new instructions for a period of time, all existing instructions will commit, and the register values will appear in the register file, which directly corresponds to the architecturally visible registers.

> 使用重新订购缓冲区(ROB)的一种替代方法是明确使用较大的物理寄存器集合与寄存器重命名。这种方法建立在 Tomasulo 算法中使用的重命名概念并扩展了它。在 tomasulo 的算法中，在执行的任何一个组合中，都包含 _architecturcible 可见寄存器_(x0，... r31 和 f0，……f31)的值。随着猜测的增加，寄存器值也可以暂时存在于 ROB 中。无论哪种情况，如果处理器都不在一段时间内发布新说明，则所有现有说明都将提交，并且寄存器值将出现在寄存器文件中，该文件直接对应于架构上可见的寄存器。

In the register-renaming approach, an extended set of physical registers is used to hold both the architecturally visible registers as well as temporary values. Thus the extended registers replace most of the function of the ROB and the reservation stations; only a queue to ensure that instructions complete in order is needed. During instruction issue, a renaming process maps the names of architectural registers to physical register numbers in the extended register set, allocating a new unused register for the destination. WAW and WAR hazards are avoided by renaming of the destination register, and speculation recovery is handled because a physical register holding an instruction destination does not become the architectural register until the instruction commits.

> 在寄存器更名的方法中，使用扩展的物理寄存器集来保存架构可见的寄存器和临时值。因此，扩展的寄存器取代了 ROB 和预订站的大部分功能；只有队列确保需要完成指令。在指令问题期间，重命名的过程将建筑寄存器的名称映射到扩展寄存器集中的物理寄存器编号，并为目的地分配新的未使用寄存器。通过重命名目的地登记册，可以避免 WAW 和战争危害，并且要处理猜测恢复，因为持有指令目的地的物理寄存器直到提交指令才成为建筑登记册。

The _renaming map_ is a simple data structure that supplies the physical register number of the register that currently corresponds to the specified architectural register, a function performed by the register status table in Tomasulo’s algorithm. When an instruction commits, the renaming table is permanently updated to indicate that a physical register corresponds to the actual architectural register, thus effectively finalizing the update to the processor state. Although an ROB is not necessary with register renaming, the hardware must still track instructions in a queue-like structure and update the renaming table in strict order.

> _RENAMING MAP_ 是一个简单的数据结构，可提供当前与指定的架构寄存器相对应的寄存器的物理寄存器号，这是由 Tomasulo 算法中的寄存器状态表执行的函数。指令提交时，重命名表将永久更新，以表明物理寄存器对应于实际的架构寄存器，从而有效地将更新最终确定为处理器状态。尽管重命名的登记量不需要 ROB，但硬件仍必须在类似队列的结构中跟踪说明，并以严格的顺序更新重命名表。

An advantage of the renaming approach versus the ROB approach is that instruction commit is slightly simplified because it requires only two simple actions: (1) record that the mapping between an architectural register number and physical register number is no longer speculative, and (2) free up any physical registers being used to hold the "older" value of the architectural register. In a design with reservation stations, a station is freed up when the instruction using it completes execution, and a ROB entry is freed up when the corresponding instruction commits.

> 重命名方法与 ROB 方法的优势是指令提交略有简化，因为它仅需要两个简单的操作：(1)记录架构寄存器编号和物理寄存器编号之间的映射不再是投机性的，并且(2)释放用于保持建筑寄存器的 "较旧" 价值的任何物理寄存器。在带有预订站的设计中，当使用该指令完成执行时，释放了一个站，并且当相应的指令提交时，ROB 条目被释放。

With register renaming, deallocating registers is more complex because before we free up a physical register, we must know that it no longer corresponds to an architectural register and that no further uses of the physical register are outstanding. A physical register corresponds to an architectural register until the architectural register is rewritten, causing the renaming table to point elsewhere. That is, if no renaming entry points to a particular physical register, then it no longer corresponds to an architectural register. There may, however, still be outstanding uses of the physical register. The processor can determine whether this is the case by examining the source register specifiers of all instructions in the functional unit queues. If a given physical register does not appear as a source and it is not designated as an architectural register, it may be reclaimed and reallocated.

> 通过注册重命名，DeallosingLosing 寄存器更加复杂，因为在我们释放物理寄存器之前，我们必须知道它不再与建筑寄存器相对应，并且没有进一步使用物理寄存器是未偿还的。物理寄存器对应于建筑寄存器，直到重写建筑寄存器为止，导致重命名表指向其他位置。也就是说，如果没有重命名的入口指向特定的物理寄存器，则它不再对应于建筑寄存器。但是，可能仍然有实物寄存器的出色用途。处理器可以通过检查功能单元队列中所有说明的源寄存器指定符来确定情况。如果给定的物理寄存器未作为源出现，也不被指定为建筑寄存器，则可以将其重新定位和重新分配。

Alternatively, the processor can simply wait until another instruction that writes the same architectural register commits. At that point, there can be no further uses of the older value outstanding. Although this method may tie up a physical register slightly longer than necessary, it is easy to implement and is used in most recent superscalars.

> 另外，处理器可以简单地等到编写相同架构寄存器提交的另一份指令。到那时，无法进一步使用未偿还的旧价值。尽管此方法可能将物理寄存器略长于必要的时间稍长，但它易于实现，并且在最近的超级标准中使用。

One question you may be asking is how do we ever know which registers are the architectural registers if they are constantly changing? Most of the time when the program is executing, it does not matter. There are clearly cases, however, where another process, such as the operating system, must be able to know exactly where the contents of a certain architectural register reside. To understand how this capability is provided, assume the processor does not issue instructions for some period of time. Eventually all instructions in the pipeline will commit, and the mapping between the architecturally visible registers and physical registers will become stable. At that point, a subset of the physical registers contains the architecturally visible registers, and the value of any physical register not associated with an architectural register is unneeded. It is then easy to move the architectural registers to a fixed subset of physical registers so that the values can be communicated to another process.

> 您可能会问的一个问题是，如果它们不断变化，我们如何知道它们是哪些寄存器？大多数情况下，该程序执行时，都无关紧要。但是，在某些情况下，另一个过程(例如操作系统)必须能够准确知道某个建筑寄存器的内容位于何处。要了解如何提供此功能，请假设处理器不会在一段时间内发布指令。最终，管道中的所有说明都将提交，并且在建筑上可见的寄存器和物理寄存器之间的映射将变得稳定。那时，物理寄存器的一个子集包含架构上可见的寄存器，并且不需要与建筑寄存器无关的任何物理寄存器的值。然后，很容易将架构寄存器移至物理寄存器的固定子集中，以便可以将值传达给另一个过程。

Both register renaming and reorder buffers continue to be used in high-end processors, which now feature the ability to have as many as 100 or more instructions (including loads and stores waiting on the cache) in flight. Whether renaming or a reorder buffer is used, the key complexity bottleneck for a dynamically scheduled superscalar remains issuing bundles of instructions with dependences within the bundle. In particular, dependent instructions in an issue bundle must be issued with the assigned virtual registers of the instructions on which they depend. A strategy for instruction issue with register renaming similar to that used for multiple issue with reorder buffers (see page 205) can be deployed, as follows:

> 寄存器重命名和重新订购缓冲区都在高端处理器中继续使用，现在，它们具有在飞行中具有多达 100 个或更多指令(包括加速器上等待的负载和存储)的能力。无论是使用重命名还是重新订购缓冲区，动态计划的超级标准的关键复杂性瓶颈仍在发出捆绑包中依赖性的指令捆绑。特别是，必须在其依赖指令的指定的虚拟登记册中发布问题捆绑包中的相关说明。指导问题的策略与重命名的登录名相似，类似于用于重新订购缓冲区的多个问题(请参阅第 205 页)，如下所示：

1. The issue logic reserves enough physical registers for the entire issue bundle (say, four registers for a four-instruction bundle with at most one register result per instruction).

> 1.问题逻辑保留在整个问题捆绑包上足够的物理寄存器(例如，每个指令最多有一个寄存器结果的四个指令捆绑包的四个寄存器)。

2. The issue logic determines what dependences exist within the bundle. If a dependence does not exist within the bundle, the register renaming structure is used to determine the physical register that holds, or will hold, the result on which instruction depends. When no dependence exists within the bundle, the result is from an earlier issue bundle, and the register renaming table will have the correct register number.

> 2.问题逻辑确定捆绑包中存在的依赖性。如果捆绑包中不存在依赖性，则使用寄存器重命名结构来确定指令所依赖的结果或将保留的物理寄存器。当捆绑包中不存在依赖性时，结果是从较早的问题捆绑包中，并且寄存器重命名表将具有正确的寄存器号码。

3. If an instruction depends on an instruction that is earlier in the bundle, then the pre-reserved physical register in which the result will be placed is used to update the information for the issuing instruction.

> 3.如果指令取决于捆绑包中较早的指令，则将将结果放置的预保留物理寄存器用于更新发行指令的信息。

Note that just as in the reorder buffer case, the issue logic must both determine dependences within the bundle and update the renaming tables in a single clock, and as before, the complexity of doing this for a larger number of instructions per clock becomes a chief limitation in the issue width.

> 请注意，就像在重新排序的情况下一样，问题逻辑必须确定捆绑包中的依赖性并在单个时钟中更新重命名表问题宽度的限制。

##### _The Challenge of More Issues per Clock_

Without speculation, there is little motivation to try to increase the issue rate beyond two, three, or possibly four issues per clock because resolving branches would limit the average issue rate to a smaller number. Once a processor includes accurate branch prediction and speculation, we might conclude that increasing the issue rate would be attractive. Duplicating the functional units is straightforward assuming silicon capacity and power; the real complications arise in the issue step and correspondingly in the commit step. The Commit step is the dual of the issue step, and the requirements are similar, so let’s take a look at what has to happen for a six-issue processor using register renaming.

> 没有猜测，几乎没有动力尝试将问题率提高到每个时钟超过两个，三个或可能的四个问题，因为解决分支机构会将平均问题率限制为较小的数量。一旦处理器包含准确的分支预测和投机，我们可以得出结论，提高问题率将很有吸引力。复制功能单元是直接的，假设硅容量和功率；实际并发症是在问题步骤中出现的，并在提交步骤中相应地出现。提交步骤是问题的双重步骤，并且要求相似，因此，让我们看一下使用寄存器重命名的六版处理器必须发生的事情。

[Figure 3.29](#_bookmark132) shows a six-instruction code sequence and what the issue step must do. Remember that this must all occur in a single clock cycle, if the processor is to maintain a peak rate of six issues per clock! All the dependences must be detected, the physical registers must be assigned, and the instructions must be rewritten using the physical register numbers: in one clock. This example makes it clear why issue rates have grown from 3–4 to only 4–8 in the past 20 years. The complexity of the analysis required during the issue cycle grows as the square of the issue width, and a new processor is typically targeted to have a higher clock rate than in the last generation! Because register renaming and the reorder buffer approaches are duals, the same complexities arise independent of the implementation scheme.

> [图 3.29](#_bookmark132) 显示了一个六指我的代码序列以及问题步骤必须做什么。请记住，如果处理器要保持每个时钟六个问题的峰值率，则必须在单个时钟周期中发生所有这些！必须检测到所有依赖性，必须分配物理寄存器，并且必须使用物理寄存器编号重写指令：在一个时钟中。这个示例清楚地表明，为什么在过去 20 年中，问题率从 3-4 增加到 4-8。随着问题宽度的正方形和新处理器的正方形，通常将分析的复杂性增长，通常是针对比上一代更高的时钟速率！因为登记量重命名和重新排序缓冲方法是双重的，因此与实施方案无关。

Figure 3.29 An example of six instructions to be issued in the same clock cycle and what has to happen. The instructions are shown in program order: 1–6; they are, however, issued in 1 clock cycle! The notation pi is used to refer to a physical register; the contents of that register at any point is determined by the renaming map. For simplicity, we assume that the physical registers holding the architectural registers x1, x2, and x3 are initially p1, p2, and p3 (they could be any physical register). The instructions are issued with physical register numbers, as shown in column four. The rename map, which appears in the last column, shows how the map would change if the instructions were issued sequentially. The difficulty is that all this renaming and replacement of architectural registers by physical renaming registers happens effectively in 1 cycle, not sequentially. The issue logic must find all the dependences and "rewrite" the instruction in parallel.

> 图 3.29 在同一时钟周期中发出的六个指令以及必须发生的事情。说明以程序顺序显示：1-6；但是，它们是在 1 个时钟周期中发出的！符号 PI 用于参考物理寄存器；该寄存器的内容在任何时候都由重命名地图确定。为简单起见，我们假设持有建筑寄存器 X1，X2 和 X3 的物理寄存器最初是 P1，P2 和 P3(它们可以是任何物理寄存器)。指令以物理寄存器编号发布，如第四列所示。重命名映射出现在最后一列中，显示了如果依次发布指令，则映射将如何更改。困难在于，通过物理重命名寄存器对建筑登记簿的所有重命名和替换都有效地发生在 1 周期中，而不是顺序地发生。问题逻辑必须找到所有依赖关系，并并行 "重写" 指令。

##### _How Much to Speculate_

One of the significant advantages of speculation is its ability to uncover events that would otherwise stall the pipeline early, such as cache misses. This potential advantage, however, comes with a significant potential disadvantage. Speculation is not free. It takes time and energy, and the recovery of incorrect speculation further reduces performance. In addition, to support the higher instruction execution rate needed to benefit from speculation, the processor must have additional resources, which take silicon area and power. Finally, if speculation causes an exceptional event to occur, such as a cache or translation lookaside buffer (TLB) miss, the potential for significant performance loss increases, if that event would not have occurred without speculation.

> 猜测的重要优势之一是它有能力揭示事件，否则可以尽早停止管道，例如高速缓存。但是，这种潜在的优势带有明显的潜在劣势。猜测不是免费的。它需要时间和精力，而不正确的投机恢复进一步降低了性能。此外，为了支持从投机中受益所需的更高的指令执行率，处理器必须具有额外的资源，这些资源需要硅区域和权力。最后，如果推测会导致出色的事件发生，例如缓存或翻译 lookAside 缓冲液(TLB)失误，则如果不推测就不会发生该事件，则可能会增加绩效损失的可能性。

To maintain most of the advantage while minimizing the disadvantages, most pipelines with speculation will allow only low-cost exceptional events (such as a first-level cache miss) to be handled in speculative mode. If an expensive exceptional event occurs, such as a second-level cache miss or a TLB miss, the processor will wait until the instruction causing the event is no longer speculative before handling the event. Although this may slightly degrade the performance of some programs, it avoids significant performance losses in others, especially those that suffer from a high frequency of such events coupled with less-than-excellent branch prediction.

> 为了在最小化缺点的同时保持大部分优势，大多数具有猜测的管道将只允许以投机模式处理低成本的异常事件(例如第一级缓存失误)。如果发生昂贵的特殊事件，例如二级缓存失误或 TLB 失误，则处理器将等到导致事件的指令不再投机，然后才能处理事件。尽管这可能会稍微降低某些程序的性能，但它避免了其他程序的重大绩效损失，尤其是那些遭受高频率此类事件的损失以及不太偏见的分支预测。

In the 1990s the potential downsides of speculation were less obvious. As processors have evolved, the real costs of speculation have become more apparent, and the limitations of wider issue and speculation have been obvious. We return to this issue shortly.

> 在 1990 年代，投机的潜在弊端不太明显。随着处理器的发展，猜测的实际成本变得越来越明显，更广泛的问题和猜测的局限性也很明显。我们很快就回到了这个问题。

##### _Speculating Through Multiple Branches_

In the examples we have considered in this chapter, it has been possible to resolve a branch before having to speculate on another. Three different situations can benefit from speculating on multiple branches simultaneously: (1) a very high branch frequency, (2) significant clustering of branches, and (3) long delays in functional units. In the first two cases, achieving high performance may mean that multiple branches are speculated, and it may even mean handling more than one branch per clock. Database programs and other less structured integer computations, often exhibit these properties, making speculation on multiple branches important. Likewise, long delays in functional units can raise the importance of speculating on multiple branches as a way to avoid stalls from the longer pipeline delays.

> 在我们在本章中考虑的示例中，在不得不推测另一个分支之前，可以解决一个分支。三种不同的情况可以通过同时推测多个分支的猜测受益：(1)非常高的分支频率，(2)分支的显着聚类，以及(3)功能单位中的长延迟。在前两种情况下，实现高性能可能意味着要猜测多个分支，甚至可能意味着每个时钟处理多个分支。数据库程序和其他结构较低的整数计算经常表现出这些属性，从而对多个分支的猜测很重要。同样，功能单元中的长时间延迟可以提高在多个分支上投机的重要性，以避免较长管道延迟的失速。

Speculating on multiple branches slightly complicates the process of speculation recovery but is straightforward otherwise. As of 2017, no processor has yet combined full speculation with resolving multiple branches per cycle, and it is unlikely that the costs of doing so would be justified in terms of performance versus complexity and power.

> 在多个分支上推测略微使投机恢复过程变得复杂，但否则很简单。截至 2017 年，尚无处理器将充分的猜测与每个周期的多个分支结合在一起，因此，这样做的成本不可能是合理的，从性能与复杂性和功率方面则是合理的。

##### _Speculation and the Challenge of Energy Efficiency_

> ##### _猜测和能源效率的挑战_

What is the impact of speculation on energy efficiency? At first glance, one might argue that using speculation always decreases energy efficiency because whenever speculation is wrong, it consumes excess energy in two ways:

> 投机对能源效率有什么影响？乍一看，人们可能会争辩说，使用猜测总是会降低能源效率，因为只要投机是错误的，它就会以两种方式消耗多余的能量：

1. Instructions that are speculated and whose results are not needed generate excess work for the processor, wasting energy.

> 1.推测且不需要其结果的说明为处理器产生过多的工作，从而浪费能量。

1. Undoing the speculation and restoring the state of the processor to continue execution at the appropriate address consumes additional energy that would not be needed without speculation.

> 1.取消猜测并恢复处理器的状态以继续在适当地址执行的情况下会消耗额外的能量，而没有投机就不需要。

Certainly, speculation will raise the power consumption, and if we could control speculation, it would be possible to measure the cost (or at least the dynamic power cost). But, if speculation lowers the execution time by more than it increases the average power consumption, then the total energy consumed may be less.

> 当然，猜测将提高功耗，如果我们能够控制猜测，则可以衡量成本(或至少动态功率成本)。但是，如果推测将执行时间降低超过增加平均功耗，那么所消耗的总能量可能会更少。

Thus, to understand the impact of speculation on energy efficiency, we need to look at how often speculation is leading to unnecessary work. If a significant number of unneeded instructions is executed, it is unlikely that speculation will improve running time by a comparable amount. [Figure 3.30](#_bookmark133) shows the fraction of instructions that are executed from misspeculation for a subset of the SPEC2000 benchmarks using a sophisticated branch predictor. As we can see, this fraction of executed misspeculated instructions is small in scientific code and significant (about 30% on average) in integer code. Thus it is unlikely that speculation is energy-efficient for integer applications, and the end of Dennard scaling makes imperfect speculation more problematic. Designers could avoid speculation, try to reduce the misspeculation, or think about new approaches, such as only speculating on branches that are known to be highly predictable.

> 因此，要了解投机对能源效率的影响，我们需要研究投机的频率导致不必要的工作。如果执行了大量的不需要说明，则猜测不太可能将运行时间提高到可比的数量。[图 3.30](#_bookmark133) 显示了使用复杂的分支预测指标从拼配中执行的指令的比例。如我们所见，这一小部分执行的拼写错误指令在科学代码中很小，整数代码中的显着(平均约为 30％)。因此，投机对于整数应用不可能有能力，而丹纳德缩放的终结使人们的猜测更加有问题。设计师可以避免猜测，尝试减少拼写错误或思考新方法，例如仅在已知的高度可预测的分支上推测。

Figure 3.30 The fraction of instructions that are executed as a result of misspeculation is typically much higher for integer programs (the first five) versus FP programs (the last five).

> 图 3.30 整数程序(前五个)与 FP 程序(最后五个)相比，由于拼写错误而执行的指令的比例通常要高得多。

##### _Address Aliasing Prediction_

_Address aliasing prediction_ is a technique that predicts whether two stores or a load and a store refer to the same memory address. If two such references do not refer to the same address, then they may be safely interchanged. Otherwise, we must wait until the memory addresses accessed by the instructions are known. Because we need not actually predict the address values, only whether such values conflict, the prediction can be reasonably accurate with small predictors. Address prediction relies on the ability of a speculative processor to recover after a misprediction; that is, if the actual addresses that were predicted to be different (and thus not alias) turn out to be the same (and thus are aliases), the processor simply restarts the sequence, just as though it had mispredicted a branch. Address value speculation has been used in several processors already and may become universal in the future.

> *address 别名预测*是一种预测是否两个存储或负载和存储的技术，请参阅相同的内存地址。如果两个这样的引用不参考相同的地址，则可以安全地互换。否则，我们必须等到已知指令访问的内存地址。因为我们实际上不需要实际预测地址值，所以只有这种值是否需要冲突，因此预测可以合理地准确地使用小型预测变量。地址预测取决于投机处理器在错误预测后恢复的能力；也就是说，如果预测的实际地址是不同的(因此不是别名)是相同的(因此是别名)，那么处理器只是重新启动序列，就好像它已经错误地预测了分支。地址价值推测已经在几个处理器中使用，并且将来可能会变得普遍。

Address prediction is a simple and restricted form of _value prediction,_ which attempts to predict the value that will be produced by an instruction. Value prediction could, if it were highly accurate, eliminate data flow restrictions and achieve higher rates of ILP. Despite many researchers focusing on value prediction in the past 15 years in dozens of papers, the results have never been sufficiently attractive to justify general value prediction in real processors.

> 地址预测是*value 预测的一种简单且受限的形式，*试图预测指令将产生的值。如果高度准确，可以消除数据流限制并获得更高的 ILP 速率。尽管许多研究人员在过去的 15 年中专注于价值预测，但结果从来没有足够吸引力，可以证明实际处理器中的一般价值预测是合理的。
