## Performance of Scientific Applications on Shared-Memory Multiprocessors

> ##科学应用程序在共享内存多处理器上的性能

This section covers the performance of the scientific applications from [Section I.3](#characteristics-of-scientific-applications) on both symmetric shared-memory and distributed shared-memory multiprocessors.

> 本节涵盖了[I.3](＃特征)对对称共享记忆和分布式共享内存多处理器的科学应用程序的性能。

### Performance of a Scientific Workload on a Symmetric Shared-Memory Multiprocessor

> ###在对称共享内存多处理器上的科学工作负载的表现

We evaluate the performance of our four scientific applications on a symmetric shared-memory multiprocessor using the following problem sizes:

> 我们使用以下问题大小评估了四个科学应用程序在对称共享内存多处理器上的性能：

- _Barnes-Hut_—16 K bodies run for six time steps (the accuracy control is set to 1.0, a typical, realistic value)

> - _barnes-hut_— 16 k 身体运行六个时间步长(精度控制设置为 1.0，典型，现实的价值)

- _FFT_—1 million complex data points

> -_fft_  -  100 万个复杂数据点

- _LU_—A 512 × 512 matrix is used with 16 × 16 blocks

> - _lu_— A 512×512 矩阵与 16×16 块一起使用

- _Ocean_—A 130 × 130 grid with a typical error tolerance

> -_ocean _- A 130×130 网格，具有典型的错误公差

In looking at the miss rates as we vary processor count, cache size, and block size, we decompose the total miss rate into _coherence misses_ and normal uni- processor misses. The normal uniprocessor misses consist of capacity, conflict, and compulsory misses. We label these misses as capacity misses because that is the dominant cause for these benchmarks. For these measurements, we include as a coherence miss any write misses needed to upgrade a block from shared to exclusive, even though no one is sharing the cache block. This mea- surement reflects a protocol that does not distinguish between a private and shared cache block.

> 当我们随着处理器计数，高速缓存大小和块大小而查看错率时，我们将总 M 率分解为 coherence misses_和正常的 Uni-Prokescor Bisses。正常的单局部处理器失误包括能力，冲突和强制性错过。我们将这些错过的标签标记为容量失误，因为这是这些基准测试的主要原因。对于这些测量值，我们将其包括在连贯性的情况下，即使没有人共享缓存块，也需要将块从共享升级到独家所需的任何写入。这种调查反映了一项协议，该协议无法区分私有缓存块和共享缓存块。

[Figure I.8](#_bookmark747) shows the data miss rates for our four applications, as we increase the number of processors from 1 to 16, while keeping the problem size constant. As we increase the number of processors, the total amount of cache increases, usually causing the capacity misses to drop. In contrast, increasing the processor count usually causes the amount of communication to increase, in turn causing the coher- ence misses to rise. The magnitude of these two effects differs by application.

> [图 i.8](#_ bookmark747)显示了我们四个应用程序的数据遗漏率，因为我们将处理器的数量从 1 增加到 16，同时保持问题大小恒定。随着我们增加处理器的数量，缓存总量增加，通常会导致容量失误下降。相比之下，增加处理器计数通常会导致沟通量增加，进而导致连贯的失误上升。这两个效应的大小因应用而有所不同。

In FFT, the capacity miss rate drops (from nearly 7% to just over 5%) but the coherence miss rate increases (from about 1% to about 2.7%), leading to a constant overall miss rate. Ocean shows a combination of effects, including some that relate to the partitioning of the grid and how grid boundaries map to cache blocks. For a typical 2D grid code the communication-generated misses are proportional to the boundary of each partition of the grid, while the capacity misses are proportional to the area of the grid. Therefore, increasing the total amount of cache while keeping the total problem size fixed will have a more significant effect on the capacity miss rate, at least until each subgrid fits within an individual processor’s cache. The sig- nificant jump in miss rate between one and two processors occurs because of con- flicts that arise from the way in which the multiple grids are mapped to the caches. This conflict is present for direct-mapped and two-way set associative caches, but fades at higher associativities. Such conflicts are not unusual in array-based appli- cations, especially when there are multiple grids in use at once. In Barnes and LU, the increase in processor count has little effect on the miss rate, sometimes causing a slight increase and sometimes causing a slight decrease.

> 在 FFT 中，容量失误率下降(从近 7％到略超过 5％)，但连贯性的失误率增加(从约 1％到约 2.7％)，导致总体失误率恒定。海洋显示出效果的组合，包括与网格分配有关的一些效果以及网格边界如何映射到缓存块。对于典型的 2D 网格代码，通信生成的错过与电网的每个分区的边界成正比，而容量遗漏与网格区域成正比。因此，增加缓存的总量同时保持总问题大小固定将对容量失误率产生更大的影响，至少直到每个子网格拟合在单个处理器的缓存中为止。在一个和两个处理器之间的失误率很大的跳跃是由于由多个网格映射到缓存的方式而产生的。直接映射和双向套件的关联缓存存在这种冲突，但在更高的关联处逐渐消失。在基于阵列的应用中，这种冲突并不罕见，尤其是在一次使用多个网格时。在 Barnes 和 Lu 中，处理器计数的增加对失误率几乎没有影响，有时会导致略有增加，有时会导致略有减少。

Increasing the cache size usually has a beneficial effect on performance, since it reduces the frequency of costly cache misses. [Figure I.9](#_bookmark748) illustrates the change in miss rate as cache size is increased for 16 processors, showing the portion of the miss rate due to coherence misses and to uniprocessor capacity misses. Two effects can lead to a miss rate that does not decrease—at least not as quickly as we might expect—as cache size increases: inherent communication and plateaus in the miss rate. Inherent communication leads to a certain frequency of coherence misses that are not significantly affected by increasing cache size. Thus, if the cache size is increased while maintaining a fixed problem size, the coherence miss rate eventually limits the decrease in cache miss rate. This effect is most obvious in Barnes, where the coherence miss rate essentially becomes the entire miss rate.

> 增加缓存尺寸通常对性能产生有益的影响，因为它会降低昂贵的缓存失误的频率。[图 i.9](#_ bookmark748)说明了错过率的变化，因为 16 个处理器的高速缓存大小增加，显示了由于连贯性错过和单层处理器容量失误而导致的错率的一部分。两种效果可能导致未降低的率，至少没有我们预期的速度，因为缓存大小增加：固有的通信和错位率的高原。固有的通信会导致一定的连贯性误差频率，而这些频率并未受到缓存大小增加的显着影响。因此，如果在保持固定的问题大小的同时增加了缓存大小，则相干损失率最终会限制缓存失误率的下降。在巴恩斯(Barnes)中，这种效果最为明显，在巴恩斯(Barnes)中，连贯性的率实际上变成了整个失误率。

Figure I.8 Data miss rates can vary in nonobvious ways as the processor count is increased from 1 to 16. The miss rates include both coherence and capacity miss rates. The compulsory misses in these benchmarks are all very small and are included in the capacity misses. Most of the misses in these applications are generated by accesses to data that are potentially shared, although in the applications with larger miss rates (FFT and Ocean), it is the capacity misses rather than the coherence misses that comprise the majority of the miss rate. Data are potentially shared if they are allocated in a portion of the address space used for shared data. In all except Ocean, the potentially shared data are heavily shared, while in Ocean only the boundaries of the subgrids are actually shared, although the entire grid is treated as a potentially shared data object. Of course, since the boundaries change as we increase the processor count (for a fixed-size prob- lem), different amounts of the grid become shared. The anomalous increase in capacity miss rate for Ocean in moving from 1 to 2 processors arises because of conflict misses in accessing the subgrids. In all cases except Ocean, the fraction of the cache misses caused by coherence transactions rises when a fixed-size problem is run on an increas- ing number of processors. In Ocean, the coherence misses initially fall as we add pro- cessors due to a large number of misses that are write ownership misses to data that are potentially, but not actually, shared. As the subgrids begin to fit in the aggregate cache (around 16 processors), this effect lessens. The single-processor numbers include write upgrade misses, which occur in this protocol even if the data are not actually shared, since they are in the shared state. For all these runs, the cache size is 64 KB, two-way set associative, with 32-byte blocks. Notice that the scale on the _y_-axis for each benchmark is different, so that the behavior of the individual benchmarks can be seen clearly.

> 图 I.8 数据误费率可能会以非明显的方式变化，因为处理器数量从 1 增加到 16。错率包括连贯性和容量失误率。这些基准测试中的强制性错过都很小，并且被包括在容量错过中。这些应用程序中的大多数错过都是通过对可能共享的数据的访问而产生的，尽管在具有较大的错率(FFT 和海洋)的应用程序中，这是构成大部分遗漏费率的能力而不是连贯性错过的。。如果将数据分配在用于共享数据的地址空间的一部分中，则可能会共享。除了海洋外，所有可能共享的数据都被大量共享，而在海洋中，只有子网格的边界实际上是共享的，尽管整个网格被视为潜在的共享数据对象。当然，由于边界随着我们增加处理器计数(对于固定大小的概率)而发生变化，因此共享不同量的网格。从 1 到 2 个处理器移动的海洋容量失误率的异常增加，由于访问子网格时的冲突错过。在所有情况下，除海洋以外，当固定大小的问题在增加的处理器上运行时，由连贯交易引起的缓存错过的比例上升。在海洋中，由于大量的失误而添加了有关潜在但实际上没有共享的数据的书面所有权，因此最初的连贯性错过了。随着子网格开始适合聚集的缓存(约 16 个处理器)，这种效果会减少。单处理器编号包括写入升级误差，即使数据实际上没有共享，也会在此协议中发生，因为它们处于共享状态。对于所有这些运行，高速缓存大小为 64 kb，双向套件关联，具有 32 字节块。请注意，每个基准测试标准的_y_轴上的比例是不同的，因此可以清楚地看到单个基准的行为。

Figure I.9 The miss rate usually drops as the cache size is increased, although coher- ence misses dampen the effect. The block size is 32 bytes and the cache is two-way set associative. The processor count is fixed at 16 processors. Observe that the scale for each graph is different.

> 图 i.9 通常随着缓存尺寸的增加而降低率，尽管相干失误会抑制效果。块大小为 32 个字节，缓存是双向集合的关联。处理器计数固定为 16 个处理器。观察每个图的比例不同。

A less important effect is a temporary plateau in the capacity miss rate that arises when the application has some fraction of its data present in cache but some significant portion of the dataset does not fit in the cache or in caches that are slightly bigger. In LU, a very small cache (about 4 KB) can capture the pair of 16 16 blocks used in the inner loop; beyond that, the next big improvement in capacity miss rate occurs when both matrices fit in the caches, which occurs when the total cache size is between 4 MB and 8 MB. This effect, sometimes called a _working set effect_, is partly at work between 32 KB and 128 KB for FFT, where the capacity miss rate drops only 0.3%. Beyond that cache size, a faster decrease in the capacity miss rate is seen, as a major data structure begins to reside in the cache. These plateaus are common in programs that deal with large arrays in a structured fashion.

> 一个不太重要的效果是，当应用程序在高速缓存中具有一定程度的数据时，出现的容量降低率是一个临时高原，但数据集的某些重要部分不适合在高速缓存或稍大一点的缓存中。在 lu 中，一个很小的缓存(约 4 kb)可以捕获内环中使用的 16 个 16 个块。除此之外，当两个矩阵拟合在缓存中时，容量失误率的下一个重大改善就会发生，这是当总高速缓存大小在 4 MB 和 8 MB 之间时发生。这种效果有时称为_工作集效果_，部分工作在 32 kb 至 128 kb 之间的 FFT 中，容量失误率仅下降 0.3％。除了缓存大小之外，由于主要数据结构开始存在于缓存中，因此可以更快地降低容量失误率。这些高原在以结构化的方式处理大型阵列的程序中很常见。

Increasing the block size is another way to change the miss rate in a cache. In uniprocessors, larger block sizes are often optimal with larger caches. In multiprocessors, two new effects come into play: a reduction in spatial locality for shared data and a potential increase in miss rate due to false sharing. Several studies have shown that shared data have lower spatial locality than unshared data. Poorer locality means that, for shared data, fetching larger blocks is less effective than in a uniprocessor because the probability is higher that the block will be replaced before all its contents are referenced. Likewise, increasing the basic size also increases the potential frequency of false sharing, increasing the miss rate.

> 增加块大小是改变缓存中错率的另一种方法。在单层处理器中，较大的块尺寸通常是最佳的，较大的缓存。在多处理器中，有两个新的效果发挥了作用：用于共享数据的空间区域降低，并且由于虚假共享而导致的错率可能增加。几项研究表明，共享数据的空间位置低于未共享数据。较差的地方意味着，对于共享数据，获取较大的块的效率不如单层处理器，因为在引用所有内容之前将更换块的概率更高。同样，增加基本大小也会增加虚假共享的潜在频率，从而增加了错率。

[Figure I.10](#_bookmark749) shows the miss rates as the cache block size is increased for a 16- processor run with a 64 KB cache. The most interesting behavior is in Barnes, where the miss rate initially declines and then rises due to an increase in the number of coherence misses, which probably occurs because of false sharing. In the other benchmarks, increasing the block size decreases the overall miss rate. In Ocean and LU, the block size increase affects both the coherence and capacity miss rates about equally. In FFT, the coherence miss rate is actually decreased at a faster rate than the capacity miss rate. This reduction occurs because the communication in FFT is structured to be very efficient. In less optimized programs, we would expect more false sharing and less spatial locality for shared data, resulting in more behavior like that of Barnes.

> [图 i.10](#_ bookmark749)显示了误费率，因为使用 64 kb 缓存的 16 处理器运行增加了缓存块的大小。最有趣的行为是在巴恩斯(Barnes)，由于连贯率的数量增加，因此失误率最初会下降，然后增加，这可能是由于虚假共享而发生的。在其他基准测试中，增加块大小会降低总体失误率。在海洋和 LU 中，块大小的增加会影响均匀的连贯性和容量失误率。在 FFT 中，相干损失率实际上比容量失误率更快地降低。这种减少之所以发生，是因为 FFT 中的通信结构非常有效。在较少优化的程序中，我们期望共享数据更多的错误共享和更少的空间局部性，从而导致更多的行为。

Figure I.10 The data miss rate drops as the cache block size is increased. All these results are for a 16-processor run with a 64 KB cache and two-way set associativity. Once again we use different scales for each benchmark.

> 图 I.11 随着缓存块大小的增加，数据误差率下降。所有这些结果都是针对具有 64 kb 缓存和双向设置关联性的 16 处理器运行。我们再次为每个基准测试使用不同的量表。

Figure I.11 Bus traffic for data misses climbs steadily as the block size in the data cache is increased. The factor of 3 increase in traffic for Ocean is the best argument against larger block sizes. Remember that our protocol treats ownership or upgrade misses the same as other misses, slightly increasing the penalty for large cache blocks; in both Ocean and FFT, this simplification accounts for less than 10% of the traffic.

> 图 i.11 数据的总线流量稳步攀升，因为数据缓存中的块大小增加。海洋交通增加的因素是针对较大块大小的最佳论点。请记住，我们的协议对所有权或升级的限制与其他失误相同，从而略微增加了对大型缓存块的罚款；在海洋和 FFT 中，这种简化占流量的 10％。

Although the drop in miss rates with longer blocks may lead you to believe that choosing a longer block size is the best decision, the bottleneck in bus-based multiprocessors is often the limited memory and bus bandwidth. Larger blocks mean more bytes on the bus per miss. [Figure I.11](#_bookmark750) shows the growth in bus traffic as the block size is increased. This growth is most serious in the programs that have a high miss rate, especially Ocean. The growth in traffic can actually lead to performance slowdowns due both to longer miss penalties and to increased bus contention.

> 尽管较长块的失误率下降可能会使您相信选择更长的块大小是最好的决定，但基于公共汽车的多处理器中的瓶颈通常是有限的内存和总线带宽。较大的块意味着每次错过的公共汽车上的字节更多。[图 I.11](#_ Bookmark750)显示了随着块大小的增加，总线流量的增长。这种增长最为严重，这些计划具有很高的失误率，尤其是海洋。交通的增长实际上会导致绩效放缓，这是由于较长的罚款和增加的巴士争夺而增加。

### Performance of a Scientific Workload on a Distributed-Memory Multiprocessor

> ###在分布式内存多处理器上进行科学工作负载的表现

The performance of a directory-based multiprocessor depends on many of the same factors that influence the performance of bus-based multiprocessors (e.g., cache size, processor count, and block size), as well as the distribution of misses to var- ious locations in the memory hierarchy. The location of a requested data item depends on both the initial allocation and the sharing patterns. We start by exam- ining the basic cache performance of our scientific/technical workload and then look at the effect of different types of misses.

> 基于目录的多处理器的性能取决于影响基于公共汽车的多处理器性能(例如，高速缓存大小，处理器计数和块大小)的许多相同因素，以及对遗物的分布在内存层次结构中。请求的数据项的位置取决于初始分配和共享模式。我们首先要检查科学/技术工作量的基本缓存性能，然后研究不同类型的错过的效果。

Because the multiprocessor is larger and has longer latencies than our snooping-based multiprocessor, we begin with a slightly larger cache (128 KB) and a larger block size of 64 bytes.

> 由于多处理器比基于窥探的多处理器更大，并且具有更长的延迟，因此我们从稍大的缓存(128 kb)开始，较大的块大小为 64 个字节。

In distributed-memory architectures, the distribution of memory requests between local and remote is key to performance because it affects both the con- sumption of global bandwidth and the latency seen by requests. Therefore, for the figures in this section, we separate the cache misses into local and remote requests. In looking at the figures, keep in mind that, for these applications, most of the remote misses that arise are coherence misses, although some capacity mis- ses can also be remote, and in some applications with poor data distribution such misses can be significant.

> 在分布式内存体系结构中，本地和远程之间的内存请求分布是性能的关键，因为它会影响全局带宽的汇总和请求所看到的延迟。因此，对于本节中的数字，我们将高速缓存遗漏分为本地和远程请求。在查看这些数字时，请记住，对于这些应用程序，尽管某些容量错误也可能是遥远的，但出现的大多数遥不可及的失误都是连贯的，并且在某些数据分布较差的应用程序中，此类错过可能是重要的。

As [Figure I.12](#_bookmark751) shows, the miss rates with these cache sizes are not affected much by changes in processor count, with the exception of Ocean, where the miss rate rises at 64 processors. This rise results from two factors: an increase in map- ping conflicts in the cache that occur when the grid becomes small, which leads to a rise in local misses, and an increase in the number of the coherence misses, which are all remote.

> 如[图 I.12](#_ bookmark751)所示，这些缓存尺寸的错率不受处理器计数的变化的影响很大，除了海洋外，在 64 个处理器上，MISS 率上升。这是两个因素引起的：当网格变小时发生的高速缓存中的地图冲突增加，这导致当地错过的增加，而连贯性遗失的数量增加，这都是遥远的。

[Figure I.13](#_bookmark752) shows how the miss rates change as the cache size is increased, assuming a 64-processor execution and 64-byte blocks. These miss rates decrease at rates that we might expect, although the dampening effect caused by little or no reduction in coherence misses leads to a slower decrease in the remote misses than in the local misses. By the time we reach the largest cache size shown, 512 KB, the remote miss rate is equal to or greater than the local miss rate. Larger caches would amplify this trend.

> [图 I.13](#_ bookmark752)表明，假设有 64 个处理器执行和 64 个字节块，随着缓存大小的增加，错率是如何变化的。这些错率以我们可能预期的速度降低，尽管由于连贯性失误几乎没有或根本没有降低导致的衰减效果导致遥控失误的降低速度慢于本地错过。到我们达到所示的最大缓存尺寸时，远程失误率等于或大于当地的失误率。较大的缓存将扩大这一趋势。

We examine the effect of changing the block size in [Figure I.14](#_bookmark753). Because these applications have good spatial locality, increases in block size reduce the miss rate, even for large blocks, although the performance benefits for going to the largest blocks are small. Furthermore, most of the improvement in miss rate comes from a reduction in the local misses.

> 我们检查了[图 I.14]中更改块大小的效果(#_ bookmark753)。由于这些应用具有良好的空间位置，因此块尺寸的增加降低了错过率，即使对于大块，到达最大块的性能优势很小。此外，大部分的遗漏率的改善都来自当地错过的减少。

Rather than plot the memory traffic, [Figure I.15](#_bookmark754) plots the number of bytes required per data reference versus block size, breaking the requirement into local and global bandwidth. In the case of a bus, we can simply aggregate the demands of each processor to find the total demand for bus and memory bandwidth. For a scal- able interconnect, we can use the data in [Figure I.15](#_bookmark754) to compute the required per- node global bandwidth and the estimated bisection bandwidth, as the next example shows.

> [图 i.15](#_ bookmark754)并没有绘制内存流量，绘制每个数据参考与块大小所需的字节数，从而将需求分解为本地和全局带宽。就公共汽车而言，我们可以简单地汇总每个处理器的需求，以找到对总线和内存带宽的总需求。对于可缩放的互连，我们可以在[图 I.15](#_ bookmark754)中使用数据来计算所需的范围全局带宽和估计的二分线带宽，如下一个示例所示。

Example Assume a 64-processor multiprocessor with 1 GHz processors that sustain one memory reference per processor clock. For a 64-byte block size, the remote miss rate is 0.7%. Find the per-node and estimated bisection bandwidth for FFT. Assume that the processor does not stall for remote memory requests; this might be true if, for example, all remote data were prefetched. How do these bandwidth requirements compare to various interconnection technologies?

> 示例假设一个具有 1 GHz 处理器的 64 处理器多处理器，该处理器每次处理器时钟维持一个内存参考。对于 64 字节块的大小，远程损坏率为 0.7％。找到 FFT 的每节点和估计的一分为二宽带。假设处理器不会停滞远程内存请求；例如，如果所有远程数据均已预取，则可能是正确的。这些带宽要求与各种互连技术相比如何？

Figure I.12 The data miss rate is often steady as processors are added for these benchmarks. Because of its grid structure, Ocean has an initially decreasing miss rate, which rises when there are 64 processors. For Ocean, the local miss rate drops from 5% at 8 processors to 2% at 32, before rising to 4% at 64. The remote miss rate in Ocean, driven primarily by communication, rises monotonically from 1% to 2.5%. Note that, to show the detailed behavior of each benchmark, different scales are used on the _y_-axis. The cache for all these runs is 128 KB, two-way set associative, with 64-byte blocks. Remote misses include any misses that require communication with another node, whether to fetch the data or to deliver an invalidate. In particular, in this figure and other data in this section, the measurement of remote misses includes write upgrade misses where the data are up to date in the local memory but cached elsewhere and, therefore, require invalidations to be sent. Such invalidations do indeed generate remote traffic, but may or may not delay the write, depending on the consistency model.

> 图 I.12 数据错率通常是稳定的，因为这些基准测试者的处理器被添加。由于其网格结构，海洋最初的错率降低了，当有 64 个处理器时，它会上升。对于海洋而言，当地的失误从 8 个处理器下降到 32 时的 2％，然后上升到 64 岁的 4％。海洋的远程失误率主要由通信驱动，单调从 1％上升到 2.5％。请注意，为了显示每个基准测试的详细行为，_y_-axis 上使用了不同的尺度。所有这些运行的缓存为 128 kb，双向套件关联，具有 64 个字节块。远程失误包括任何需要与其他节点进行通信的错过，无论是获取数据还是输送无效。特别是，在本图和本节中的其他数据中，远程错过的测量包括写入升级错过，其中数据在本地内存中最新，但在其他地方缓存，因此需要发送无效。这样的无效确实会产生远程流量，但可能会或可能不会延迟写入，具体取决于一致性模型。

FFT performs all-to-all communication, so the bisection bandwidth is equal to the number of processors times the per-node bandwidth, or about 64 448 MB/ sec 28.7 GB/sec. The SGI Origin 3000 with 64 processors has a bisection band- width of about 50 GB/sec. No standard networking technology comes close.

> FFT 执行全能的通信，因此一分为二的带宽等于处理器的数量乘以每个节点带宽，或大约 64 448 MB/ sec 28.7 GB/ sec。具有 64 个处理器的 SGI Origin 3000 具有约 50 GB/sec 的一分配带宽。没有标准的网络技术接近。

_Answer_ The per-node bandwidth is simply the number of data bytes per reference times the reference rate: 0.7% 1 GB/sec 64 448 MB/sec. This rate is somewhat higher than the hardware sustainable transfer rate for the CrayT3E (using a block prefetch)

> _answer _每节点带宽仅是每个参考时间的数据字节数：0.7％1 GB/sec 64 448 MB/sec。该速率比 Crayt3E 的硬件可持续转移率高(使用块预取)高度高

Figure I.13 Miss rates decrease as cache sizes grow. Steady decreases are seen in the local miss rate, while the remote miss rate declines to varying degrees, depending on whether the remote miss rate had a large capacity component or was driven primarily by communication misses. In all cases, the decrease in the local miss rate is larger than the decrease in the remote miss rate. The plateau in the miss rate of FFT, which we men- tioned in the last section, ends once the cache exceeds 128 KB. These runs were done with 64 processors and 64-byte cache blocks.

> 图 I.13 随着缓存尺寸的增长，错率降低。在当地的失误率中看到稳定的下降，而远程失误率下降到不同程度，具体取决于远程失误率是否具有较大的容量组件或主要由通信失误驱动。在所有情况下，局部错率的降低都大于遥控率的降低。一旦缓存超过 128 kb，我们在最后一部分中提到的 FFT 的高原率就结束了。这些运行是使用 64 个处理器和 64 个字节缓存块完成的。

and lower than that for an SGI Origin 3000 (1.6 GB/processor pair). The FFT per-node bandwidth demand exceeds the bandwidth sustainable from the fastest standard networks by more than a factor of 5.

> 比 SGI Origin 3000(1.6 GB/处理器对)低。FFT 每节点带宽需求超过最快标准网络的带宽可持续性超过 5 倍。

The previous example looked at the bandwidth demands. The other key issue for a parallel program is remote memory access time, or latency. To get insight into this, we use a simple example of a directory-based multiprocessor. Figure I.16 shows the parameters we assume for our simple multiprocessor model. It assumes that the time to first word for a local memory access is 85 processor cycles and that the path to local memory is 16 bytes wide, while the network interconnect is 4 bytes wide. This model ignores the effects of contention, which are probably not too serious in the parallel benchmarks we examine, with the possible exception of FFT, which uses all-to-all communication. Contention could have a serious performance impact in other workloads.

> 上一个示例查看了带宽的需求。并行程序的另一个关键问题是远程内存访问时间或延迟。为了了解这一点，我们使用一个基于目录的多处理器的简单示例。图 I.16 显示了我们为简单多处理器模型假设的参数。它假设局部内存访问的第一个单词的时间为 85 个处理器周期，而通往本地内存的路径为 16 个字节宽，而网络互连为 4 字节宽。该模型忽略了争论的效果，在我们检查的平行基准中可能并不是太严重了，但可能使用全部通信的 FFT 除外。争夺可能会对其他工作量产生严重的性能影响。

Figure I.14 Data miss rate versus block size assuming a 128 KB cache and 64 pro- cessors in total. Although difficult to see, the coherence miss rate in Barnes actually rises for the largest block size, just as in the last section.

> 图 I.14 数据误率与块大小，假设总共有 128 kb 的缓存和 64 名专科管理者。尽管很难看到，但巴恩斯的一致性失误实际上上升了最大的块大小，就像上一部分一样。

[Figure I.17](#_bookmark755) shows the cost in cycles for the average memory reference, assuming the parameters in Figure I.16. Only the latencies for each reference type are counted. Each bar indicates the contribution from cache hits, local misses, remote misses, and three-hop remote misses. The cost is influenced by the total frequency of cache misses and upgrades, as well as by the distri- bution of the location where the miss is satisfied. The cost for a remote mem- ory reference is fairly steady as the processor count is increased, except for Ocean. The increasing miss rate in Ocean for 64 processors is clear in [Figure I.12](#_bookmark751). As the miss rate increases, we should expect the time spent on memory references to increase also.

> [图 I.17](#_ bookmark755)假设图 i.16 中的参数，显示了平均内存参考的周期成本。仅计算每种参考类型的潜伏期。每个栏表示缓存命中，本地错过，遥控失误和三跳远程失误的贡献。成本受到缓存错过和升级的总频率的影响，也受到满足失误的位置的分辨率。除了海洋外，随着处理器数量的增加，远程内存参考的成本相当稳定。[图 I.12](#_ bookmark751)中，64 个处理器的海洋率提高的海洋率提高。随着错过率的增加，我们应该期望在内存参考上花费的时间也会增加。

Although [Figure I.17](#_bookmark755) shows the memory access cost, which is the dominant multiprocessor cost in these benchmarks, a complete performance model would need to consider the effect of contention in the memory system, as well as the losses arising from synchronization delays.

> 尽管[图 I.17](#_ bookmark755)显示了内存访问成本，这是这些基准中的主要多处理器成本，但完整的性能模型将需要考虑在内存系统中的争夺效果以及产生的损失来自同步延迟。

Figure I.15 The number of bytes per data reference climbs steadily as block size is increased. These data can be used to determine the bandwidth required per node both internally and globally. The data assume a 128 KB cache for each of 64 processors.

> 图 I.15 随着块大小的增加，每个数据参考的字节数量稳步攀升。这些数据可用于确定内部和全球节点所需的带宽。数据假设 64 个处理器中的每一个都假设了 128 kb 的缓存。

Figure I.16 Characteristics of the example directory-based multiprocessor. Misses can be serviced locally (including from the local directory), at a remote home node, or using the services of both the home node and another remote node that is caching an exclusive copy. This last case is called a three-hop miss and has a higher cost because it requires interrogating both the home directory and a remote cache. Note that this simple model does not account for invalidation time but does include some factor for increasing interconnect time. These remote access latencies are based on those in an SGI Origin 3000, the fastest scalable interconnect system in 2001, and assume a 500 MHz processor.

> 图 I.16 基于目录的多处理器的特征。可以在本地(包括来自本地目录)，远程家庭节点或使用 Home 节点的服务和另一个正在缓存独家副本的远程节点的服务。最后一个案例称为三跳失误，成本更高，因为它需要询问主目录和远程缓存。请注意，这个简单的模型无法解释无效时间，但确实包含了增加互连时间的某些因素。这些远程访问潜伏期基于 SGI Origin 3000(2001 年最快的可扩展互连系统)中的远程访问潜伏期，并假设一个 500 MHz 处理器。

Figure I.17 The effective latency of memory references in a DSM multiprocessor depends both on the relative frequency of cache misses and on the location of the memory where the accesses are served. These plots show the memory access cost (a metric called average memory access time in [Chapter 2](#_bookmark46)) for each of the benchmarks for 8, 16, 32, and 64 processors, assuming a 512 KB data cache that is two-way set asso- ciative with 64-byte blocks. The average memory access cost is composed of four dif- ferent types of accesses, with the cost of each type given in Figure I.16. For the Barnes and LU benchmarks, the low miss rates lead to low overall access times. In FFT, the higher access cost is determined by a higher local miss rate (1–4%) and a significant three-hop miss rate (1%). The improvement in FFT comes from the reduction in local miss rate from 4% to 1%, as the aggregate cache increases. Ocean shows the biggest change in the cost of memory accesses, and the highest overall cost at 64 processors. The high cost is driven primarily by a high local miss rate (average 1.6%). The memory access cost drops from 8 to 16 processors as the grids more easily fit in the individual caches. At 64 processors, the dataset size is too small to map properly and both local misses and coherence misses rise, as we saw in [Figure I.12](#_bookmark751).

> 图 I.17 DSM 多处理器中内存引用的有效延迟既取决于高速缓存错过的相对频率，也取决于存储器的位置。这些图显示了每个基准为 8、16、32 和 64 处理器的内存访问成本(在[第 2 章](#_ bookmark46)中的平均内存访问时间)，假设有两个 kb 数据缓存，则为两个，16、32 和 64 个处理器 - 设置为 64 个字节块的协会。平均内存访问成本由四种不同类型的访问组成，每种类型的成本在图 I.16 中给出。对于 Barnes 和 Lu 基准，低失误率导致总体访问时间较低。在 FFT 中，较高的访问成本取决于较高的本地失误率(1-4％)和明显的三跳率(1％)确定。FFT 的改善来自局部失误率从 4％降低到 1％，随着总缓存的增加。海洋显示出内存访问成本的最大变化，以及 64 个处理器的总成本最高。高成本主要由高地遗漏率(平均 1.6％)驱动。由于网格更容易适合单个缓存，内存访问的成本从 8 到 16 个处理器下降到 16 个处理器。在 64 个处理器时，数据集的大小太小，无法正确地绘制，并且本地的错过和连贯的错过都上升了，正如我们在[图 I.12]中看到的那样(#_ bookmark751)。
