### References

<span id="_bookmark701" class="anchor"><span id="_bookmark702" class="anchor"></span></span>[Alliant Computer Systems Corp, 1987. Alliant FX/Series: Product Summary. Mass, Acton (June).](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0010) [Asanovic, K., 1998. Vector microprocessors,"  Ph.D. thesis, Computer Science Division. University of](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0015)

<span id="_bookmark703" class="anchor"></span>[California at Berkeley (May).](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0015)

[Bailey, D.H., Barszcz, E., Barton, J.T., Browning, D.S., Carter, R.L., Dagum, L., Fatoohi, R.A.,](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0020) [Frederickson, P.O., Lasinski, T.A., Schreiber, R.S., Simon, H.D., Venkatakrishnan, V.,](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0020) [Weeratunga, S.K., 1991. The NAS parallel benchmarks. Int’l. J. Supercomputing Applications](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0020) <span id="_bookmark704" class="anchor"></span>[5, 63–73.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0020)

[Banerjee, U., 1979. Speedup of ordinary programs,"  Ph.D. thesis, Department of Computer Science.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0025)

[University of Illinois at Urbana-Champaign (October).](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0025)

[Baskett, F., Keller, T.W., 1977. An Evaluation of the Cray-1 Processor. In: Kuck, D.J., Lawrie, D.H.,](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0030) [Sameh, A.H. (Eds.), High Speed Computer and Algorithm Organization. Academic Press, San](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0030) [Diego, pp. 71–84.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0030)

[Brandt, M., Brooks, J., Cahir, M., Hewitt, T., Lopez-Pineda, E., Sandness, D., 2000. The Benchmarker’s](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0035) [Guide for Cray SV1 Systems. Cray Inc., Seattle, Wash.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0035)

[Bucher, I.Y., 1983. The computational speed of supercomputers. In: Proc. ACM SIGMETRICS Conf.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0040) [on Measurement and Modeling of Computer Systems, August 29–31, 1983. Minneapolis, Minn,](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0040)

<span id="_bookmark705" class="anchor"></span>[pp. 151–165.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0040)

[Callahan, D., Dongarra, J., Levine, D., 1988. Vectorizing compilers: A test suite and results.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0045) [In: Supercomputing ’88: Proceedings of the 1988 ACM/IEEE Conference on Supercomputing,](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0045) [November 12–17, pp. 98–105. Orlando, FL.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0045)

[Chen, S., 1983. Large-scale and high-speed multiprocessor system for scientific applications.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0050) [In: Hwang, K. (Ed.), Superprocessors: Design and applications. Proc. NATO Advanced Research](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0050) [Workshop on High-Speed Computing, June 20–22, 1983, Julich, Kernforschungsanlage, Federal](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0050) <span id="_bookmark706" class="anchor"></span>[Republic of Germany. IEEE, (August), 1984.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0050)

<span id="_bookmark707" class="anchor"></span>[Dongarra, J.J., 1986. A survey of high performance processors. COMPCON, IEEE, 8–11 (March).](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0055) [Dunnigan, T.H., Vetter, J.S., White III, J.B., Worley, P.H., 2005. Performance evaluation of the Cray](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0060)

<span id="_bookmark708" class="anchor"></span>[X1 distributed shared-memory architecture. IEEE Micro 25 (1 (January–February)), 30–40.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0060)

[Fazio, D., 1987. It’s really much more fun building a supercomputer than it is simply inventing one.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0065)

[COMPCON, IEEE, 102–105 (February).](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0065)

[Flynn, M.J., 1966. Very high-speed computing systems. In: Proc. IEEE 54:12 (December),](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0070)

[pp. 1901–1909.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0070)

[Hintz, R.G., Tate, D.P., 1972. Control data STAR-100 processor design. COMPCON, IEEE 1–4](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0075) <span id="_bookmark709" class="anchor"></span>[(September).](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0075)

[Jordan, K.E., 1987. Performance comparison of large-scale scientific processors: Scalar mainframes,](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0080) [mainframes with vector facilities, and supercomputers. Computer 20 (3 (March)), 10–23.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0080)

[Kitagawa, K., Tagaya, S., Hagihara, Y., Kanoh, Y., 2003. A hardware overview of SX-6 and SX-7](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0085) <span id="_bookmark710" class="anchor"></span>[supercomputer. NEC Research & Development J 44 (1 (January)), 2–7.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0085)

[Kuck, D., Budnik, P.P., Chen, S.-C., Lawrie, D.H., Towle, R.A., Strebendt, R.E., Davis Jr., E.W.,](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0090) [Han, J., Kraska, P.W., Muraoka, Y., 1974. Measurements of parallelism in ordinary FORTRAN](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0090) [programs. Computer 7 (1 (January)), 37–46.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0090)

[Lincoln, N.R., 1982. Technology and design trade offs in the creation of a modern supercomputer. IEEE](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0095) <span id="_bookmark711" class="anchor"></span>[Trans. on Computers, 363–376. C-31:5 (May).](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0095)

[Lubeck, O., Moore, J., Mendez, R., 1985. A benchmark comparison of three supercomputers: Fujitsu](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0100) <span id="_bookmark712" class="anchor"></span>[VP-200, Hitachi S810/20, and Cray X-MP/2. Computer 18 (1 (January)), 10–29.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0100)

Luszczek, P., Dongarra, J.J., Koester, D., Rabenseifner, R., Lucas, B., Kepner, J., McCalpin, J., Bailey, D., Takahashi, D., 2005. In: Introduction to the HPC challenge benchmark suite,"  Lawrence Berkeley National Laboratory, Paper LBNL-57493 (April 25). [http://repositories.cdlib.org/lbnl/LBNL-57493](http://repositories.cdlib.org/lbnl/LBNL-57493).

[Miranker, G.S., Rubenstein, J., Sanguinetti, J., 1988. Squeezing a Cray-class supercomputer into a](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0110) [single-user package. COMPCON, IEEE, 452–456 (March).](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0110)

[Miura, K., Uchida, K., 1983. FACOM vector processing system: VP100/200. In: Proc. NATO](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0115) [Advanced Research Workshop on High-Speed Computing. June 20–22, 1983, Julich, Ker-](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0115) [nforschungsanlage, Federal Republic of Germany; also in K. Hwang, ed.,  "Superprocessors: Design](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0115) <span id="_bookmark713" class="anchor"></span>[and applications,"  `IEEE` (August), 1984, 59–73.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0115)

[Moore, B., Padegs, A., Smith, R., Bucholz, W., 1987. Concepts of the System/370 vector architecture.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0120) [In: Proc. 14th Int’l. Symposium on Computer Architecture, June 3–6, 1987. Pittsburgh, Penn,](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0120)

<span id="_bookmark714" class="anchor"></span>[pp. 282–292.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0120)

[Padua, D., Wolfe, M., 1986. Advanced compiler optimizations for supercomputers. Comm. ACM](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0125) [29 (12 (December)), 1184–1201.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0125)

<span id="_bookmark715" class="anchor"></span>[Russell, R.M., 1978. The Cray-1 processor system. Comm. of the ACM 21 (1 (January)), 63–72.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0130) [Schneck, P.B., 1987. Superprocessor Architecture. Kluwer Academic Publishers, Norwell, Mass.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0135) [Smith, B.J., 1981. Architecture and applications of the HEP multiprocessor system. Real-Time Signal](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0140)

[Processing IV 298, 241–248. August.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0140)

[Sporer, M., Moss, F.H., Mathais, C.J., 1988. An introduction to the architecture of the Stellar Graphics](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0145) <span id="_bookmark716" class="anchor"></span>[supercomputer. COMPCON, IEEE 464 (March).](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0145)

[Tanqueray, D., 2002. The Cray X1 and supercomputer road map. In: Proc. 13th Daresbury Machine](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0150) [Evaluation Workshop, December 11–12. Cheshire, England.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0150)

[Vajapeyam, S., 1991. Instruction-level characterization of the Cray Y-MP processor. Ph.D. thesis, Com-](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0155) [puter Sciences Department. University of Wisconsin-Madison.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0155)

[Watanabe, T., 1987. Architecture and performance of the NEC supercomputer SX system. Parallel](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0160) [Computing 5, 247–255.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0160)

[Watson, W.J., 1972. The TI ASC—a highly modular and flexible super processor architecture. In: Proc.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0165)

[AFIPS Fall Joint Computer Conf, pp. 221–228.](http://refhub.elsevier.com/B978-0-12-811905-1.09973-9/rf0165)

## Exercises

In these exercises assume VMIPS has a clock rate of 500 MHz and that T<sub>loop</sub> 15. Use the start-up times from Figure G.2, and assume that the store latency is always included in the running time.

1. \[10] &lt;G.1, G.2 &gt;Write a VMIPS vector sequence that achieves the peak MFLOPS performance of the processor (use the functional unit and instruction description in
   [Section G.2](#vector-performance-in-more-depth)). Assuming a 500-MHz clock rate, what is the peak MFLOPS?
2. \[20/15/15] &lt;G.1–G.6 &gt;Consider the following vector code run on a 500 MHz version of VMIPS for a fixed vector length of 64:
   LV V1,Ra MULV.D V2,V1,V3 ADDV.D V4,V1,V3 SV Rb,V2

SV Rc,V4

Ignore all strip-mining overhead, but assume that the store latency must be included in the time to perform the loop. The entire sequence produces 64 results.

1. \[20] &lt;G.1–G.4 &gt;Assuming no chaining and a single memory pipeline, how many chimes are required? How many clock cycles per result (including both stores as one result) does this vector sequence require, including start-up
   overhead?
2. \[15] &lt;G.1–G.4 &gt;If the vector sequence is chained, how many clock cycles per result does this sequence require, including overhead?
3. \[15] &lt; G.1–G.6 &gt;Suppose VMIPS had three memory pipelines and
   chaining.
   If there were no bank conflicts in the accesses for the above loop, how many

clock cycles are required per result for this sequence?

1. \[20/20/15/15/20/20/20] &lt;G.2–G.6 &gt;Consider the following
   FORTRAN code:
   do 10 i= 1,n

A(i) =A(i) +B(i)

B(i) =x \* B(i)

10 continue

Use the techniques of [Section G.6](#_bookmark695) to estimate performance throughout this exercise, assuming a 500 MHz version of VMIPS.

1. \[20] &lt;G.2–G.6 &gt;Write the best VMIPS vector code for the inner portion of the loop. Assume x is in F0 and the addresses of A and B are in Ra and Rb, respectively.
2. \[20] &lt;G.2–G.6 &gt;Find the total time for this loop on VMIPS (T<sub>100</sub>). What is the MFLOPS rating for the loop (R<sub>100</sub>)?
3. \[15] &lt;G.2–G.6 &gt;Find R<sub>∞</sub> for this loop.
4. \[15] &lt;G.2–G.6 &gt;Find N<sub>1/2</sub> for this loop.
5. \[20] &lt;G.2–G.6 &gt;Find N<sub>v</sub> for this loop. Assume the
   scalar code has been pipe- line scheduled so that each memory
   reference takes six cycles and each FP oper-
   ation takes three cycles. Assume the scalar overhead is also T<sub>loop</sub>.
6. \[20] &lt;G.2–G.6 &gt;Assume VMIPS has two memory pipelines. Write
   vector code that takes advantage of the second memory pipeline. Show
   the layout in
   convoys.
7. \[20] &lt;G.2–G.6 &gt;Compute T<sub>100</sub> and R<sub>100</sub>
   for VMIPS with two memory pipelines.

<!-- -->

1. \[20/10] &lt;G.2 &gt;Suppose we have a version of VMIPS with eight
   memory banks (each a double word wide) and a memory access time of
   eight cycles.

   1. \[20] &lt;G.2 &gt;If a load vector of length 64 is executed

      > with a stride of 20 double words, how many cycles will the
      > load take to complete?
      >
   2. \[10] &lt;G.2 &gt;What percentage of the memory bandwidth do

      > you achieve on a 64-element load at stride 20 versus stride 1?
      >
2. \[12/12] &lt;G.5–G.6 &gt;Consider the following loop:
   C= 0.0

do 10 i= 1,64

A(i) =A(i) +B(i) C=C+A(i)

10 continue

1. \[12] &lt;G.5–G.6 &gt;Split the loop into two loops: one with no dependence and one with a dependence. Write these loops in FORTRAN—as a source-to-source
   transformation. This optimization is called _loop fission._
2. \[12] &lt;G.5–G.6 &gt;Write the VMIPS vector code for the loop without a dependence.

<!-- -->

1. \[20/15/20/20] &lt;G.5–G.6 &gt;The compiled Linpack performance of the Cray-1 (designed in 1976) was almost doubled by a better compiler in 1989.
   Let’s look at a simple example of how this might occur. Consider the DAXPY-like loop (where `k` is a parameter to the procedure containing the loop):

do 10 i= 1,64

do 10 j= 1,64 Y(k,j) =a\*X(i,j) +Y(k,j)

10 continue

1. \[20] &lt;G.5–G.6 &gt;Write the `straightforward` code sequence for just the inner loop in VMIPS vector instructions.
2. \[15] &lt;G.5–G.6 &gt;Using the techniques of [Section G.6](#_bookmark695), estimate the perfor- mance of this code on VMIPS by finding T<sub>64</sub> in clock cycles. You may assume
   that T<sub>loop</sub> of overhead is incurred for each iteration of the outer loop. What limits the performance?
3. \[20] &lt;G.5–G.6 &gt;Rewrite the VMIPS code to reduce the performance limita- tion; show the resulting inner loop in VMIPS vector instructions. (_Hint:_ Think
   about what establishes T<sub>chime</sub>; can you affect it?) Find the total time for the resulting sequence.
4. \[20] &lt;G.5–G.6 &gt;Estimate the performance of your new version, using the techniques of [Section G.6](#_bookmark695) and finding T<sub>64</sub>.

<!-- -->

1. \[15/15/25] &lt;G.4 &gt;Consider the following code:
   do 10 i= 1,64

if (B(i) .ne. 0) then

A(i) =A(i)/B(i)

10 continue

Assume that the addresses of A and B are in Ra and Rb, respectively, and that F0 contains 0.

1. \[15] &lt; G.4 &gt;Write the VMIPS code for this loop using the
   vector-mask capability.
2. \[15] &lt;G.4 &gt;Write the VMIPS code for this loop using
   scatter-gather.
3. \[25] &lt;G.4 &gt;Estimate the performance (T<sub>100</sub> in
   clock cycles) of these two vector loops, assuming a divide latency
   of 20 cycles. Assume that all vector instruc-
   tions run at one result per clock, independent of the setting of the vector-mask register. Assume that 50% of the entries of B are 0. Considering hardware costs, which would you build if the above loop were typical?
4. \[15/20/15/15] &lt;G.1–G.6 &gt;The difference between peak and
   sustained perfor- mance can be large. For one problem, a Hitachi
   S810 had a peak speed twice as
   high as that of the Cray X-MP, while for another more realistic problem, the Cray X-MP was twice as fast as the Hitachi processor. Let’s examine why this might occur using two versions of VMIPS and the following code sequences:

C Code sequence 1
do 10 i= 1,10000

A(i) =x \* A(i) +y \* A(i)

10 continue

C Code sequence 2

do 10 i= 1,100

A(i) =x \* A(i)

10 continue

Assume there is a version of VMIPS (call it VMIPS-II) that has two copies of every floating-point functional unit with full chaining among them. Assume that both VMIPS and VMIPS-II have two load-store units. Because of the extra functional units and the increased complexity of assigning operations to units, all the over- heads (T<sub>loop</sub> and T<sub>start</sub>) are doubled for VMIPS-II.

1. \[15] &lt;G.1–G.6 &gt;Find the number of clock cycles on code sequence 1 on VMIPS.
2. \[20] &lt;G.1–G.6 &gt;Find the number of clock cycles on code sequence 1 for VMIPS-II. How does this compare to VMIPS?
3. \[15] &lt; G.1–G.6 &gt;Find the number of clock cycles on code
   sequence 2 for VMIPS.
4. \[15] &lt;G.1–G.6 &gt;Find the number of clock cycles on code sequence 2 for VMIPS-II. How does this compare to VMIPS?

<!-- -->

1. \[20] &lt;G.5 &gt;Here is a tricky piece of code with
   two-dimensional arrays. Does this loop have dependences? Can these
   loops be written so they are parallel? If so, how?
   Rewrite the `source` code so that it is clear that the loop can be vectorized, if possible.

do 290 j= 2,n

do 290 i= 2,j

aa(i,j) =aa(i-1,j)\*aa(i-1,j) +bb(i,j)

290 continue

1. \[12/15] &lt;G.5 &gt;Consider the following loop:
   do 10 i= 2,n A(i) =B

10 C(i) =A(i - 1)

1. \[12] &lt;G.5 &gt;Show there is a loop-carried dependence in this code fragment.
2. \[15] &lt;G.5 &gt;Rewrite the code in FORTRAN so that it can be vectorized as two separate vector sequences.

<!-- -->

1. \[15/25/25] &lt;G.5 &gt;As we saw in [Section G.5](#effectiveness-of-compiler-vectorization), some loop structures are not easily vectorized. One common structure is a _reduction_—a loop that reduces an array to a
   single value by repeated application of an operation. This is a special case of a recurrence. A common example occurs in dot product:

dot = 0.0

do 10 i= 1,64

10 dot=dot +A(i) \* B(i)

This loop has an obvious loop-carried dependence (on dot) and cannot be vec- torized in a straightforward fashion. The first thing a good vectorizing compiler would do is split the loop to separate out the vectorizable portion and the recurrence and perhaps rewrite the loop as:

do 10 i= 1,64

10 dot(i) =A(i) \* B(i) do 20 i= 2,64

20 dot(1) =dot(1) +dot(i)

The variable dot has been expanded into a vector; this transformation is called _scalar expansion_. We can try to vectorize the second loop either relying strictly on the compiler (part (a)) or with hardware support as well (part (b)). There is an important caveat in the use of vector techniques for reduction. To make reduction work, we are relying on the associativity of the operator being used for the reduc- tion. Because of rounding and finite range, however, floating-point arithmetic is not strictly associative. For this reason, most compilers require the programmer to indicate whether associativity can be used to more efficiently compile reductions.

1. \[15] &lt;G.5 &gt;One simple scheme for compiling the loop with the recurrence is to add sequences of progressively shorter vectors—two 32-element vectors, then
   two 16-element vectors, and so on. This technique has been called _recursive doubling_. It is faster than doing all the operations in scalar mode. Show how the FORTRAN code would look for execution of the second loop in the preced- ing code fragment using recursive doubling.
2. \[25] &lt;G.5 &gt;In some vector processors, the vector registers are addressable, and the operands to a vector operation may be two different parts of the same vector
   register. This allows another solution for the reduction, called _partial sums_.

The key idea in partial sums is to reduce the vector to `m` sums where `m` is the total latency through the vector functional unit, including the operand read and write times. Assume that the VMIPS vector registers are addressable (e.g., you can initiate a vector operation with the operand V1(16), indicating that the input operand began with element 16). Also, assume that the total latency for adds, including operand read and write, is eight cycles. Write a VMIPS code sequence that reduces the contents of V1 to eight partial sums. It can be done with one vector operation.

1. \[25] &lt;G.5 &gt;Discuss how adding the extension in part (b)
   would affect a machine that had multiple lanes.

<!-- -->

1. \[40] &lt;G.3–G.4 &gt;Extend the MIPS simulator to be a VMIPS
   simulator, including the ability to count clock cycles. Write some
   short benchmark programs in MIPS
   and VMIPS assembly language. Measure the speedup on VMIPS, the percentage of vectorization, and usage of the functional units.
2. \[50] &lt;G.5 &gt;Modify the MIPS compiler to include a dependence
   checker. Run some scientific code and loops through it and measure
   what percentage of the state- ments could be vectorized.
3. \[Discussion] Some proponents of vector processors might argue that
   the vector processors have provided the best path to ever-increasing
   amounts of processor power by focusing their attention on boosting
   peak vector performance. Others would argue that the emphasis on
   peak performance is misplaced because an increasing percentage of
   the programs are dominated by nonvector performance. (Remember
   Amdahl’s law?) The proponents would respond that programmers should
   work to make their programs vectorizable. What do you think about
   this argument?
