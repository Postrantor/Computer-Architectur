## Switch Microarchitecture

Network switches implement the routing, arbitration, and switching functions of switched-media networks. Switches also implement buffer management mecha- nisms and, in the case of lossless networks, the associated flow control. For some networks, switches also implement part of the network management functions that explore, configure, and reconfigure the network topology in response to boot-up and failures. Here, we reveal the internal structure of network switches by describ- ing a basic switch microarchitecture and various alternatives suitable for different routing, arbitration, and switching techniques presented previously.

> 网络交换机实现了交换机网络的路由，仲裁和交换功能。交换机还实施缓冲管理机制，在无损网络的情况下，相关的流量控制。对于某些网络，交换机还实现了网络管理功能的一部分，该功能探索，配置和重新配置网络拓扑以响应启动和失败。在这里，我们通过描述基本的开关微体系结构和各种替代方案来揭示网络开关的内部结构，适合于先前介绍的不同路由，仲裁和开关技术。

### Basic Switch Microarchitecture

The internal data path of a switch provides connectivity among the input and output ports. Although a shared bus or a multiported central memory could be used, these solutions are insufficient or too expensive, respectively, when the required aggre- gate switch bandwidth is high. Most high-performance switches implement an internal crossbar to provide nonblocking connectivity within the switch, thus allowing concurrent connections between multiple input-output port pairs. Buffer- ing of blocked packets can be done using first in, first out (FIFO) or circular queues, which can be implemented as `dynamically allocatable multi-queues` (DAMQs) in static RAM to provide high capacity and flexibility. These queues can be placed at input ports (i.e., `input buffered switch`), output ports (i.e., `output buffered switch`), centrally within the switch (i.e., `centrally buffered switch`), or at both the input and output ports of the switch (i.e., `input-output-buffered switch`). [Figure F.21](#_bookmark618) shows a block diagram of an input-output-buffered switch.

> 交换机的内部数据路径提供输入和输出端口之间的连接。尽管可以使用共享总线或多端口中央存储器，但当所需的聚合交换机带宽很高时，这些解决方案分别不够用或过于昂贵。大多数高性能交换机实施内部交叉开关以在交换机内提供无阻塞连接，从而允许多个输入-输出端口对之间的并发连接。可以使用先进先出 (FIFO) 或循环队列来缓冲阻塞的数据包，这些队列可以在静态 RAM 中实现为 "动态可分配多队列" (DAMQ)，以提供高容量和灵活性。这些队列可以放置在输入端口(即 "输入缓冲开关" )、输出端口(即 "输出缓冲开关" )、开关的中央(即 "中央缓冲开关" )，或同时放置在输入和 开关的输出端口(即 "输入输出缓冲开关" )。[图 F.21](#_bookmark618) 显示了输入输出缓冲开关的框图。

Routing can be implemented using a finite-state machine or forwarding table within the routing control unit of switches. In the former case, the routing infor- mation given in the packet header is processed by a finite-state machine that deter- mines the allowed switch output port (or ports if routing is adaptive), according to the routing algorithm. Portions of the routing information in the header are usually stripped off or modified by the routing control unit after use to simplify processing at the next switch along the path. When routing is implemented using forwarding tables, the routing information given in the packet header is used as an address to access a forwarding table entry that contains the allowed switch output port(s) pro- vided by the routing algorithm. Forwarding tables must be preloaded into the switches at the outset of network operation. Hybrid approaches also exist where the forwarding table is reduced to a small set of routing bits and combined with a small logic block. Those routing bits are used by the routing control unit to know what paths are allowed and decide the output ports the packets need to take. The goal with those approaches is to build flexible yet compact routing control units, eliminating the area and power wastage of a large forwarding table and thus being suitable for OCNs. The routing control unit is usually implemented as a centralized resource, although it could be replicated at every input port so as not to become a bottleneck. Routing is done only once for every packet, and packets typically are large enough to take several cycles to flow through the switch, so a centralized routing control unit rarely becomes a bottleneck. [Figure F.21](#_bookmark618) assumes a centralized routing control unit within the switch.

> 路由可以使用有限状态机或交换机路由控制单元内的转发表来实现。在前一种情况下，数据包标头中给出的路由信息 ​​ 由有限状态机处理，根据路由算法确定允许的交换机输出端口(或端口，如果路由是自适应的)。报头中的部分路由信息通常在使用后由路由控制单元剥离或修改，以简化沿路径的下一个交换机的处理。当使用转发表实现路由时，数据包标头中给出的路由信息 ​​ 用作访问转发表条目的地址，该条目包含路由算法提供的允许的交换机输出端口。转发表必须在网络运行开始时预加载到交换机中。还存在混合方法，其中转发表被缩减为一小组路由位并与一个小逻辑块组合。路由控制单元使用这些路由位来了解允许哪些路径并决定数据包需要采用的输出端口。这些方法的目标是构建灵活而紧凑的路由控制单元，消除大型转发表的面积和功率浪费，从而适用于 OCN。路由控制单元通常作为集中式资源实现，尽管它可以在每个输入端口进行复制以免成为瓶颈。每个数据包只进行一次路由，而且数据包通常大到需要几个周期才能流过交换机，因此集中式路由控制单元很少成为瓶颈。[图 F.21](#_bookmark618) 假定交换机内有一个集中式路由控制单元。

Figure F.21 Basic microarchitectural components of an input-output-buffered switch.

Arbitration is required when two or more packets concurrently request the same output port, as described in the previous section. Switch arbitration can be implemented in a centralized or distributed way. In the former case, all of the requests and status information are transmitted to the central switch arbitration unit; in the latter case, the arbiter is distributed across the switch, usually among the input and/or output ports. Arbitration may be performed multiple times on packets, and there may be multiple queues associated with each input port, increasing the number of arbitration requests that must be processed. Thus, many implementations use a hierarchical arbitration approach, where arbitration is first performed locally at every input port to select just one request among the corre- sponding packets and queues, and later arbitration is performed globally to process the requests made by each of the local input port arbiters. [Figure F.21](#_bookmark618) assumes a centralized arbitration unit within the switch.

> 如前一节所述，当两个或多个数据包同时请求同一个输出端口时，需要进行仲裁。交换机仲裁可以采用集中式或分布式方式实现。在前一种情况下，所有的请求和状态信息都被传送到中央交换仲裁单元；在后一种情况下，仲裁器分布在整个交换机中，通常分布在输入和/或输出端口之间。仲裁可能会对数据包执行多次，并且可能有多个队列与每个输入端口相关联，从而增加了必须处理的仲裁请求的数量。因此，许多实现使用分层仲裁方法，其中首先在每个输入端口本地执行仲裁，以在相应的数据包和队列中选择一个请求，然后全局执行仲裁以处理每个本地发出的请求 输入端口仲裁器。[图 F.21](#_bookmark618) 假定交换机内有一个集中式仲裁单元。

> [!note]
> 这里有一句，仲裁是分布式实现的，这个机制是否可以引入到实时系统中的节点备份，健康检查？

The basic switch microarchitecture depicted in [Figure F.21](#_bookmark618) functions in the fol- lowing way. When a packet starts to arrive at a switch input port, the link controller decodes the incoming signal and generates a sequence of bits, possibly deserializ- ing data to adapt them to the width of the internal data path if different from the external link width. Information is also extracted from the packet header or link control signals to determine the queue to which the packet should be buffered. As the packet is being received and buffered (or after the entire packet has been buffered, depending on the switching technique), the header is sent to the routing unit. This unit supplies a request for one or more output ports to the arbitration unit. Arbitration for the requested output port succeeds if the port is free and has enough space to buffer the entire packet or flit, depending on the switching technique. If wormhole switching with virtual channels is implemented, additional arbitration and allocation steps may be required for the transmission of each individual flit. Once the resources are allocated, the packet is transferred across the internal cross- bar to the corresponding output buffer and link if no other packets are ahead of it and the link is free. Link-level flow control implemented by the link controller pre- vents input queue overflow at the neighboring switch on the other end of the link. If virtual channel switching is implemented, several packets may be time- multiplexed across the link on a flit-by-flit basis. As the various input and output ports operate independently, several incoming packets may be processed concur- rently in the absence of contention.

> [图 F.21](#_bookmark618) 中描述的基本开关微架构按以下方式运行。当数据包开始到达交换机输入端口时，链路控制器解码传入信号并生成位序列，如果不同于外部链路宽度，可能会反序列化数据以适应内部数据路径的宽度。还从数据包报头或链路控制信号中提取信息，以确定数据包应缓冲到的队列。当数据包被接收和缓冲时(或者在整个数据包被缓冲之后，取决于交换技术)，报头被发送到路由单元。该单元向仲裁单元提供一个或多个输出端口的请求。如果端口空闲并且有足够的空间来缓冲整个数据包或 flit，则请求的输出端口的仲裁成功，具体取决于交换技术。如果使用虚拟通道实现虫洞切换，则可能需要额外的仲裁和分配步骤来传输每个单独的微片。一旦分配了资源，如果没有其他数据包在其前面且链路空闲，则数据包将通过内部交叉开关传输到相应的输出缓冲区和链路。链路控制器实现的链路级流量控制可防止链路另一端相邻交换机的输入队列溢出。如果实施了虚拟信道切换，则几个数据包可能会在逐个迁移的基础上在链路上进行时间复用。由于各种输入和输出端口独立运行，因此在没有争用的情况下可以同时处理多个传入数据包。

### Buffer Organizations

As mentioned above, queues can be located at the switch input, output, or both sides. Output-buffered switches have the advantage of completely eliminating `head-of-line blocking`. Head-of-line (HOL) blocking occurs when two or more packets are buffered in a queue, and a blocked packet at the head of the queue blocks other packets in the queue that would otherwise be able to advance if they were at the queue head. This cannot occur in output-buffered switches as all the packets in a given queue have the same status; they require the same output port. However, it may be the case that all the switch input ports simultaneously receive a packet for the same output port. As there are no buffers at the input side, output buffers must be able to store all those incoming packets at the same time. This requires implementing output queues with an internal switch `speedup` of `k`. That is, output queues must have a write bandwidth `k` times the link bandwidth, where `k` is the number of switch ports. This oftentimes is too expensive. Hence, this solu- tion by itself has rarely been implemented in lossless networks. As the probability of concurrently receiving many packets for the same output port is usually small, commercial systems that use output-buffered switches typically implement only moderate switch speedup, dropping packets on rare buffer overflow.

> 如上所述，队列可以位于交换机输入端、输出端或两侧。输出缓冲开关具有完全消除 "队头阻塞" 的优势。队头 (HOL) 阻塞发生在两个或多个数据包缓冲在队列中时，并且队列头部的阻塞数据包阻塞队列中的其他数据包，否则这些数据包如果在队列中则能够前进 头。这不会发生在输出缓冲交换机中，因为给定队列中的所有数据包都具有相同的状态；他们需要相同的输出端口。然而，情况可能是所有交换机输入端口同时接收同一输出端口的分组。由于输入端没有缓冲区，输出缓冲区必须能够同时存储所有这些传入数据包。这需要使用 `k` 的内部开关 `speedup` 来实现输出队列。也就是说，输出队列的写入带宽必须是链路带宽的 "k" 倍，其中 "k" 是交换机端口的数量。这通常太昂贵了。因此，这种解决方案本身很少在无损网络中实施。由于同一输出端口同时接收到许多数据包的可能性通常很小，因此使用输出缓冲交换机的商业系统通常只实现适度的交换机加速，在罕见的缓冲区溢出时丢弃数据包。

Switches with buffers on the input side are able to receive packets without hav- ing any switch speedup; however, HOL blocking can occur within input port queues, as illustrated in [Figure F.22(a)](#_bookmark619). This can reduce switch output port utiliza- tion to less than 60% even when packet destinations are uniformly distributed. As shown in [Figure F.22(b)](#_bookmark619), the use of virtual channels (two in this case) can mitigate HOL blocking but does not eliminate it. A more effective solution is to organize the input queues as `virtual output queues` (VOQs), shown in [Figure F.22(c)](#_bookmark619). With this, each input port implements as many queues as there are output ports, thus provid- ing separate buffers for packets destined to different output ports. This is a popular technique widely used in ATM switches and IP routers. The main drawbacks of

> 输入端带有缓冲区的交换机能够在没有任何交换机加速的情况下接收数据包；但是，HOL 阻塞可能发生在输入端口队列中，如 [图 F.22(a)](#_bookmark619) 所示。这可以将交换机输出端口的利用率降低到 60% 以下，即使数据包目的地是均匀分布的。如 [图 F.22(b)](#_bookmark619) 所示，使用虚拟通道(在本例中为两个)可以减轻 HOL 阻塞，但不能消除它。一种更有效的解决方案是将输入队列组织为 "虚拟输出队列" (VOQ)，如 [图 F.22(c)](#_bookmark619) 所示。这样，每个输入端口实现与输出端口一样多的队列，从而为发往不同输出端口的数据包提供单独的缓冲区。这是一种广泛用于 ATM 交换机和 IP 路由器的流行技术。的主要缺点

![](../media/image661.png)

Figure F.22 (a) Head-of-line blocking in an input buffer, (b) the use of two virtual channels to reduce HOL block- ing, and (c) the use of virtual output queuing to eliminate HOL blocking within a switch. The shaded input buffer is the one to which the crossbar is currently allocated. This assumes each input port has only one access port to the switch’s internal crossbar.

> 图 F.22(a)输入缓冲区中的线路阻塞，(b)使用两个虚拟通道来减少 HOL 阻塞，以及(c)使用虚拟输出排队以消除在一个内部的 hol 封锁转变。阴影输入缓冲区是当前分配的横杆的一种。假设每个输入端口只有一个访问交换机内部横梁的访问端口。

VOQs, however, are cost and lack of scalability: The number of VOQs grows qua- dratically with switch ports. Moreover, although VOQs eliminate HOL blocking within a switch, HOL blocking occurring at the network level end-to-end is not solved. Of course, it is possible to design a switch with VOQ support at the network level also—that is, to implement as many queues per switch input port as there are output ports across the entire network—but this is extremely expensive. An alter- native is to dynamically assign only a fraction of the queues to store (cache) sep- arately only those packets headed for congested destinations.

> 但是，VOQ 是成本且缺乏可扩展性的：VOQ 的数量随交换机端口而急剧增长。此外，尽管 VOQ 消除了开关中的 HOL 阻塞，但在网络级别端到端的 HOL 阻塞尚未解决。当然，还可以在网络级别上设计带有 VOQ 支持的开关，也就是说，随着整个网络中的输出端口，每个交换机输入端口都会实现尽可能多的队列 - 但这非常昂贵。一个改变的天然是动态分配一小部分排队存储(缓存)，仅将这些数据包用于拥挤目的地。

Combined input-output-buffered switches minimize HOL blocking when there is sufficient buffer space at the output side to buffer packets, and they minimize the switch speedup required due to buffers being at the input side. This solution has the further benefit of decoupling packet transmission through the internal crossbar of the switch from transmission through the external links. This is especially useful for cut-through switching implementations that use virtual channels, where flit transmissions are time-multiplexed over the links. Many designs used in commer- cial systems implement input-output-buffered switches.

> 当输出端有足够的缓冲空间来缓冲数据包时，组合式输入输出缓冲开关可最大限度地减少 HOL 阻塞，并且由于缓冲器位于输入端，它们可最大限度地减少所需的开关加速。该解决方案的另一个好处是将通过交换机内部交叉开关的数据包传输与通过外部链路的传输分离开来。这对于使用虚拟通道的直通交换实现特别有用，其中快速传输在链路上进行时分复用。商业系统中使用的许多设计都实现了输入输出缓冲开关。

### Routing Algorithm Implementation

It is important to distinguish between the routing algorithm and its implementation. While the routing algorithm describes the rules to forward packets across the net- work and affects packet latency and network throughput, its implementation affects the delay suffered by packets when reaching a node, the required silicon area, and the power consumption associated with the routing computation. Several techniques have been proposed to pre-compute the routing algorithm and/or hide the routing computation delay. However, significantly less effort has been devoted to reduce silicon area and power consumption without significantly affecting routing flexibil- ity. Both issues have become very important, particularly for OCNs. Many existing designs address these issues by implementing relatively simple routing algorithms, but more sophisticated routing algorithms will likely be needed in the future to deal with increasing manufacturing defects, process variability, and other complications arising from continued technology scaling, as discussed briefly below.

> 区分路由算法及其实现非常重要。路由算法描述了在整个网络上转发数据包并影响数据包延迟和网络吞吐量的规则，但其实现会影响数据包到达节点时数据包所遭受的延迟，所需的硅区域以及与路由计算相关的功耗。已经提出了几种技术来预先计算路由算法和/或隐藏路由计算延迟。然而，努力大大减少了减少硅面积和功耗，而不会显着影响路由屈曲。这两个问题都变得非常重要，尤其是对于 OCN。许多现有的设计通过实施相对简单的路由算法来解决这些问题，但是将来可能需要更复杂的路由算法来处理不断增加的制造缺陷，过程可变性以及持续技术扩展引起的其他并发症，如下所述。

As mentioned in a previous section, depending on where the routing algorithm is computed, two basic forms of routing exist: source and distributed routing. In source routing, the complexity of implementation is moved to the end nodes where paths need to be stored in tables, and the path for a given packet is selected based on the destination end node identifier. In distributed routing, however, the complexity is moved to the switches where, at each hop along the path of a packet, a selection of the output port to take is performed. In distributed routing, two basic implemen- tations exist. The first one consists of using a logic block that implements a fixed routing algorithm for a particular topology. The most common example of such an implementation is dimension-order routing, where dimensions are offset in an established order. Alternatively, distributed routing can be implemented with for- warding tables, where each entry encodes the output port to be used for a particular destination. Therefore, in the worst case, as many entries as destination nodes are required.

> 如前一节所述，取决于计算路由算法的位置，存在两种基本的路由形式：源和分布式路由。在源路由中，实现的复杂性被移至需要存储在表中的路径的末端节点，并根据目标末端节点标识符选择给定数据包的路径。但是，在分布式路由中，复杂性移至开关，在该开关中，在每个跳动的路径上，每个跳跃的路径都可以选择要采用的输出端口。在分布式路由中，存在两个基本的实现。第一个是使用逻辑块，该逻辑块用于特定拓扑的固定路由算法。这种实现的最常见示例是维订单路由，其中尺寸以既定顺序的抵消。另外，可以使用填写表来实现分布式路由，其中每个条目编码要用于特定目的地的输出端口。因此，在最坏的情况下，需要许多目的地节点的条目。

Both methods for implementing distributed routing have their benefits and drawbacks. Logic-based routing features a very short computation delay, usually requires a small silicon area, and has low power consumption. However, logic- based routing needs to be designed with a specific topology in mind and, therefore, is restricted to that topology. Table-based distributed routing is quite flexible and supports any topology and routing algorithm. Simply, tables need to be filled with the proper contents based on the applied routing algorithm (e.g., the up\*/down\* routing algorithm can be defined for any irregular topology). However, the down side of table-based distributed routing is its non-negligible area and power cost. Also, scalability is problematic in table-based solutions as, in the worst case, a sys- tem with N end nodes (and switches) requires as many as N tables each with N entries, thus having quadratic cost.

> 实施分布式路由的两种方法都有其好处和缺点。基于逻辑的路由具有非常短的计算延迟，通常需要一个小的硅区域，并且功耗低。但是，基于逻辑的路由需要考虑特定的拓扑，因此仅限于该拓扑。基于表的分布式路由非常灵活，并支持任何拓扑和路由算法。简而言之，需要根据应用的路由算法填充适当的内容(例如，可以针对任何不规则拓扑来定义向上\*/down \*路由算法)。但是，基于桌子的分布式路由的下降是其不可忽略的区域和电源成本。同样，在基于表的解决方案中，可伸缩性是有问题的，因为在最坏的情况下，具有 n 端节点(和开关)的系统需要每个 n 台上的 n 个桌子，因此具有二次成本。

Depending on the network domain, one solution is more suitable than the other. For instance, in SANs, it is usual to find table-based solutions as is the case with InfiniBand. In other environments, like OCNs, table-based implementations are avoided due to the aforementioned costs in power and silicon area. In such envi- ronments, it is more advisable to rely on logic-based implementations. Herein lies some of the challenges OCN designers face: ever continuing technology scaling through device miniaturization leads to increases in the number of manufacturing defects, higher failure rates (either transient or permanent), significant process var- iations (transistors behaving differently from design specs), the need for different clock frequency and voltage domains, and tight power and energy budgets. All of these challenges translate to the network needing support for heterogeneity. Dif- ferent—possibly irregular—regions of the network will be created owing to failed components, powered down switches and links, disabled components (due to unacceptable variations in performance) and so on. Hence, heterogeneous systems may emerge from a homogeneous design. In this framework, it is important to effi- ciently implement routing algorithms designed to provide enough flexibility to address these new challenges.

> 根据网络域，一个解决方案比另一个解决方案更合适。例如，在 SANS 中，通常可以像 Infiniband 一样找到基于表的解决方案。在其他环境(例如 OCN)中，由于上述电力和硅区域的成本，因此可以避免基于桌子的实现。在这样的环境中，建议依靠基于逻辑的实现。OCN 设计师面临的一些挑战所在：通过设备微型化的持续技术扩展会导致制造缺陷的数量增加，更高的故障率(瞬态或永久性)，重要的过程量(晶体管与设计规格的行为不同)，需要不同的时钟频率和电压域以及紧密的功率和能量预算。所有这些挑战都转化为需要支持异质性的网络。由于组件失败，开关和链接电源，禁用组件(由于性能的不可接受的变化)等，将创建网络区域的区别。因此，异质系统可能会从同质设计中出现。在此框架中，重要的是有效地实施旨在提供足够灵活性来应对这些新挑战的路由算法。

A well-known solution for providing a certain degree of flexibility while being much more compact than traditional table-based approaches is interval routing \[[Leeuwen 1987](#_bookmark682)], where a range of destinations is defined for each output port. Although this approach is not flexible enough, it provides a clue on how to address emerging challenges. A more recent approach provides a plausible implementation design point that lies between logic-based implementation (efficiency) and table- based implementation (flexibility). Logic-Based Distributed Routing (LBDR) is a hybrid approach that takes as a reference a regular 2D mesh but allows an irregular network to be derived from it due to changes in topology induced by manufactur- ing defects, failures, and other anomalies. Due to the faulty, disabled, and powered- down components, regularity is compromised and the dimension-order routing algorithm can no longer be used. To support such topologies, LBDR defines a set of configuration bits at each switch. Four connectivity bits are used at each switch to indicate the connectivity of the switch to the neighbor switches in the topology. Thus, one connectivity bit per port is used. Those connectivity bits are used, for instance, to disable an output port leading to a faulty component. Addi- tionally, eight routing bits are used, two per output port, to define the available routing options. The value of the routing bits is set at power-on and is computed from the routing algorithm to be implemented in the network. Basically, when a routing bit is set, it indicates that a packet can leave the switch through the asso- ciated output port and is allowed to perform a certain turn at the next switch. In this respect, LBDR is similar to interval routing, but it defines geographical areas instead of ranges of destinations. [Figure F.23](#_bookmark620) shows an example where a topology-agnostic routing algorithm is implemented with LBDR on an irregular topology. The figure shows the computed configuration bits.

> 一种提供一定程度灵活性同时比传统的基于表格的方法更紧凑的著名解决方案是间隔路由 [[Leeuwen 1987](#_bookmark682)]，其中为每个输出端口定义了一系列目的地。尽管这种方法不够灵活，但它为如何应对新出现的挑战提供了线索。最近的一种方法提供了一个合理的实现设计点，它位于基于逻辑的实现(效率)和基于表的实现(灵活性)之间。基于逻辑的分布式路由 (LBDR) 是一种混合方法，它以规则的 2D 网格作为参考，但由于制造缺陷、故障和其他异常引起的拓扑变化，允许从中派生出不规则网络。由于故障、禁用和掉电的组件，规则性受到损害，维度顺序路由算法不能再使用。为支持此类拓扑，LBDR 在每个交换机上定义了一组配置位。每个交换机使用四个连接位来指示该交换机与拓扑中相邻交换机的连接性。因此，每个端口使用一个连接位。例如，这些连接位用于禁用导致故障组件的输出端口。此外，还使用八个路由位(每个输出端口两个)来定义可用的路由选项。路由位的值在上电时设置，并根据要在网络中实现的路由算法计算得出。基本上，当一个路由位被设置时，它表明一个数据包可以通过相关的输出端口离开交换机，并被允许在下一个交换机执行特定的转弯。在这方面，LBDR 类似于区间路由，但它定义的是地理区域而不是目的地范围。[图 F.23](#_bookmark620) 显示了一个示例，其中在不规则拓扑上使用 LBDR 实现了与拓扑无关的路由算法。该图显示了计算出的配置位。

The connectivity and routing bits are used to implement the routing algorithm. For that purpose, a small set of logic gates are used in combination with the con- figuration bits. Basically, the LBDR approach takes as a reference the initial topol- ogy (a 2D mesh), and makes a decision based on the current coordinates of the router, the coordinates of the destination router, and the configuration bits. [Figure F.24](#_bookmark621) shows the required logic, and [Figure F.25](#_bookmark622) shows an example of where a packet is forwarded from its source to its destination with the use of the config- uration bits. As can be noticed, routing restrictions are enforced by preventing the use of the west port at switch 10.

> 连接和路由位用于实现路由算法。为此，一小组逻辑门与配置位结合使用。基本上，LBDR 方法将初始拓扑(2D 网格)作为参考，并根据路由器的当前坐标、目标路由器的坐标和配置位做出决定。[图 F.24](#_bookmark621) 显示了所需的逻辑，[图 F.25](#_bookmark622) 显示了使用配置位将数据包从源转发到目的地的示例 . 可以注意到，路由限制是通过阻止使用交换机 10 的西向端口来实施的。

LBDR represents a method for efficient routing implementation in OCNs. This mechanism has been recently extended to support non-minimal paths, collective communication operations, and traffic isolation. All of these improve- ments have been made while maintaining a compact and efficient implementation with the use of a small set of configuration bits. A detailed description of LBDR and its extensions, and the current research on OCNs can be found in [Flich \[2010]](#\_bookmark655).

> LBDR 代表了一种在 OCN 中实现高效路由的方法。该机制最近已扩展为支持非最小路径、集体通信操作和流量隔离。所有这些改进都是在使用一小组配置位保持紧凑和高效实施的同时进行的。关于 LBDR 及其扩展的详细描述，以及当前对 OCN 的研究可以在 [Flich \[2010]](#\_bookmark655) 中找到。

Figure F.23 Shown is an example of an irregular network that uses LBDR to implement the routing algorithm.
![](../media/image662.png)
<img src="../media/image669.png" style="width:1.22966in;height:1.71354in" />
![](../media/image670.png)
Figure F.24 LBDR logic at each input port of the router.
![](../media/image671.png)
Figure F.25 Example of routing a message from Router 14 to Router 5 using LBDR at each router.

### Pipelining the Switch Microarchitecture

Performance can be enhanced by pipelining the switch microarchitecture. Pipe- lined processing of packets in a switch has similarities with pipelined execution of instructions in a vector processor. In a vector pipeline, a single instruction indi- cates what operation to apply to all the vector elements executed in a pipelined way. Similarly, in a switch pipeline, a single packet header indicates how to pro- cess all of the internal data path physical transfer units (or `phits`) of a packet, which are processed in a pipelined fashion. Also, as packets at different input ports are independent of each other, they can be processed in parallel similar to the way mul- tiple independent instructions or threads of pipelined instructions can be executed in parallel.

> 可以通过流水线化开关微体系结构来提高性能。交换机中数据包的流水线处理与向量处理器中指令的流水线执行有相似之处。在向量流水线中，单个指令指示将什么操作应用于以流水线方式执行的所有向量元素。类似地，在交换机管道中，单个数据包标头指示如何处理数据包的所有内部数据路径物理传输单元(或 "phits" )，这些单元以流水线方式处理。此外，由于不同输入端口的数据包彼此独立，因此可以并行处理它们，类似于可以并行执行多个独立指令或流水线指令线程的方式。

The switch microarchitecture can be pipelined by analyzing the basic functions performed within the switch and organizing them into several stages. [Figure F.26](#_bookmark623) shows a block diagram of a five-stage pipelined organization for the basic switch microarchitecture given in [Figure F.21](#_bookmark618), assuming cut-through switching and the use of a forwarding table to implement routing. After receiving the header portion of the packet in the first stage, the routing information (i.e., destination address) is used in the second stage to look up the allowed routing option(s) in the forwarding table. Concurrent with this, other portions of the packet are received and buffered in the input port queue at the first stage. Arbitration is performed in the third stage. The crossbar is configured to allocate the granted output port for the packet in the fourth stage, and the packet header is buffered in the switch output port and ready for transmission over the external link in the fifth stage. Note that the second and third stages are used only by the packet header; the payload and trailer portions of the packet use only three of the stages—those used for data flow-thru once the internal data path of the switch is set up.

> 交换机微架构可以通过分析交换机内执行的基本功能并将它们组织成几个阶段来流水线化。[图 F.26](#_bookmark623) 显示了 [图 F.21](#_bookmark618) 中给出的基本交换机微体系结构的五级流水线组织框图，假设直通交换和转发的使用 表来实现路由。在第一阶段接收到数据包的标头部分后，在第二阶段使用路由信息(即目标地址)在转发表中查找允许的路由选项。与此同时，数据包的其他部分在第一阶段被接收并缓存在输入端口队列中。第三阶段进行仲裁。交叉开关被配置为在第四阶段为数据包分配允许的输出端口，并且在第五阶段将数据包头缓存在交换机输出端口并准备通过外部链路传输。请注意，第二和第三阶段仅由数据包标头使用；数据包的有效负载和尾部部分仅使用三个阶段——一旦设置了交换机的内部数据路径，用于数据流的阶段。

Figure F.26 Pipelined version of the basic input-output-buffered switch. The notation in the figure is as follows: IB is the input link control and buffer stage, RC is the route computation stage, SA is the crossbar switch arbitration stage, ST is the crossbar switch traversal stage, and OB is the output buffer and link control stage. Packet fragments (flits) coming after the header remain in the IB stage until the header is processed and the crossbar switch resources are provided.

> 图 F.26 基本输入输出缓冲开关的流水线版本。图中符号如下：IB 为输入链路控制和缓冲阶段，RC 为路由计算阶段，SA 为 crossbar 交换机仲裁阶段，ST 为 crossbar 交换机遍历阶段，OB 为输出缓冲区和链路 控制阶段。报头之后的数据包片段 (flits) 保留在 IB 阶段，直到处理报头并提供交叉开关资源。

A virtual channel switch usually requires an additional stage for virtual channel allocation. Moreover, arbitration is required for every flit before transmission through the crossbar. Finally, depending on the complexity of the routing and arbi- tration algorithms, several clock cycles may be required for these operations.

> 虚拟通道开关通常需要一个额外的阶段来分配虚拟通道。此外，在通过交叉开关传输之前，每个微片都需要仲裁。最后，根据路由和仲裁算法的复杂性，这些操作可能需要几个时钟周期。

### Other Switch Microarchitecture Enhancements

As mentioned earlier, internal switch speedup is sometimes implemented to increase switch output port utilization. This speedup is usually implemented by increasing the clock frequency and/or the internal data path width (i.e., phit size) of the switch. An alternative solution consists of implementing several parallel data paths from each input port’s set of queues to the output ports. One way of doing this is by increasing the number of crossbar input ports. When implementing several physical queues per input port, this can be achieved by devoting a separate crossbar port to each input queue. For example, the IBM Blue Gene/L implements two crossbar access ports and two read ports per switch input port.

> 如前所述，有时会实施内部交换机加速以提高交换机输出端口的利用率。这种加速通常通过增加开关的时钟频率和/或内部数据路径宽度(即 phit 大小)来实现。另一种解决方案包括从每个输入端口的队列集到输出端口实施多个并行数据路径。一种方法是增加交叉开关输入端口的数量。当每个输入端口实现多个物理队列时，这可以通过为每个输入队列分配一个单独的交叉开关端口来实现。例如，IBM Blue Gene/L 在每个开关输入端口实现了两个交叉访问端口和两个读取端口。

Another way of implementing parallel data paths between input and output ports is to move the buffers to the crossbar crosspoints. This switch architecture is usually referred to as a `buffered crossbar switch`. A buffered crossbar provides independent data paths from each input port to the different output ports, thus mak- ing it possible to send up to `k` packets at a time from a given input port to `k` different output ports. By implementing independent crosspoint memories for each input- output port pair, HOL blocking is eliminated at the switch level. Moreover, arbi- tration is significantly simpler than in other switch architectures. Effectively, each output port can receive packets from only a disjoint subset of the crosspoint mem- ories. Thus, a completely independent arbiter can be implemented at each switch output port, each of those arbiters being very simple.

> 在输入和输出端口之间实现并行数据路径的另一种方法是将缓冲区移动到 crossbar 交叉点。这种交换机架构通常称为 "缓冲交叉开关" 。缓冲交叉开关提供从每个输入端口到不同输出端口的独立数据路径，因此可以一次从给定输入端口向 "k" 个不同输出端口发送多达 "k" 个数据包。通过为每个输入输出端口对实现独立的交叉点存储器，在开关级消除了 HOL 阻塞。此外，仲裁比其他交换机架构要简单得多。实际上，每个输出端口只能从交叉点存储器的一个不相交子集中接收数据包。因此，可以在每个开关输出端口实现一个完全独立的仲裁器，每个仲裁器都非常简单。

A buffered crossbar would be the ideal switch architecture if it were not so expensive. The number of crosspoint memories increases quadratically with the number of switch ports, dramatically increasing its cost and reducing its scalability with respect to the basic switch architecture. In addition, each crosspoint memory must be large enough to efficiently implement link-level flow control. To reduce cost, most designers prefer input-buffered or combined input-output-buffered switches enhanced with some of the mechanisms described previously.

> 如果缓冲交叉开关不是那么昂贵，它将是理想的开关架构。交叉点存储器的数量随交换机端口数量的增加呈二次方增长，从而显着增加其成本并降低其相对于基本交换机架构的可扩展性。此外，每个交叉点内存必须足够大才能有效地实现链路级流量控制。为了降低成本，大多数设计人员更喜欢输入缓冲或组合输入输出缓冲开关，这些开关通过前面描述的某些机制得到增强。
