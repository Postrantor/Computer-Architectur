## Performance of Symmetric Shared-Memory Multiprocessors

> ##对称共享内存多处理器的性能

In a multicore using a snooping coherence protocol, several different phenomena combine to determine performance. In particular, the overall cache performance is a combination of the behavior of uniprocessor cache miss traffic and the traffic caused by communication, which results in invalidations and subsequent cache misses. Changing t he processor count, cache size, and block size can affect these two components of the miss rate in different ways, leading to overall system behav- ior that is a combination of the two effects.

> 在使用窥探一致性方案的多核心中，几种不同的现象结合在一起以确定性能。特别是，总体缓存性能是单层缓存失误和通信引起的流量的行为的组合，这导致无效和随后的缓存失误。更改处理器计数，缓存大小和块大小会以不同的方式影响错过率的这两个组成部分，从而导致整体系统行为，这是两种效果的组合。

[Appendix B](#_bookmark436) breaks the uniprocessor miss rate into the three C’s classification (capacity, compulsory, and conflict) and provides insight into both application behavior and potential improvements to the cache design. Similarly, the misses that arise from interprocessor communication, which are often called _coherence misses_, can be broken into two separate sources.

> [附录 B]（#_ bookmark436）将 UniProcessor 的率分类为三个 C 的分类（容量，强制性和冲突），并提供了对应用程序行为和缓存设计的潜在改进的见解。同样，通常称为_ Coherence Misses_的解释器通信产生的错过可以分为两个单独的来源。

The first source is the _true sharing misses_ that arise from the communication of data through the cache coherence mechanism. In an invalidation-based protocol, the first write by a processor to a shared cache block causes an invalidation to establish ownership of that block. Additionally, when another processor attempts to read a modified word in that cache block, a miss occurs and the resultant block is transferred. Both these misses are classified as true sharing misses because they directly arise from the sharing of data among processors.

> 第一个来源是_true 共享 MISSES _是通过缓存相干机制传达数据的通信。在基于无效的协议中，处理器对共享缓存块的第一次写入会导致无效建立该块的所有权。此外，当另一个处理器试图在该缓存块中读取修改后的单词时，会发生遗漏，并将结果块转移。这两种错过都被归类为真正的共享错过，因为它们直接来自处理器之间的数据共享。

The second effect, called _false sharing_, arises from the use of an invalidation- based coherence algorithm with a single valid bit per cache block. False sharing occurs when a block is invalidated (and a subsequent reference causes a miss) because some word in the block, other than the one being read, is written into. If the word written into is actually used by the processor that received the inval- idate, then the reference was a true sharing reference and would have caused a miss independent of the block size. If, however, the word being written and the word read are different and the invalidation does not cause a new value to be commu- nicated, but only causes an extra cache miss, then it is a false sharing miss. In a false sharing miss, the block is shared, but no word in the cache is actually shared, and the miss would not occur if the block size were a single word. The following exam- ple makes the sharing patterns clear.

> 第二个效果称为_false 共享_，是由使用基于无效的连贯算法的使用，每个缓存块有一个有效的位。当块无效（随后的引用导致错过）时，会发生错误的共享，因为该块中的某些单词（除了被读取的单词）被写入其中。如果已收到无效的处理器实际使用了写入的单词，则该引用是一个真实的共享参考，并且会导致单独的块大小。但是，如果编写的单词和单词读是不同的，并且无效不会导致新价值被调整，而是导致额外的缓存失误，那是一个错误的分享失误。在一个错误的共享中，该块共享，但实际上没有共享缓存中的单词，如果块大小是一个单词，则不会发生遗漏。以下考试使共享模式清晰明了。

Example Assume that words z1 and z2 are in the same cache block, which is in the shared state in the caches of both P1 and P2. Assuming the following sequence of events, identify each miss as a true sharing miss, a false sharing miss, or a hit. Any miss that would occur if the block size were one word is designated a true sharing miss.

> 示例假设单词 z1 和 z2 位于同一缓存块中，该缓存块处于 P1 和 P2 的缓存中的共享状态。假设有以下事件序列，请确定每个失误是真正的分享失误，虚假的分享失误或命中率。如果块大小为一个单词，则会发生任何遗漏。

_Answer_ Here are the classifications by time step:

> _answer_这是按时间步骤按分类：

1. This event is a true sharing miss, since z1 is in the shared state in P2 and needs to be invalidated from P2.

> 1.此事件是一个真正的分享失误，因为 Z1 处于 P2 中的共享状态，需要从 P2 中无效。

2. This event is a false sharing miss, since z2 was invalidated by the write of z1 in P1, but that value of z1 is not used in P2.

> 2.此事件是一个错误的共享失误，因为 Z2 在 P1 中的 Z1 写入无效，但是 Z1 的值未在 P2 中使用。

3. This event is a false sharing miss, since the block containing z1 is marked shared due to the read in P2, but P2 did not read z1. The cache block containing z1 will be in the shared state after the read by P2; a write miss is required to obtain exclusive access to the block. In some protocols, this will be handled as an _upgrade request_, which generates a bus invalidate, but does not transfer the cache block.

> 3.此事件是一个错误的共享错过，因为由于 P2 中的读取，包含 Z1 的块被标记为共享，但 P2 没有读取 Z1。读取 P2 后，包含 Z1 的缓存块将处于共享状态；需要写信以获得对该块的独家访问。在某些协议中，这将作为_upgrade request_处理，该请求_会生成总线无效，但不会传输缓存块。

4. This event is a false sharing miss for the same reason as step 3.

> 4.此事件是一个错误的分享失误，其原因与步骤 3 相同。

5. This event is a true sharing miss, since the value being read was written by P2.

> 5.此事件是一个真正的分享失误，因为所读取的值是由 P2 撰写的。

Although we will see the effects of true and false sharing misses in commercial workloads, the role of coherence misses is more significant for tightly coupled applications that share significant amounts of user data. We examine their effects in detail in Appendix I when we consider the performance of a parallel scientific workload.

> 尽管我们将看到真实和虚假共享在商业工作负载中的影响，但对于共享大量用户数据的紧密耦合应用程序，连贯性错过的作用更为重要。当我们考虑平行科学工作负载的性能时，我们在附录 I 中详细检查了它们的效果。

### A Commercial Workload

> ###商业工作量

In this section, we examine the memory system behavior of a 4-processor shared- memory multiprocessor when running an online transaction processing workload. The study we examine was done with a 4-processor Alpha system in 1998, but it remains the most comprehensive and insightful study of the performance of a mul- tiprocessor for such workloads. We will focus on understanding the multiprocessor cache activity, and particularly the behavior in L3, where much of the traffic is coherence-related.

> 在本节中，我们在运行在线交易处理工作负载时检查 4 处理器共享 - 内存多处理器的内存系统行为。我们研究的研究是在 1998 年通过 4 处理器 Alpha 系统完成的，但它仍然是对此类工作负载的 Mul-Tipersessor 的性能最全面，最有见地的研究。我们将专注于理解多处理器缓存活动，尤其是 L3 中的行为，其中许多流量与相干相关。

Figure 5.9 The characteristics of the cache hierarchy of the Alpha 21164 used in this study and the Intel i7. Although the sizes are larger and the associativity is higher on the i7, the miss penalties are also higher, so the behav- ior may differ only slightly. Both systems have a high penalty (125 cycles or more) for a transfer required from a private cache. A key difference is that L3 is shared in the i7 versus four separate, unshared caches in the Alpha server.

> 图 5.9 本研究中使用的 Alpha 21164 的缓存层次结构和 Intel I7 的特征。尽管尺寸较大，并且在 i7 上的关联性更高，但罚款也更高，因此行为可能仅略有不同。对于私人缓存所需的转移，这两个系统都有很高的罚款（125 个或更多循环）。一个关键区别在于，L3 与 Alpha Server 中的四个单独的未共享缓存中共享。

The results were collected either on an AlphaServer 4100 or using a configur- able simulator modeled after the AlphaServer 4100. Each processor in the Alpha- Server 4100 is an Alpha 21164, which issues up to four instructions per clock and runs at 300 MHz. Although the clock rate of the Alpha processor in this system is considerably slower than processors in systems designed in 2017, the basic struc- ture of the system, consisting of a four-issue processor and a three-level cache hier- archy, is very similar to the multicore Intel i7 and other processors, as shown in [Figure 5.9](#_bookmark226). Rather than focus on the performance details, we consider data that looks at the simulated L3 behavior for L3 caches varying from 2 to 8 MiB per processor.

> 结果是在 Alphapherver 4100 上收集的，或者使用以 Alphacterver 4100 建立建模的配置模拟器。Alpha-Server 4100 中的每个处理器都是 Alpha 21164，该 Alpha 21164，每个时钟最多发出四个说明，并以 300 MHz 运行。尽管该系统中 Alpha 处理器的时钟速率比 2017 年设计的系统中的处理器要慢得多，但是该系统的基本结构，包括四件问题处理器和三级 cache Hier-Archy，非常非常类似于多核 Intel I7 和其他处理器，如[图 5.9]（#_ bookmark226）所示。我们没有专注于性能细节，而是考虑着查看 L3 缓存的模拟 L3 行为的数据，每个处理器从 2 到 8 MIB 不等。

Although the original study considered three different workloads, we focus our attention on the online transaction-processing (OLTP) workload modeled after TPC-B (which has memory behavior similar to its newer cousin TPC-C, described in [Chapter 1](#_bookmark2)) and using Oracle 7.3.2 as the underlying database. The workload con- sists of a set of client processes that generate requests and a set of servers that han- dle them. The server processes consume 85% of the user time, with the remaining going to the clients. Although the I/O latency is hidden by careful tuning and enough requests to keep the processor busy, the server processes typically block for I/O after about 25,000 instructions. Overall, 71% of the execution time is spent in user mode, 18% in the operating system, and 11% idle, primarily waiting for I/O. Of the commercial applications studied, the OLTP application stresses the memory system the hardest and shows significant challenges even when evaluated with much larger L3 caches. For example, on the AlphaServer, the processors are stalled for approximately 90% of the cycles with memory accesses occupying almost half the stall time and L2 misses 25% of the stall time.

> 尽管最初的研究考虑了三种不同的工作负载，但我们将注意力集中在以 TPC-B 为模型的在线交易处理（OLTP）工作量（其记忆行为类似于其较新的 Cousin TPC-C，[第 1 章]（＃）（＃）（＃）_Bookmark2））和使用 Oracle 7.3.2 作为基础数据库。工作负载由生成请求的一组客户流程和一组服务器组成。服务器处理消耗的 85％的用户时间，其余的则将流向客户端。尽管通过仔细的调整和足够的请求使处理器忙碌，但服务器的处理通常会在大约 25,000 个说明后阻止 I/O。总体而言，执行时间的 71％用于用户模式，在操作系统中的 18％和 11％的空闲时间，主要等待 I/O。在研究的商业应用中，OLTP 应用程序强调记忆系统最困难，即使使用更大的 L3 缓存进行评估，也会显示出巨大的挑战。例如，在 Alphapherver 上，处理器停滞了大约 90％的周期，内存访问几乎占据了一半的失速时间，而 L2 却错过了 25％的失速时间。

Figure 5.10 The relative performance of the OLTP workload as the size of the L3 cache, which is set as two-way set associative, grows from 1 to 8 MiB. The idle time also grows as cache size is increased, reducing some of the performance gains. This growth occurs because, with fewer memory system stalls, more server processes are needed to cover the I/O latency. The workload could be retuned to increase the com- putation/communication balance, holding the idle time in check. The PAL code is a set of sequences of specialized OS-level instructions executed in privileged mode; an exam- ple is the TLB miss handler.

> 图 5.10 OLTP 工作负载的相对性能作为 L3 高速缓存的大小，该尺寸为双向集合关联，从 1 个 MIB 增长到 8 MIB。随着缓存大小的增加，空闲时间也会增长，从而减少了一些性能增长。这种增长之所以发生，是因为，由于存储系统摊位较少，需要更多的服务器进程来覆盖 I/O 延迟。可以重新调整工作量以增加计算/沟通余额，并控制空闲时间。PAL 代码是在特权模式下执行的专用 OS 级指令的一组序列；TLB 小姐处理程序是考试。

We start by examining the effect of varying the size of the L3 cache. In these studies, the L3 cache is varied from 1 to 8 MiB per processor; at 2 MiB per pro- cessor, the total size of L3 is equal to that of the Intel i7 6700. In the case of the i7, however, the cache is shared, which provides both some advantages and disadvan- tages. It is unlikely that the shared 8 MiB cache will outperform separate L3 caches with a total size of 16 MiB. [Figure 5.10](#_bookmark227) shows the effect of increasing the cache size, using two-way set associative caches, which reduces the large number of con- flict misses. The execution time is improved as the L3 cache grows because of the reduction in L3 misses. Surprisingly, almost all of the gain occurs in going from 1 to 2 MiB (or 4 to 8 MiB of total cache for the four processors). There is little addi- tional gain beyond that, despite the fact that cache misses are still a cause of sig- nificant performance loss with 2 MiB and 4 MiB caches. The question is, Why? To better understand the answer to this question, we need to determine what factors contribute to the L3 miss rate and how they change as the L3 cache grows. [Figure 5.11](#_bookmark228) shows these data, displaying the number of memory access cycles con- tributed per instruction from five sources. The two largest sources of L3 memory access cycles with a 1 MiB L3 are instruction and capacity/conflict misses. With a larger L3, these two sources shrink to be minor contributors. Unfortunately, the compulsory, false sharing, and true sharing misses are unaffected by a larger L3. Thus, at 4 and 8 MiB, the true sharing misses generate the dominant fraction of the misses; the lack of change in true sharing misses leads to the limited reductions in the overall miss rate when increasing the L3 cache size beyond 2 MiB.

> 我们首先检查改变 L3 缓存大小的效果。在这些研究中，L3 缓存从每个处理器 1 到 8 MIB 差异。在每个专门的 2 MIB 时，L3 的总大小等于英特尔 i7 6700。但是，对于 i7，共享缓存，这既可以提供一些优势和缺点。共享的 8 个 MIB 缓存不太可能超过单独的 L3 缓存，总尺寸为 16 MIB。[图 5.10]（#_ bookmark227）使用双向套件的关联缓存显示了增加缓存大小的效果，从而减少了大量的 Conflict 错过。随着 L3 缓存的增长，由于 L3 失误的减少，执行时间得到了改善。令人惊讶的是，几乎所有的增益都在从 1 到 2 MIB（或四个处理器的总缓存 4 至 8 MIB）中发生。除此之外，除此之外，几乎没有增加的收益，尽管缓存失误仍然是造成 2 个 MIB 和 4 个 MIB 缓存的性能损失的原因。问题是，为什么？为了更好地理解这个问题的答案，我们需要确定哪些因素导致 L3 失误率以及它们随着 L3 缓存的增长而变化。[图 5.11]（#_ bookmark228）显示了这些数据，显示了从五个来源通过指令进行的记忆访问周期的数量。具有 1 MIB L3 的 L3 内存访问周期的两个最大来源是指令和容量/冲突。随着 L3 的较大，这两个来源缩小为较小的贡献者。不幸的是，强制性，虚假共享和真实的分享失误不受较大的 L3 的影响。因此，在 4 和 8 MIB 时，真正的分享失误产生了失误的主要部分。真正的共享错过的变化缺乏变化会导致总体失误率的降低有限，当时 L3 高速缓存大小超过 2 MIB。

Figure 5.11 The contributing causes of memory access cycle shift as the cache size is increased. The L3 cache is simulated as two-way set associative.

> 图 5.11 随着缓存大小的增加，内存访问周期移动的原因。L3 缓存模拟为双向集合关联。

Increasing the cache size eliminates most of the uniprocessor misses while leaving the multiprocessor misses untouched. How does increasing the processor count affect different types of misses? [Figure 5.12](#_bookmark229) shows these data assuming a base configuration with a 2 MiB, two-way set associative L3 cache (the same effective per processor cache size as the i7 but with less associativity). As we might expect, the increase in the true sharing miss rate, which is not compensated for by any decrease in the uniprocessor misses, leads to an overall increase in the memory access cycles per instruction.

> 增加缓存尺寸会消除大多数 UniproCessor 错过的遗漏，同时使多处理器未经触及。增加处理器计数如何影响不同类型的错过？[图 5.12]（#_ bookmark229）显示了这些数据，假设具有 2 MIB 的基本配置，则具有 2 MIB，双向集合关联 L3 缓存（每个处理器缓存大小与 i7 相同，但关联性较小）。正如我们可能期望的那样，真正的共享率的增加，这不会因单层失误的任何减少而弥补，这会导致每条指令的内存访问周期的总体增加。

The final question we examine is whether increasing the block size—which should decrease the instruction and cold miss rate and, within limits, also reduce the capacity/conflict miss rate and possibly the true sharing miss rate—is helpful for this workload. [Figure 5.13](#_bookmark230) shows the number of misses per 1000 instructions as the block size is increased from 32 to 256 bytes. Increasing the block size from 32 to 256 bytes affects four of the miss rate components:

> 我们检查的最后一个问题是，增加块大小是否会降低指令和冷率率，并且在限制内还会降低容量/冲突失误率，甚至可能是真正的分享失误率 - 对此工作量有所帮助。[图 5.13]（#_ bookmark230）显示了每 1000 个说明的错过数量，因为块大小从 32 个字节增加到 256 个字节。将块的大小从 32 个字节增加到 256 个字节会影响四个遗漏率组件：

- The true sharing miss rate decreases by more than a factor of 2, indicating some locality in the true sharing patterns.

> - 真正的共享率降低了 2 倍以上，表明真正的共享模式中的某些地方。

Figure 5.12 The contribution to memory access cycles increases as processor count increases primarily because of increased true sharing. The compulsory misses slightly increase because each processor must now handle more compulsory misses.

> 图 5.12 对内存访问周期的贡献随着处理器计数的增加而增加，这主要是由于真实共享的增加。强制性错过略有增加，因为现在每个处理器都必须处理更多的强制性错过。

Figure 5.13 The number of misses per 1000 instructions drops steadily as the block size of the L3 cache is increased, making a good case for an L3 block size of at least 128 bytes. The L3 cache is 2 MiB, two-way set associative.

> 图 5.13 随着 L3 缓存的块大小增加，每 1000 个指令的错过数量稳定下降，这是 L3 块大小至少 128 个字节的好案例。L3 缓存是 2 个 MIB，双向套件关联。

- The compulsory miss rate significantly decreases, as we would expect.

> - 正如我们预期的那样，强制性的失误率大大降低。

- The conflict/capacity misses show a small decrease (a factor of 1.26 compared to a factor of 8 increase in block size), indicating that the spatial locality is not high in the uniprocessor misses that occur with L3 caches larger than 2 MiB.

> - 冲突/容量误差显示出很小的减少（比例增加了 1.26，块大小增加了 8 倍），这表明在 L3 缓存大于 2 MIB 的 UniproCessor 遗漏中，空间位置不高。

- The false sharing miss rate, although small in absolute terms, nearly doubles.

> - 错误的分享率，尽管从绝对的角度来看，虽然很小，但几乎翻了一番。

The lack of a significant effect on the instruction miss rate is startling. If there were an instruction-only cache with this behavior, we would conclude that the spa- tial locality is very poor. In the case of mixed L2 and L3 caches, other effects such as instruction-data conflicts may also contribute to the high instruction cache miss rate for larger blocks. Other studies have documented the low spatial locality in the instruction stream of large database and OLTP workloads, which have lots of short basic blocks and special-purpose code sequences. Based on these data, the miss penalty for a larger block size L3 to perform as well as the 32-byte block size L3 can be expressed as a multiplier on the 32-byte block size penalty.

> 缺乏对教学的影响不足的失误率令人震惊。如果这种行为只有只有指令的缓存，我们将得出结论，规范的位置非常差。在混合 L2 和 L3 缓存的情况下，其他效果（例如指示数据冲突）也可能有助于较大块的高指令缓存率。其他研究已经记录了大数据库和 OLTP 工作负载的指令流中的低空间位置，它们具有许多简短的基本块和特殊用途的代码序列。基于这些数据，较大的块大小 L3 的罚款以及 32 字节块大小 L3 可以表示为 32 字节块大小的惩罚上的乘数。

With modern DDR SDRAMs that make block access fast, these numbers are attainable, especially at the 64 byte (the i7 block size) and the 128 byte block size. Of course, we must also worry about the effects of the increased traffic to memory and possible contention for the memory with other cores. This latter effect may easily negate the gains obtained from improving the performance of a single processor.

> 有了现代 DDR SDRAM，可以快速访问块，因此可以实现这些数字，尤其是在 64 个字节（i7 块大小）和 128 个字节块大小时。当然，我们还必须担心增加流量增加对内存的影响以及与其他内核的内存可能存在的争夺。后一种效果很容易消除通过提高单个处理器的性能而获得的收益。

### A Multiprogramming and OS Workload

> ###多编程和操作系统工作负载

Our next study is a multiprogrammed workload consisting of both user activity and OS activity. The workload used is two independent copies of the compile phases of the Andrew benchmark, a benchmark that emulates a software development envi- ronment. The compile phase consists of a parallel version of the UNIX “make” command executed using eight processors. The workload runs for 5.24 seconds on eight processors, creating 203 processes and performing 787 disk requests on three different file systems. The workload is run with 128 MiB of memory, and no paging activity takes place.

> 我们的下一项研究是由用户活动和 OS 活动组成的多编程工作负载。使用的工作量是安德鲁基准的编译阶段的两个独立副本，安德鲁基准是模拟软件开发环境的基准。编译阶段由使用八个处理器执行的 UNIX“ make”命令的并行版本组成。工作负载在八个处理器上运行 5.24 秒，创建 203 个进程并在三个不同的文件系统上执行 787 个磁盘请求。工作负载运行 128 个 MIB 内存，并且不进行分页活动。

The workload has three distinct phases: compiling the benchmarks, which involves substantial compute activity; installing the object files in a library; and removing the object files. The last phase is completely dominated by I/O, and only two processes are active (one for each of the runs). In the middle phase, I/O also plays a major role, and the processor is largely idle. The overall workload is much more system- and I/O-intensive than the OLTP workload.

> 工作量有三个不同的阶段：编译基准，涉及实质性的计算活动；在库中安装对象文件；并删除对象文件。最后一个阶段完全由 I/O 主导，只有两个过程是活动的（每个运行中一个）。在中间阶段，I/O 也起着重要作用，并且处理器在很大程度上是空闲的。与 OLTP 工作负载相比，整体工作负载更加系统和 I/O 密集型。

For the workload measurements, we assume the following memory and I/O systems:

> 对于工作负载测量，我们假设以下内存和 I/O 系统：

- _Level 1 instruction cache_—32 KB, two-way set associative with a 64-byte block, 1 clock cycle hit time.

> - _level 1 指令 cache_— 32 kb，双向设置的关联，带有 64 字节块，1 个时钟周期命中时间。

- _Level 1 data cache_—32 KB, two-way set associative with a 32-byte block, 1 clock cycle hit time. Our focus is on examining the behavior in the Level 1 data cache, in contrast to the OLTP study, which focused on the L3 cache.

> - _ level 1 数据缓存_— 32 kb，双向设置的关联，带有 32 字节块，1 个时钟周期命中时间。与 OLTP 研究相比，我们的重点是检查 1 级数据缓存中的行为，该研究的重点是 L3 缓存。

- _Level 2 cache_—1 MiB unified, two-way set associative with a 128-byte block, 10 clock cycle hit time.

> - _ level 2 缓存_— 1 MIB 统一，双向套件与 128 字节块，10 个时钟周期命中时间。

- _Main memory_—Single memory on a bus with an access time of 100 clock cycles.

> - _main Memory_  - 在访问时间为 100 个时钟周期的总线上的单个内存。

- _Disk system_—Fixed-access latency of 3 ms (less than normal to reduce idle time).

> - _disk System _-固定胶条延迟为 3 ms（比平常小，减少空闲时间）。

[Figure 5.14](#_bookmark231) shows how the execution time breaks down for the eight processors using the parameters just listed. Execution time is broken down into four components:

> [图 5.14]（#_ bookmark231）显示了使用刚刚列出的参数为八个处理器分解的执行时间如何分解。执行时间分为四个组成部分：

1. _Idle_—Execution in the kernel mode idle loop

> 1. _idle_  - 在内核模式怠速循环中执行

2. _User_—Execution in user code

> 2. _user_  - 用户代码中的执行

3. _Synchronization_—Execution or waiting for synchronization variables

> 3. _synchronization_-执行或等待同步变量

4. _Kernel_—Execution in the OS that is neither idle nor in synchronization access

> 4. _KERNEL_  - 在 OS 中的执行既不闲置，也不是同步访问

This multiprogramming workload has a significant instruction cache perfor- mance loss, at least for the OS. The instruction cache miss rate in the OS for a 64-byte block size, two-way set associative cache varies from 1.7% for a 32 KB cache to 0.2% for a 256 KB cache. User-level instruction cache misses are roughly one-sixth of the OS rate, across the variety of cache sizes. This partially accounts for the fact that, although the user code executes nine times as many instructions as the kernel, those instructions take only about four times as long as the smaller num- ber of instructions executed by the kernel.

> 这个多编程工作负载至少对于 OS，具有重要的指令缓存损失。OS 中的指令缓存速率为 64 字节块大小，双向集合缓存从 32 KB 缓存的 1.7％到 256 KB 缓存的 0.2％。在各种缓存尺寸的情况下，用户级指令缓存误差大约是 OS 速率的六分之一。这部分解释了以下事实：尽管用户代码执行了 9 次指令，但这些说明仅占内核执行的较小指令的四倍。

Figure 5.14 The distribution of execution time in the multiprogrammed parallel “make” workload. The high fraction of idle time is due to disk latency when only one of the eight processors is active. These data and the subsequent measurements for this workload were collected with the SimOS system (Rosenblum et al., 1995). The actual runs and data collection were done by M. Rosenblum, S. Herrod, and E. Bugnion of Stanford University.

> 图 5.14 多编程并行“制造”工作负载中的执行时间分布。当仅八个处理器中的一个处于活动状态时，怠速时间的高部分是由于磁盘延迟所致。这些数据和此工作负载的后续测量是通过 SIMOS 系统收集的（Rosenblum 等，1995）。实际运行和数据收集是由斯坦福大学的 Rosenblum，S。Herrod 和 E. Bugnion 完成的。

### Performance of the Multiprogramming and OS Workload

> ###多编程和操作系统工作负载的性能

In this section, we examine the cache performance of the multiprogrammed work- load as the cache size and block size are changed. Because of differences between the behavior of the kernel and that of the user processes, we keep these two com- ponents separate. Remember, though, that the user processes execute more than eight times as many instructions, so the overall miss rate is determined primarily by the miss rate in user code, which as we will see, is often one-fifth of the kernel miss rate.

> 在本节中，我们检查了随着缓存大小和块大小的更改，多编程工作负载的缓存性能。由于内核的行为与用户流程的行为之间存在差异，因此我们将这两个共同点保持分开。但是请记住，用户处理执行的指令八倍以上，因此总体率主要取决于用户代码中的错过率，这是我们将看到的，通常是内核错率的五分之一。

Although the user code executes more instructions, the behavior of the oper- ating system can cause more cache misses than the user processes for two reasons beyond larger code size and lack of locality. First, the kernel initializes all pages before allocating them to a user, which significantly increases the compulsory component of the kernel’s miss rate. Second, the kernel actually shares data and thus has a nontrivial coherence miss rate. In contrast, user processes cause coher- ence misses only when the process is scheduled on a different processor, and this component of the miss rate is small. This is a major difference between a multi- programmed workload and one like the OLTP workload.

> 尽管用户代码执行更多的指令，但操作系统的行为可能会导致比用户流程更多的缓存失误，这是由于两个原因超出了更大的代码大小和缺乏区域的原因。首先，内核在将它们分配给用户之前初始化了所有页面，从而大大增加了内核错率的强制性组件。其次，内核实际上共享数据，因此具有非平凡的连贯性失误率。相比之下，仅当该过程安排在另一个处理器上时，用户流程才会导致连贯性错过，而错过率的该组成部分很小。这是多编程工作负载和诸如 OLTP 工作负载之类的工作负载之间的主要区别。

[Figure 5.15](#_bookmark232) shows the data miss rate versus data cache size and versus block size for the kernel and user components. Increasing the data cache size affects the user miss rate more than it affects the kernel miss rate. Increasing the block size has beneficial effects for both miss rates because a larger fraction of the misses arise from compulsory and capacity, both of which can be potentially improved with larger block sizes. Because coherence misses are relatively rarer, the negative effects of increasing block size are small. To understand why the kernel and user processes behave differently, we can look at how the kernel misses behave.

> [图 5.15]（#_ bookmark232）显示了数据缓存大小与数据缓存大小以及内核和用户组件的块大小相对于块大小。增加数据缓存大小会影响用户错率的影响超过其影响内核失误率。增加块大小对两者的失误率都具有有益的效果，因为较大的失误是由强制性和容量引起的，这两者都可以通过较大的块大小来可能改善。由于连贯性的错过相对稀有，因此增加块大小的负面影响很小。要了解为什么内核和用户流程的行为不同，我们可以研究内核的表现。

[Figure 5.16](#_bookmark233) shows the variation in the kernel misses versus increases in cache size and in block size. The misses are broken into three classes: compulsory misses, coherence misses (from both true and false sharing), and capacity/conflict misses (which include misses caused by interference between the OS and the user process and between multiple user processes). [Figure 5.16](#_bookmark233) confirms that, for the kernel ref- erences, increasing the cache size reduces only the uniprocessor capacity/conflict miss rate. In contrast, increasing the block size causes a reduction in the compulsory miss rate. The absence of large increases in the coherence miss rate as block size is increased means that false sharing effects are probably insignificant, although such misses may be offsetting some of the gains from reducing the true sharing misses. If we examine the number of bytes needed per data reference, as in [Figure 5.17](#_bookmark234),

> [图 5.16]（#_ bookmark233）显示了内核失误与缓存大小和块大小的增加的变化。这些错过分为三类：强制性错过，连贯的错过（来自真实和虚假共享）以及容量/冲突错过（包括由 OS 和用户过程之间以及多个用户流程之间的干扰引起的错过）。[图 5.16]（#_ bookmark233）证实，对于内核参考，增加缓存大小仅降低了单层处理器容量/冲突率率。相反，增加块大小会导致强制性失误率的降低。由于块大小的增加，连贯率的率不大，这意味着虚假的共享效应可能微不足道，尽管这些错过可能会抵消一些减少真正的共享失误的收益。如果我们检查每个数据参考所需的字节数，如[图 5.17]（#_ bookmark234），

we see that the kernel has a higher traffic ratio that grows with block size. It is easy to see why this occurs: when going from a 16-byte block to a 128-byte block, the miss rate drops by about 3.7, but the number of bytes transferred per miss increases by 8, so the total miss traffic increases by just over a factor of 2. The user program also more than doubles as the block size goes from 16 to 128 bytes, but it starts out at a much lower level.

> 我们看到内核具有较高的交通比率，该比率随块大小而增长。很容易看出为什么会发生这种情况：从 16 字节块到一个 128 字节块时，错率下降了约 3.7，但是每次失误的字节数增加了 8 个，因此总的错过流量增加了超过 2 倍以上。随着块大小从 16 到 128 个字节，用户程序也增加了一倍以上，但它以较低的级别开始。

Figure 5.15 The data miss rates for the user and kernel components behave differently for increases in the L1 data cache size (on the left) versus increases in the L1 data cache block size (on the right). Increasing the L1 data cache from 32 to 256 KB (with a 32-byte block) causes the user miss rate to decrease proportionately more than the kernel miss rate: the user-level miss rate drops by almost a factor of 3, whereas the kernel-level miss rate drops by a factor of only 1.3. At the largest size, the L1 is closer to the size of L2 in a modern multicore processors. Thus the data indicates that the kernel miss rate will still be significant in an L2 cache. The miss rate for both user and kernel com- ponents drops steadily as the L1 block size is increased (while keeping the L1 cache at 32 KB). In contrast to the effects of increasing the cache size, increasing the block size improves the kernel miss rate more significantly (just under a factor of 4 for the kernel references when going from 16-byte to 128-byte blocks versus just under a factor of 3 for the user references).

> 图 5.15 用户和内核组件的数据错率对 L1 数据缓存大小（左侧）的增加而与 L1 数据缓存块大小（右图）的增加相对于增加。将 L1 数据缓存从 32 kb 增加到 256 kb（具有 32 字节块）会导致用户失误率比内核失误率的比例降低更多：用户级的失误率几乎下降了 3 倍，而内核则降低了。 - 级别的失误率仅下降了 1.3 倍。在最大尺寸的情况下，L1 更接近现代多层处理器中的 L2 大小。因此，数据表明内核失误率在 L2 缓存中仍然很重要。随着 L1 块大小的增加（同时将 L1 缓存保持在 32 kb），用户和内核组合的错率都会稳定下降。与增加缓存尺寸的影响相反，增加块大小会更大地提高内核失误率（当从 16 字节到 128 字节块中，核心参考的 4 倍不足 4 倍，而不是仅在于刚好的因素。3 对于用户参考）。

Figure 5.16 The components of the kernel data miss rate change as the L1 data cache size is increased from 32 to 256 KB, when the multiprogramming workload is run on eight processors. The compulsory miss rate component stays constant because it is unaffected by cache size. The capacity component drops by more than a factor of 2, whereas the coherence component nearly doubles. The increase in coherence misses occurs because the probability of a miss being caused by an invalidation increases with cache size, since fewer entries are bumped due to capacity. As we would expect, the increasing block size of the L1 data cache substantially reduces the compulsory miss rate in the kernel references. It also has a significant impact on the capacity miss rate, decreasing it by a factor of 2.4 over the range of block sizes. The increased block size has a small reduction in coherence traffic, which appears to stabilize at 64 bytes, with no change in the coherence miss rate in going to 128-byte lines. Because there are no significant reduc- tions in the coherence miss rate as the block size increases, the fraction of the miss rate caused by coherence grows from about 7% to about 15%.

> 图 5.16 当 L1 数据缓存大小从 32 kb 增加到 256 kb 时，内核数据丢失率的变化，当多编程工作负载在八个处理器上运行时。强制性的错率组件保持恒定，因为它不受缓存大小的影响。容量成分下降了 2 倍以上，而相干分量几乎翻了一番。相干误差的增加之所以发生，是因为由于容量而导致较少的条目撞击，因此由于无效而导致的遗漏概率增加。正如我们所期望的那样，L1 数据缓存的块大小的增加大大降低了内核参考中的强制性失误率。它还对容量损失率有重大影响，在块大小范围内将其降低了 2.4 倍。增加的块大小的相干流量较小，这似乎稳定在 64 个字节上，而连贯性损失率没有变化为 128 字节线。由于随着块大小的增加，连贯率没有显着降低，因此连贯性率引起的失误率的比例从约 7％增加到约 15％。

Figure 5.17 The number of bytes needed per data reference grows as block size is increased for both the kernel and user components. It is interesting to compare this chart with the data on scientific programs shown in Appendix I.

> 图 5.17 随着内核和用户组件的块大小增加，每个数据参考所需的字节数会增加。将此图表与附录 I 中显示的科学程序的数据进行比较很有趣。

For the multiprogrammed workload, the OS is a much more demanding user of the memory system. If more OS or OS-like activity is included in the workload, and the behavior is similar to what was measured for this workload, it will become very difficult to build a sufficiently capable memory system. One possible route to improving performance is to make the OS more cache-aware through either better programming environments or through programmer assistance. For example, the OS reuses memory for requests that arise from different system calls. Despite the fact that the reused memory will be completely overwritten, the hardware, not rec- ognizing this, will attempt to preserve coherency and the possibility that some por- tion of a cache block may be read, even if it is not. This behavior is analogous to the reuse of stack locations on procedure invocations. The IBM Power series has sup- port to allow the compiler to indicate this type of behavior on procedure invoca- tions, and the newest AMD processors have similar support. It is harder to detect such behavior by the OS, and doing so may require programmer assistance, but the payoff is potentially even greater.

> 对于多图工作负载，OS 是内存系统的要求更高的用户。如果工作量中包含更多的 OS 或 OS 样活动，并且该行为类似于该工作负载的测量，那么构建足够功能的内存系统将变得非常困难。提高性能的一项可能途径是通过更好的编程环境或程序员帮助使操作系统更加了解。例如，操作系统将重新用于内存，以引起不同系统调用引起的请求。尽管重复使用的内存将被完全覆盖，但硬件而不是对此进行重新审议，将试图保留相干性，即使没有读取缓存块的可能性也可能会被读取。这种行为类似于在过程调用中堆栈位置的重复使用。IBM Power 系列具有支持，以允许编译器在过程中指示这种类型的行为，并且最新的 AMD 处理器具有类似的支持。很难检测到 OS 的行为，这样做可能需要程序员的帮助，但是收益可能更大。

OS and commercial workloads pose tough challenges for multiprocessor mem- ory systems, and unlike scientific applications, which we examine in Appendix I, they are less amenable to algorithmic or compiler restructuring. As the number of cores increases, predicting the behavior of such applications is likely to get more difficult. Emulation or simulation methodologies that allow the simulation of tens to hundreds of cores with large applications (including operating systems) will be crucial to maintaining an analytical and quantitative approach to design.

> 操作系统和商业工作负载对多处理器内存系统构成艰巨的挑战，与我们在附录 I 中检查的科学应用不同，它们不太适合算法或编译器重组。随着核心数量的增加，预测此类应用的行为可能会变得更加困难。模拟或模拟方法，允许将数十枚到具有大型应用的核心模拟（包括操作系统）对于维持分析和定量设计方法至关重要。
