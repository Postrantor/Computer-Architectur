> [!note]
> 这里对网络概念有明确的定义解释，挺值得看的
> 另外对于网络延迟也有一些介绍，这块在实时性上应该有用

## Interconnecting Two Devices

This section introduces the basic concepts required to understand how communi- cation between just two networked devices takes place. This includes concepts that deal with situations in which the receiver may not be ready to process incoming data from the sender and situations in which transport errors may occur. To ease understanding, the black box network at this point can be conceptualized as an ideal network that behaves as simple dedicated links between the two devices. [Figure F.3](#_bookmark600) illustrates this, where unidirectional wires run from device A to device B and `vice versa`, and each end node contains a buffer to hold the data. Regardless of the network complexity, whether dedicated link or not, a connection exists from each end node device to the network to inject and receive information to/from the network. We first describe the basic functions that must be performed at the end nodes to commence and complete communication, and then we discuss network media and the basic functions that must be performed by the network to carry out communication. Later, a simple performance model is given, along with sev- eral examples to highlight implications of key network parameters.

> 本节介绍理解两个联网设备之间如何进行通信所需的基本概念。这包括处理接收方可能未准备好处理来自发送方的传入数据的情况以及可能发生传输错误的情况的概念。为了便于理解，此时的黑盒网络可以概念化为一个理想网络，它充当两个设备之间的简单专用链路。[图 F.3](#_bookmark600) 说明了这一点，其中单向线路从设备 A 延伸到设备 B，反之亦然，每个终端节点都包含一个缓冲区来保存数据。无论网络复杂程度如何，无论是否有专用链路，从每个终端节点设备到网络都存在连接，以向网络注入信息或从网络接收信息。我们首先描述端节点必须执行的基本功能以开始和完成通信，然后我们讨论网络媒体和网络必须执行的基本功能以进行通信。随后，给出了一个简单的性能模型，以及几个例子来强调关键网络参数的含义。

### Network Interface Functions: Composing and Processing Messages

Suppose we want two networked devices to read a word from each other’s mem- ory. The unit of information sent or received is called a `message`. To acquire the desired data, the two devices must first compose and send a certain type of message in the form of a `request` containing the address of the data within the other device. The address (i.e., memory or operand location) allows the receiver to identify where to find the information being requested. After processing the request, each device then composes and sends another type of message, a `reply`, containing the data. The address and data information is typically referred to as the message `payload.`

> 假设我们希望两个联网设备从彼此的内存中读取一个单词。发送或接收的信息单元称为 "消息" 。为了获取所需的数据，两个设备必须首先以 "请求" 的形式编写并发送某种类型的消息，其中包含另一个设备中数据的地址。地址(即内存或操作数位置)允许接收方识别在哪里可以找到所请求的信息。处理完请求后，每个设备都会组合并发送另一种类型的消息，即 "回复" ，其中包含数据。地址和数据信息通常称为消息 "有效载荷" 。

Figure F.3 A simple dedicated link network bidirectionally interconnecting two devices.

In addition to payload, every message contains some control bits needed by the network to deliver the message and process it at the receiver. The most typical are bits to distinguish between different types of messages (e.g., request, reply, request acknowledge, reply acknowledge) and bits that allow the network to transport the information properly to the destination. These additional control bits are encoded in the `header` and/or `trailer` portions of the message, depending on their location relative to the message payload. As an example, [Figure F.4](#_bookmark601) shows the format of a message for the simple dedicated link network shown in [Figure F.3](#_bookmark600). This example shows a single-word payload, but messages in some interconnection networks can include several thousands of words.

> 除了有效负载之外，每条消息还包含网络传递消息并在接收方处理消息所需的一些控制位。最典型的是区分不同类型消息(例如，请求、回复、请求确认、回复确认)的位和允许网络将信息正确传输到目的地的位。这些额外的控制位被编码在消息的 "header" 和/或 "trailer" 部分，具体取决于它们相对于消息有效负载的位置。例如，[图 F.4](#_bookmark601) 显示了[图 F.3](#_bookmark600) 中所示的简单专用链路网络的消息格式。此示例显示单个单词的有效负载，但某些互连网络中的消息可能包含数千个单词。

Before message transport over the network occurs, messages have to be com- posed. Likewise, upon receipt from the network, they must be processed. These and other functions described below are the role of the `network interface` (also referred to as the `channel adapter`) residing at the end nodes. Together with some direct memory access (DMA) engine and link drivers to transmit/receive messages to/from the network, some dedicated memory or register(s) may be used to buffer outgoing and incoming messages. Depending on the network domain and design specifications for the network, the network interface hardware may consist of noth- ing more than the communicating device itself (i.e., for OCNs and some SANs) or a separate card that integrates several embedded processors and DMA engines with thousands of megabytes of RAM (i.e., for many SANs and most LANs and WANs).

> 在通过网络进行消息传输之前，必须组合消息。同样，从网络收到后，必须对其进行处理。这些和下面描述的其他功能是驻留在端节点的 "网络接口" (也称为 "通道适配器" )的作用。与一些直接内存访问 (DMA) 引擎和链接驱动程序一起向/从网络传输/接收消息，一些专用内存或寄存器可用于缓冲传出和传入消息。根据网络域和网络设计规范，网络接口硬件可能只包含通信设备本身(即，对于 OCN 和某些 SAN)或集成了多个嵌入式处理器和 DMA 引擎的单独卡 数千兆字节的 RAM(即，对于许多 SAN 和大多数 LAN 和 WAN)。

In addition to hardware, network interfaces can include software or firmware to perform the needed operations. Even the simple example shown in [Figure F.3](#_bookmark600) may invoke messaging software to translate requests and replies into messages with the appropriate headers. This way, user applications need not worry about composing and processing messages as these tasks can be performed automatically at a lower level. An application program usually cooperates with the operating or runtime system to send and receive messages. As the network is likely to be shared by many processes running on each device, the operating system cannot allow messages intended for one process to be received by another. Thus, the messaging software must include protection mechanisms that distinguish between processes. This dis- tinction could be made by expanding the header with a `port` number that is known by both the sender and intended receiver processes.

> 除了硬件之外，网络接口还可以包括软件或固件来执行所需的操作。即使 [图 F.3](#_bookmark600) 中显示的简单示例也可以调用消息传递软件将请求和回复转换为具有适当标头的消息。这样，用户应用程序无需担心编写和处理消息，因为这些任务可以在较低级别自动执行。应用程序通常与操作系统或运行时系统协作来发送和接收消息。由于网络很可能由每个设备上运行的许多进程共享，因此操作系统不允许一个进程接收到另一个进程的消息。因此，消息传递软件必须包括区分进程的保护机制。这种区别可以通过使用发送方和预期的接收方进程都知道的 "端口" 号扩展标头来实现。

Figure F.4 An example packet format with header, payload, and checksum in the trailer.

In addition to composing and processing messages, additional functions need to be performed by the end nodes to establish communication among the commu- nicating devices. Although hardware support can reduce the amount of work, some can be done by software. For example, most networks specify a maximum amount of information that can be transferred (i.e., `maximum transfer unit`) so that network buffers can be dimensioned appropriately. Messages longer than the maximum transfer unit are divided into smaller units, called `packets` (or `datagrams`), that are transported over the network. Packets are reassembled into messages at the des- tination end node before delivery to the application. Packets belonging to the same message can be distinguished from others by including a `message ID` field in the packet header. If packets arrive out of order at the destination, they are reordered when reassembled into a message. Another field in the packet header containing a `sequence number` is usually used for this purpose.

> 除了编写和处理消息之外，端节点还需要执行其他功能以在通信设备之间建立通信。虽然硬件支持可以减少工作量，但有些可以通过软件来完成。例如，大多数网络都指定了可以传输的最大信息量(即 "最大传输单元" )，以便可以适当地确定网络缓冲区的大小。超过最大传输单元的消息被分成更小的单元，称为 "数据包" (或 "数据报" )，通过网络传输。在交付给应用程序之前，数据包在目标端节点重新组合成消息。属于同一消息的数据包可以通过在数据包标头中包含 "消息 ID" 字段来与其他数据包区分开来。如果数据包乱序到达目的地，它们会在重新组合成消息时重新排序。包头中包含 "序列号" 的另一个字段通常用于此目的。

The sequence of steps the end node follows to commence and complete com- munication over the network is called a `communication protocol`. It generally has symmetric but reversed steps between sending and receiving information. Commu- nication protocols are implemented by a combination of software and hardware to accelerate execution. For instance, many network interface cards implement hard- ware timers as well as hardware support to split messages into packets and reas- semble them, compute the cyclic redundancy check (CRC) `checksum,` handle virtual memory addresses, and so on.

> 端节点开始并完成网络通信所遵循的步骤序列称为 "通信协议" 。它通常在发送和接收信息之间具有对称但相反的步骤。通信协议通过软件和硬件的组合来实现以加速执行。例如，许多网络接口卡实现硬件定时器以及硬件支持，将消息拆分为数据包并重新组合它们，计算循环冗余校验 (CRC) "校验和" ，处理虚拟内存地址等。

Some network interfaces include extra hardware to offload protocol processing from the host computer, such as TCP `offload engines` for LANs and WANs. But, for interconnection networks such as SANs that have low latency requirements, this may not be enough even when lighter-weight communication protocols are used such as message passing interface (MPI). Communication performance can be further improved by bypassing the operating system (OS). OS bypassing can be implemented by directly allocating message buffers in the network interface memory so that applications directly write into and read from those buffers. This avoids extra memory-to-memory copies. The corresponding protocols are referred to as `zero-copy` protocols or `user-level communication` protocols. Protection can still be maintained by calling the OS to allocate those buffers at initialization and preventing unauthorized memory accesses in hardware.

> 一些网络接口包括额外的硬件来从主机卸载协议处理，例如用于 LAN 和 WAN 的 TCP "卸载引擎" 。但是，对于具有低延迟要求的 SAN 等互连网络，即使使用消息传递接口 (MPI) 等轻量级通信协议，这也可能不够。通过绕过操作系统 (OS) 可以进一步提高通信性能。操作系统绕过可以通过直接在网络接口内存中分配消息缓冲区来实现，这样应用程序就可以直接写入和读取这些缓冲区。这避免了额外的内存到内存拷贝。相应的协议被称为 "零拷贝" 协议或 "用户级通信" 协议。仍然可以通过调用操作系统在初始化时分配这些缓冲区并防止在硬件中进行未经授权的内存访问来维持保护。

In general, some or all of the following are the steps needed to send a message at end node devices over a network:

> 通常，以下一些或全部是通过网络在末端节点设备发送消息所需的步骤：

1. The application executes a system call, which copies data to be sent into an operating system or network interface buffer, divides the message into packets (if needed), and composes the header and trailer for packets.

> 1.该应用程序执行系统调用，该系统调用要将要发送到操作系统或网络接口缓冲区的数据将消息划分为数据包(如果需要的话)，并组成数据包的标题和预告片。

2. The checksum is calculated and included in the header or trailer of packets.

> 2.计算校验和包含在数据包的标题或拖车中。

3. The timer is started, and the network interface hardware sends the packets.

> 3.启动计时器，网络接口硬件发送数据包。

Message reception is in the reverse order:

> 消息接收是相反的顺序：

3\. The network interface hardware receives the packets and puts them into its buffer or the operating system buffer.

> 3 \。网络接口硬件接收数据包并将其放入其缓冲区或操作系统缓冲区中。

2\. The checksum is calculated for each packet. If the checksum matches the sender’s checksum, the receiver sends an acknowledgment back to the packet sender. If not, it deletes the packet, assuming that the sender will resend the packet when the associated timer expires.

> 2 \。计算每个数据包的校验和。如果校验和匹配发件人的校验和，则接收器将确认回到数据包发件人。如果没有，它会删除数据包，假设在关联的计时器到期时发件人将重新发送数据包。

1. Once all packets pass the test, the system reassembles the message, copies the data to the user’s address space, and signals the corresponding application.

> 1.所有数据包通过测试后，系统将重新组装消息，将数据复制到用户的地址空间，并向相应的应用程序发出信号。

The sender must still react to packet acknowledgments:

> 发件人仍然必须对数据包确认：

- When the sender gets an acknowledgment, it releases the copy of the corre- sponding packet from the buffer.

> - 当发件人获得确认时，它将从缓冲区中释放相应数据包的副本。

- If the sender reaches the time-out instead of receiving an acknowledgment, it resends the packet and restarts the timer.

> - 如果发件人到达超时而不是收到确认，则将重新发送数据包并重新启动计时器。

Just as a protocol is implemented at network end nodes to support communi- cation, protocols are also used across the network structure at the physical, data link, and network layers responsible primarily for packet transport, flow control, error handling, and other functions described next.

> 就像在网络端节点上实施协议以支持通信一样，在物理，数据链路和网络层的网络结构上也使用协议，主要负责数据包运输，流量控制，错误处理以及其他所描述的功能下一个。

### Basic Network Structure and Functions: Media and Form Factor, Packet Transport, Flow Control, and Error Handling

Once a packet is ready for transmission at its source, it is injected into the network using some dedicated hardware at the network interface. The hardware includes some transceiver circuits to drive the physical network media—either electrical or optical. The type of `media` and `form factor` depends largely on the interconnect distances over which certain signaling rates (e.g., transmission speed) should be sustainable. For centimeter or less distances on a chip or multichip module, typi- cally the middle to upper copper metal layers can be used for interconnects at multi- Gbps signaling rates per line. A dozen or more layers of copper traces or tracks imprinted on circuit boards, midplanes, and backplanes can be used for Gbps differential-pair signaling rates at distances of about a meter or so. Category 5E unshielded twisted-pair copper wiring allows 0.25 Gbps transmission speed over distances of 100 meters. Coaxial copper cables can deliver 10 Mbps over kilometer distances. In these conductor lines, distance can usually be traded off for higher transmission speed, up to a certain point. Optical media enable faster transmission speeds at distances of kilometers. Multimode fiber supports 100 Mbps transmis- sion rates over a few kilometers, and more expensive single-mode fiber supports Gbps transmission speeds over distances of several kilometers. Wavelength divi- sion multiplexing allows several times more bandwidth to be achieved in fiber (i.e., by a factor of the number of wavelengths used).

> 一旦数据包在其源头准备好传输，就会使用网络接口上的一些专用硬件将其注入网络。硬件包括一些收发器电路来驱动物理网络介质——无论是电气的还是光学的。"媒体" 的类型和 "外形因素" 在很大程度上取决于互连距离，在该距离上某些信号速率(例如，传输速度)应该是可持续的。对于芯片或多芯片模块上的厘米或更短距离，通常可以使用中间到上层铜金属层以每条线路多 Gbps 的信号速率进行互连。印在电路板、中间板和背板上的十几层或更多层铜迹线或轨道可用于在大约一米左右的距离内实现 Gbps 差分对信号传输速率。5E 类非屏蔽双绞线铜线允许 0.25 Gbps 的传输速度超过 100 米的距离。同轴铜缆可以在千米距离内提供 10 Mbps 的速度。在这些导线中，距离通常可以在一定程度上换取更高的传输速度。光学介质可在千米距离内实现更快的传输速度。多模光纤支持数公里范围内的 100 Mbps 传输速率，而更昂贵的单模光纤支持数公里范围内的 Gbps 传输速率。波分复用允许在光纤中实现多几倍的带宽(即，使用的波长数量的一个因子)。

The hardware used to drive network links may also include some encoders to encode the signal in a format other than binary that is suitable for the given trans- port distance. Encoding techniques can use multiple voltage levels, redundancy, data and control rotation (e.g., 4b5b encoding), and/or a guaranteed minimum number of signal transitions per unit time to allow for clock recovery at the receiver. The signal is decoded at the receiver end, and the packet is stored in the corresponding buffer. All of these operations are performed at the network physical layer, the details of which are beyond the scope of this appendix. Fortu- nately, we do not need to worry about them. From the perspective of the data link and higher layers, the physical layer can be viewed as a long linear pipeline without staging in which signals propagate as waves through the network transmission medium. All of the above functions are generally referred to as `packet transport`. Besides packet transport, the network hardware and software are jointly responsible at the data link and network protocol layers for ensuring reliable delivery of packets. These responsibilities include: (1) preventing the sender from sending packets at a faster rate than they can be processed by the receiver, and (2) ensuring that the packet is neither garbled nor lost in transit. The first responsibility is met by either discarding packets at the receiver when its buffer is full and later notifying the sender to retransmit them, or by notifying the sender to stop sending packets when the buffer becomes full and to resume later once it has room for more packets. The latter strategy is generally known as `flow control`.

> 用于驱动网络链路的硬件还可能包括一些编码器，以适合给定传输距离的非二进制格式对信号进行编码。编码技术可以使用多个电压电平、冗余、数据和控制旋转(例如 4b5b 编码)和/或每单位时间保证的最小信号转换次数，以允许在接收器处恢复时钟。信号在接收端解码，数据包存储在相应的缓冲区中。所有这些操作都在网络物理层执行，其详细信息超出了本附录的范围。幸运的是，我们不需要担心它们。从数据链路和更高层的角度来看，物理层可以看作是一条没有分段的长线性管道，信号在网络传输介质中以波的形式传播。以上所有功能统称为 "数据包传输" 。除了数据包传输之外，网络硬件和软件还共同负责数据链路和网络协议层，以确保数据包的可靠传送。这些职责包括：(1) 防止发送方以比接收方处理速度更快的速度发送数据包，以及 (2) 确保数据包在传输过程中既不会出现乱码也不会丢失。第一个责任是在接收方的缓冲区已满时丢弃数据包，然后通知发送方重新传输数据包，或者通知发送方在缓冲区变满时停止发送数据包，并在有空间容纳更多数据包后继续发送数据包。数据包。后一种策略通常称为 "流量控制" 。

There are several interesting techniques commonly used to implement flow control beyond simple `handshaking` between the sender and receiver. The more popular techniques are `Xon/Xoff` (also referred to as `Stop & Go`) and `credit-based` flow control. Xon/Xoff consists of the receiver notifying the sender either to stop or to resume sending packets once high and low buffer occupancy levels are reached, respectively, with some hysteresis to reduce the number of notifications. Notifica- tions are sent as "stop" and "go" signals using additional control wires or encoded in control packets. Credit-based flow control typically uses a credit counter at the sender that initially contains a number of credits equal to the number of buffers at the receiver. Every time a packet is transmitted, the sender decrements the credit counter. When the receiver consumes a packet from its buffer, it returns a credit to the sender in the form of a control packet that notifies the sender to increment its counter upon receipt of the credit. These techniques essentially control the flow of packets into the network by `throttling` packet injection at the sender when the receiver reaches a low watermark or when the sender runs out of credits.

> 除了发送方和接收方之间简单的 "握手" 之外，还有一些有趣的技术常用于实现流量控制。更流行的技术是 "Xon/Xoff" (也称为 "Stop & Go" )和 "基于信用的" 流量控制。Xon/Xoff 由接收方通知发送方在分别达到高和低缓冲区占用水平时停止或恢复发送数据包组成，并带有一些滞后以减少通知数量。通知使用额外的控制线或编码在控制数据包中作为 "停止" 和 "继续" 信号发送。基于信用的流量控制通常在发送方使用信用计数器，该计数器最初包含的信用数等于接收方缓冲区的数量。每次传输数据包时，发送方都会减少信用计数器。当接收方使用其缓冲区中的数据包时，它会以控制数据包的形式向发送方返回信用，通知发送方在收到信用后增加其计数器。当接收方达到低水位线或发送方用完信用时，这些技术实质上通过在发送方 "节流" 数据包注入来控制数据包流入网络。

Xon/Xoff usually generates much less control traffic than credit-based flow control because notifications are only sent when the high or low buffer occupancy levels are crossed. On the other hand, credit-based flow control requires less than half the buffer size required by Xon/Xoff. Buffers for Xon/Xoff must be large enough to prevent overflow before the "stop" control signal reaches the sender. Overflow cannot happen when using credit-based flow control because the sender will run out of credits, thus stopping transmission. For both schemes, full link bandwidth utilization is possible only if buffers are large enough for the distance over which communication takes place.

> Xon/Xoff 通常生成比基于信用的流量控制少得多的控制流量，因为只有在超过高或低缓冲区占用水平时才会发送通知。另一方面，基于信用的流量控制需要的缓冲区大小不到 Xon/Xoff 所需缓冲区大小的一半。Xon/Xoff 的缓冲区必须足够大，以防止在 "停止" 控制信号到达发送器之前发生溢出。使用基于信用的流量控制时不会发生溢出，因为发送方将用完信用，从而停止传输。对于这两种方案，仅当缓冲区对于发生通信的距离而言足够大时，才可能充分利用链路带宽。

Let’s compare the buffering requirements of the two flow control techniques in a simple example covering the various interconnection network domains.

> 让我们在一个涵盖各种互连网络域的简单示例中比较两种流控制技术的缓冲要求。

Example Suppose we have a dedicated-link network with a raw data bandwidth of 8 Gbps for each link in each direction interconnecting two devices. Packets of 100 bytes (including the header) are continuously transmitted from one device to the other to fully utilize network bandwidth. What is the minimum amount of credits and buffer space required by credit-based flow control assuming interconnect distances of 1 cm, 1 m, 100 m, and 10 km if only link propagation delay is taken into account? How does the minimum buffer space compare against Xon/Xoff?

> 示例 假设我们有一个专用链路网络，每个方向上的每个链路的原始数据带宽为 8 Gbps，互连两个设备。100 字节的数据包(包括标头)从一台设备连续传输到另一台设备，以充分利用网络带宽。如果仅考虑链路传播延迟，假定互连距离为 1 cm、1 m、100 m 和 10 km，基于信用的流量控制所需的最小信用量和缓冲空间是多少？ 最小缓冲区空间与 Xon/Xoff 相比如何？

_Answer_ At the start, the receiver buffer is initially empty and the sender contains a number of credits equal to buffer capacity. The sender will consume a credit every time a packet is transmitted. For the sender to continue transmitting packets at network speed, the first returned credit must reach the sender before the sender runs out of credits. After receiving the first credit, the sender will keep receiving credits at the same rate it transmits packets. As we are considering only propagation delay over the link and no other sources of delay or overhead, null processing time at the sender and receiver are assumed. The time required for the first credit to reach the sender since it started transmission of the first packet is equal to the round-trip propagation delay for the packet transmitted to the receiver and the return credit transmitted back to the sender. This time must be less than or equal to the packet transmission time multiplied by the initial credit count:

> _Answer_ 开始时，接收方缓冲区最初是空的，发送方包含的信用点数等于缓冲区容量。每传输一个数据包，发送方都会消耗一个信用点。为了使发送方继续以网络速度传输数据包，第一个返回的信用必须在发送方用完信用之前到达发送方。收到第一个信用后，发送方将继续以与传输数据包相同的速率接收信用。由于我们只考虑链路上的传播延迟而不考虑其他延迟或开销来源，因此假定发送方和接收方的处理时间为零。自发送方开始传输第一个数据包以来，第一个信用到达发送方所需的时间等于传输到接收方的数据包和返回发送方的返回信用的往返传播延迟。此时间必须小于或等于数据包传输时间乘以初始信用计数：

As each credit represents one packet-sized buffer entry, the minimum amount of credits (and, likewise, buffer space) needed by each device is one for the 1 cm and 1 m distances, 10 for the 100 m distance, and 1000 packets for the 10 km distance. For Xon/Xoff, this minimum buffer size corresponds to the buffer fragment from the high occupancy level to the top of the buffer and from the low occupancy level to the bottom of the buffer. With the added hysteresis between both occupancy levels to reduce notifications, the minimum buffer space for Xon/Xoff turns out to be more than twice that for credit-based flow control.

> 由于每个信用代表一个数据包大小的缓冲区条目，因此每个设备所需的最小信用量(以及同样的缓冲区空间)对于 1 厘米和 1 米距离为 1，对于 100 米距离为 10，对于 1000 数据包 10 公里的距离。对于 Xon/Xoff，此最小缓冲区大小对应于从高占用级别到缓冲区顶部以及从低占用级别到缓冲区底部的缓冲区片段。通过在两个占用级别之间增加滞后以减少通知，Xon/Xoff 的最小缓冲区空间结果是基于信用的流量控制的两倍多。

Networks that implement flow control do not need to drop packets and are sometimes referred to as `lossless` networks; networks that drop packets are some- times referred to as `lossy` networks. This single difference in the way packets are handled by the network drastically constrains the kinds of solutions that can be implemented to address other related network problems, including packet routing, congestion, deadlock, and reliability, as we will see later in this appendix. This difference also affects performance significantly as dropped packets need to be retransmitted, thus consuming more link bandwidth and suffering extra delay. These behavioral and performance differences ultimately restrict the interconnec- tion network domains for which certain solutions are applicable. For instance, most networks delivering packets over relatively short distances (e.g., OCNs and SANs) tend to implement flow control; on the other hand, networks delivering packets over relatively long distances (e.g., LANs and WANs) tend to be designed to drop packets. For the shorter distances, the delay in propagating flow control informa- tion back to the sender can be negligible, but not so for longer distance scales. The kinds of applications that are usually run also influence the choice of lossless ver- sus lossy networks. For instance, dropping packets sent by an Internet client like a Web browser affects only the delay observed by the corresponding user. However, dropping a packet sent by a process from a parallel application may lead to a sig- nificant increase in the overall execution time of the application if that packet’s delay is on the critical path.

> 实现流量控制的网络不需要丢弃数据包，有时被称为 "无损" 网络；丢弃数据包的网络有时被称为 "有损" 网络。网络处理数据包方式的这种单一差异极大地限制了可以实施以解决其他相关网络问题的解决方案的种类，包括数据包路由、拥塞、死锁和可靠性，我们将在本附录后面看到。这种差异也会显着影响性能，因为丢弃的数据包需要重新传输，从而消耗更多的链路带宽并遭受额外的延迟。这些行为和性能差异最终限制了某些解决方案适用的互连网络域。例如，大多数在相对较短的距离内传输数据包的网络(例如 OCN 和 SAN)倾向于实施流量控制；另一方面，在相对较长的距离上传输数据包的网络(例如 LAN 和 WAN)往往被设计为丢弃数据包。对于较短的距离，将流量控制信息传播回发送方的延迟可以忽略不计，但对于较长的距离尺度则不然。通常运行的应用程序类型也会影响无损网络和有损网络的选择。例如，丢弃由 Internet 客户端(如 Web 浏览器)发送的数据包只会影响相应用户观察到的延迟。但是，如果该数据包的延迟位于关键路径上，则丢弃来自并行应用程序的进程发送的数据包可能会导致应用程序的整体执行时间显着增加。

The second responsibility of ensuring that packets are neither garbled nor lost in transit can be met by implementing some mechanisms to detect and recover from transport errors. Adding a checksum or some other error detection field to the packet format, as shown in [Figure F.4](#_bookmark601), allows the receiver to detect errors. This redundant information is calculated when the packet is sent and checked upon receipt. The receiver then sends an acknowledgment in the form of a control packet if the packet passes the test. Note that this acknowledgment control packet may simultaneously contain flow control information (e.g., a credit or stop signal), thus reducing control packet overhead. As described earlier, the most common way to recover from errors is to have a timer record the time each packet is sent and to presume the packet is lost or erroneously transported if the timer expires before an acknowledgment arrives. The packet is then resent.

> 通过实施某些机制来检测和从运输错误中恢复，可以确保既不弄乱或丢失的数据包的第二个责任。如[图 F.4](#_bookmark601) 所示，将校验和一些错误检测字段添加到数据包格式中，允许接收器检测错误。当收据时发送并检查数据包时，将计算此冗余信息。然后，如果数据包通过测试，则接收器以控制数据包的形式发送确认。请注意，此确认控制数据包可以同时包含流控制信息(例如，信用或停止信号)，从而减少了控制数据包的开销。如前所述，从错误中恢复的最常见方法是将一个计时器记录发送到每个数据包发送的时间，并假定如果计时器在确认之前到期，则将数据包丢失或错误地运输。然后，数据包是不满的。

The communication protocol across the network and network end nodes must handle many more issues other than packet transport, flow control, and reliability. For example, if two devices are from different manufacturers, they might order bytes differently within a word (Big Endian versus Little Endian byte ordering). The protocol must reverse the order of bytes in each word as part of the delivery system. It must also guard against the possibility of duplicate packets if a delayed packet were to become unstuck. Depending on the system requirements, the pro- tocol may have to implement `pipelining` among operations to improve perfor- mance. Finally, the protocol may need to handle network congestion to prevent performance degradation when more than two devices are connected, as described later in [Section F.7](#practical-issues-for-commercial-interconnection-networks).

> 除数据包传输，流控制和可靠性外，整个网络和网络端节点上的通信协议必须处理更多问题。例如，如果两个设备来自不同的制造商，则它们可能会在一个单词中以不同的方式订购字节(Big Endian 和 Little Endian Byte Ordering)。该协议必须作为交付系统的一部分扭转每个单词中字节的顺序。如果延迟的数据包被拆除，它还必须防止重复包装的可能性。根据系统的要求，预托可能必须在操作中实现\_PIPELINGIND，以改善穿孔。最后，该协议可能需要处理网络拥塞，以防止连接两个以上的设备时性能降解，如[F.7 节]后面所述(＃实用 - 商业交流连接网络网络)中所述。

### Characterizing Performance: Latency and Effective Bandwidth

Now that we have covered the basic steps for sending and receiving messages between two devices, we can discuss performance. We start by discussing the latency when transporting a single packet. Then we discuss the effective bandwidth (also known as throughput) that can be achieved when the transmission of multiple packets is pipelined over the network at the packet level.

> 现在，我们已经涵盖了两个设备之间发送和接收消息的基本步骤，我们可以讨论性能。我们首先讨论运输单个数据包时的延迟。然后，我们讨论有效的带宽(也称为吞吐量)，当在数据包级别上通过网络管道输送多个数据包的传输可以实现。

[Figure F.5](#_bookmark602) shows the basic components of latency for a single packet. Note that some latency components will be broken down further in later sections as the inter- nals of the "black box" network are revealed. The timing parameters in [Figure F.5](#_bookmark602) apply to many interconnection network domains: inside a chip, between chips on a board, between boards in a chassis, between chassis within a computer, between computers in a cluster, between clusters, and so on. The values may change, but the components of latency remain the same.

> [图 F.5](#_bookmark602) 显示了单个数据包的延迟的基本组成部分。请注意，由于揭示了 "黑匣子" 网络的间隔，因此某些延迟组件将进一步分解。[图 F.5](#_bookmark602) 中的定时参数适用于许多互连网络域：在芯片内部，木板上的芯片之间，底盘之间的木板之间，底盘之间的底盘，底盘之间，计算机之间的计算机之间的计算机之间的计算机，之间，之间集群，等等。值可能会改变，但是延迟的组成部分保持不变。

The following terms are often used loosely, leading to confusion, so we define them here more precisely:

> 以下术语通常被松散地使用，导致混乱，因此我们在这里更精确地定义它们：

- `Bandwidth`—Strictly speaking, the `bandwidth` of a transmission medium refers to the range of frequencies for which the attenuation per unit length introduced by that medium is below a certain threshold. It must be distinguished from the `transmission speed`, which is the amount of information transmitted over a medium per unit time. For example, modems successfully increased transmis- sion speed in the late 1990s for a fixed bandwidth (i.e., the 3 KHz bandwidth provided by voice channels over telephone lines) by encoding more voltage levels and, hence, more bits per signal cycle. However, to be consistent with its more widely understood meaning, we use the term `band-width` to refer to the maximum rate at which information can be transferred, where information includes packet header, payload, and trailer. The units are traditionally bits per second, although bytes per second is sometimes used. The term `bandwidth` is also used to mean the measured speed of the medium (i.e., network links). `Aggregate bandwidth` refers to the total data bandwidth supplied by the net- work, and `effective bandwidth` or `throughput` is the fraction of aggregate band- width delivered by the network to an application.

> - ‘带宽’——严格来说，传输介质的‘带宽’是指该介质引入的单位长度衰减低于某个阈值的频率范围。它必须与 "传输速度" 区分开来，传输速度是单位时间内通过介质传输的信息量。例如，调制解调器在 20 世纪 90 年代后期通过编码更多的电压电平并因此在每个信号周期中增加比特数，成功地提高了固定带宽(即电话线上语音通道提供的 3 KHz 带宽)的传输速度。然而，为了与其更广泛理解的含义保持一致，我们使用术语 "带宽" 来指代信息可以传输的最大速率，其中信息包括包头、有效载荷和尾部。单位传统上是比特每秒，尽管有时使用字节每秒。术语 "带宽" 也用于表示媒体(即网络链接)的测量速度。"聚合带宽" 是指网络提供的总数据带宽，而 "有效带宽" 或 "吞吐量" 是网络提供给应用程序的聚合带宽的一部分。

Figure F.5 Components of packet latency. Depending on whether it is an OCN, SAN, LAN, or WAN, the relative amounts of sending and receiving overhead, time of flight, and transmission time are usually quite different from those illustrated here.

- `Time of flight`—This is the time for the first bit of the packet to arrive at the receiver, including the propagation delay over the links and delays due to other hardware in the network such as link repeaters and network switches. The unit of measure for time of flight can be in milliseconds for WANs, microseconds for LANs, nanoseconds for SANs, and picoseconds for OCNs.

> - "飞行时间" ——这是数据包的第一位到达接收器的时间，包括链路上的传播延迟和网络中其他硬件(如链路中继器和网络交换机)引起的延迟。飞行时间的度量单位对于 WAN 可以是毫秒，对于 LAN 可以是微秒，对于 SAN 可以是纳秒，对于 OCN 可以是皮秒。

- `Transmission time`—This is the time for the packet to pass through the network, not including time of flight. One way to measure it is the difference in time between when the first bit of the packet arrives at the receiver and when the last bit of that packet arrives at the receiver. By definition, transmission time is equal to the size of the packet divided by the data bandwidth of network links. This measure assumes there are no other packets contending for that bandwidth (i.e., a zero-load or no-load network).

> - `Transmission time`——这是数据包通过网络的时间，不包括飞行时间。测量它的一种方法是数据包的第一位到达接收器和该数据包的最后一位到达接收器之间的时间差。根据定义，传输时间等于数据包的大小除以网络链路的数据带宽。该措施假设没有其他数据包争用该带宽(即零负载或无负载网络)。

- `Transport latency`—This is the sum of time of flight and transmission time. Transport latency is the time that the packet spends in the interconnection net- work. Stated alternatively, it is the time between when the first bit of the packet is injected into the network and when the last bit of that packet arrives at the receiver. It does not include the overhead of preparing the packet at the sender or processing it when it arrives at the receiver.

> - `Transport latency`—这是飞行时间和传输时间的总和。传输延迟是数据包在互连网络中花费的时间。换句话说，它是从数据包的第一位注入网络到该数据包的最后一位到达接收方之间的时间。它不包括在发送方准备数据包或在数据包到达接收方时对其进行处理的开销。

- `Sending overhead`—This is the time for the end node to prepare the packet (as opposed to the message) for injection into the network, including both hard- ware and software components. Note that the end node is busy for the entire time, hence the use of the term `overhead`. Once the end node is free, any sub- sequent delays are considered part of the transport latency. We assume that overhead consists of a constant term plus a variable term that depends on packet size. The constant term includes memory allocation, packet header preparation, setting up DMA devices, and so on. The variable term is mostly due to copies from buffer to buffer and is usually negligible for very short packets.

> - `发送开销`——这是端节点准备数据包(而不是消息)以注入网络的时间，包括硬件和软件组件。请注意，端节点一直处于忙碌状态，因此使用术语 "开销" 。一旦终端节点空闲，任何后续延迟都被视为传输延迟的一部分。我们假设开销由一个常数项加上一个取决于数据包大小的变量项组成。常数项包括内存分配、包头准备、设置 DMA 设备等。可变项主要是由于从缓冲区到缓冲区的复制，对于非常短的数据包通常可以忽略不计。

- `Receiving overhead`—This is the time for the end node to process an incoming packet, including both hardware and software components. We also assume here that overhead consists of a constant term plus a variable term that depends on packet size. In general, the receiving overhead is larger than the sending overhead. For example, the receiver may pay the cost of an interrupt or may have to reorder and reassemble packets into messages.

> - `接收开销`——这是端节点处理传入数据包的时间，包括硬件和软件组件。我们还假设开销由一个常数项加上一个取决于数据包大小的变量项组成。一般来说，接收开销大于发送开销。例如，接收方可能需要支付中断费用，或者可能必须将数据包重新排序并重新组合成消息。

The total latency of a packet can be expressed algebraically by the following:

> 数据包的总延迟可以通过以下代数表示：

Let’s see how the various components of transport latency and the sending and receiving overheads change in importance as we go across the interconnection network domains: from OCNs to SANs to LANs to WANs.

> 让我们看看当我们跨越互连网络域时，运输延迟的各个组成部分以及发送和接收开销的重要性如何变化：从 OCN 到 SANS 到 SANS，再到 LAN 到 LAN 到 WANS。

Example Assume that we have a dedicated link network with a data bandwidth of 8 Gbps for each link in each direction interconnecting two devices within an OCN, SAN, LAN, or WAN, and we wish to transmit packets of 100 bytes (including the header) between the devices. The end nodes have a per-packet sending overhead of `x` + 0.05 ns/byte and receiving overhead of 4/3(_x_) + 0.05 ns/byte, where `x` is 0 μs for the OCN, 0.3 μs for the SAN, 3 μs for the LAN, and 30 μs for the WAN, which are typical for these network types. Calculate the total latency to send packets from one device to the other for interconnection distances of 0.5 cm, 5 m, 5000 m, and 5000 km assuming that time of flight consists only of link propagation delay (i.e., no switching or other sources of delay).

> 示例假设我们有一个专用链路网络，每个方向上每个链路的数据带宽为 8 Gbps，互连 OCN、SAN、LAN 或 WAN 中的两个设备，我们希望传输 100 字节的数据包(包括标头) 设备之间。端节点的每个数据包发送开销为 `x` + 0.05 ns/字节，接收开销为 4/3(_x_) + 0.05 ns/字节，其中 `x` 对于 OCN 为 0 μs，对于 OCN 为 0.3 μs SAN，LAN 为 3 μs，WAN 为 30 μs，这是这些网络类型的典型值。对于 0.5 cm、5 m、5000 m 和 5000 km 的互连距离，计算从一台设备向另一台设备发送数据包的总延迟，假设飞行时间仅包含链路传播延迟(即没有切换或其他延迟源) ).

_Answer_ Using the above expression and the calculation for propagation delay through a conductor given in the previous example, we can plug in the parameters for each of the networks to find their total packet latency. For the OCN:

> `answer` 使用上述表达式和通过上一个示例中给出的导体进行传播延迟的计算，我们可以插入每个网络的参数，以找到其总数据包延迟。对于 OCN：

The increased fraction of the latency required by time of flight for the longer distances along with the greater likelihood of errors over the longer distances are among thereasons why WANs and LANs usemore sophisticated and time-consuming communication protocols, which increase sending and receiving overheads. The need for standardization is another reason. Complexity also increases due to the require- ments imposed on the protocol by the typical applications that run over the various interconnectionnetwork domains aswe go from tens to hundreds to thousandsto many thousands of devices. We will consider this in later sections when we discuss connect- ing more than two devices. The above example shows that the propagation delay com- ponent of time of flight for WANs and some LANs is so long that other latency components—including the sending and receiving overheads—can practically be ignored. This is not so for SANs and OCNs where the propagation delay pales in com- parison to the overheads and transmission delay. Remember that time-of-flight latency due to switches and other hardware in the network besides sheer propagation delay through the links is neglected in the above example. For noncongested networks, switch latency generally is small compared to the overheads and propagation delay through the links in WANs and LANs, but this is not necessarily so for multiprocessor SANs and multicore OCNs, as we will see in later sections.

> 更长距离的飞行时间所需的延迟部分增加，以及更长距离上出错的可能性更大，是 WAN 和 LAN 使用更复杂和耗时的通信协议的原因之一，这增加了发送和接收开销。标准化的需要是另一个原因。当我们从数十个到数百个到数千个到数千个设备时，运行在各种互连网络域上的典型应用程序对协议施加的要求也增加了复杂性。当我们讨论连接两个以上的设备时，我们将在后面的章节中考虑这一点。上面的示例表明，WAN 和某些 LAN 的飞行时间的传播延迟分量非常长，以至于其他延迟分量(包括发送和接收开销)实际上可以忽略不计。对于 SAN 和 OCN，情况并非如此，与开销和传输延迟相比，传播延迟显得微不足道。请记住，除了通过链路的纯粹传播延迟之外，由于网络中的交换机和其他硬件引起的飞行时间延迟在上面的示例中被忽略了。对于非拥塞网络，与通过 WAN 和 LAN 中的链路的开销和传播延迟相比，交换机延迟通常较小，但对于多处理器 SAN 和多核 OCN 而言并非如此，我们将在后面的部分中看到。

So far, we have considered the transport of a single packet and computed the associated end-to-end total packet latency. In order to compute the effective band- width for two networked devices, we have to consider a continuous stream of packets transported between them. We must keep in mind that, in addition to min- imizing packet latency, the goal of any network optimized for a given cost and power consumption target is to transfer the maximum amount of available infor- mation in the shortest possible time, as measured by the effective bandwidth deliv- ered by the network. For applications that do not require a response before sending the next packet, the sender can overlap the sending overhead of later packets with the transport latency and receiver overhead of prior packets. This essentially pipe- lines the transmission of packets over the network, also known as `link pipelining`. Fortunately, as discussed in prior chapters of this book, there are many application areas where communication from either several applications or several threads from the same application can run concurrently (e.g., a Web server concurrently serving thousands of client requests or streaming media), thus allowing a device to send a stream of packets without having to wait for an acknowledgment or a reply. Also, as long messages are usually divided into packets of maximum size before transport, a number of packets are injected into the network in succession for such cases. If such overlap were not possible, packets would have to wait for prior packets to be acknowledged before being transmitted and thus suffer signif- icant performance degradation.

> 到目前为止，我们已经考虑了单个数据包的传输并计算了相关的端到端总数据包延迟。为了计算两个联网设备的有效带宽，我们必须考虑在它们之间传输的连续数据包流。我们必须牢记，除了最小化数据包延迟之外，任何针对给定成本和功耗目标优化的网络的目标是在尽可能短的时间内传输最大数量的可用信息，如通过 网络提供的有效带宽。对于在发送下一个数据包之前不需要响应的应用程序，发送方可以将后续数据包的发送开销与先前数据包的传输延迟和接收方开销重叠。这本质上是通过网络传输数据包，也称为 "链路流水线" 。幸运的是，正如本书前面章节所讨论的，在许多应用领域中，来自多个应用程序或来自同一应用程序的多个线程的通信可以并发运行(例如，Web 服务器同时服务数千个客户端请求或流媒体)，因此 允许设备发送数据包流而无需等待确认或回复。此外，由于长消息在传输之前通常被分成最大大小的数据包，因此在这种情况下，许多数据包会被连续注入网络。如果这种重叠是不可能的，数据包将不得不等待先前的数据包在传输之前得到确认，从而遭受显着的性能下降。

Packets transported in a pipelined fashion can be acknowledged quite straight- forwardly simply by keeping a copy at the source of all unacknowledged packets that have been sent and keeping track of the correspondence between returned acknowledgments and packets stored in the buffer. Packets will be removed from the buffer when the corresponding acknowledgment is received by the sender. This can be done by including the message ID and packet sequence number associated with the packet in the packet’s acknowledgment. Furthermore, a separate timer must be associated with each buffered packet, allowing the packet to be resent if the associated time-out expires.

> 以流水线方式传输的数据包可以非常直接地得到确认，只需在源头保留一份所有已发送的未确认数据包的副本，并跟踪返回的确认和存储在缓冲区中的数据包之间的对应关系。当发送方收到相应的确认后，数据包将从缓冲区中删除。这可以通过在数据包的确认中包含与数据包关联的消息 ID 和数据包序列号来完成。此外，一个单独的计时器必须与每个缓冲的数据包相关联，允许在相关超时到期时重新发送数据包。

Pipelining packet transport over the network has many similarities with pipe- lining computation within a processor. However, among some differences are that it does not require any staging latches. Information is simply propagated through network links as a sequence of signal waves. Thus, the network can be considered as a logical pipeline consisting of as many stages as are required so that the time of flight does not affect the effective bandwidth that can be achieved. Transmission of a packet can start immediately after the transmission of the previous one, thus over- lapping the sending overhead of a packet with the transport and receiver latency of previous packets. If the sending overhead is smaller than the transmission time, packets follow each other back-to-back, and the effective bandwidth approaches the raw link bandwidth when continuously transmitting packets. On the other hand, if the sending overhead is greater than the transmission time, the effective band- width at the injection point will remain well below the raw link bandwidth. The resulting `link injection bandwidth`, BW<sub>LinkInjection</sub>, for each link injecting a contin- uous stream of packets into a network is calculated with the following expression:

> 网络上的流水线数据包传输与处理器内的流水线计算有许多相似之处。然而，一些不同之处在于它不需要任何暂存锁存器。信息只是作为信号波序列通过网络链接传播。因此，网络可以被视为由所需级数组成的逻辑管道，因此飞行时间不会影响可实现的有效带宽。数据包的传输可以在前一个数据包传输之后立即开始，从而将数据包的发送开销与之前数据包的传输和接收延迟重叠。如果发送开销小于传输时间，则数据包一个接一个地紧随其后，连续发送数据包时有效带宽接近原始链路带宽。另一方面，如果发送开销大于传输时间，则注入点的有效带宽将保持远低于原始链路带宽。生成的 "链路注入带宽" BW<sub>LinkInjection</sub>，对于将连续数据包流注入网络的每个链路，使用以下表达式计算：

We must also consider what happens if the receiver is unable to consume packets at the same rate they arrive. This occurs if the receiving overhead is greater than the sending overhead and the receiver cannot process incoming packets fast enough. In this case, the `link reception bandwidth`, BW<sub>LinkReception</sub>, for each reception link of the network is less than the link injection bandwidth and is obtained with this expression:

> 我们还必须考虑如果接收方无法以相同的速率接收数据包，会发生什么情况。如果接收开销大于发送开销并且接收方不能足够快地处理传入的数据包，则会发生这种情况。在这种情况下，网络每个接收链路的 "链路接收带宽" BW<sub>LinkReception</sub> 小于链路注入带宽，并通过以下表达式获得：

When communication takes place between two devices interconnected by ded- icated links, all the packets sent by one device will be received by the other. If the receiver cannot process packets fast enough, the receiver buffer will become full, and flow control will throttle transmission at the sender. As this situation is produced by causes external to the network, we will not consider it further here. Moreover, if the receiving overhead is greater than the sending overhead, the receiver buffer will fill up and flow control will, likewise, throttle transmission at the sender. In this case, the effect of flow control is, on average, the same as if we replace sending overhead with receiving overhead. Assuming an ideal network that behaves like two dedi- cated links running in opposite directions at the full link bandwidth between the two devices—which is consistent with our black box view of the network to this point—the resulting effective bandwidth is the smaller of twice the injection band- width (to account for the two injection links, one for each device) or twice the recep- tion bandwidth. This results in the following expression for effective bandwidth:

> 当通过专用链路互连的两个设备之间发生通信时，一个设备发送的所有数据包都将被另一个设备接收。如果接收方不能足够快地处理数据包，接收方缓冲区将变满，流量控制将限制发送方的传输。由于这种情况是网络外部原因造成的，这里不再赘述。此外，如果接收开销大于发送开销，接收缓冲区将填满，流量控制同样会限制发送端的传输。在这种情况下，流量控制的效果平均而言与我们将发送开销替换为接收开销相同。假设一个理想网络的行为类似于两个专用链路以两个设备之间的全链路带宽在相反的方向运行——这与我们目前的网络黑盒视图一致——得到的有效带宽是两倍中的较小者 注入带宽(考虑到两个注入链路，每个设备一个)或接收带宽的两倍。这导致以下有效带宽表达式：

where Overhead max(Sending overhead, Receiving overhead). Taking into account the expression for the transmission time, it is obvious that the effective bandwidth delivered by the network is identical to the aggregate network band- width when the transmission time is greater than the overhead. Therefore, full network utilization is achieved regardless of the value for the time of flight and, thus, regardless of the distance traveled by packets, assuming ideal network behavior (i.e., enough credits and buffers are provided for credit-based and Xon/ Xoff flow control). This analysis assumes that the sender and receiver network interfaces can process only one packet at a time. If multiple packets can be pro- cessed in parallel (e.g., as is done in IBM’s Federation network interfaces), the overheads for those packets can be overlapped, which increases effective band- width by that overlap factor up to the amount bounded by the transmission time. Let’s use the equation on page F-17 to explore the impact of packet size, trans- mission time, and overhead on BW<sub>Link</sub> <sub>Injection</sub>, BW<sub>LinkReception</sub>, and effective bandwidth for the various network domains: OCNs, SANs, LANs, and WANs.

> 顶部最大的位置(发送开销，接收开销)。考虑到传输时间的表达式，很明显，当传输时间大于开销时，网络传递的有效带宽与聚集网络带宽相同。因此，无论飞行时间的价值如何，都可以实现完整的网络利用，因此，无论数据包的距离如何控制)。该分析假设发件人和接收器网络界面一次只能处理一个数据包。如果可以并行处理多个数据包(例如，就像在 IBM 的联邦网络界面中所做的那样)，则可以重叠这些数据包的开销，从而将有效的频段宽度增加到该重叠因子，最高为传输限制的量时间。让我们使用第 F-17 页的方程式来探索数据包大小，传输时间和开销对 BW <ub> link </sub> <sub> indection </sub>，bw <ub> linkRection </子 > 和各种网络领域的有效带宽：OCN，SANS，LAN 和 WANS。

Example As in the previous example, assume we have a dedicated link network with a data bandwidth of 8 Gbps for each link in each direction interconnecting the two devices within an OCN, SAN, LAN, or WAN. Plot effective bandwidth versus packet size for each type of network for packets ranging in size from 4 bytes (i.e., a single 32-bit word) to 1500 bytes (i.e., the maximum transfer unit for Ether- net), assuming that end nodes have the same per-packet sending and receiving overheads as before: `x` + 0.05 ns/byte and 4/3(_x_) + 0.05 ns/byte, respectively, where `x` is 0 μs for the OCN, 0.3 μs for the SAN, 3 μs for the LAN, and 30 μs for the WAN. What limits the effective bandwidth, and for what packet sizes is the effec- tive bandwidth within 10% of the aggregate network bandwidth?

> 示例 与前面的示例一样，假设我们有一个专用链路网络，每个方向上的每个链路的数据带宽为 8 Gbps，将 OCN、SAN、LAN 或 WAN 中的两个设备互连。假设终端节点具有 4 字节(即单个 32 位字)到 1500 字节(即以太网的最大传输单元)的数据包大小，针对每种类型的网络绘制有效带宽与数据包大小的关系图 与之前相同的每个数据包发送和接收开销：分别为 `x` + 0.05 ns/byte 和 4/3(_x_) + 0.05 ns/byte，其中 `x` 对于 OCN 为 0 μs，对于 OCN 为 0.3 μs SAN，LAN 3 μs，WAN 30 μs。什么限制了有效带宽，对于什么数据包大小，有效带宽在总网络带宽的 10% 以内？

Figure F.6 Effective bandwidth versus packet size plotted in semi-log form for the four network domains. Overhead can be amortized by increasing the packet size, but for too large of an overhead (e.g., for WANs and some LANs) scaling the packet size is of little help. Other considerations come into play that limit the maximum packet size.

_Answer_ [Figure F.6](#_bookmark603) plots effective bandwidth versus packet size for the four network domains using the simple equation and parameters given above. For all packet sizes in the OCN, transmission time is greater than overhead (sending or receiv- ing), allowing full utilization of the aggregate bandwidth, which is 16 Gbps—that is, injection link (alternatively, reception link) bandwidth times two to account for both devices. For the SAN, overhead—specifically, receiving overhead—is larger than transmission time for packets less than about 800 bytes; consequently, packets of 655 bytes and larger are needed to utilize 90% or more of the aggregate band- width. For LANs and WANs, most of the link bandwidth is not utilized since over- head in this example is many times larger than transmission time for all packet sizes.

> `answer` [图 F.6](#_bookmark603) 使用上面给出的简单方程式和参数来绘制四个网络域的有效带宽与四个网络域的数据包大小。对于 OCN 中的所有数据包尺寸，传输时间大于开销(发送或收到)，允许完全利用聚集体带宽(即 16 Gbps)，也就是说，注射链接(替代，接收链接)带宽乘以 2 次到 2 次。解释两个设备。对于 SAN 的开销(特别是接收开销)，小于传输时间小于大约 800 个字节的传输时间；因此，需要 655 个字节和较大的数据包来利用 90％或更多的聚集体宽度。对于 LAN 和 WAN，大多数链路带宽都没有使用，因为在此示例中，对于所有数据包尺寸的尺寸比传输时间大很多倍。

This example highlights the importance of reducing the sending and receiving overheads relative to packet transmission time in order to maximize the effective bandwidth delivered by the network.

> 这个示例强调了减少与数据包传输时间的发送和接收开销的重要性，以最大程度地提高网络传递的有效带宽。

The analysis above suggests that it is possible to provide some upper bound for the effective bandwidth by analyzing the path followed by packets and determining where the bottleneck occurs. We can extend this idea beyond the network interfaces by defining a model that considers the entire network from end to end as a pipe and identifying the narrowest section of that pipe. There are three areas of interest in that pipe: the aggregate of all network injection links and the corresponding `network injection bandwidth` (BW<sub>NetworkInjection</sub>), the aggregate of all network reception links and the corresponding `network reception bandwidth` (BW<sub>NetworkReception</sub>), and the aggregate of all network links and the corresponding `network bandwidth` (BW<sub>Network</sub>). Expressions for these will be given in later sections as various layers of the black box view of the network are peeled away.

> 上面的分析表明，通过分析数据包所遵循的路径并确定瓶颈发生的位置，可以为有效带宽提供一些上限。我们可以通过定义一个模型将这个想法扩展到网络接口之外，该模型将整个网络从一端到另一端视为管道并识别该管道的最窄部分。该管道包含三个感兴趣的区域：所有网络注入链接的聚合和相应的 "网络注入带宽" (BW<sub>NetworkInjection</sub>)，所有网络接收链接的聚合和相应的 "网络接收"  bandwidth ` (BW<sub>NetworkReception</sub>)，以及所有网络链接和对应的` network bandwidth` (BW<sub>Network</sub>) 的总和。随着网络黑盒视图的各个层被剥离，这些表达式将在后面的部分给出。

To this point, we have assumed that for just two interconnected devices the black box network behaves ideally and the network bandwidth is equal to the aggregate raw network bandwidth. In reality, it can be much less than the aggre- gate bandwidth as we will see in the following sections. In general, the effective bandwidth delivered end-to-end by the network to an application is upper bounded by the minimum across all three potential bottleneck areas:

> 为此，我们假设仅对于两个互连设备，黑匣子网络的行为是理想的，并且网络带宽等于汇总的原始网络带宽。实际上，它可能比以下各节看到的那样远远远远远远少于大门带宽。通常，网络向应用程序端对端交付的有效带宽在所有三个潜在瓶颈区域的最低限度上都限制：

Effective bandwidth = min BWNetworkInjection, BWNetwork, BWNetworkReception

> 有效带宽= min bwnetworkIndoction，bwnetwork，bwnetwork reception

We will expand upon this expression further in the following sections as we reveal more about interconnection networks and consider the more general case of inter- connecting more than two devices.

> 在以下各节中，我们将进一步扩展该表达式，因为我们透露了有关互连网络的更多信息，并考虑了连接两个以上设备的更一般情况。

In some sections of this appendix, we show how the concepts introduced in the section take shape in example high-end commercial products. Figure F.7 lists several commercial computers that, at one point in time in their existence, were among the highest-performing systems in the world within their class. Although these systems are capable of interconnecting more than two devices, they implement the basic functions needed for interconnecting only two devices. In addition to being applicable to the SANs used in those systems, the issues discussed in this section also apply to other interconnect domains: from OCNs to WANs.

> 在本附录的某些部分中，我们展示了本节中引入的概念如何在示例高端商业产品中形成。图 F.7 列出了几台商业计算机，这些计算机在其存在的一个时间点上是全球表现最高的系统之一。尽管这些系统能够互连两个以上的设备，但它们仅实现了仅互连两个设备所需的基本功能。除了适用于这些系统中使用的 SAN 外，本节中讨论的问题还适用于其他互连域：从 OCN 到 WANS。
