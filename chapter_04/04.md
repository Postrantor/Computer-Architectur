## Graphics Processing Units

> ##图形处理单元

People can buy a GPU chip with thousands of parallel floating-point units for a few hundred dollars and plug it into their desk side PC. Such affordability and convenience makes high performance computing available to many. The interest in GPU computing blossomed when this potential was combined with a programming language that made GPUs easier to program. Therefore many programmers of scientific and multimedia applications today are pondering whether to use GPUs or CPUs. For programmers interested in machine learning, which is the subject of [Chapter 7](#_bookmark322), GPUs are currently the preferred platform.

> 人们可以以数百美元的价格购买具有数千个平行浮点单元的 GPU 芯片，并将其插入他们的桌子侧 PC。这种负担能力和便利性使许多人都可以使用高性能计算。当将这种潜力与一种使 GPU 更易于编程的编程语言结合使用时，对 GPU 计算的兴趣就会蓬勃发展。因此，当今许多科学和多媒体应用程序的程序员都在思考是否使用 GPU 或 CPU。对于对机器学习感兴趣的程序员来说，这是[第 7 章]（#\_ bookmark322）的主题，GPU 当前是首选平台。

GPUs and CPUs do not go back in computer architecture genealogy to a common ancestor; there is no “missing link” that explains both. As [Section 4.10](#historical-perspective-and-references-1) describes, the primary ancestors of GPUs are graphics accelerators, as doing graphics well is the reason why GPUs exist. While GPUs are moving toward mainstream computing, they can’t abandon their responsibility to continue to excel at graphics. Thus the design of GPUs may make more sense when architects ask, given the hardware invested to do graphics well, how can we supplement it to improve the performance of a wider range of applications?

> GPU 和 CPU 不会在计算机体系结构的家谱中回到一个共同的祖先。没有“缺少链接”可以解释两者。正如[4.10]（＃历史谱图和参考 1）所描述的那样，GPU 的主要祖先是图形加速器，因为良好的图形是 GPU 存在的原因。当 GPU 朝主流计算迈进时，他们不能放弃继续在图形上表现出色的责任。因此，当建筑师要求使用硬件投资以做好图形处理时，GPU 的设计可能更有意义，我们如何补充它以提高更广泛的应用程序的性能？

Note that this section concentrates on using GPUs for computing. To see how GPU computing combines with the traditional role of graphics acceleration, see “Graphics and Computing GPUs,” by John Nickolls and David Kirk ([Appendix](#_bookmark391) [A](#_bookmark391) in the 5th edition of _Computer Organization and Design_ by the same authors as this book).

> 请注意，本节专注于使用 GPU 进行计算。要查看 GPU 计算如何与图形加速的传统作用结合在一起，请参见 John Nickolls 和 David Kirk（[Appendix]（#_ bookmark391）[a]（#_ bookmark391）的“图形和计算 GPU”，第 5 版的* commoputer 组织和 Design*与本书同一作者）。

Because the terminology and some hardware features are quite different from vector and SIMD architectures, we believe it will be easier if we start with the simplified programming model for GPUs before we describe the architecture.

> 由于术语和某些硬件功能与向量和 SIMD 架构完全不同，因此我们认为，如果我们先从 GPU 的简化编程模型开始，然后再描述架构，这将变得更加容易。

### Programming the GPU

> ###编程 GPU

CUDA is an elegant solution to the problem of representing parallelism in algorithms, not all algorithms, but enough to matter. It seems to resonate in some way with the way we think and code, allowing an easier, more natural expression of parallelism beyond the task level.

> CUDA 是对代表算法中平行性的问题的优雅解决方案，不是所有算法，而是足够重要。它似乎以某种方式与我们的思考和编码方式产生共鸣，从而使并行性更轻松，更自然地表达超出任务级别。

The challenge for the GPU programmer is not simply getting good performance on the GPU, but also in coordinating the scheduling of computation on the system processor and the GPU and the transfer of data between system memory and GPU memory. Moreover, as we see will see later in this section, GPUs have virtually every type of parallelism that can be captured by the programming environment: multithreading, MIMD, SIMD, and even instruction-level.

> GPU 程序员面临的挑战不仅是在 GPU 上获得良好的性能，而且还在协调系统处理器和 GPU 的计算计划以及系统内存和 GPU 内存之间的数据传输。此外，正如我们看到的那部分，GPU 几乎具有通过编程环境捕获的所有类型的并行性：多线程，MIMD，SIMD，甚至指令级别。

NVIDIA decided to develop a C-like language and programming environment that would improve the productivity of GPU programmers by attacking both the challenges of heterogeneous computing and of multifaceted parallelism. The name of their system is _CUDA_, for Compute Unified Device Architecture. CUDA produces C/C++ for the system processor (_host_) and a C and C++ dialect for the GPU (_device_, thus the D in CUDA). A similar programming language is _OpenCL_, which several companies are developing to offer a vendor-independent language for multiple platforms.

> NVIDIA 决定开发一种类似 C 的语言和编程环境，该语言将通过攻击异质计算和多方面并行性的挑战，从而提高 GPU 程序员的生产率。其系统的名称为 *cuda*，用于计算统一的设备体系结构。CUDA 为系统处理器（_HOST_）和 GPU 的 C 和 C ++ 方言产生 C/C ++（_device_，因此 CUDA 中的 D）。类似的编程语言是 *OPENCL*，几家公司正在开发该语言，以为多个平台提供独立于供应商的语言。

NVIDIA decided that the unifying theme of all these forms of parallelism is the _CUDA Thread_. Using this lowest level of parallelism as the programming primitive, the compiler and the hardware can gang thousands of CUDA Threads together to utilize the various styles of parallelism within a GPU: multithreading, MIMD, SIMD, and instruction-level parallelism. Therefore NVIDIA classifies the CUDA programming model as single instruction, multiple thread (_SIMT_). For reasons we will soon see, these threads are blocked together and executed in groups of threads, called a _Thread Block_. We call the hardware that executes a whole block of threads a _multithreaded SIMD Processor_.

> NVIDIA 决定所有这些并行形式的统一主题是 *cuda thread*。编译器和硬件将最低级别的并行级别作为编程原始，可以将数千个 CUDA 线程组合在一起，以利用 GPU 中的各种并行风格：多线程阅读，MIMD，SIMD 和说明级并行性。因此，NVIDIA 将 CUDA 编程模型分类为单个指令，多线程（_simt_）。由于我们很快就会看到的原因，这些线程被阻止并以线程组（称为* Thread Block*）的组进行执行。我们调用硬件，该硬件执行整个线程 *multithReaded Simd 处理器*。

We need just a few details before we can give an example of a CUDA program:

> 我们只需要几个细节，然后才能提供一个 CUDA 程序的示例：

To distinguish between functions for the GPU (device) and functions for the system processor (host), CUDA uses device or global for the former and host for the latter.

> 为了区分 GPU（设备）的函数和系统处理器（主机）的功能，CUDA 使用设备或全局为前者和主机。

CUDA variables declared with device are allocated to the GPU Memory (see below), which is accessible by all multithreaded SIMD Processors.

> 用设备声明的 CUDA 变量分配给 GPU 内存（见下文），所有多线 SIMD 处理器都可以访问。

The extended function call syntax for the function _name_ that runs on the GPU is _name_ &lt; &lt;&lt;dimfirid, dimBlock&gt;&gt; &gt; (… _parameter list_…)

> 在 GPU 上运行的函数 *name* 的扩展函数调用语法为 *name*＆lt;＆lt;＆lt; dimfirid，dimblock＆gt;＆gt;＆gt;（…_参数列表_…）

where dimfirid and dimBlock specify the dimensions of the code (in Thread Blocks) and the dimensions of a block (in threads).

> 其中 dimfirid 和 dimblock 指定代码的尺寸（在线程块中）和块的尺寸（在线程中）。

In addition to the identifier for blocks (blockIdx) and the identifier for each thread in a block (threadIdx), CUDA provides a keyword for the number of threads per block (blockDim), which comes from the dimBlock parameter in the preceding bullet.

> 除了块（blockIDX）的标识符和块中每个线程的标识符（threadiDx）中的标识符外，CUDA 还提供了每个块线程数（blockDim）的关键字，该螺纹来自前面子弹中的 Dimblock 参数。

Before seeing the CUDA code, let’s start with conventional C code for the DAXPY loop from [Section 4.2](#vector-architecture):

> 在查看 CUDA 代码之前，让我们从[4.2]（＃vector-Architecture）的 Daxpy 循环的常规 C 代码开始：

Following is the CUDA version. We launch n threads, one per vector element, with 256 CUDA Threads per Thread Block in a multithreaded SIMD Processor. The GPU function starts by calculating the corresponding element index i based on the block ID, the number of threads per block, and the thread ID. As long as this index is within the array (i &lt; n), it performs the multiply and add.

> 以下是 CUDA 版本。我们启动 n 个线程，每个向量元素一个，在多线程 SIMD 处理器中，每个线程块有 256 个 CUDA 线程。GPU 函数首先基于块 ID，每个块线程数和线程 ID 来计算相应的元素索引 i。只要此索引在数组（I＆lt; n）内，它就会执行乘法并添加。

Comparing the C and CUDA codes, we see a common pattern to parallelizing data-parallel CUDA code. The C version has a loop where each iteration is independent from the others, allowing the loop to be transformed straightforwardly into a parallel code where each loop iteration becomes a separate thread. (As previously mentioned and described in detail in [Section 4.5](#detecting-and-enhancing-loop-level-parallelism), vectorizing compilers also rely on a lack of dependences between iterations of a loop, which are called _loop-carried dependences_.) The programmer determines the parallelism in CUDA explicitly by specifying the grid dimensions and the number of threads per SIMD Processor. By assigning a single thread to each element, there is no need to synchronize between threads when writing results to memory.

> 比较 C 和 CUDA 代码，我们看到了一个共同的模式，可以并行数据并行 CUDA 代码。C 版本具有一个循环，每个迭代都与其他迭代独立，可以将循环直接转换为平行代码，在该代码中，每个环路迭代成为一个单独的线程。（如前所述，在[第 4.5 节]中详细描述（＃检测和增强的环路级并行性），矢量化编译器还依赖于循环的迭代之间缺乏依赖性的循环，这称为* loop carlied 依赖*。）程序员通过指定网格尺寸和每个 SIMD 处理器的线程数来明确确定 CUDA 中的并行性。通过为每个元素分配一个线程，在将结果写入内存时无需在线程之间同步。

The GPU hardware handles parallel execution and thread management; it is not done by applications or by the operating system. To simplify scheduling by the hardware, CUDA requires that Thread Blocks be able to execute independently and in any order. Different Thread Blocks cannot communicate directly, although they can _coordinate_ using atomic memory operations in global memory.

> GPU 硬件处理并行执行和线程管理；它不是由应用程序或操作系统完成的。为了简化硬件计划，CUDA 要求线程块能够以任何顺序独立执行。不同的线程块无法直接通信，尽管它们可以使用全局内存中的原子内存操作 *coorcord*。

As we will soon see, many GPU hardware concepts are not obvious in CUDA. Writing efficient GPU code requires that programmers think in terms of SIMD operations, even though the CUDA programming model looks like MIMD. Performance programmers must keep the GPU hardware in mind when writing in CUDA. That could hurt programmer productivity, but then most programmers are using GPUs instead of CPUs to get performance. For reasons explained shortly, they know that they need to keep groups of 32 threads together in control flow to get the best performance from multithreaded SIMD Processors and to create many more threads per multithreaded SIMD Processor to hide latency to DRAM. They also need to keep the data addresses localized in one or a few blocks of memory to get the expected memory performance.

> 正如我们将很快看到的那样，在 CUDA 中，许多 GPU 硬件概念并不明显。编写有效的 GPU 代码要求程序员对 SIMD 操作进行思考，即使 CUDA 编程模型看起来像 MIMD。性能程序员在 CUDA 写作时必须牢记 GPU 硬件。这可能会损害程序员的生产力，但是大多数程序员都使用 GPU 而不是 CPU 来获得性能。由于很快解释的原因，他们知道他们需要将 32 个线程组保持在控制流程中，以从多线程 SIMD 处理器中获得最佳性能，并为每个多线程 SIMD 处理器创建更多线程，以隐藏延迟到 DRAM。他们还需要将数据地址定位在一个或几个内存块中以获得预期的内存性能。

Like many parallel systems, a compromise between productivity and performance is for CUDA to include intrinsics to give programmers explicit control over the hardware. The struggle between productivity on the one hand versus allowing the programmer to be able to express anything that the hardware can do on the other hand happens often in parallel computing. It will be interesting to see how the language evolves in this classic productivity-performance battle as well as to see whether CUDA becomes popular for other GPUs or even other architectural styles.

> 像许多并行系统一样，生产率和性能之间的折衷是 CUDA 包括内在的信息，以使程序员明确控制对硬件。一方面生产力与允许程序员能够表达硬件可以做的任何事情之间经常发生在并行计算中发生的任何事情之间的斗争。有趣的是，在这场经典的生产力 - 表现之战中，语言如何发展，以及查看 CUDA 是否在其他 GPU 甚至其他建筑风格中流行。

### NVIDIA GPU Computational Structures

> ### nvidia gpu 计算结构

The uncommon heritage mentioned above helps explain why GPUs have their own architectural style and their own terminology independent from CPUs. One obstacle to understanding GPUs has been the jargon, with some terms even having misleading names. This obstacle has been surprisingly difficult to overcome, as the many rewrites of this chapter can attest.

> 上面提到的不常见遗产有助于解释为什么 GPU 具有自己的建筑风格和独立于 CPU 的术语。理解 GPU 的一个障碍是行话，有些术语甚至具有误导性名称。由于本章的许多重写可以证明，因此这一障碍很难克服。

To try to bridge the twin goals of making the architecture of GPUs understandable _and_ learning the many GPU terms with nontraditional definitions, our approach is to use the CUDA terminology for software but initially use more descriptive terms for the hardware, sometimes borrowing terms from OpenCL. Once we explain the GPU architecture in our terms, we’ll map them into the official jargon of NVIDIA GPUs.

> 为了弥合使 GPU 架构可以理解的*和*学习许多 GPU 术语的双重目标，我们的方法是将 CUDA 术语用于软件，但最初对硬件使用更具描述性术语，有时是从 OpenCL 中借用术语。一旦我们用术语解释了 GPU 架构，我们将其映射到 Nvidia GPU 的官方术语中。

From left to right, [Figure 4.12](#_bookmark182) lists the descriptive term used in this section, the closest term from mainstream computing, the official NVIDIA GPU jargon in case you are interested, and then a short explanation of the term. The rest of this section explains the microarchitectural features of GPUs using the descriptive terms on the left in the figure.

> 从左到右，[图 4.12]（#\_ bookmark182）列出了本节中使用的描述性术语，即主流计算中最接近的术语，如果您有兴趣，则官方的 NVIDIA GPU 术语，然后是对该术语的简短说明。本节的其余部分使用图中左侧的描述术语解释了 GPU 的微体系特征。

We use NVIDIA systems as our example as they are representative of GPU architectures. Specifically, we follow the terminology of the preceding CUDA parallel programminglanguageand use the NVIDIAPascal GPU as the example (see [Section 4.7](#_bookmark198)). Like vector architectures, GPUs work well only with data-level parallel problems. Both styles have gather-scatter data transfers and mask registers, and GPU processors have even more registers than do vector processors. Sometimes, GPUs implement certain features in hardware that vector processors would implement in software. This difference is because vector processors have a scalar processor that can execute a software function. Unlike most vector architectures,

> 我们将 NVIDIA 系统作为我们的示例，因为它们代表了 GPU 架构。具体而言，我们遵循前面的 CUDA 并行编程的术语使用 NVIDIAPASCAL GPU 作为示例（请参阅[第 4.7]节（#\_ bookmark198））。像矢量体系结构一样，GPU 仅在数据级并行问题上效果很好。两种样式都有聚会片段数据传输和蒙版寄存器，而 GPU 处理器的寄存器比向量处理器更多。有时，GPU 在硬件中实现某些功能，矢量处理器将在软件中实现。此差异是因为向量处理器具有可以执行软件函数的标量处理器。与大多数矢量体系结构不同，

Figure 4.12 Quick guide to GPU terms used in this chapter. We use the first column for hardware terms. Four groups cluster these 11 terms. From top to bottom: program abstractions, machine objects, processing hardware, and memory hardware. [Figure 4.21](#_bookmark191) on page 312 associates vector terms with the closest terms here, and [Figure 4.24](#_bookmark195) on page 317 and [Figure 4.25](#_bookmark196) on page 318 reveal the official CUDA/NVIDIA and AMD terms and definitions along with the terms used by OpenCL.

> 图 4.12 本章中使用的 GPU 术语的快速指南。我们将第一列用于硬件项。四个小组聚集了这 11 个术语。从上到下：程序抽象，机器对象，处理硬件和内存硬件。[图 4.21]（#_ bookmark191）第 312 页上的矢量术语与最接近的术语相关联，以及[图 4.24]（#_ bookmark195）第 317 页和[图 4.25]（图 4.25]（#_ bookmark196）（#_ bookmark196）和 AMD 术语和定义以及 OpenCL 使用的术语。

GPUs also rely on multithreading within a single multithreaded SIMD Processor to hide memory latency (see Chapters [2](#_bookmark46) and [3](#_bookmark93)). However, efficient code for both vector architectures and GPUs requires programmers to think in groups of SIMD operations.

> GPU 还依靠单个多线程 SIMD 处理器中的多线程来隐藏内存延迟（请参阅第[2]（#_ bookmark46）和[3]（#_ bookmark93））。但是，矢量体系结构和 GPU 的有效代码要求程序员在 SIMD 操作组中进行思考。

A _Grid_ is the code that runs on a GPU that consists of a set of _Thread Blocks_. [Figure 4.12](#_bookmark182) draws the analogy between a grid and a vectorized loop and between a Thread Block and the body of that loop (after it has been strip-mined, so that it is a full computation loop). To give a concrete example, let’s suppose we want to multiply two vectors together, each 8192 elements long: A=B\* C. We’ll return to this example throughout this section. [Figure 4.13](#_bookmark183) shows the relationship between this example and these first two GPU terms. The GPU code that works on the whole 8192 element multiply is called a _Grid_ (or vectorized loop). To break it down into more manageable sizes, a Grid is composed of _Thread Blocks_ (or body of a vectorized loop), each with up to 512 elements. Note that a SIMD instruction executes 32 elements at a time. With 8192 elements in the vectors, this example thus has 16 Thread Blocks because 16 8192 512. The Grid and Thread Block are programming abstractions implemented in GPU hardware that help programmers organize their CUDA code. (The Thread Block is analogous to a strip-mined vector loop with a vector length of 32.)

> *grid* 是在 GPU 上运行的代码，该代码由一组 *thread 块*组成。[图 4.12]（#_ bookmark182）在网格和矢量循环之间以及螺纹块和该循环的主体之间绘制类比（在循环进行了剥离之后，因此它是一个完整的计算循环）。为了给出一个具体的示例，我们假设我们要将两个向量乘在一起，每个 8192 个元素长：a = b \* C。我们将在本节中返回此示例。[图 4.13]（#_ bookmark183）显示了此示例与前两个 GPU 项之间的关系。在整个 8192 元素乘上工作的 GPU 代码称为*grid*（或矢量化循环）。为了将其分解为更易于管理的尺寸，网格由* thread blocks*（或矢量循环的主体）组成，每个元素最多为 512 个元素。请注意，SIMD 指令一次执行 32 个元素。由于向量中有 8192 个元素，因此该示例具有 16 个线程块，因为 168192512。网格和线程块是在 GPU 硬件中实现的编程摘要，可帮助程序员组织其 CUDA 代码。（螺纹块类似于矢量长度为 32 的条带媒介环。）

A Thread Block is assigned to a processor that executes that code, which we call a _multithreaded SIMD Processor_, by the _Thread Block Scheduler_. The programmer tells the Thread Block Scheduler, which is implemented in hardware, how many Thread Blocks to run. In this example, it would send 16 Thread Blocks to multithreaded SIMD Processors to compute all 8192 elements of this loop

> 将线程块分配给执行该代码的处理器，我们称之为 *multithReaded Simd 处理器*，由 *thread Block Scheduler*。程序员告诉在硬件中实现的线程块调度程序，要运行多少个线程块。在此示例中，它将将 16 个线程块发送到多线程 SIMD 处理器以计算此循环的所有 8192 个元素

[Figure 4.14](#_bookmark184) shows a simplified block diagram of a multithreaded SIMD Processor. It is similar to a vector processor, but it has many parallel functional units instead of a few that are deeply pipelined, as in a vector processor. In the programming example in [Figure 4.13](#_bookmark183), each multithreaded SIMD Processor is assigned 512 elements of the vectors to work on. SIMD Processors are full processors with separate PCs and are programmed using threads (see [Chapter 3](#_bookmark93)).

> [图 4.14]（#_ bookmark184）显示了多线程 SIMD 处理器的简化框图。它类似于向量处理器，但是它具有许多平行的功能单元，而不是像矢量处理器一样深处管道的少数单元。在[图 4.13]（#_ bookmark183）的编程示例中，每个多线程 SIMD 处理器都分配了 512 个向量的元素。SIMD 处理器是带有独立 PC 的完整处理器，并使用线程进行编程（请参阅[第 3 章]（#\_ bookmark93））。

The GPU hardware then contains a collection of multithreaded SIMD Processors that execute a Grid of Thread Blocks (bodies of vectorized loop); that is, a GPU is a multiprocessor composed of multithreaded SIMD Processors.

> 然后，GPU 硬件包含一个多线程 SIMD 处理器的集合，该处理器执行螺纹块网格（矢量化循环的主体）；也就是说，GPU 是由多线程 SIMD 处理器组成的多处理器。

A GPU can have from one to several dozen multithreaded SIMD Processors. For example, the Pascal P100 system has 56, while the smaller chips may have as few as one or two. To provide transparent scalability across models of GPUs with a differing number of multithreaded SIMD Processors, the Thread Block Scheduler assigns Thread Blocks (bodies of a vectorized loop) to multithreaded SIMD Processors. [Figure 4.15](#_bookmark185) shows the floor plan of the P100 implementation of the Pascal architecture. Dropping down one more level of detail, the machine object that the hardware creates, manages, schedules, and executes is a _thread of SIMD instructions_. It is a traditional thread that contains exclusively SIMD instructions. These threads of SIMD instructions have their own PCs, and they run on a multithreaded SIMD Processor. The _SIMD Thread Scheduler_ knows which threads of SIMD instructions are ready to run and then sends them off to a dispatch unit to be run on the multithreaded SIMD Processor. Thus GPU hardware has two levels of hardware schedulers: (1) the _Thread Block Scheduler_ that assigns Thread Blocks (bodies of vectorized loops) to multithreaded SIMD Processors and (2) the SIMD Thread Scheduler _within_ a SIMD Processor, which schedules when threads of SIMD instructions should run.

> GPU 可以从一到几十个多线程 SIMD 处理器。例如，Pascal P100 系统具有 56，而较小的芯片可能只有一两个。为了提供不同数量的多线程 SIMD 处理器的 GPU 模型的透明可伸缩性，线程块调度程序将线程块（矢量化循环的主体）分配给多线程 SIMD 处理器。[图 4.15]（#* bookmark185）显示了 Pascal 架构实施 P100 的平面图。删除另一个级别的细节，硬件创建，管理，计划和执行的机器对象是 SIMD 指令的*线程*。这是一个传统的线程，其中包含 SIMD 指令。这些 SIMD 指令的线程具有自己的 PC，并且在多线程 SIMD 处理器上运行。\_simd 线程调度程序*知道 SIMD 指令的哪些线程可以运行，然后将它们发送到派遣单元以在多线程 SIMD 处理器上运行。因此，GPU 硬件具有两个级别的硬件调度程序：（1）*thread 块调度程序*将线程块（矢量化循环的主体）分配到多线程 SIMD 处理器和（2）SIMD 线程调度程序*within* a SIMD 处理器，该 simd simd simd 时说明应运行。

![](./media/image247.png)

> ！[]（./ Media/image247.png）

<img src="./media/image248.png" style="width:0.35238in;height:0.33854in" />

> <img src =“ ./媒体/image248.png” style =“宽度：0.35238in;高度：0.33854in”/>

<img src="./media/image249.png" style="width:0.35236in;height:0.33854in" />

> <img src =“ ./媒体/image249.png” style =“宽度：0.35236in;高度：0.33854in”/>

Figure 4.13 The mapping of a Grid (vectorizable loop), Thread Blocks (SIMD basic blocks), and threads of SIMD instructions to a vector-vector multiply, with each vector being 8192 elements long. Each thread of SIMD instructions calculates 32 elements per instruction, and in this example, each Thread Block contains 16 threads of SIMD instructions and the Grid contains 16 Thread Blocks. The hardware Thread Block Scheduler assigns Thread Blocks to multithreaded SIMD Processors, and the hardware Thread Scheduler picks which thread of SIMD instructions to run each clock cycle within a SIMD Processor. Only SIMD Threads in the same Thread Block can communicate via local memory. (The maximum number of SIMD Threads that can execute simultaneously per Thread Block is 32 for Pascal GPUs.)

> 图 4.13 网格（可矢量循环），螺纹块（SIMD 基本块）和 SIMD 指令的线程的映射到向量 - 向量乘数，每个矢量为 8192 个元素。SIMD 指令的每个线程都计算每个指令的 32 个元素，在此示例中，每个线程块包含 16 个 SIMD 指令的线程，并且网格包含 16 个线程块。硬件线程块调度程序将线程块分配给多线程 SIMD 处理器，以及硬件线程调度程序选择 SIMD 指令的线程以在 SIMD 处理器中运行每个时钟周期。只有同一线程块中的 SIMD 线程才能通过本地内存进行通信。（对于 Pascal GPU，可以同时执行每个线程块的最大 SIMD 线程数为 32。）

Figure 4.14 Simplified block diagram of a multithreaded SIMD Processor. It has 16 SIMD Lanes. The SIMD Thread Scheduler has, say, 64 independent threads of SIMD instructions that it schedules with a table of 64 program counters (PCs). Note that each lane has 1024 32-bit registers.

> 图 4.14 多线程 SIMD 处理器的简化框图。它有 16 个 SIMD 车道。SIMD 线程调度程序具有 64 个独立的 SIMD 指令的独立线程，并使用 64 个程序计数器（PC）进行调度。请注意，每个车道都有 1024 32 位登记册。

The SIMD instructions of these threads are 32 wide, so each thread of SIMD instructions in this example would compute 32 of the elements of the computation. In this example, Thread Blocks would contain 512/32 16 SIMD Threads (see [Figure 4.13](#_bookmark183)).

> 这些线程的 SIMD 指令为 32 宽，因此在此示例中，SIMD 指令的每个线程将计算计算的元素的 32 个。在此示例中，线程块将包含 512/32 16 SIMD 线程（请参见[图 4.13]（#\_ bookmark183））。

Because the thread consists of SIMD instructions, the SIMD Processor must have parallel functional units to perform the operation. We call them _SIMD Lanes_, and they are quite similar to the Vector Lanes in [Section 4.2](#vector-architecture).

> 由于线程由 SIMD 指令组成，因此 SIMD 处理器必须具有并行功能单元才能执行操作。我们称它们为 *simd lanes*，它们与[4.2]（＃vector-Architecture）中的向量通道非常相似。

<img src="./media/image261.jpeg" style="width:4.33924in;height:2.56in" />

> <img src =“ ./媒体/image261.jpeg” style =“宽度：4.33924in;高度：2.56in”/>

Figure 4.15 Full-chip block diagram of the Pascal P100 GPU. It has 56 multithreaded SIMD Processors, each with an L1 cache and local memory, 32 L2 units, and a memory-bus width of 4096 data wires. (It has 60 blocks, with four spares to improve yield.) The P100 has 4 HBM2 ports supporting up to 16 GB of capacity. It contains 15.4 billion transistors.

> 图 4.15 Pascal P100 GPU 的全芯片框图。它具有 56 个多线程 SIMD 处理器，每个处理器都有 L1 缓存和本地内存，32 L2 单元，并且存储器宽度为 4096 个数据线。（它有 60 个块，有 4 个备件可提高产量。）P100 具有 4 个 HBM2 端口，可支持高达 16 GB 的容量。它包含 154 亿晶体管。

With the Pascal GPU, each 32-wide thread of SIMD instructions is mapped to 16 physical SIMD Lanes, so each SIMD instruction in a thread of SIMD instructions takes 2 clock cycles to complete. Each thread of SIMD instructions is executed in lock step and scheduled only at the beginning. Staying with the analogy of a SIMD Processor as a vector processor, you could say that it has 16 lanes, the vector length is 32, and the chime is 2 clock cycles. (This wide but shallow nature is why we use the more accurate term SIMD Processor rather than vector.) Note that the number of lanes in a GPU SIMD Processor can be anything up to the number of threads in a Thread Block, just as the number of lanes in a vector processor can vary between 1 and the maximum vector length. For example, across GPU generations, the number of lanes per SIMD Processor has fluctuated between 8 and 32. Because by definition the threads of SIMD instructions are independent, the SIMD Thread Scheduler can pick whatever thread of SIMD instructions is ready, and need not stick with the next SIMD instruction in the sequence within a thread. The SIMD Thread Scheduler includes a scoreboard (see [Chapter 3](#_bookmark93)) to keep track of up to 64 threads of SIMD instructions to see which SIMD instruction is ready to go. The latency of memory instructions is variable because of hits and misses in the caches and the TLB, thus the requirement of a scoreboard to determine when these instructions are complete. [Figure 4.16](#_bookmark186) shows the SIMD Thread Scheduler picking threads of SIMD instructions in a different order over time. The assumption of GPU architects is that GPU applications have so many threads of SIMD instructions that multithreading can both hide the latency to DRAM and increase utilization of multithreaded SIMD Processors.

> 使用 Pascal GPU，SIMD 指令的每个 32 宽线程都映射到 16 个物理 SIMD 车道，因此 SIMD 指令线中的每个 SIMD 指令都需要 2 个时钟周期来完成。SIMD 指令的每个线程均以锁定步骤执行，并且仅在开始时计划。与 SIMD 处理器作为矢量处理器的类比，您可以说它有 16 条车道，矢量长度为 32，而循环为 2 个时钟周期。（这种宽而浅的性质是为什么我们使用更准确的术语 SIMD 处理器而不是向量。）请注意，GPU SIMD 处理器中的车道数可以是线程块中线程数的任何内容，就像数字一样矢量处理器中的车道可以在 1 和最大矢量长度之间变化。例如，在 GPU 几代中，每个 SIMD 处理器的车道数量在 8 到 32 之间波动。由于根据定义，SIMD 指令的线程是独立的，因此 SIMD 线程调度程序可以选择 SIMD 指令的任何线程，并且不需要粘贴使用线程中的序列中的下一个 SIMD 指令。SIMD 线程调度程序包括一个记分牌（请参阅[第 3 章]（#_ bookmark93）），以跟踪多达 64 个 SIMD 指令的 64 个线程，以查看已准备好使用 SIMD 指令。由于缓存和 TLB 的命中和错过，内存指令的延迟是可变的，因此计分板需要确定何时完成这些说明。[图 4.16]（#_ bookmark186）显示了 SIMD 线程调度程序随着时间的推移以不同的顺序选择 SIMD 指令的线程。GPU 架构师的假设是 GPU 应用程序具有许多 SIMD 指令的线程，以至于多线程可以隐藏 DRAM 的延迟，并增加了多线程 SIMD 处理器的利用率。

<img src="./media/image262.png" style="width:1.57999in;height:0.79in" />

> <img src =“ ./媒体/image262.png” style =“宽度：1.57999in;高度：0.79in/>

Figure 4.16 Scheduling of threads of SIMD instructions. The scheduler selects a ready thread of SIMD instructions and issues an instruction synchronously to all the SIMD Lanes executing the SIMD Thread. Because threads of SIMD instructions are independent, the scheduler may select a different SIMD Thread each time.

> 图 4.16 SIMD 指令线程的调度。调度程序选择一个 Ready 线程 SIMD 指令，并同步向执行 SIMD 线程的所有 SIMD 车道发送指令。由于 SIMD 指令的线程是独立的，因此调度程序可以每次选择其他 SIMD 线程。

Continuing our vector multiply example, each multithreaded SIMD Processor must load 32 elements of two vectors from memory into registers, perform the multiply by reading and writing registers, and store the product back from registers into memory. To hold these memory elements, a SIMD Processor has between an impressive 32,768–65,536 32-bit registers (1024 per lane in [Figure 4.14](#_bookmark184)), depending on the model of the Pascal GPU. Just like a vector processor, these registers are divided logically across the Vector Lanes or, in this case, SIMD Lanes.

> 继续我们的矢量乘法示例，每个多线程 SIMD 处理器都必须将两个向量的 32 个元素从内存中加载到寄存器中，通过读取和写作寄存器执行乘法，然后将产品从寄存器存储回存储器中。为了持有这些内存元素，SIMD 处理器具有令人印象深刻的 32,768–65,536 32 位寄存器（图 4.14]（#\_ bookmark184）），具体取决于 Pascal GPU 的模型。就像矢量处理器一样，这些寄存器在逻辑上在矢量车道上或在这种情况下为 SIMD 车道进行了划分。

Each SIMD Thread is limited to no more than 256 registers, so you might think of a SIMD Thread as having up to 256 vector registers, with each vector register having 32 elements and each element being 32 bits wide. (Because doubleprecision floating-point operands use two adjacent 32-bit registers, an alternative view is that each SIMD Thread has 128 vector registers of 32 elements, each of which is 64 bits wide.)

> 每个 SIMD 线程的限制不超过 256 个寄存器，因此您可能会认为 SIMD 线程具有多达 256 个向量寄存器，每个向量寄存器具有 32 个元素，每个元素宽 32 位。（因为双重浮点操作数使用两个相邻的 32 位寄存器，所以另一种视图是，每个 SIMD 线程都有 128 个向量寄存器，为 32 个元素，每个元素均为 64 位宽。）

There is a trade-off between register use and maximum number of threads; fewer registers per thread means more threads are possible, and more registers mean fewer threads. That is, not all SIMD Threads need to have the maximum number of registers. Pascal architects believe much of this precious silicon area would be idle if all threads had the maximum number of registers.

> 寄存器使用和最大线程数之间存在权衡；每个线程较少的寄存器意味着更多的线程是可能的，而更多的寄存器意味着更少的线程。也就是说，并非所有 SIMD 线程都需要具有最大寄存器数。帕斯卡尔建筑师认为，如果所有线程都有最大数量的寄存器，那么这个珍贵的硅区域的大部分将是闲置的。

To be able to execute many threads of SIMD instructions, each is dynamically allocated a set of the physical registers on each SIMD Processor when threads of SIMD instructions are created and freed when the SIMD Thread exits. For example, a programmer can have a Thread Block that uses 36 registers per thread with, say, 16 SIMD Threads alongside another Thread Block that has 20 registers per thread with 32 SIMD Threads. Subsequent Thread Blocks may show up in any order, and the registers have to be allocated on demand. While this variability can lead to fragmentation and make some registers unavailable, in practice most Thread Blocks use the same number of registers for a given vectorizable loop (“grid”). The hardware must know where the registers for each Thread Block are in the large register file, and this is recorded on a per Thread-Block basis. This flexibility requires routing, arbitration, and banking in the hardware because a specific register for a given Thread Block could end up in any location in the register file.

> 为了能够执行 SIMD 指令的许多线程，当 SIMD 线程退出时创建并释放 SIMD 指令的线程时，每个 SIMD 处理器上都会动态分配一组物理寄存器。例如，程序员可以具有一个线程块，该线程块每个线程使用 36 个寄存器，例如，另一个线程块旁边有 16 个 SIMD 线程，该线程块，每个线程每个线程具有 20 个寄存器，带有 32 个 SIMD 线程。随后的螺纹块可能会按任何顺序显示，并且必须根据需要分配寄存器。尽管这种可变性可能会导致分散性并使某些寄存器不可用，但实际上，大多数线程块都使用相同数量的寄存器进行给定的矢量循环（“ GRID”）。硬件必须知道每个线程块的寄存器在大寄存器文件中的位置，并且以每个线程块为基础记录。这种灵活性需要在硬件中的路由，仲裁和银行，因为给定线程块的特定寄存器最终可能会在寄存器文件中的任何位置进入。

Note that a CUDA Thread is just a vertical cut of a thread of SIMD instructions, corresponding to one element executed by one SIMD Lane. Beware that CUDA Threads are very different from POSIX Threads; you can’t make arbitrary system calls from a CUDA Thread.

> 请注意，CUDA 线程只是 SIMD 指令线程的垂直切割，对应于一个 SIMD 巷执行的一个元素。注意 CUDA 线程与 Posix 线程大不相同；您无法从 CUDA 线程进行任意系统调用。

We’re now ready to see what GPU instructions look like.

> 现在，我们已经准备好查看 GPU 说明的外观。

### NVIDA GPU Instruction Set Architecture

> ### nvidia gpu 指令集体系结构

Unlike most system processors, the instruction set target of the NVIDIA compilers is an abstraction of the hardware instruction set. _PTX_ (_Parallel Thread Execution_) provides a stable instruction set for compilers as well as compatibility across generations of GPUs. The hardware instruction set is hidden from the programmer. PTX instructions describe the operations on a single CUDA Thread and usually map one-to-one with hardware instructions, but one PTX instruction can expand to many machine instructions, and vice versa. PTX uses an unlimited number of write-once registers and the compiler must run a register allocation procedure to map the PTX registers to a fixed number of read-write hardware registers available on the actual device. The optimizer runs subsequently and can reduce register use even further. This optimizer also eliminates dead code, folds instructions together, and calculates places where branches might diverge and places where diverged paths could converge.

> 与大多数系统处理器不同，NVIDIA 编译器的指令集目标是硬件指令集的抽象。_ptx_（_Parallelthal 线程执行_）为编译器提供了一个稳定的指令集以及 GPU 的几代人的兼容性。硬件指令集隐藏在程序员中。PTX 指令描述了单个 CUDA 线程上的操作，通常用硬件说明一对一地绘制一对一，但是一个 PTX 指令可以扩展到许多机器说明，反之亦然。PTX 使用无限数量的写入寄存器数量，编译器必须运行寄存器分配过程，以将 PTX 寄存器映射到实际设备上可用的固定数量的读取版本硬件寄存器。优化器随后运行，可以进一步减少寄存器使用。该优化器还消除了死亡代码，将指令折叠在一起，并计算出分支机构可能分歧和分歧路径可能收敛的地方的地方。

Although there is some similarity between the x86 microarchitecture and PTX, in that both translate to an internal form (microinstructions for x86), the difference is that this translation happens in hardware at runtime during execution on the x86 versus in software and load time on a GPU.

> 尽管 X86 微结构结构和 PTX 之间存在一些相似之处，但这两者都转化为内部形式（x86 的微观结构），但区别在于，此翻译在 X86 vers the x86 vers in Software in Software in Software in Software in Software in Software in Software in Software in Software in Software in Software in thectution ins thection and of of a poad of of tocal ins of of Software and Load of of a interion trange。GPU。

[Figure 4.17](#_bookmark187) shows the basic PTX instruction set. All instructions can be predicated by 1-bit predicate registers, which can be set by a set predicate instruction (setp). The control flow instructions are functions call and return, thread exit, branch, and barrier synchronization for threads within a Thread Block (bar.sync). Placing a predicate in front of a branch instruction gives us conditional branches. The compiler or PTX programmer declares virtual registers as 32-bit or 64-bit typed or untyped values. For example, R0, R1, ... are for 32-bit values and RD0, RD1, ... are for 64-bit registers. Recall that the assignment of virtual registers to physical registers occurs at load time with PTX.

> [图 4.17]（#\_ bookmark187）显示了基本的 PTX 指令集。所有指令都可以通过 1 位谓词寄存器来鉴定，可以通过集合谓词指令（SETP）设置。控制流量指令是函数呼叫和返回，线程出口，分支和屏障同步线程中的线程（bar.sync）。将谓词放在分支指导的前面，为我们提供条件分支。编译器或 PTX 程序员将虚拟寄存器声明为 32 位或 64 位键入或未型值。例如，R0，R1，...适用于 32 位值，RD0，RD1，...用于 64 位寄存器。回想一下，将虚拟寄存器分配到物理寄存器时，使用 PTX 发生在加载时间。

As demonstrated above, the CUDA programming model assigns one CUDA Thread to each loop iteration and offers a unique identifier number to each Thread Block (blockIdx) and one to each CUDA Thread within a block (threadIdx). Thus it creates 8192 CUDA Threads and uses the unique number to address each element within the array, so there is no incrementing or branching code. The first three PTX instructions calculate that unique element byte offset in R8, which is added to the base of the arrays. The following PTX instructions load two double-precision floating-point operands, multiply and add them, and store the sum. (We’ll describe the PTX code corresponding to the CUDA code “if (i &lt; n)” below.)

> 如上所述，CUDA 编程模型将一个 CUDA 线程分配给每个循环迭代，并为每个线程块（blockIdx）提供一个唯一的标识符编号，并且在一个块（threadiDx）内为每个 cuda 线程提供了一个。因此，它创建了 8192 个 CUDA 线程，并使用唯一数字来解决数组中的每个元素，因此没有增量或分支代码。前三个 PTX 指令计算 R8 中的唯一元素字节偏移，该字节偏移添加到阵列的底部。以下 PTX 指令加载两个双精度浮点操作数，乘以并添加它们，然后存储总和。（我们将描述与下面的 CUDA 代码相对应的 PTX 代码。）

All data transfers are gather-scatter! To regain the efficiency of sequential (unitstride) data transfers, GPUs include special Address Coalescing hardware to recognize when the SIMD Lanes within a thread of SIMD instructions are collectively issuing sequential addresses. That runtime hardware then notifies the Memory Interface Unit to request a block transfer of 32 sequential words. To get this important performance improvement, the GPU programmer must ensure that adjacent CUDA Threads access nearby addresses at the same time so that they can be coalesced into one or a few memory or cache blocks, which our example does.

> 所有数据传输都是聚集的！为了恢复顺序（单位）数据传输的效率，GPU 包括特殊地址合并硬件，以识别 SIMD 指令线程中的 SIMD 车道何时共同发出顺序地址。然后，该运行时硬件通知内存接口单元以请求 32 个顺序单词的块传输。为了改善这种重要的性能，GPU 程序员必须确保同时确保相邻的 CUDA 线程访问附近的地址，以便将它们合并为一个或几个内存或高速缓存块，我们的示例可以做到这一点。

### Conditional Branching in GPUs

> ### GPU 中的条件分支

Just like the case with unit-stride data transfers, there are strong similarities between how vector architectures and GPUs handle IF statements, with the former implementing the mechanism largely in software with limited hardware support and the latter making use of even more hardware. As we will see, in addition to explicit predicate registers, GPU branch hardware uses internal masks, a branch synchronization stack, and instruction markers to manage when a branch diverges into multiple execution paths and when the paths converge.

> 就像单位式数据传输的情况一样，矢量体系结构和 GPU 处理 IF 语句之间的相似之处很强，前者主要在软件中实现了具有有限的硬件支持的软件和后者使用更多硬件的机制。正如我们将看到的那样，除了显式谓词寄存器外，GPU 分支硬件还使用内部掩码，分支同步堆栈和指令标记，当分支偏离多个执行路径以及路径收敛时，可以管理。

At the PTX assembler level, control flow of one CUDA Thread is described by the PTX instructions branch, call, return, and exit, plus individual per-thread-lane predication of each instruction, specified by the programmer with per-thread-lane 1-bit predicate registers. The PTX assembler analyzes the PTX branch graph and optimizes it to the fastest GPU hardware instruction sequence. Each can make its own decision on a branch and does not need to be in lock step.

> 在 PTX 汇编器级别上，一个 CUDA 线程的控制流由 PTX 指令分支，呼叫，返回和退出，以及每个指令的单个每条线程式估计，由程序员指定的每个指令的每个指令的每个指令。 - 位谓词寄存器。PTX 汇编器分析 PTX 分支图并将其优化为最快的 GPU 硬件指令序列。每个人都可以在分支机构上做出自己的决定，并且不需要进入锁定步骤。

At the GPU hardware instruction level, control flow includes branch, jump, jump indexed, call, call indexed, return, exit, and special instructions that manage the branch synchronization stack. GPU hardware provides each SIMD Thread with its own stack; a stack entry contains an identifier token, a target instruction address, and a target thread-active mask. There are GPU special instructions that push stack entries for a SIMD Thread and special instructions and instruction markers that pop a stack entry or unwind the stack to a specified entry and branch to the target instruction address with the target thread-active mask. GPU hardware instructions also have an individual per-lane predication (enable/disable), specified with a 1-bit predicate register for each lane.

> 在 GPU 硬件指令级别，控制流包括分支，跳跃，跳跃索引，呼叫，呼叫索引，返回，退出和管理分支同步堆栈的特殊指令。GPU 硬件为每个 SIMD 线程提供了自己的堆栈；堆栈条目包含标识符令牌，目标指令地址和目标螺纹激活掩码。有一些 GPU 的特殊说明可以将 SIMD 线程的堆栈条目和特殊说明和指令标记弹出堆栈条目，或将堆栈放在指定的条目中，并使用目标线程式掩码分支到目标指令地址。GPU 硬件指令还具有单个车道谓词（启用/禁用），并为每个车道提供 1 位谓词寄存器。

The PTX assembler typically optimizes a simple outer-level IF-THEN-ELSE statement coded with PTX branch instructions to solely predicated GPU instructions, without any GPU branch instructions. A more complex control flow often results in a mixture of predication and GPU branch instructions with special instructions and markers that use the branch synchronization stack to push a stack entry when some lanes branch to the target address, while others fall through. NVIDIA says a branch _diverges_ when this happens. This mixture is also used when a SIMD Lane executes a synchronization marker or _converges_, which pops a stack entry and branches to the stack-entry address with the stack-entry threadactive mask.

> PTX 汇编程序通常优化了使用 PTX 分支指令编码的简单外部级别，以完全预测 GPU 指令，而无需任何 GPU 分支指令。更复杂的控制流程通常会导致鉴定和 GPU 分支指令与特殊指令和标记的混合物，这些指令和标记使用分支同步堆栈在某些车道分支到目标地址时将堆栈条目推入堆栈条目，而另一些车道则掉落。NVIDIA 说发生了一个分支 *Diverges*。当 Simd Lane 执行同步标记或 *Converges* 时，还会使用此混合物，该标记或 *Converges* 弹出堆栈条目并使用堆栈输入的螺纹式掩码分支到堆栈输入地址。

The PTX assembler identifies loop branches and generates GPU branch instructions that branch to the top of the loop, along with special stack instructions to handle individual lanes breaking out of the loop and converging the SIMD Lanes when all lanes have completed the loop. GPU indexed jump and indexed call instructions push entries on the stack so that when all lanes complete the switch statement or function call, the SIMD Thread converges.

> PTX 汇编器识别环路分支，并生成 GPU 分支指令，将分支到循环顶部的分支，以及特殊的堆栈指令，以处理所有车道完成环路时，可以处理单个车道闯入环路并收敛 SIMD 车道。GPU 索引跳跃和索引呼叫指令在堆栈上推入条目，以便当所有车道都完成交换机语句或功能调用时，SIMD 线程会收敛。

A GPU set predicate instruction (setp in [Figure 4.17](#_bookmark187)) evaluates the conditional part of the IF statement. The PTX branch instruction then depends on that predicate. If the PTX assembler generates predicated instructions with no GPU branch instructions, it uses a per-lane predicate register to enable or disable each SIMD Lane for each instruction. The SIMD instructions in the threads inside the THEN part of the IF statement broadcast operations to all the SIMD Lanes. Those lanes with the predicate set to 1 perform the operation and store the result, and the other SIMD Lanes don’t perform an operation or store a result. For the ELSE statement, the instructions use the complement of the predicate (relative to the THEN statement), so the SIMD Lanes that were idle now perform the operation and store the result while their formerly active siblings don’t. At the end of the ELSE statement, the instructions are unpredicated so the original computation can proceed. Thus, for equal length paths, an IF-THEN-ELSE operates at 50% efficiency or less.

> GPU 设置谓词指令（图 4.17]（#\_ bookmark187）中的 SETP）评估 IF 语句的条件部分。然后，PTX 分支指令取决于该谓词。如果 PTX 汇编器在没有 GPU 分支指令的情况下生成了预测指令，则它使用每条谓词寄存器来启用或禁用每个指令的每个 SIMD 泳道。IF 语句广播操作的一部分内部的线程中的 SIMD 指令向所有 SIMD 车道。那些具有谓词设置为 1 的车道执行操作并存储结果，而其他 SIMD 车道则无法执行操作或存储结果。对于其他语句，指令使用谓词的补充（相对于当时的语句），因此，空闲的 SIMD 车道现在执行操作并存储结果，而其以前活跃的兄弟姐妹则没有。在 Else 语句的结尾，指令未预言，因此可以进行原始计算。因此，对于相等的长度路径，IF-THE-ELSE 的效率为 50％或更低。

IF statements can be nested, thus the use of a stack, and the PTX assembler typically generates a mix of predicated instructions and GPU branch and special synchronization instructions for complex control flow. Note that deep nesting can mean that most SIMD Lanes are idle during execution of nested conditional statements. Thus, doubly nested IF statements with equal-length paths run at 25% efficiency, triply nested at 12.5% efficiency, and so on. The analogous case would be a vector processor operating where only a few of the mask bits are ones.

> 如果语句可以嵌套，因此使用堆栈，而 PTX 汇编器通常会生成谓词指令和 GPU 分支的混合，以及用于复杂控制流的特殊同步指令。请注意，深嵌套可能意味着在执行嵌套条件语句时，大多数 SIMD 车道都是空闲的。因此，如果具有相等长度路径的语句以 25％的效率运行，三倍以 12.5％的效率嵌套，则双重嵌套。类似的情况将是一个矢量处理器操作，其中只有少数掩模位是一个。

Dropping down a level of detail, the PTX assembler sets a “branch synchronization” marker on appropriate conditional branch instructions that pushes the current active mask on a stack inside each SIMD Thread. If the conditional branch diverges (some lanes take the branch but some fall through), it pushes a stack entry and sets the current internal active mask based on the condition. A branch synchronization marker pops the diverged branch entry and flips the mask bits before the ELSE portion. At the end of the IF statement, the PTX assembler adds another branch synchronization marker that pops the prior active mask off the stack into the current active mask.

> 删除一定程度的细节，PTX 汇编器在适当的条件分支指令上设置了“分支同步”标记，该指令将当前的活动掩码推在每个 SIMD 线程内的堆栈上。如果条件分支有所不同（某些车道占据分支，但有些车道掉落），它将按下堆栈条目并根据条件设置当前的内部活动掩码。分支同步标记弹出散开的分支条目，并在其他部分之前翻转掩模位。在 IF 语句的末尾，PTX 汇编器添加了另一个分支同步标记，该标记将堆栈从堆栈中弹出到当前的活动掩码中。

If all the mask bits are set to 1, then the branch instruction at the end of the THEN skips over the instructions in the ELSE part. There is a similar optimization for the THEN part in case all the mask bits are 0 because the conditional branch jumps over the THEN instructions. Parallel IF statements and PTX branches often use branch conditions that are unanimous (all lanes agree to follow the same path) such that the SIMD Thread does not diverge into a different individual lane control flow. The PTX assembler optimizes such branches to skip over blocks of instructions that are not executed by any lane of a SIMD Thread. This optimization is useful in conditional error checking, for example, where the test must be made but is rarely taken.

> 如果将所有掩码位设置为 1，则在然后跳过 Esson 部分的指令时的分支指令。如果所有蒙版位均为 0，则对当时的部分也有类似的优化，因为条件分支跳过当时的指令。平行如果语句和 PTX 分支通常使用一致的分支条件（所有车道都同意遵循相同的路径），以使 SIMD 线程不会偏离其他单独的车道控制流。PTX 汇编器优化了此类分支，以跳过 SIMD 线程任何车道执行的指令块。该优化在有条件错误检查中很有用，例如，必须进行测试但很少进行测试。

The code for a conditional statement similar to the one in [Section 4.2](#vector-architecture) is

> 有条件语句的代码类似于[4.2]（＃vector-Architecture）中的代码

This IF statement could compile to the following PTX instructions (assuming that R8 already has the scaled thread ID), with _\*Push_, _\*Comp_, _\*Pop_ indicating the branch synchronization markers inserted by the PTX assembler that push the old mask, complement the current mask, and pop to restore the old mask:

> 如果语句可以编译为以下 PTX 指令（假设 R8 已经具有缩放线程 ID），则使用* \*push*，_ \*comp_，* \*pop*表示由 PTX 汇编器插入的分支同步标记，以推动 PTX 汇编器旧面具，补充当前面具，然后弹出以恢复旧面具：

Once again, normally all instructions in the IF-THEN-ELSE statement are executed by a SIMD Processor. It’s just that only some of the SIMD Lanes are enabled for the THEN instructions and some lanes for the ELSE instructions. As previously mentioned, in the surprisingly common case that the individual lanes agree on the predicated branch—such as branching on a parameter value that is the same for all lanes so that all active mask bits are 0s or all are 1s—the branch skips the THEN instructions or the ELSE instructions.

> 再一次，通常在 if-then-else 语句中的所有指令均由 SIMD 处理器执行。只是仅启用了某些 SIMD 车道，用于当时的说明和一些指令的车道。如前所述，在令人惊讶的常见情况下，单个车道在谓语分支上一致 - 例如，在所有车道上都相同的参数值分支，以使所有活动掩码位均为 0 或全部为 1，分支是 1s 然后说明或其他说明。

This flexibility makes it appear that an element has its own program counter; however, in the slowest case, only one SIMD Lane could store its result every 2 clock cycles, with the rest idle. The analogous slowest case for vector architectures is operating with only one mask bit set to 1. This flexibility can lead naive GPU programmers to poor performance, but it can be helpful in the early stages of program development. Keep in mind, however, that the only choice for a SIMD Lane in a clock cycle is to perform the operation specified in the PTX instruction or be idle; two SIMD Lanes cannot simultaneously execute different instructions.

> 这种灵活性使元素似乎具有自己的程序计数器。但是，在最慢的情况下，只有一个 SIMD 巷可以在其余空闲的情况下每 2 个时钟周期存储每 2 个时钟周期。矢量体系结构最慢的情况下，只有一个面具位设置为 1。这种灵活性可以使天真的 GPU 程序员的性能差，但在程序开发的早期阶段可能会有所帮助。但是请记住，在时钟周期中，SIMD 巷的唯一选择是执行 PTX 指令中指定的操作或闲置。两个 SIMD 车道不能同时执行不同的说明。

This flexibility also helps explain the name _CUDA Thread_ given to each element in a thread of SIMD instructions, because it gives the illusion of acting independently. A naive programmer may think that this thread abstraction means GPUs handle conditional branches more gracefully. Some threads go one way, the rest go another, which seems true as long as you’re not in a hurry. Each CUDA Thread is either executing the same instruction as every other thread in the Thread Block or it is idle. This synchronization makes it easier to handle loops with conditional branches because the mask capability can turn off SIMD Lanes and it detects the end of the loop automatically.

> 此灵活性还有助于解释 SIMD 指令线程中给出每个元素的名称 *cuda thread*，因为它给出了独立行动的幻想。幼稚的程序员可能会认为此线程抽象意味着 GPU 的手柄更优雅地处理条件分支。有些线程采取一种方式，其余的是另一种方式，只要您不着急，这似乎是正确的。每个 CUDA 线程要么与线程块中的其他每个线程执行相同的指令，要么是空闲的。这种同步使使用条件分支的循环更加容易，因为掩模功能可以关闭 SIMD 车道，并且可以自动检测到环路的末端。

The resulting performance sometimes belies that simple abstraction. Writing programs that operate SIMD Lanes in this highly independent MIMD mode is like writing programs that use lots of virtual address space on a computer with a smaller physical memory. Both are correct, but they may run so slowly that the programmer will not be pleased with the result.

> 由此产生的表演有时会掩盖简单的抽象。在此高度独立的 MIMD 模式下编写操作 SIMD 车道的程序就像编写程序，这些程序在具有较小物理内存的计算机上使用大量虚拟地址空间。两者都是正确的，但是它们的运行速度可能很慢，以至于程序员对结果不满意。

Conditional execution is a case where GPUs do in runtime hardware what vector architectures do at compile time. Vector compilers do a double IF-conversion, generating four different masks. The execution is basically the same as GPUs, but there are some more overhead instructions executed for vectors. Vector architectures have the advantage of being integrated with a scalar processor, allowing them to avoid the time for the 0 cases when they dominate a calculation. Although it will depend on the speed of the scalar processor versus the vector processor, the crossover point when it’s better to use scalar might be when less than 20% of the mask bits are 1s. One optimization available at runtime for GPUs, but not at compile time for vector architectures, is to skip the THEN or ELSE parts when mask bits are all 0s or all 1s.

> 有条件执行是 GPU 在运行时硬件中执行的一种情况，在编译时进行了矢量体系结构。向量编译器进行双重交流，生成四个不同的掩码。执行基本上与 GPU 相同，但是对于向量执行了更多的开销说明。向量体系结构具有与标量处理器集成在一起的优点，从而使它们避免在计算统治时的 0 情况下的时间。尽管这将取决于标量处理器与矢量处理器的速度，但使用标量的交叉点可能是少于 20％的掩码钻头为 1。GPU 在运行时可用的一种优化，但在矢量体系结构的编译时间没有可用，是在掩码位均为 0 或全部 1s 时跳过当时的零件。

Thus the efficiency with which GPUs execute conditional statements comes down to how frequently the branches will diverge. For example, one calculation of eigenvalues has deep conditional nesting, but measurements of the code show that around 82% of clock cycle issues have between 29 and 32 out of the 32 mask bits set to 1, so GPUs execute this code more efficiently than one might expect.

> 因此，GPU 执行条件语句的效率取决于分支的差异。例如，特征值的一个计算具有深度的条件嵌套，但是该法规的测量表明，在设置为 1 的 32 个蒙版钻头中，大约 82％的时钟周期问题在 29 至 32 之间，因此 GPU 比一个比一个更有效地执行此代码。可能期望。

Note that the same mechanism handles the strip-mining of vector loops—when the number of elements doesn’t perfectly match the hardware. The example at the beginning of this section shows that an IF statement checks to see if this SIMD Lane element number (stored in R8 in the preceding example) is less than the limit (i &lt; n), and it sets masks appropriately.

> 请注意，相同的机制可以处理矢量循环的剥离，当时元素的数量与硬件不完全匹配。本节开头的示例表明，IF 语句检查以查看此 SIMD 车道元素号（以前示例中的 R8 存储）是否小于限制（I＆lt; n），并且适当地设置了掩码。

### NVIDIA GPU Memory Structures

> ### nvidia gpu 内存结构

[Figure 4.18](#_bookmark188) shows the memory structures of an NVIDIA GPU. Each SIMD Lane in a multithreaded SIMD Processor is given a private section of off-chip DRAM, which we call the _private memory_. It is used for the stack frame, for spilling registers, and for private variables that don’t fit in the registers. SIMD Lanes do _not_ share private memories. GPUs cache this private memory in the L1 and L2 caches to aid register spilling and to speed up function calls.

> [图 4.18]（#_ bookmark188）显示了 NVIDIA GPU 的内存结构。为多线程 SIMD 处理器中的每个 SIMD 车道都有一个非芯片 DRAM 的私有部分，我们将其称为\_private Memory_。它用于堆栈框架，用于溢出寄存器以及不适合寄存器的私人变量。SIMD LANES DO *NOT* 共享私人记忆。GPU 缓存在 L1 和 L2 缓存中的此私人内存，以帮助注册溢出并加快功能呼叫。

We call the on-chip memory that is local to each multithreaded SIMD Processor _local memory_. It is a small scratchpad memory with low latency (a few dozen clocks) and high bandwidth (128 bytes/clock) where the programmer can store data that needs to be reused, either by the same thread or another thread in the same

> 我们调用每个多线程 SIMD 处理器*LOCAL MOMENE *的片段内存。这是一个小的刮擦记忆，具有低延迟（几十个时钟）和高带宽（128 个字节/时钟），程序员可以存储需要重复使用的数据，无论是同一线程还是在同一线程中的另一个线程

![](./media/image263.png)Thread block

> ！[]（./媒体/image263.png）线程块

![](./media/image264.png)Grid 0 Sequence

> ！[]（./媒体/image264.png）网格 0 序列

<img src="./media/image265.png" style="width:1.49834in;height:0.47917in" />

> <img src =“ ./媒体/image265.png” style =“宽度：1.49834in;高度：0.47917in”/>

![](./media/image266.png)

> ！[]（./ Media/image266.png）

Figure 4.18 GPU memory structures. GPU memory is shared by all Grids (vectorized loops), local memory is shared by all threads of SIMD instructions within a Thread Block (body of a vectorized loop), and private memory is private to a single CUDA Thread. Pascal allows preemption of a Grid, which requires that all local and private memory be able to be saved in and restored from global memory. For completeness sake, the GPU can also access CPU memory via the PCIe bus. This path is commonly used for a final result when its address is in host memory. This option eliminates a final copy from the GPU memory to the host memory.

> 图 4.18 GPU 内存结构。GPU 存储器均由所有网格共享（矢量化循环），本地内存均由线程块中的所有 SIMD 指令的所有线程共享（矢量化环的主体），并且私有内存是单个 CUDA 线程的私有内存。帕斯卡（Pascal）允许先发出网格，这要求所有本地和私人内存都能从全球内存中保存并恢复。为了完整的清晰，GPU 还可以通过 PCIE 总线访问 CPU 内存。当其地址在主机内存中时，该路径通常用于最终结果。此选项将最终副本从 GPU 内存到主机内存。

Thread Block. Local memory is limited in size, typically to 48 KiB. It carries no state between Thread Blocks executed on the same processor. It is shared by the SIMD Lanes within a multithreaded SIMD Processor, but this memory is not shared between multithreaded SIMD Processors. The multithreaded SIMD Processor dynamically allocates portions of the local memory to a Thread Block when it creates the Thread Block, and frees the memory when all the threads of the Thread Block exit. That portion of local memory is private to that Thread Block. Finally, we call the off-chip DRAM shared by the whole GPU and all Thread

> 线块。本地内存的大小限制，通常为 48 KIB。它在同一处理器上执行的线程块之间没有任何状态。它是由 SIMD 车道共享的多线程 SIMD 处理器中的，但是此内存并未在多线程 SIMD 处理器之间共享。多线程 SIMD 处理器在创建线程块时，将本地内存的一部分动态分配给线程块，并在线程块的所有线程退出时释放内存。本地内存的那部分是该线程块的私人。最后，我们称整个 GPU 和所有线程共享的片外 DRAM

Blocks _GPU Memory_. Our vector multiply example used only GPU Memory.

> blocks _gpu memory_。我们的矢量乘法示例仅使用 GPU 内存。

The system processor, called the _host_, can read or write GPU Memory. Local memory is unavailable to the host, as it is private to each multithreaded SIMD Processor. Private memories are unavailable to the host as well.

> 系统处理器称为 *host*，可以读取或编写 GPU 内存。主机不可用的本地内存，因为它是每个多线程 SIMD 处理器的私有内存。主机也无法获得私人记忆。

Rather than rely on large caches to contain the whole working sets of an application, GPUs traditionally use smaller streaming caches and, because their working sets can be hundreds of megabytes, rely on extensive multithreading of threads of SIMD instructions to hide the long latency to DRAM. Given the use of multithreading to hide DRAM latency, the chip area used for large L2 and L3 caches in system processors is spent instead on computing resources and on the large number of registers to hold the state of many threads of SIMD instructions. In contrast, as mentioned, vector loads and stores amortize the latency across many elements because they pay the latency only once and then pipeline the rest of the accesses.

> GPU 传统上使用较小的流式缓存，而不是依靠大型缓存来包含整个应用程序集，因为它们的工作组可能是数百个兆字节，而是依靠大量的 SIMD 说明线程的多线程来隐藏长延迟来掩盖 DRAM DRAM DRAM DRAM DRAM DRAM。鉴于使用多线程来隐藏 DRAM 延迟，因此将用于系统处理器中大的 L2 和 L3 缓存的芯片区域用于计算资源和大量寄存器上，以保留 SIMD 指令的许多线程的状态。相反，如前所述，向量负载和存储会摊销许多元素的延迟，因为它们仅支付一次延迟，然后将其余的访问权限输送。

Although hiding memory latency behind many threads was the original philosophy of GPUs and vector processors, all recent GPUs and vector processors have caches to reduce latency. The argument follows Little’s Law from queuing theory: the longer the latency, the more threads need to run during a memory access, which in turn requires more registers. Thus GPU caches are added to lower average latency and thereby mask potential shortages of the number of registers.

> 尽管许多线程背后的记忆潜伏期是 GPU 和向量处理器的原始理念，但所有最近的 GPU 和矢量处理器都有缓存以减少延迟。该论点遵循排队理论的 Little 定律：延迟的时间越长，在内存访问期间需要运行的线程越多，这又需要更多的寄存器。因此，GPU 缓存添加到较低的平均延迟中，从而掩盖了寄存器数量的潜在短缺。

To improve memory bandwidth and reduce overhead, as mentioned, PTX data transfer instructions in cooperation with the memory controller coalesce individual parallel thread requests from the same SIMD Thread together into a single memory block request when the addresses fall in the same block. These restrictions are placed on the GPU program, somewhat analogous to the guidelines for system processor programs to engage hardware prefetching (see [Chapter 2](#_bookmark46)). The GPU memory controller will also hold requests and send ones together to the same open page to improve memory bandwidth (see [Section 4.6](#cross-cutting-issues-1)). [Chapter 2](#_bookmark46) describes DRAM in sufficient detail for readers to understand the potential benefits of grouping related addresses.

> 为了改善内存带宽并减少开销，如前所述，PTX 数据传输指令与内存控制器合并合并单个并行线程请求从同一 SIMD 线程一起将地址落入同一块时，将其从同一 SIMD 线程一起进入单个内存块请求。这些限制放在 GPU 程序上，有点类似于系统处理器程序的准则，以参与硬件预取用（请参阅[第 2 章]（#_ bookmark46））。GPU 内存控制器还将保存请求并将其发送到同一打开页面以改善内存带宽（请参阅[4.6]（＃cross-Cutting-issues-1））。[第 2 章]（#_ bookmark46）描述了 DRAM 足够详细地说明读者了解与分组相关地址的潜在好处。

### Innovations in the Pascal GPU Architecture

> ### Pascal GPU 架构中的创新

The multithreaded SIMD Processor of Pascal is more complicated than the simplified version in [Figure 4.20](#_bookmark190). To increase hardware utilization, each SIMD Processor has two SIMD Thread Schedulers, each with multiple instruction dispatch units (some GPUs have four thread schedulers). The dual SIMD Thread Scheduler selects two threads of SIMD instructions and issues one instruction from each to two sets of 16 SIMD Lanes, 16 load/store units, or 8 special function units. With multiple execution units available, two threads of SIMD instructions are scheduled each clock cycle, allowing 64 lanes to be active. Because the threads are independent, there is no need to check for data dependences in the instruction stream. This innovation would be analogous to a multithreaded vector processor that can issue vector instructions from two independent threads. [Figure 4.19](#_bookmark189) shows the Dual Scheduler issuing instructions, and [Figure 4.20](#_bookmark190) shows the block diagram of the multithreaded SIMD Processor of a Pascal GP100 GPU.

> Pascal 的多线程 SIMD 处理器比[图 4.20]中的简化版本（#_ bookmark190）中的简化版本更为复杂。为了增加硬件利用率，每个 SIMD 处理器都有两个 SIMD 线程调度程序，每个调度程序都有多个指令调度单元（某些 GPU 具有四个线程调度程序）。Dual Simd 线程调度程序选择 SIMD 指令的两个线程，并从每个指令中发出一组指令，到两组 16 个 SIMD 车道，16 个负载/存储单元或 8 个特殊功能单元。有了多个可用的执行单元，每个时钟周期都安排了两个 SIMD 指令的线程，使 64 个车道处于活动状态。由于线程是独立的，因此无需检查指令流中的数据依赖性。该创新将类似于可以从两个独立线程发出向量指令的多线程量处理器。[图 4.19]（#_ bookmark189）显示了双调度程序发行说明，[图 4.20]（#\_ bookmark190）显示了 Pascal GP100 GPU 的多线 Readed Simd 处理器的块图。

Each new generation of GPU typically adds some new features that increase performance or make it easier for programmers. Here are the four main innovations of Pascal:

> 每个新一代 GPU 通常都会添加一些新功能，以提高性能或使程序员更容易。这是帕斯卡尔的四个主要创新：

Figure 4.19 Block diagram of Pascal’s dual SIMD Thread scheduler. Compare this design to the single SIMD Thread design in [Figure 4.16](#_bookmark186).

> 图 4.19 Pascal 双重 SIMD 线程调度程序的框图。将此设计与[图 4.16]（#\_ Bookmark186）中的单个 SIMD 线程设计进行比较。

_Fast single-precision, double-precision, and half-precision floating-point arithmetic_—Pascal GP100 chip has significant floating-point performance in three sizes, all part of the IEEE standard for floating-point. The singleprecision floating-point of the GPU runs at a peak of 10 TeraFLOP/s. Double-precision is roughly half-speed at 5 TeraFLOP/s, and half-precision is about double-speed at 20 TeraFLOP/s when expressed as 2-element vectors. The atomic memory operations include floating-point add for all three sizes. Pascal GP100 is the first GPU with such high performance for half-precision.

> _fast 单精制，双重精确和半精度的浮点算点_-Pascal GP100 芯片具有三种尺寸的浮点功能，这是 IEEE 浮点数标准的所有部分。GPU 的单个浮点数以 10 teraflop/s 的峰值运行。在 5 teraflop/s 的情况下，双速度大约是半速，而将半速度表示为 20 teraflop/s 的双速，当表示为 2 元素矢量时。原子记忆操作包括所有三种尺寸的浮点添加。Pascal GP100 是第一个具有这样高表现的 GPU。

_High-bandwidth memory_—The next innovation of the Pascal GP100 GPU is the use of stacked, high-bandwidth memory (_HBM2_). This memory has a wide bus with 4096 data wires running at 0.7 GHz offering a peak bandwidth of 732 GB/s, which is more than twice as fast as previous GPUs.

> _-High-Bandwidth Memory_ - Pascal GP100 GPU 的下一个创新是使用堆叠的高带宽内存（_HBM2_）。该内存的总线宽，4096 个数据线以 0.7 GHz 运行，峰带宽为 732 GB/s，其速度是以前的 GPU 的两倍以上。

_High-speed chip-to-chip interconnect_—Given the coprocessor nature of GPUs, the PCI bus can be a communications bottleneck when trying to use multiple GPUs with one CPU. Pascal GP100 introduces the _NVLink_ communications channel that supports data transfers of up to 20 GB/s in each direction. Each GP100 has 4 NVLink channels, providing a peak aggregate chip-tochip bandwidth of 160 GB/s per chip. Systems with 2, 4, and 8 GPUs are available for multi-GPU applications, where each GPU can perform load, store, and atomic operations to any GPU connected by NVLink. Additionally, an NVLink channel can communicate with the CPU in some cases. For example, the IBM Power9 CPU supports CPU-GPU communication. In this chip, NVLink provides a coherent view of memory between all GPUs and CPUs connected together. It also provides cache-to-cache communication instead of memory-to-memory communication.

> _高速芯片到芯片互连_-赋予 GPU 的协处理器性质，当试图与一个 CPU 一起使用多个 GPU 时，PCI 总线可以成为通信瓶颈。Pascal GP100 引入了 *NVLINK* 通信通道，该通道支持每个方向上最多 20 GB/s 的数据传输。每个 GP100 都有 4 个 NVLINK 通道，每芯片提供 160 GB/s 的峰值聚集芯片芯片带宽。具有 2、4 和 8 GPU 的系统可用于多 GPU 应用，每个 GPU 都可以在 NVLink 连接的任何 GPU 上执行负载，存储和原子操作。此外，在某些情况下，NVLink 通道可以与 CPU 通信。例如，IBM Power9 CPU 支持 CPU-GPU 通信。在此芯片中，NVLink 提供了连接在一起的所有 GPU 和 CPU 之间的记忆的连贯视图。它还提供缓存到缓存通信，而不是内存到内存通信。

Figure 4.20 Block diagram of the multithreaded SIMD Processor of a Pascal GPU. Each of the 64 SIMD Lanes (cores) has a pipelined floating-point unit, a pipelined integer unit, some logic for dispatching instructions and operands to these units, and a queue for holding results. The 64 SIMD Lanes interact with 32 double-precision ALUs (DP units) that perform 64-bit floating-point arithmetic, 16 load-store units (LD/STs), and 16 special function units (SFUs) that calculate functions such as square roots, reciprocals, sines, and cosines.

> 图 4.20 Pascal GPU 的多线程 SIMD 处理器的框图。64 个 SIMD 车道中的每一个（内核）都有一个管道的浮点单元，一个管道的整数单元，一些用于将指令和操作数分配给这些单元的逻辑，以及用于保持结果的队列。64 个 SIMD 车道与执行 64 位浮点算术的 32 个双精度 ALU（DP 单元）相互作用根，互惠，罪恶和余弦。

_Unified virtual memory and paging support_—The Pascal GP100 GPU adds page-fault capabilities within a unified virtual address space. This feature allows a single virtual address for every data structure that is identical across all the GPUs and CPUs in a single system. When a thread accesses an address that is remote, a page of memory is transferred to the local GPU for subsequent use. Unified memory simplifies the programming model by providing demand paging instead of explicit memory copying between the CPU and GPU or between GPUs. It also allows allocating far more memory than exists on the GPU to solve problems with large memory requirements. As with any virtual memory system, care must be taken to avoid excessive page movement.

> _unified 虚拟内存和分页支持_- Pascal GP100 GPU 在统一的虚拟地址空间中添加了页面故障功能。此功能允许每个数据结构的单个虚拟地址，该地址在单个系统中的所有 GPU 和 CPU 中相同。当线程访问远程地址时，将一页内存的页面传输到本地 GPU 以备后续使用。统一内存通过提供需求分页而不是在 CPU 和 GPU 之间或 GPU 之间的明确内存复制来简化编程模型。它还允许分配比 GPU 上的内存更多的内存，以解决具有较大内存需求的问题。与任何虚拟内存系统一样，必须注意避免过多的页面移动。

### Similarities and Differences Between Vector Architectures and GPUs

> ###向量体系结构和 GPU 之间的相似性和差异

As we have seen, there really are many similarities between vector architectures and GPUs. Along with the quirky jargon of GPUs, these similarities have contributed to the confusion in architecture circles about how novel GPUs really are. Now that you’ve seen what is under the covers of vector computers and GPUs, you can appreciate both the similarities and the differences. Because both architectures are designed to execute data-level parallel programs, but take different paths, this comparison is in depth in order to provide a better understanding of what is needed for DLP hardware. [Figure 4.21](#_bookmark191) shows the vector term first and then the closest equivalent in a GPU.

> 如我们所见，向量体系结构和 GPU 之间确实有许多相似之处。与 GPU 的古怪行话一起，这些相似之处也导致了建筑圈中关于新颖 GPU 的真实情况的混乱。现在，您已经看到了矢量计算机和 GPU 的封面下的内容，您可以欣赏相似之处和差异。由于两种体系结构均设计用于执行数据级并行程序，但要采用不同的路径，因此该比较是深入的，以便更好地了解 DLP 硬件所需的内容。[图 4.21]（#\_ bookmark191）首先显示矢量术语，然后显示 GPU 中最接近的等效词。

A SIMD Processor is like a vector processor. The multiple SIMD Processors in GPUs act as independent MIMD cores, just as many vector computers have multiple vector processors. This view will consider the NVIDIA Tesla P100 as a 56-core machine with hardware support for multithreading, where each core has 64 lanes. The biggest difference is multithreading, which is fundamental to GPUs and missing from most vector processors.

> SIMD 处理器就像矢量处理器。GPU 中的多个 SIMD 处理器充当独立的 MIMD 核心，就像许多矢量计算机具有多个向量处理器一样。该视图将 NVIDIA TESLA P100 视为一台 56 核机器，具有用于多线程的硬件支持，每个核心都有 64 个车道。最大的区别是多线程，这对于 GPU 至关重要，大多数矢量处理器缺少。

Looking at the registers in the two architectures, the RV64V register file in our implementation holds entire vectors—that is, a contiguous block of elements. In contrast, a single vector in a GPU will be distributed across the registers of all SIMD Lanes. A RV64V processor has 32 vector registers with perhaps 32 elements, or 1024 elements total. A GPU thread of SIMD instructions has up to 256 registers with 32 elements each, or 8192 elements. These extra GPU registers support multithreading.

> 查看两个架构中的寄存器，我们实现中的 RV64V 寄存器文件保存了整个向量，即连续的元素块。相比之下，GPU 中的单个向量将分布在所有 SIMD 车道的寄存器中。RV64V 处理器具有 32 个矢量寄存器，其中约为 32 个元素，或总共 1024 个元素。SIMD 指令的 GPU 线程最多有 256 个寄存器，每个寄存器具有 32 个元素，或 8192 个元素。这些额外的 GPU 寄存器支持多线程。

[Figure 4.22](#_bookmark192) is a block diagram of the execution units of a vector processor on the left and a multithreaded SIMD Processor of a GPU on the right. For pedagogic purposes, we assume the vector processor has four lanes and the multithreaded SIMD Processor also has four SIMD Lanes. This figure shows that the four SIMD Lanes act in concert much like a four-lane vector unit, and that a SIMD Processor acts much like a vector processor.

> [图 4.22]（#\_ bookmark192）是左侧矢量处理器的执行单元的框图，右侧是 GPU 的多线程 SIMD 处理器。出于教学目的，我们假设矢量处理器有四个车道，并且多线程 SIMD 处理器还具有四个 SIMD 车道。该图表明，四个 SIMD 车道的共同作用类似于四车道矢量单元，而 SIMD 处理器的作用很像矢量处理器。

In reality, there are many more lanes in GPUs, so GPU “chimes” are shorter. While a vector processor might have 2 to 8 lanes and a vector length of, say, 32— making a chime 4 to 16 clock cycles—a multithreaded SIMD Processor might have 8 or 16 lanes. A SIMD Thread is 32 elements wide, so a GPU chime would just be 2 or 4 clock cycles. This difference is why we use “SIMD Processor” as the more descriptive term because it is closer to a SIMD design than it is to a traditional vector processor design.

> 实际上，GPU 中还有更多的车道，因此 GPU“钟声”较短。矢量处理器可能具有 2 到 8 个车道，例如 32 个矢量长度（制作 4 至 16 个时钟周期），一个多线 SIMD 处理器可能具有 8 或 16 个车道。SIMD 线程为 32 个元素，因此 GPU 铃声仅为 2 或 4 个时钟周期。这种差异就是为什么我们使用“ SIMD 处理器”作为更具描述性的术语的原因，因为它比传统的矢量处理器设计更接近 SIMD 设计。

All GPU loads and stores are gather and scatter, in that each SIMD Lane sends a unique address. It’s up to the GPU Coalescing Unit to get unit-stride performance when addresses from the SIMD Lanes allow it

> 所有的 GPU 负载和商店都是收集和分散的，因为每个 Simd Lane 都会发送一个唯一的地址。当 Simd Lanes 的地址允许它时

Vector mask registers are explicitly part of the architectural state, while GPU mask registers are internal to the hardware. The GPU conditional hardware adds a new feature beyond predicate registers to manage masks dynamically

> 向量蒙版寄存器明确是体系结构状态的一部分，而 GPU 蒙版寄存器则是硬件的内部。GPU 条件硬件添加了一个新功能，超出了谓词寄存器，以动态管理面具

These are similar, but SIMD Processors tend to have many lanes, taking a few clock cycles per lane to complete a vector, while vector architectures have few lanes and take many cycles to complete a vector. They are also multithreaded where vectors usually are not

> 它们是相似的，但是 SIMD 处理器往往有许多车道，每条车道需要几个时钟周期才能完成矢量，而向量体系结构的车道很少，并且需要许多周期才能完成矢量。它们也是矢量通常不是的多线程

Figure 4.21 GPU equivalent to vector terms.

> 图 4.21 GPU 等效于矢量项。

![](./media/image274.png)

> ！[]（./ Media/Image274.png）

![](./media/image281.png)

> ！[]（./ Media/image281.png）

<img src="./media/image290.png" style="width:0.15454in;height:0.11687in" />

> <img src =“ ./媒体/image290.png” style =“宽度：0.15454in;高度：0.11687in”/>

<img src="./media/image291.png" style="width:0.15268in;height:0.11458in" />

> <img src =“ ./媒体/image291.png” style =“宽度：0.15268in;高度：0.11458in”/>

<img src="./media/image292.png" style="width:0.15454in;height:0.11687in" />

> <img src =“ ./媒体/image292.png” style =“宽度：0.15454in;高度：0.11687in”/>

<img src="./media/image290.png" style="width:0.15454in;height:0.11687in" />

> <img src =“ ./媒体/image290.png” style =“宽度：0.15454in;高度：0.11687in”/>

![](./media/image290.png)

> ！[]（./ Media/image290.png）

<img src="./media/image293.png" style="width:0.15326in;height:0.1275in" />

> <img src =“ ./媒体/image293.png” style =“宽度：0.15326in;高度：0.1275in”/>

<img src="./media/image294.png" style="width:0.1527in;height:0.11458in" />

> <img src =“ ./媒体/image294.png” style =“宽度：0.1527in;高度：0.11458in”/>

Figure 4.22 A vector processor with four lanes on the left and a multithreaded SIMD Processor of a GPU with four SIMD Lanes on the right. (GPUs typically have 16 or 32 SIMD Lanes.) The Control Processor supplies scalar operands for scalar-vector operations, increments addressing for unit and nonunit stride accesses to memory, and performs other accounting-type operations. Peak memory performance occurs only in a GPU when the Address Coalescing Unit can discover localized addressing. Similarly, peak computational performance occurs when all internal mask bits are set identically. Note that the SIMD Processor has one PC per SIMD Thread to help with multithreading.

> 图 4.22 矢量处理器，左侧有四个车道和一个 GPU 的多线程 SIMD 处理器，右侧有四个 SIMD 车道。（GPU 通常有 16 或 32 个 SIMD 泳道。）控制处理器提供标量矢量操作的标量操作数，单位的地址和非单元踏板访问存储器的增量，并执行其他会计操作。峰值记忆性能仅在地址合并单元可以发现本地化的地址时发生在 GPU 中。同样，当所有内部掩码位均设置相同时，就会发生峰值计算性能。请注意，SIMD 处理器有一个 PC 每个 SIMD 线程可帮助多线程。

The closest GPU term to a vectorized loop is Grid, and a PTX instruction is the closest to a vector instruction because a SIMD Thread broadcasts a PTX instruction to all SIMD Lanes.

> 最接近矢量循环的 GPU 术语是网格，并且 PTX 指令是最接近矢量指令的指令，因为 SIMD 线程将 PTX 指令广播到所有 SIMD 车道。

With respect to memory access instructions in the two architectures, all GPU loads are gather instructions and all GPU stores are scatter instructions. If data addresses of CUDA Threads refer to nearby addresses that fall into the same cache/memory block at the same time, the Address Coalescing Unit of the GPU will ensure high memory bandwidth. The _explicit_ unit-stride load and store instructions of vector architectures versus the _implicit_ unit stride of GPU programming is why writing efficient GPU code requires that programmers think in terms of SIMD operations, even though the CUDA programming model looks like MIMD. Because CUDA Threads can generate their own addresses, strided as well as gather-scatter, addressing vectors are found in both vector architectures and GPUs.

> 关于两个体系结构中的内存访问指令，所有 GPU 负载均为收集说明，所有 GPU 商店均为散点说明。如果 CUDA 线程的数据地址是指同时落入同一缓存/内存块的附近地址，则 GPU 的地址合并单元将确保高内存带宽。*EXPLITIC* 单位式负载和存储向量体系结构的指令与 GPU 编程的 *implitic* 单位步幅是为什么编写有效的 GPU 代码要求程序员在 SIMD 操作方面进行思考，即使 CUDA 编程模型看起来像 MIMD。由于 CUDA 线程可以生成自己的地址，也可以艰难地划分，因此在矢量体系结构和 GPU 中都可以找到向量。

As we mentioned several times, the two architectures take very different approaches to hiding memory latency. Vector architectures amortize it across all the elements of the vector by having a deeply pipelined access, so you pay the latency only once per vector load or store. Therefore vector loads and stores are like a block transfer between memory and the vector registers. In contrast, GPUs hide memory latency using multithreading. (Some researchers are investigating adding multithreading to vector architectures to try to capture the best of both worlds.)

> 正如我们几次提到的那样，这两个体系结构采用了非常不同的方法来隐藏内存延迟。向量体系结构通过深入管道访问来使其在矢量的所有元素中摊销，因此您只需支付每个向量负载或存储的延迟一次。因此，向量负载和存储就像内存和向量寄存器之间的块传输一样。相反，GPU 使用多线程隐藏内存延迟。（一些研究人员正在调查将多线程添加到矢量体系结构中，以尝试捕捉两全其美。）

With respect to conditional branch instructions, both architectures implement them using mask registers. Both conditional branch paths occupy time and/or space even when they do not store a result. The difference is that the vector compiler manages mask registers explicitly in software while the GPU hardware and assembler manages them implicitly using branch synchronization markers and an internal stack to save, complement, and restore masks.

> 关于条件分支指令，两个架构都使用掩模寄存器实现它们。有条件的分支路径也占据了时间和/或空间，即使它们不存储结果。不同之处在于，矢量编译器在软件中明确管理蒙版寄存器，而 GPU 硬件和汇编器则使用分支同步标记和内部堆栈隐式管理它们，以保存，补充和还原掩码。

The Control Processor of a vector computer plays an important role in the execution of vector instructions. It broadcasts operations to all the Vector Lanes and broadcasts a scalar register value for vector-scalar operations. It also does implicit calculations that are explicit in GPUs, such as automatically incrementing memory addresses for unit-stride and nonunit-stride loads and stores. The Control Processor is missing in the GPU. The closest analogy is the Thread Block Scheduler, which assigns Thread Blocks (bodies of vector loop) to multithreaded SIMD Processors. The runtime hardware mechanisms in a GPU that both generate addresses and then discover if they are adjacent, which is commonplace in many DLP applications, are likely less power-efficient than using a Control Processor.

> 向量计算机的控制处理器在执行向量指令中起着重要作用。它将操作广播到所有矢量车道，并为矢量量表操作广播标量寄存器值。它还进行了在 GPU 中显式的隐式计算，例如自动增加单位式和非单位式载荷和商店的内存地址。GPU 中缺少控制处理器。最接近的类比是线程块调度程序，它将线程块（向量循环的主体）分配给多线程 SIMD 处理器。GPU 中的运行时硬件机制既生成地址，然后发现它们是否相邻，这在许多 DLP 应用程序中很常见，可能比使用控制处理器的功率低。

The scalar processor in a vector computer executes the scalar instructions of a vector program; that is, it performs operations that would be too slow to do in the vector unit. Although the system processor that is associated with a GPU is the closest analogy to a scalar processor in a vector architecture, the separate address spaces plus transferring over a PCIe bus means thousands of clock cycles of overhead to use them together. The scalar processor can be slower than a vector processor for floating-point computations in a vector computer, but not by the same ratio as the system processor versus a multithreaded SIMD Processor (given the overhead).

> 向量计算机中的标量处理器执行向量程序的标量指令；也就是说，它执行的操作太慢，无法在矢量单元中执行。尽管与 GPU 关联的系统处理器是与矢量体系结构中标量处理器最接近的类比，但单独的地址空间加上 PCIE 总线上的传输意味着成千上万的开销时钟循环将它们一起使用。标量处理器可以比矢量计算机中的浮点计算的矢量处理器慢，但与系统处理器相比，与多线程 SIMD 处理器（给定开销）相同。

Therefore each “vector unit” in a GPU must do computations that you would expect to do using a scalar processor in a vector computer. That is, rather than calculate on the system processor and communicate the results, it can be faster to disable all but one SIMD Lane using the predicate registers and built-in masks and do the scalar work with one SIMD Lane. The relatively simple scalar processor in a vector computer is likely to be faster and more power-efficient than the GPU solution. If system processors and GPUs become more closely tied together in the future, it will be interesting to see if system processors can play the same role as scalar processors do for vector and multimedia SIMD architectures.

> 因此，GPU 中的每个“向量单元”必须进行您期望使用矢量计算机中标量处理器进行的计算。也就是说，与其在系统处理器上计算并传达结果，不如使用谓词寄存器和内置蒙版禁用除一个 SIMD 泳道并使用一个 SIMD 泳道进行标量工作可以更快。矢量计算机中相对简单的标量处理器可能比 GPU 解决方案更快，更有效率。如果将来系统处理器和 GPU 在将来变得更加紧密地联系在一起，那么有趣的是，系统处理器是否可以扮演与标量处理器对向量和多媒体 SIMD 体系结构相同的角色。

### Similarities and Differences Between Multimedia SIMD Computers and GPUs

> ###多媒体 SIMD 计算机和 GPU 之间的相似性和差异

At a high level, multicore computers with multimedia SIMD instruction extensions do share similarities with GPUs. [Figure 4.23](#_bookmark193) summarizes the similarities and differences.

> 在高水平上，具有多媒体 SIMD 指令扩展的多核算计算机确实与 GPU 共享相似之处。[图 4.23]（#\_ bookmark193）总结了相似性和差异。

Both are multiprocessors whose processors use multiple SIMD Lanes, although GPUs have more processors and many more lanes. Both use hardware multithreading to improve processor utilization, although GPUs have hardware support for many more threads. Both have roughly 2:1 performance ratios between peak performance of single-precision and double-precision floating-point arithmetic. Both use caches, although GPUs use smaller streaming caches, and multicore computers use large multilevel caches that try to contain whole working sets completely. Both use a 64-bit address space, although the physical main memory is much smaller in GPUs. Both support memory protection at the page level as well as demand paging, which allows them to address far more memory than they have on board.

> 两者都是多处理器，其处理器使用多个 SIMD 车道，尽管 GPU 具有更多的处理器和更多的车道。两者都使用硬件多线程来改善处理器的利用率，尽管 GPU 对更多线程具有硬件支持。两者在单精制的峰值性能和双精度浮点算术算术之间的性能比率约为 2：1。两者都使用缓存，尽管 GPU 使用较小的流式缓存，并且多计算机使用大型多级缓存，这些缓存试图完全包含整个工作组。两者都使用 64 位地址空间，尽管 GPU 中的物理主内存要小得多。两者都支持页面级别的内存保护以及需求分页，这使他们可以解决比起船上更多的内存。

In addition to the large numerical differences in processors, SIMD Lanes, hardware thread support, and cache sizes, there are many architectural differences. The scalar processor and multimedia SIMD instructions are tightly integrated in traditional computers; they are separated by an I/O bus in GPUs, and they even have separate main memories. The multiple SIMD Processors in a GPU use a single address space and can support a coherent view of all memory on some systems given support from CPU vendors (such as the IBM Power9). Unlike GPUs, multimedia SIMD instructions historically did not support gather-scatter memory accesses, which [Section 4.7](#_bookmark198) shows is a significant omission.

> 除了处理器，SIMD 车道，硬件线程支持和缓存尺寸的较大数值差异外，还有许多架构差异。标量处理器和多媒体 SIMD 指令紧密整合到传统计算机中；它们被 GPU 中的 I/O 公共汽车分开，甚至有单独的主要记忆。GPU 中的多个 SIMD 处理器使用单个地址空间，并可以在 CPU 供应商（例如 IBM Power9）的某些系统上支持所有内存的连贯视图。与 GPU 不同，历史上多媒体 SIMD 指令不支持聚集筛记忆访问，[第 4.7 节]（#\_ bookmark198 节）显示，这是一个很大的遗漏。

Figure 4.23 Similarities and differences between multicore with multimedia SIMD extensions and recent GPUs.

> 图 4.23 多核心与多媒体 SIMD 扩展和最近的 GPU 之间的相似性和差异。

### Summary

> ＃＃＃ 概括

Now that the veil has been lifted, we can see that GPUs are really just multithreaded SIMD Processors, although they have more processors, more lanes per processor, and more multithreading hardware than do traditional multicore computers. For example, the Pascal P100 GPU has 56 SIMD Processors with

> 既然已经取消了面纱，我们可以看到 GPU 实际上只是多线程 SIMD 处理器，尽管它们具有更多的处理器，每个处理器的车道更多，并且比传统的多核算计算机更多的是多线程硬件。例如，Pascal P100 GPU 具有 56 个 SIMD 处理器

64 lanes per processor and hardware support for 64 SIMD Threads. Pascal embraces instruction-level parallelism by issuing instructions from two SIMD Threads to two sets of SIMD Lanes. GPUs also have less cache memory—Pascal’s L2 cache is 4 MiB—and it can be coherent with a cooperative distant scalar processor or distant GPUs.

> 每个处理器和硬件支持 64 个 SIMD 线程的 64 个车道。帕斯卡（Pascal）通过将指令从两个 SIMD 线程发布到两组 SIMD 车道来包含指令级的并行性。GPU 也具有较少的缓存内存 - Pascal 的 L2 高速缓存为 4 MIB，并且可以与合作的远处标量处理器或远处 GPU 相干。

The CUDA programming model wraps up all these forms of parallelism around a single abstraction, the CUDA Thread. Thus the CUDA programmer can think of programming thousands of threads, although they are really executing each block of 32 threads on the many lanes of the many SIMD Processors. The CUDA programmer who wants good performance keeps in mind that these threads are organized in blocks and executed 32 at a time and that addresses need to be to adjacent addresses to get good performance from the memory system.

> CUDA 编程模型将所有这些并行形式包裹在单个抽象的 CUDA 线程周围。因此，CUDA 程序员可以考虑编程数千个线程，尽管它们确实在许多 SIMD 处理器的许多车道上确实执行了 32 个线程的每个块。想要良好性能的 CUDA 程序员请记住，这些线程是在块中组织的，并且一次执行 32 个，并且该地址需要是相邻地址，以从内存系统中获得良好的性能。

Although we’ve used CUDA and the NVIDIA GPU in this section, rest assured that the same ideas are found in the OpenCL programming language and in GPUs from other companies.

> 尽管我们在本节中使用了 CUDA 和 NVIDIA GPU，但请确保在 OpenCL 编程语言和其他公司的 GPU 中找到相同的想法。

Now that you understand better how GPUs work, we reveal the real jargon. [Figures 4.24](#_bookmark195) and [4.25](#_bookmark196) match the descriptive terms and definitions of this section with the official CUDA/NVIDIA and AMD terms and definitions. We also include the OpenCL terms. We believe the GPU learning curve is steep in part because of using terms such as “streaming multiprocessor” for the SIMD Processor, “thread processor” for the SIMD Lane, and “shared memory” for local memory— especially because local memory is _not_ shared between SIMD Processors! We hope that this two-step approach gets you up that curve quicker, even if it’s a bit indirect.

> 现在您可以更好地了解 GPU 的工作方式，我们揭示了真正的行话。[图 4.24]（#_ bookmark195）和[4.25]（#_ bookmark196）将本节的描述术语和定义与官方的 cuda/nvidia 和 AMD 术语和定义匹配。我们还包括 OpenCL 条款。我们认为，GPU 学习曲线的一部分是因为使用诸如 SIMD 处理器的“流多处理器”，SIMD 巷的“线程处理器”和本地内存的“共享内存”之类的术语，尤其是因为 local Memory *not*共享在 SIMD 处理器之间！我们希望这种两步的方法可以使您更快地提高曲线，即使有点间接。
