## Cross-Cutting Issues

> ##交叉切割问题

Because multiprocessors redefine many system characteristics (e.g., performance assessment, memory latency, and the importance of scalability), they introduce interesting design problems that cut across the spectrum, affecting both hardware and software. In this section, we give several examples related to the issue of mem- ory consistency. We then examine the performance gained when multithreading is added to multiprocessing.

> 由于多处理器重新定义了许多系统特征（例如，性能评估，内存延迟以及可扩展性的重要性），因此他们引入了有趣的设计问题，这些问题跨越了整个频谱，从而影响了硬件和软件。在本节中，我们提供了与 Mem-Ory 一致性问题有关的几个示例。然后，我们检查将多线程添加到多处理中时获得的性能。

### Compiler Optimization and the Consistency Model

> ###编译器优化和一致性模型

Another reason for defining a model for memory consistency is to specify the range of legal compiler optimizations that can be performed on shared data. In explicitly parallel programs, unless the synchronization points are clearly defined and the pro- grams are synchronized, the compiler cannot interchange a read and a write of two different shared data items because such transformations might affect the semantics of the program. This restriction prevents even relatively simple optimizations, such as register allocation of shared data, because such a process usually interchanges reads and writes. In implicitly parallelized programs—for example, those written in High Performance Fortran (HPF)—programs must be synchronized and the synchronization points are known, so this issue does not arise. Whether compilers can get significant advantage from more relaxed consistency models remains an open question, both from a research viewpoint and from a practical viewpoint, where the lack of uniform models is likely to retard progress on deploying compilers.

> 定义内存一致性模型的另一个原因是指定可以在共享数据上执行的法律编译器优化范围。在明确的并行程序中，除非明确定义同步点并且对程序进行了同步，否则编译器无法互换读取和两个不同共享数据项的写入，因为这种转换可能会影响程序的语义。这种限制甚至可以防止相对简单的优化，例如共享数据的注册分配，因为这样的过程通常会互换读取和写入。在隐式平行的程序中（例如，那些以高性能 Fortran（HPF）编写的程序）进行了同步，并且同步点是已知的，因此不会出现此问题。从研究的角度和实际观点，缺乏统一模型可能会阻碍部署编译器的进展。

### Using Speculation to Hide Latency in Strict Consistency Models

> ###使用猜测将延迟隐藏在严格的一致性模型中

As we saw in [Chapter 3](#_bookmark93), speculation can be used to hide memory latency. It can also be used to hide latency arising from a strict consistency model, giving much of the benefit of a relaxed memory model. The key idea is for the processor to use dynamic scheduling to reorder memory references, letting them possibly execute out of order. Executing the memory references out of order may generate violations of sequential consistency, which might affect the execution of the program. This possibility is avoided by using the delayed commit feature of a speculative proces- sor. Assume the coherency protocol is based on invalidation. If the processor receives an invalidation for a memory reference before the memory reference is committed, the processor uses speculation recovery to back out of the computation and restart with the memory reference whose address was invalidated.

> 正如我们在[第 3 章]（#_ bookmark93）中看到的那样，可以使用猜测来隐藏内存延迟。它也可以用来掩盖严格的一致性模型产生的潜伏期，从而给予轻松记忆模型的大部分好处。关键想法是处理器使用动态调度来重新排序内存引用，使它们可能无法执行。从顺序执行内存引用可能会产生违反顺序一致性的行为，这可能会影响程序的执行。通过使用投机性程序的延迟提交功能，可以避免这种可能性。假设相干协议基于无效。如果处理器在进行内存参考之前收到了用于内存参考的无效，则处理器使用投机恢复来退出计算，并重新启动其地址无效的内存参考。

If the reordering of memory requests by the processor yields an execution order that could result in an outcome that differs from what would have been seen under sequential consistency, the processor will redo the execution. The key to using this approach is that the processor need only guarantee that the result would be the same as if all accesses were completed in order, and it can achieve this by detecting when the results might differ. The approach is attractive because the speculative restart will rarely be triggered. It will be triggered only when there are unsynchronized accesses that actually cause a race ([Gharachorloo et al., 1992](#_bookmark952)).

> 如果处理器对内存请求的重新排序产生的执行顺序可能会导致结果与顺序一致性下的结果不同，则处理器将重做执行。使用这种方法的关键是，处理器需要仅保证结果与所有访问订单完成相同，并且可以通过检测结果何时差异来实现这一目标。该方法很有吸引力，因为投机重新启动很少会触发。仅当有不同步的访问实际引起种族时，才会触发它（[Gharachorloo 等，1992]（#_ bookmark952））。

[Hill (1998)](#_bookmark961) advocated the combination of sequential or processor consistency together with speculative execution as the consistency model of choice. His argu- ment has three parts. First, an aggressive implementation of either sequential con- sistency or processor consistency will gain most of the advantage of a more relaxed model. Second, such an implementation adds very little to the implementation cost of a speculative processor. Third, such an approach allows the programmer to rea- son using the simpler programming models of either sequential or processor con- sistency. The MIPS R10000 design team had this insight in the mid-1990s and used the R10000’s out-of-order capability to support this type of aggressive imple- mentation of sequential consistency.

> [Hill（1998）]（#_ bookmark961）提倡连续或处理器一致性以及投机执行作为选择的一致性模型的组合。他的论证有三个部分。首先，顺序连贯性或处理器一致性的积极实现将获得更轻松的模型的大部分优势。其次，这样的实施几乎没有增加投机处理器的实施成本。第三，这种方法允许程序员使用顺序或处理器连贯的更简单的编程模型来调整。MIPS R10000 设计团队在 1990 年代中期拥有了这个见解，并使用 R10000 的端外功能来支持这种类型的顺序一致性。

One open question is how successful compiler technology will be in optimizing memory references to shared variables. The state of optimization technology and the fact that shared data are often accessed via pointers or array indexing have lim- ited the use of such optimizations. If this technology were to become available and lead to significant performance advantages, compiler writers would want to be able to take advantage of a more relaxed programming model. This possibility and the desire to keep the future as flexible as possible led the RISC V designers to opt for release consistency, after a long series of debates.

> 一个开放的问题是编译器技术在优化共享变量的内存引用方面将如何成功。优化技术的状态以及经常通过指针或数组索引访问共享数据的事实，可以使用此类优化。如果这项技术可用并带来了显着的性能优势，则编译器作家希望能够利用更轻松的编程模型。这种可能性以及保持未来尽可能灵活的愿望，导致 RISC V 设计师在经过一系列的辩论之后选择发行一致性。

### Inclusion and Its Implementation

> ###包容及其实施

All multiprocessors use multilevel cache hierarchies to reduce both the demand on the global interconnect and the latency of cache misses. If the cache also provides _multilevel inclusion_—every level of cache hierarchy is a subset of the level farther away from the processor—then we can use the multilevel structure to reduce the contention between coherence traffic and processor traffic that occurs when snoops and processor cache accesses must contend for the cache. Many multiprocessors with multilevel caches enforce the inclusion property, although recent multiproces- sors with smaller L1 caches and different block sizes have sometimes chosen not to enforce inclusion. This restriction is also called the _subset property_ because each cache is a subset of the cache below it in the hierarchy.

> 所有多处理器都使用多级高速缓存层次结构来减少对全局互连的需求和高速缓存误差的延迟。如果缓存还提供了_multilevel 包含_-缓存层次结构的各个级别是离处理器较远的级别的一个子集，那么我们可以使用多级结构来减少 Snoops 和 Processor Cache Cacke 访问时发生的相干流量和处理器之间的争论必须竞争缓存。许多具有多级缓存的多处理器会强制执行包含性能，尽管最近有较小的 L1 缓存和不同块大小的近期多个多个处理器有时选择不强制纳入包容性。该限制也称为_subset 属性_，因为每个缓存是层次结构中下方缓存的子集。

At first glance, preserving the multilevel inclusion property seems trivial. Con- sider a two-level example: Any miss in L1 either hits in L2 or generates a miss in L2, causing it to be brought into both L1 and L2. Likewise, any invalidate that hits in L2 must be sent to L1, where it will cause the block to be invalidated if it exists. The catch is what happens when the block sizes of L1 and L2 are different.

> 乍一看，保留多级包容性能似乎微不足道。考虑一个两层的示例：L1 中的任何失误要么在 L2 中击中或在 L2 中产生失误，因此将其带入 L1 和 L2。同样，必须将击中 L2 击中的任何无效发送到 L1，如果存在，它将导致块无效。当 L1 和 L2 的块大小不同时，就会发生这种情况。

Choosing different block sizes is quite reasonable, since L2 will be much larger and have a much longer latency component in its miss penalty, and thus will want to use a larger block size. What happens to our “automatic” enforcement of inclu- sion when the block sizes differ? A block in L2 represents multiple blocks in L1, and a miss in L2 causes the replacement of data that is equivalent to multiple L1 blocks. For example, if the block size of L2 is four times that of L1, then a miss in L2 will replace the equivalent of four L1 blocks. Let’s consider a detailed example.

> 选择不同的块大小是相当合理的，因为 L2 会更大，并且在其错过的罚款中具有更长的延迟部分，因此希望使用更大的块大小。当块尺寸不同时，我们对包容性的“自动”执行会发生什么？L2 中的一个块代表 L1 中的多个块，而 L2 中的一个块导致替换数据等效于多个 L1 块。例如，如果 L2 的块大小是 L1 的四倍，则 L2 中的错过将替换相当于四个 L1 块。让我们考虑一个详细的例子。

Example Assume that L2 has a block size four times that of L1. Show how a miss for an address that causes a replacement in L1 and L2 can lead to violation of the inclu- sion property.

> 示例假设 L2 的块大小是 L1 的四倍。显示导致 L1 和 L2 替换的地址的错过如何导致侵犯围栏财产。

To maintain inclusion with multiple block sizes, we must probe the higher levels of the hierarchy when a replacement is done at the lower level to ensure that any words replaced in the lower level are invalidated in the higher-level caches; different levels of associativity create the same sort of problems. [Baer and](#_bookmark922) [Wang (1988)](#_bookmark922) described the advantages and challenges of inclusion in detail, and in 2017 most designers have opted to implement inclusion, often by settling on one block size for all levels in the cache. For example, the Intel i7 uses inclusion for L3, meaning that L3 always includes the contents of all of L2 and L1. This decision allows the i7 to implement a straightforward directory scheme at L3 and to minimize the interference from snooping on L1 and L2 to those circum- stances where the directory indicates that L1 or L2 have a cached copy. The AMD Opteron, in contrast, makes L2 inclusive of L1 but has no such restriction for L3. It uses a snooping protocol, but only needs to snoop at L2 unless there is a hit, in which case a snoop is sent to L1.

> 为了维持多个块大小的包含，我们必须在较低级别进行替换时探测较高的层次结构，以确保在较高级别的较高级别的缓存中无效。不同级别的关联性会产生相同的问题。[Baer and]（#_ bookmark922）[Wang（1988）]（#_ bookmark922）描述了详细包容的优点和挑战，2017 年，大多数设计师选择实施包容性，通常是通过在一个块大小上解决一个块大小来实现包容性。缓存。例如，英特尔 i7 使用包含在 L3 中，这意味着 L3 始终包含所有 L2 和 L1 的内容。该决定允许 i7 在 L3 上实现直接目录方案，并最大程度地减少从 L1 和 L2 上的侦听到目录表明 L1 或 L2 具有缓存副本的情况的干扰。相比之下，AMD Opteron 使 L2 包括 L1，但对 L3 没有这种限制。它使用窥探协议，但是除非有命中率，否则只需要在 L2 上偷窥，在这种情况下，将窥探发送到 L1。

### Performance Gains From Multiprocessing and Multithreading

> ###多处理和多线程的性能获得

In this section, we briefly look at a study of the effectiveness of using multithread- ing on a multicore processor, the IBM Power5; we will return to this topic in the next section, when we examine the performance of the Intel i7. The IBM Power5 is a dual-core that supports simultaneous multithreading (SMT); its basic architecture is very similar to the more recent Power8 (which we examine in the next section), but it has only two cores per processor.

> 在本节中，我们简要介绍了在多核心处理器 IBM Power5 上使用多纹理的有效性的研究。当我们检查英特尔 i7 的性能时，我们将在下一节中返回此主题。IBM Power5 是一个双核，支持同时多线程（SMT）；它的基本体系结构与最近的 Power8 非常相似（我们在下一节中对此进行了检查），但是每个处理器只有两个内核。

Figure 5.25 A comparison of SMT and single-thread (ST) performance on the 8-processor IBM eServer p5 575 using SPECfpRate (top half) and SPECintRate (bottom half) as benchmarks. Note that the _x_-axis starts at a speedup of 0.9, a performance loss. Only one processor in each Power5 core is active, which should slightly improve the results from SMT by decreasing destructive interference in the memory system. The SMT results are obtained by creating 16 user threads, whereas the ST results use only eight threads; with only one thread per processor, the Power5 is switched to single-threaded mode by the OS. These results were collected by John McCalpin at IBM. As we can see from the data, the standard deviation of the results for the SPECfpRate is higher than for SPECintRate (0.13 versus 0.07), indicating that the SMT improvement for FP programs is likely to vary widely.

> 图 5.25 使用 SPECFPRATE（上半场）和 Specintrate（下半部分）作为基准测试的 8 处理器 IBM Eserver P5 575 的 SMT 和单线程（ST）性能的比较。请注意，_x_-axis 以 0.9 的速度开始，性能损失。每个 Power5 核心中只有一个处理器是活跃的，这应该通过减少存储器系统的破坏性干扰来稍微改善 SMT 的结果。SMT 结果是通过创建 16 个用户线程获得的，而 ST 结果仅使用 8 个线程。每个处理器只有一个线程，power5 由 OS 切换到单线读取模式。这些结果由 IBM 的 John McCalpin 收集。从数据中可以看出，SpecFrate 的结果的标准偏差高于 Spectrate（0.13 对 0.07），这表明 FP 程序的 SMT 改进可能会差异很大。

To examine the performance of multithreading in a multiprocessor, measure- ments were made on an IBM system with eight Power5 processors, using only one core on each processor. [Figure 5.25](#_bookmark246) shows the speedup for an 8-processor Power5 multiprocessor, with and without SMT, for the SPECRate2000 benchmarks, as described in the caption. On average, the SPECintRate is 1.23 times faster, and the SPECfpRate is 1.16 times faster. Note that a few floating-point benchmarks experience a slight decrease in performance in SMT mode, with the maximum reduction in speedup being 0.93. Although one might expect that SMT would do a better job of hiding the higher miss rates of the SPECFP benchmarks, it appears that limits in the memory system are encountered when running in SMT mode on such benchmarks.

> 为了检查多线程在多处理器中的性能，在具有八个 Power5 处理器的 IBM 系统上进行了测量，每个处理器上仅使用一个核心。[图 5.25]（#_ bookmark246）如标题中所述，显示了 8 个处理器 power5 多处理器的加速度。平均而言，指定速度的速度快 1.23 倍，而 SPECFPRATE 的速度更快为 1.16 倍。请注意，在 SMT 模式下，一些浮点基准的性能会略有下降，而速度最大降低为 0.93。尽管人们可能会期望 SMT 可以更好地隐藏 SPECFP 基准的较高的错过率，但是在此类基准测试下以 SMT 模式运行时，似乎遇到了内存系统中的限制。

For roughly 10 years, multicore has been the primary focus for scaling perfor- mance, although the implementations vary widely, as does their support for larger multichip multiprocessors. In this section, we examine the design of three different multicores, the support they provide for larger multiprocessors, and some perfor- mance characteristics, before doing a broader evaluation of small to large multi- processor Xeon systems, and concluding with a detailed evaluation of the multicore i7 920, a predecessor of the i7 6700.

> 在大约 10 年的时间里，Multicore 一直是扩展性能的主要重点，尽管这些实现方式差异很大，它们对较大的 Multichip 多处理器的支持也差异很大。在本节中，我们在对小型至大型多处理器 Xeon 系统进行更广泛的评估之前，在更广泛的评估之前检查了三个不同的多门的设计，它们为更大的多处理器提供了支持，并进行了更广泛的评估，并以详细的评估评估 i7 6700 的前身 Multicore I7 920。

### Performance of Multicore-Based Multiprocessors on a Multiprogrammed Workload

> ###多编程工作负载上基于多核的多处理器的性能

[Figure 5.26](#_bookmark248) shows the key characteristics of three multicore processors designed for server applications and available in 2015 through 2017. The Intel Xeon E7 is based on the same basic design as the i7, but it has more cores, a slightly slower clock rate (power is the limitation), and a larger L3 cache. The Power8 is the new- est in the IBM Power series and features more cores and bigger caches. The Fujitsu SPARC64 X+ is the newest SPARC server chip; unlike the T-series mentioned in [Chapter 3](#_bookmark93), it uses SMT. Because these processors are configured for multicore and multiprocessor servers, they are available as a family, varying processor count, cache size, and so on, as the figure shows.

> [图 5.26]（#_ bookmark248）显示了为服务器应用程序设计的三个多功能处理器的关键特性，并在 2015 年至 2017 年提供。英特尔 Xeon E7 基于与 i7 相同的基本设计，但它具有更多的核心，但有点稍微时钟速率较慢（功率为限制）和较大的 L3 缓存。Power8 是 IBM Power 系列中的新产品，并具有更多的核心和更大的缓存。Fujitsu SPARC64 X+ 是最新的 SPARC 服务器芯片；与[第 3 章]中提到的 T 系列不同（#_ bookmark93），它使用 SMT。由于这些处理器是为多功能和多处理器服务器配置的，因此它们可作为家庭使用，如图所示，变化的处理器计数，缓存大小等。

These three systems show a range of techniques both for connecting the on- chip cores and for connecting multiple processor chips. First, let’s look at how the cores are connected within a chip. The SPARC64 X+ is the simplest: it shares a single L2 cache, which is 24-way set associative, among the 16 cores. There are four separate DIMM channels to attach memory accessible with a 16 4 switch between the cores and the channels.

> 这三个系统显示了连接芯片芯和连接多个处理器芯片的一系列技术。首先，让我们看一下芯片如何连接在芯片中。SPARC64 X+ 是最简单的：它共享一个 16 个内核中的单个 L2 缓存，该缓存是 24 向相关的。有四个独立的 DIMM 通道，可以在内核和通道之间使用 16 4 开关连接内存。

[Figure 5.27](#_bookmark249) shows how the Power8 and Xeon E7 chips are organized. Each core in the Power8 has an 8 MiB bank of L3 directly connected; other banks are accessed via the interconnection network, which has 8 separate buses. Thus the Power8 is a true NUCA (_Nonuniform Cache Architecture_), because the access time to the attached bank of L3 will be much faster than accessing another L3. Each Power8 chip has a set of links that can be used to build a large multiprocessor using an organization we will see shortly. The memory links are connected to a special memory controller that includes an L4 and interfaces directly with DIMMs.

> [图 5.27]（#_ bookmark249）显示了如何组织功率 8 和 Xeon E7 芯片。Power8 中的每个核心都有一个直接连接的 8 个 MIB 库；其他银行将通过互连网络访问，该网络有 8 个单独的总线。因此，power8 是一个真实的核（_ noniniform Cache 架构_），因为与访问另一个 L3 相比，对 L3 附件的访问时间要快得多。每个 Power8 芯片都有一组链接，可用于使用我们将在不久将看到的组织来构建大型多处理器。内存链接连接到包含 L4 和直接与 DIMM 的接口的特殊内存控制器。

Part B of [Figure 5.27](#_bookmark249), shows how the Xeon E7 processor chip is organized when there are 18 or more cores (20 cores are shown in this figure). Three rings connect the cores and the L3 cache banks, and each core and each bank of L3 is connected to two rings. Thus any cache bank or any core is accessible from any other core by choosing the right ring. Therefore, within the chip, the E7 has uni- form access time. In practice, however, the E7 is normally operated as a NUMA architecture by logically associating half the cores with each memory channel; this

> [图 5.27]的 B 部分（#_ bookmark249）显示了 Xeon E7 处理器芯片在有 18 个或更多核时如何组织起来（该图中显示了 20 个核心）。三个环连接了核心和 L3 缓存库，每个核心和 L3 的每个库都连接到两个环。因此，通过选择正确的环，可以从任何其他核心访问任何缓存库或任何核心。因此，在芯片中，E7 具有联合访问时间。但是，实际上，通过将一半的内核与每个内存通道相关联，E7 通常作为 NUMA 架构操作；这个

Figure 5.26 Summary of the characteristics of three recent high-end multicore processors (2015–2017 releases) designed for servers. The table shows the range of processor counts, clock rates, and cache sizes within each pro- cessor family. The Power8 L3 is a NUCA (Nonuniform Cache Access) design, and it also supports off-chip L4 of up to 128 MiB using EDRAM. A 32-core Xeon has recently been announced, but no system shipments have occurred. The Fujitsu SPARC64 is also available as an 8-core design, which is normally configured as a single processor system. The last row shows the range of configured systems with published performance data (such as SPECintRate) with both processor chip counts and total core counts. The Xeon systems include multiprocessors that extend the basic inter- connect with additional logic; for example, using the standard Quickpath interconnect limits the processor count to 8 and the largest system to 8 24 192 cores, but SGI extends the interconnect (and coherence mechanisms) with extra logic to offer a 32 processor system using 18-core processor chips for a total size of 576 cores. Newer releases of these processors increased clock rates (significantly in the Power8 case, less so in others) and core counts (signifi- cantly in the case of Xeon).

> 图 5.26 为服务器设计了三个最近的三个高端多层处理器（2015–2017 版本）的特性摘要。该表显示了每个专业家族中处理器计数，时钟率和缓存大小的范围。Power8 L3 是一种核（不均匀的高速缓存访问）设计，它还支持使用 EDRAM 的芯片外 L4 高达 128 MIB。最近宣布了 32 个核心的 Xeon，但没有发生系统货物。Fujitsu SPARC64 也可以作为 8 核设计提供，通常将其配置为单个处理器系统。最后一行显示了具有已发布的性能数据（例如 Specintrate）的配置系统的范围，并具有处理器芯片计数和总核心计数。Xeon 系统包含多处理器，以扩展与其他逻辑相关的基本交流；例如，使用标准快速路径互连将处理器计数限制为 8，最大的系统将最大的系统限制为 8 24 192 核，但是 SGI 使用额外的逻辑扩展了互连（和连贯的机制），以使用 18 核处理器系统使用 18 核处理器芯片提供 32 个处理器系统。总尺寸为 576 个核心。这些处理器的较新版本提高了时钟率（在 Power8 情况下显着，在其他情况下更少）和核心计数（在 Xeon 的情况下显着）。

<img src="./media/image299.png" style="width:4.97386in;height:2.24065in" />

> <img src =“ ./媒体/image299.png” style =“宽度：4.97386in;高度：2.24065in”/>

![](./media/image300.png)

> ！[]（./ Media/image300.png）

<img src="./media/image301.png" style="width:5.97502in" />

> <img src =“ ./媒体/image301.png” style =“ width：5.97502in”/>

Figure 5.27 The on-chip organizations of the Power8 and Xeon E7 are shown. The Power8 uses 8 separate buses between L3 and the CPU cores. Each Power8 also has two sets of links for connecting larger multiprocessors. The Xeon uses three rings to connect processors and L3 cache banks, as well QPI for interchip links. Software is used to logically associate half the cores with each memory channel.

> 图 5.27 显示了 Power8 和 Xeon E7 的片上组织。Power8 使用 L3 和 CPU 内核之间的 8 个单独的总线。每个 Power8 还具有连接较大多处理器的两组链接。Xeon 使用三个戒指连接处理器和 L3 缓存库，以及用于 Interchip 链接的 QPI。软件用于将核心一半与每个内存通道相关联。

increases the probability that a desired memory page is open on a given access. The E7 provides 3 QuickPath Interconnect (QPI) links for connecting multiple E7s.

> 增加了在给定访问时打开所需内存页面的概率。E7 提供了 3 个快速路径互连（QPI）链接，用于连接多个 E7。

Multiprocessors consisting of these multicores use a variety of different inter- connection strategies, as [Figure 5.28](#_bookmark250) shows. The Power8 design provides support for connecting 16 Power8 chips for a total of 192 cores. The intragroup links pro- vide higher bandwidth interconnect among a completely connected module of 4 processor chips. The intergroup links are used to connect each processor chip to the 3 other modules. Thus each processor is two hops from any other, and the mem- ory access time is determined by whether an address resides in local memory, clus- ter memory, or intercluster memory (actually the latter can have two different values, but the difference is swamped by the intercluster time).

> 由这些多功能组成的多处理器使用各种不同的相互连接策略，如[图 5.28]（#_ bookmark250）所示。Power8 Design 为连接 16 个 Power8 芯片的支持提供了支持，共有 192 个核心。该组链接在 4 个处理器芯片的完全连接的模块之间提供更高的带宽互连。组间链接用于将每个处理器芯片连接到其他 3 个模块。因此，每个处理器是其他任何一个啤酒花的两个啤酒被集群时间淹没）。

The Xeon E7 uses QPI to interconnect multiple multicore chips. In a 4-chip, multiprocessor, which with the latest announced Xeon could have 128 cores, the three QPI links on each processor are connected to three neighbors, yielding a 4-chip fully connected multiprocessor. Because memory is directly connected to each E7 multicore, even this 4-chip arrangement has nonuniform memory access time (local versus remote). [Figure 5.28](#_bookmark250) shows how 8 E7 processors can be con- nected; like the Power8, this leads to a situation where every processor is one or two hops from every other processor. There are a number of Xeon-based mul- tiprocessor servers that have more than 8 processor chips. In such designs, the typ- ical organization is to connect 4 processor chips together in a square, as a module, with each processor connecting to two neighbors. The third QPI in each chip is connected to a crossbar switch. Very large systems can be created in this fashion. Memory accesses can then occur at four locations with different timings: local to the processor, an immediate neighbor, the neighbor in the cluster that is two hops away, and through the crossbar. Other organizations are possible and require less than a full crossbar in return for more hops to get to remote memory.

> Xeon E7 使用 QPI 互连多重芯片。在一个 4 芯片的多处理器中，最新宣布的 Xeon 可能有 128 个内核，每个处理器上的三个 QPI 链接都连接到三个邻居，产生了一个 4 芯片完全连接的多处理器。由于内存直接连接到每个 E7 多核，因此即使此 4 芯片布置也具有不均匀的内存访问时间（本地与远程）。[图 5.28]（#_ bookmark250）显示了如何连接 8 个 E7 处理器；像 Power8 一样，这会导致每个处理器都是其他每个处理器的一两个啤酒花的情况。有许多基于 Xeon 的 Mul-tip 处理器服务器，它们具有 8 个以上的处理器芯片。在这样的设计中，典型组织是将 4 个处理器芯片连接在一起，将 4 个处理器作为一个模块，每个处理器连接到两个邻居。每个芯片中的第三个 QPI 连接到横梁开关。可以以这种方式创建非常大的系统。然后，内存访问可以在四个位置发生，这些位置具有不同的时间：处理器本地，直接邻居，群集中的邻居在两个啤酒花之外，并通过横梁。其他组织是可能的，并且需要比完整的横杆，以换取更多的啤酒花才能到达遥控内存。

The SPARC64 X+ also uses a 4-processor module, but each processor has three connections to its immediate neighbors plus two (or three in the largest con- figuration) connections to a crossbar. In the largest configuration, 64 processor chips can be connected to two crossbar switches, for a total of 1024 cores. Memory access is NUMA (local, within a module, and through the crossbar), and coherency is directory-based.

> SPARC64 X+ 还使用了 4 处理器模块，但是每个处理器都与其直接邻居以及两个（或最大的配置中的三个）连接到横杆。在最大的配置中，可以将 64 个处理器芯片连接到两个横梁开关，总共 1024 个内核。内存访问为 numa（本地，在模块内和通过横梁），相干性基于目录。

### Performance of Multicore-Based Multiprocessors on a Multiprogrammed Workload

> ###多编程工作负载上基于多核的多处理器的性能

First, we compare the performance scalability of these three multicore processors using SPECintRate, considering configurations up to 64 cores. [Figure 5.29](#_bookmark251) shows how the performance scales relative to the performance of the smallest configura- tion, which varies between 4 and 16 cores. In the plot, the smallest configuration is assumed to have perfect speedup (i.e., 8 for 8 cores, 12 for 12 cores, etc.). This figure does _not_ show performance among these different processors. Indeed such performance varies significantly: in the 4-core configuration, the IBM Power8 is

> 首先，我们使用 Specintrate 比较了这三个多核处理器的性能可伸缩性，考虑到最高 64 个内核的配置。[图 5.29]（#_ bookmark251）显示了性能如何相对于最小配置的性能，在 4 到 16 个内核之间变化。在图中，假定最小的配置具有完美的加速度（即 8 核 8，12 个核心等）。该图确实_not_显示了这些不同处理器之间的性能。确实，这种性能差异很大：在 4 核配置中，IBM Power8 是

![](./media/image302.png)

> ！[]（./ Media/image302.png）

![](./media/image303.png)

> ！[]（./ Media/image303.png）

Figure 5.28 The system architecture for three multiprocessors built from multicore chips.

> 图 5.28 由多核芯片构建的三个多处理器的系统体系结构。

Figure 5.29 The performance scaling on the SPECintRate benchmarks for four multicore processors as the number of cores is increased to 64. Performance for each processor is plotted relative to the smallest configuration and assuming that configuration had perfect speedup. Although this chart shows how a given multiprocessor scales with additional cores, it does not supply any data about performance among processors. There are differences in the clock rates, even within a given processor family. These are generally swamped by the core scaling effects, except for the Power8 that shows a clock range spread of 1.5 from the smallest configuration to the 64 core configuration.

> 图 5.29 四个多层处理器的规范基准上的性能缩放，因为核心数增加到 64。每个处理器的性能相对于最小的配置绘制了绘制的性能，并且假设配置具有完美的加速。尽管此图表显示了给定的多处理器如何使用其他内核，但它没有提供有关处理器性能的任何数据。即使在给定的处理器家族中，时钟速率也存在差异。这些通常会被核心缩放效果淹没，除了 Power8 显示了从最小配置到 64 核心配置的时钟范围 1.5 的范围。

1.5 times as fast as the SPARC64 X+ on a per core basis! Instead [Figure 5.29](#_bookmark251) shows how the performance scales for each processor family as additional cores are added.

> 每核的速度是 SPARC64 X+ 的 1.5 倍！相反，[图 5.29]（#_ bookmark251）显示了如何添加每个处理器家族的性能量表。

Two of the three processors show diminishing returns as they scale to 64 cores. The Xeon systems appear to show the most degradation at 56 and 64 cores. This may be largely due to having more cores share a smaller L3. For example, the 40- core system uses 4 chips, each with 60 MiB of L3, yielding 6 MiB of L3 per core. The 56-core and 64-core systems also use 4 chips but have 35 or 45 MiB of L3 per chip, or 2.5–2.8 MiB per core. It is likely that the resulting larger L3 miss rates lead to the reduction in speedup for the 56-core and 64-core systems.

> 三个处理器中的两个显示回报率降低，因为它们扩展到 64 个核心。Xeon 系统似乎显示出 56 和 64 个核心的降解最多。这可能主要是由于拥有更多的核心共享较小的 L3。例如，40 个核心系统使用 4 个芯片，每个芯片具有 60 个 MIB L3，每核的 L3 6 MIB。56 核和 64 核系统还使用 4 个芯片，但每芯片具有 35 或 45 MIB 的 L3，或每个核心的 2.5-2.8 MIB。由此产生的较大的 L3 失误可能导致 56 核和 64 核系统的加速降低。

The IBM Power8 results are also unusual, appearing to show significant super- linear speedup. This effect, however, is due largely to differences in the clock rates, which are much larger across the Power8 processors than for the other processors in this figure. In particular, the 64-core configuration has the highest clock rate (4.4 GHz), whereas the 4-core configuration has a 3.0 GHz clock. If we normalize the relative speedup for the 64-core system based on the clock rate differential with the 4-core system, the effective speedup is 57 rather than 84. Therefore, while the Power8 system scales well, and perhaps the best among these processors, it is not miraculous.

> IBM Power8 结果也很不寻常，似乎显示出明显的超线性加速。但是，这种效果主要是由于时钟速率的差异，而在整个 Power8 处理器中，该效果比该图中的其他处理器要大得多。特别是，64 核配置的时钟速率最高（4.4 GHz），而 4 核配置的时钟率为 3.0 GHz 时钟。如果我们基于 4 核系统的时钟速率差异 64 核系统的相对加速，则有效的加速为 57 而不是 84。，这不是奇迹。

![](./media/image305.png)

> ！[]（./ Media/Image305.png）

Figure 5.30 The scaling of relative performance for multiprocessor multicore. As before, performance is shown relative to the smallest available system. The Xeon result at 80 cores is the same L3 effect that showed up for smaller configurations. All systems larger than 80 cores have between 2.5 and 3.8 MiB of L3 per core, and the 80-core, or smaller, systems have 6 MiB per core.

> 图 5.30 多处理器多核的相对性能缩放。和以前一样，相对于最小的可用系统显示性能。80 核的 Xeon 结果与显示较小配置的 L3 效应相同。大于 80 个核心的所有系统均在每个核心的 2.5 至 3.8 MIB 之间，并且 80 核或较小的系统每个核心具有 6 个 MIB。

[Figure 5.30](#_bookmark252) shows scaling for these three systems at configurations above 64 processors. Once again, the clock rate differential explains the Power8 results; the clock-rate equivalent scaled speedup with 192 processors is 167, versus 223, when not accounting for clock rate differences. Even at 167, the Power8 scaling is some- what better than that on the SPARC64 X+ or Xeon systems. Surprisingly, although there are some effects on speedup in going from the smallest system to 64 cores, they do not seem to get dramatically worse at these larger configurations. The nature of the workload, which is highly parallel and user-CPU-intensive, and the overheads paid in going to 64 cores probably lead to this result.

> [图 5.30]（#_ bookmark252）在 64 个处理器以上的配置下显示了这三个系统的缩放。时钟速率差再次解释了 power8 结果。与 192 处理器相同的时钟等效缩放速度为 167，而 223，当时不考虑时钟率差异。即使在 167，Power8 缩放也比 SPARC64 X+ 或 Xeon 系统上的缩放量更好。令人惊讶的是，尽管从最小的系统到 64 个内核会对加速有一些影响，但在这些较大的配置方面，它们似乎并没有变得更糟。工作量的性质高度平行且用户 CPU 密集型，而在 64 个内核中支付的间接费用可能会导致这一结果。

### Scalability in an Xeon MP With Different Workloads

> ###在具有不同工作负载的 Xeon MP 中的可伸缩性

In this section, we focus on the scalability of the Xeon E7 multiprocessors on three different workloads: a Java-based commercially oriented workload, a virtual machine workload, and a scientific parallel processing workload, all from the SPEC benchmarking organization, as described next.

> 在本节中，我们将重点介绍了三个不同的工作负载上的 Xeon E7 多处理器的可伸缩性：基于 Java 的商业定向工作负载，虚拟机工作负载和科学并行处理工作负载，所有这些工作都是从 Spec Benchmarking 组织中进行的，如接下来所述。

- SPECjbb2015: Models a supermarket IT system that handles a mix of point-of-sale requests, online purchases, and data-mining operations. The performance metric is throughput-oriented, and we use the maximum per- formance measurement on the server side running multiple Java virtual machines.

> -SpecJBB2015：建模一个超市 IT 系统，该系统处理销售点请求，在线购买和数据挖掘操作的混合。性能指标是面向吞吐量的，我们使用运行多个 Java 虚拟机的服务器端上使用最大功能测量。

- SPECVirt2013: Models a collection of virtual machines running independent mixes of other SPEC benchmarks, including CPU benchmarks, web servers, and mail servers. The system must meet a quality of service guarantee for each virtual machine.

> - SpecVirt2013：建模一组虚拟机的集合，运行其他规格基准的独立混合物，包括 CPU 基准测试，Web 服务器和邮件服务器。该系统必须符合每台虚拟机的服务质量保证。

- SPECOMP2012: A collection of 14 scientific and engineering programs writ- ten with the OpenMP standard for shared-memory parallel processing. The codes are written in Fortran, C, and C++ and range from fluid dynamics to molecular modeling to image manipulation.

> - Specomp2012：14 个科学和工程计划的集合 - 十个具有共享记忆并行处理的 OpenMP 标准。这些代码以 Fortran，C 和 C ++ 的形式编写，从流体动力学到分子建模再到图像操作。

As with the previous results, [Figure 5.31](#_bookmark253) shows performance assuming linear speedup on the smallest configuration, which for these benchmarks varies from 48 cores to 72 cores, and plotting performance relative to the that smallest configu- ration. The SPECjbb2015 and SPECVirt2013 include significant systems soft- ware, including the Java VM software and the VM hypervisor. Other than the system software, the interaction among the processes is very small. In contrast, SPECOMP2012 is a true parallel code with multiple user processes sharing data and collaborating in the computation.

> 与先前的结果一样，[图 5.31]（#_ bookmark253）显示了性能，假设最小的配置上有线性加速，对于这些基准测试，该基准从 48 个内核到 72 个内核，并且相对于最小的配置而绘制性能。SpecJBB2015 和 SpecVirt2013 包括重要的系统软件，包括 Java VM 软件和 VM 机床管理器。除了系统软件外，过程之间的交互非常小。相比之下，Specomp2012 是一个真正的并行代码，其中多个用户流程共享数据并在计算中进行协作。

Let’s begin by examining SPECjbb2015. It obtains speedup efficiency (speedup/processor ratio) of between 78% and 95%, showing good speedup, even in the largest configuration. SPECVirt2013 does even better (for the range of sys- tem measured), obtaining almost linear speedup at 192 cores. Both SPECjbb2015 and SPECVirt2013 are benchmarks that scale up the application size (as in the TPC benchmarks discussed in [Chapter 1](#_bookmark2)) with larger systems so that the effects of Amdahl’s Law and interprocess communication are minor.

> 让我们从检查 SpecJBB2015 开始。它的加速效率（加速/处理器比率）在 78％至 95％之间，即使在最大的配置中也显示出良好的加速。SpecVirt2013 的功能甚至更好（对于测量的系统范围），在 192 个内核下获得了几乎线性的加速。SpecJBB2015 和 SpecVirt2013 都是具有较大系统的应用程序大小（如[第 1 章]（#_ bookmark2）中讨论的 TPC 基准测试的基准（如 TPC 基准），因此 Amdahl 的定律和分解沟通的效果是次要的。

Finally, let’s turn to SPECOMP2012, the most compute-intensive of these benchmarks and the one that truly involves parallel processing. The major trend visible here is a steady loss of efficiency as we scale from 30 to 576 cores so that by 576 cores, the system exhibits only half the efficiency it showed at 30 cores. This reduction leads to a relative speedup of 284, assuming that the 30-core speedup is 30. These are probably Amdahl’s Law effects resulting from limited parallelism as well as synchronization and communication overheads. Unlike the SPECjbb2015 and SPECVirt2013, these benchmarks are not scaled for larger systems.

> 最后，让我们转到 Specomp2012，这是这些基准中最密集的，也是真正涉及并行处理的基准。这里可见的主要趋势是效率的稳定丧失，因为我们从 30 到 576 个核心缩放到 576 个核心，该系统仅显示出在 30 个核心处显示的效率的一半。假设 30 核的速度为 30，这种降低导致相对加速为 284。这些可能是 Amdahl 的定律效应，这是由于有限的并行性以及同步和通信开销而引起的。与 SpecJBB2015 和 SpecVirt2013 不同，这些基准不是针对较大系统的缩放。

Figure 5.31 Scaling of performance on a range of Xeon E7 systems showing performance relative to the smallest benchmark configuration, and assuming that configuration gets perfect speedup (e.g., the smallest SPEWCOMP configuration is 30 cores and we assume a performance of 30 for that system). Only relative performance can be assessed from this data, and comparisons across the benchmarks have no relevance. Note the difference in the scale of the vertical and horizontal axes.

> 图 5.31 在一系列 Xeon E7 系统上的性能缩放显示相对于最小的基准配置，并且假设配置获得了完美的加速度（例如，最小的 SpewComp 配置为 30 个内核，我们对该系统的性能为 30 个）。只能从这些数据中评估相对性能，并且基准之间的比较没有相关性。注意垂直和水平轴的尺度的差异。

### Performance and Energy Efficiency of the Intel i7 920 Multicore

> ###英特尔 i7 920 多功能的性能和能源效率

In this section, we closely examine the performance of the i7 920, a predecessor of the 6700, on the same two groups of benchmarks we considered in [Chapter 3](#_bookmark93): the parallel Java benchmarks and the parallel PARSEC benchmarks (described in detail in [Figure 3.32](#_bookmark93) on page 247). Although this study uses the older i7 920, it remains, by far, the most comprehensive study of energy efficiency in multicore processors and the effects of multicore combined with SMT. The fact that the i7 920 and 6700 are similar indicates that the basic insights should also apply to the 6700.

> 在本节中，我们仔细研究了 6700 的前身 i7 920 的性能，在[第 3 章]中考虑的相同两组基准（#_ bookmark93）：并行 Java 基准和 Parallel Parsec 基准（所描述）（所描述）在第 247 页的[图 3.32]（#_ bookmark93）中详细介绍。尽管这项研究使用了较旧的 i7 920，但到目前为止，它仍然是多核处理器中能源效率的最全面研究，以及与 SMT 结合的多核心的效果。i7 920 和 6700 相似的事实表明，基本见解也应适用于 6700。

First, we look at the multicore performance and scaling versus a single-core without the use of SMT. Then we combine both the multicore and SMT capabil- ity. All the data in this section, like that in the earlier i7 SMT evaluation ([Chapter 3](#_bookmark93)) come from [Esmaeilzadeh et al. (2011)](#_bookmark945). The dataset is the same as that used earlier (see [Figure 3.32](#_bookmark93) on page 247), except that the Java benchmarks tradebeans and pjbb2005 are removed (leaving only the five scalable Java bench- marks); tradebeans and pjbb2005 never achieve speedup above 1.55 even with four cores and a total of eight threads, and thus are not appropriate for evaluating more cores.

> 首先，我们查看了多项性性能和缩放与单核的比例，而无需使用 SMT。然后，我们将多核和 SMT Capabil-结合在一起。本节中的所有数据，就像较早的 i7 SMT 评估（[第 3 章]（#_ bookmark93））中的所有数据都来自[Esmaeilzadeh 等人。（2011）]（#_ bookmark945）。数据集与前面使用的数据集相同（请参见第 247 页上的[图 3.32]（#_ bookmark93）），除了删除了 Java 基准 Markecs TradeBeans 和 PJBB2005（仅保留五个可伸缩的 Java 台上标记）；即使有四个核心和总共八个线程，贸易面和 PJBB2005 也从未达到 1.55 以上的加速度，因此不适合评估更多的核心。

[Figure 5.32](#_bookmark254) plots both the speedup and energy efficiency of the Java and PARSEC benchmarks without the use of SMT. Energy efficiency is computed by the ratio: energy consumed by the single-core run divided by the energy con- sumed by the two- or four-core run (i.e., efficiency is the inverse of energy con- sumed). Higher energy efficiency means that the processor consumes less energy for the same computation, with a value of 1.0 being the break-even point. The unused cores in all cases were in deep sleep mode, which minimized their power consumption by essentially turning them off. In comparing the data for the single-core and multicore benchmarks, it is important to remember that the full energy cost of the L3 cache and memory interface is paid in the single- core (as well as the multicore) case. This fact increases the likelihood that energy consumption will improve for applications that scale reasonably well. Harmonic mean is used to summarize results with the implication described in the caption.

> [图 5.32]（#_ bookmark254）在不使用 SMT 的情况下绘制了 Java 和 Parsec 基准的加速和能源效率。能源效率是按比率：通过单核运行消耗的能量，除以两核或四核运行的能量（即效率是能量相关的倒数）。较高的能源效率意味着处理器在同一计算中消耗的能量较少，其值为 1.0 是收获点。在所有情况下，未使用的核心都处于深度睡眠模式，这可以通过基本关闭它们来最大程度地减少其功耗。在比较单核和多项基准测试的数据时，重要的是要记住，L3 缓存和内存接口的全部能源成本是在单核（以及多项式）情况下支付的。这一事实增加了能源消耗的可能性，而能源消耗将改善合理扩展的应用。谐波平均值用于总结结果中描述的结果。

Figure 5.32 This chart shows the speedup and energy efficiency for two- and four-core executions of the par- allel Java and PARSEC workloads without SMT. These data were collected by [Esmaeilzadeh et al. (2011)](#_bookmark945) using the same setup as described in [Chapter 3](#_bookmark93). Turbo Boost is turned off. The speedup and energy efficiency are summarized using harmonic mean, implying a workload where the total time spent running each benchmark on 2 cores is equivalent.

> 图 5.32 此图表显示了 Par-Allel Java 和 Parsec 工作负载的两核执行的加速和能源效率，没有 SMT。这些数据是通过[Esmaeilzadeh 等人收集的。（2011）]（#_ bookmark945）使用与[第 3 章]（#_ bookmark93）相同的设置。涡轮增压被关闭。使用谐波平均值总结了加速度和能源效率，这意味着工作量的总时间是在 2 个核心上运行每个基准的总时间。

As the figure shows, the PARSEC benchmarks get better speedup than the Java benchmarks, achieving 76% speedup efficiency (i.e., actual speedup divided by processor count) on four cores, whereas the Java benchmarks achieve 67% speedup efficiency on four cores. Although this observation is clear from the data, analyzing why this difference exists is difficult. It is quite possible that Amdahl’s Law effects have reduced the speedup for the Java workload, which includes some typically serial parts, such as the garbage collector. In addition, interaction between the processor architecture and the application, which affects issues such as the cost of synchronization or commu- nication, may also play a role. In particular, well-parallelized applications, such as those in PARSEC, sometimes benefit from an advantageous ratio between computation and communication, which reduces the dependence on communi- cations costs (see Appendix I).

> 如图所示，PARSEC 基准的加速度比 Java 基准测试更好，在四个内核上实现了 76％的加速效率（即实际加速度除以处理器计数），而 Java 基准测试实现了 67％的加速效率。尽管从数据中可以清楚地看出这一观察结果，但分析为什么存在这种差异很困难。Amdahl 的法律效应很有可能减少了 Java 工作量的加速，其中包括一些通常的串行零件，例如垃圾收集器。此外，处理器体系结构与应用程序之间的交互作用会影响同步或交流成本等问题，也可能起作用。特别是，平行合理的应用程序（例如 PARSEC 中的应用程序）有时受益于计算和通信之间的有利比率，从而降低了对通讯成本的依赖（请参阅附录 I）。

These differences in speedup translate to differences in energy efficiency. For example, the PARSEC benchmarks actually slightly improve energy effi- ciency over the single-core version; this result may be significantly affected by the fact that the L3 cache is more effectively used in the multicore runs than in the single-core case and the energy cost is identical in both cases. Thus, for the PARSEC benchmarks, the multicore approach achieves what designers hoped for when they switched from an ILP-focused design to a multicore design; namely, it scales performance as fast or faster than scaling power, resulting in constant or even improved energy efficiency. In the Java case, we see that neither the two- nor four-core runs break even in energy efficiency because of the lower speedup levels of the Java workload (although Java energy efficiency for the 2p run is the same as for PARSEC). The energy efficiency in the four-core Java case is reasonably high (0.94). It is likely that an ILP-centric processor would need _even more_ power to achieve a comparable speedup on either the PARSEC or Java workload. Thus the TLP-centric approach is also certainly better than the ILP-centric approach for improving performance for these applications. As we will see in [Section 5.10](#the-future-of-multicore-scaling), there are reasons to be pessimistic about simple, efficient, long-term scaling of multicore.

> 这些速度上的差异转化为能源效率的差异。例如，PARSEC 基准实际上略微提高了单核版本的能量效率。该结果可能受到以下事实的显着影响：L3 缓存在多核运行中比单核情况更有效，并且在两种情况下的能源成本都是相同的。因此，对于 parsec 的基准，多项式方法实现了设计师从以 ILP 为中心的设计转换为多层设计时所希望的。也就是说，它比缩放功率更快或更快地缩放性能，从而导致恒定甚至提高能源效率。在 Java 情况下，我们看到，由于 Java 工作负载的速度较低，即使在能源效率上，两核和四核的运行都不会中断（尽管 2P 运行的 Java 能源效率与 PARSEC 相同）。四核 Java 情况的能源效率相当高（0.94）。以 ILP 为中心的处理器可能需要更多的功率来实现 PARSEC 或 JAVA 工作负载上的可比较加速。因此，以 TLP 为中心的方法当然也比以 ILP 为中心的方法更好，以改善这些应用程序的性能。正如我们将在[第 5.10 节]（＃变形的尺度）中看到的那样，有理由对简单，高效，长期缩放的多项式感到悲观。

##### _Putting Multicore and SMT Together_

> ##### _ tuputting 多核心和 SMT _

Finally, we consider the combination of multicore and multithreading by measur- ing the two benchmark sets for two to four processors and one to two threads (a total of four data points and up to eight threads). [Figure 5.33](#_bookmark256) shows the speedup and energy efficiency obtained on the Intel i7 when the processor count is two or four and SMT is or is not employed, using harmonic mean to summarize the two benchmarks sets. Clearly, SMT can add to performance when there is sufficient thread-level parallelism available even in the multicore situation. For example, in the four-core, no-SMT case, the speedup efficiencies were 67% and 76% for Java and PARSEC, respectively. With SMT on four cores, those ratios are an astonishing 83% and 97%.

> 最后，我们通过测量两到四个处理器和一到两个线程的两个基准集（总共四个数据点和多达八个线程）来考虑多核和多线程的组合。[图 5.33]（#_ bookmark256）显示了当处理器计数为 2 或四个，而 SMT 是或不使用 SMT 时在英特尔 i7 上获得的加速和能量效率，使用谐波均值来总结两个基准集。显然，即使在多核心情况下，有足够的线程级并行性，SMT 也可以增加性能。例如，在四核，无 SMT 情况下，Java 和 Parsec 的加速效率分别为 67％和 76％。在四个核心上有 SMT，这些比率令人惊讶的是 83％和 97％。

Figure 5.33 This chart shows the speedup for two- and four-core executions of the parallel Java and PARSEC workloads both with and without SMT. Remember that the preceding results vary in the number of threads from two to eight and reflect both archi- tectural effects and application characteristics. Harmonic mean is used to summarize results, as discussed in the [Figure 5.32](#_bookmark254) caption.

> 图 5.33 此图显示了在有或没有 SMT 的情况下，平行 Java 和 Parsec 工作负载的两核执行的加速度。请记住，前面的结果在两个到 8 的线程数量上有所不同，并反映了拱形效应和应用特征。谐波平均值用于总结结果，如[图 5.32]（#_ bookmark254）标题中所述。

Energy efficiency presents a slightly different picture. In the case of PARSEC, speedup is essentially linear for the four-core SMT case (eight threads), and power scales more slowly, resulting in an energy efficiency of 1.1 for that case. The Java situation is more complex; energy efficiency peaks for the two-core SMT (four- thread) run at 0.97 and drops to 0.89 in the four-core SMT (eight-thread) run. It seems highly likely that the Java benchmarks are encountering Amdahl’s Law effects when more than four threads are deployed. As some architects have observed, multicore does shift more responsibility for performance (and thus energy efficiency) to the programmer, and the results for the Java workload certainly bear this out.

> 能源效率呈现出略有不同的情况。在 PARSEC 的情况下，对于四核 SMT 案例（八个线程），加速基本上是线性的，并且功率更慢，导致该案例的能源效率为 1.1。Java 情况更加复杂。两核 SMT（四线）的能源效率峰值为 0.97，在四核 SMT（八线）运行中降至 0.89。当部署了四个以上的线程时，Java 基准似乎很有可能遇到 Amdahl 的法律效果。正如一些建筑师所观察到的那样，多记者确实将对性能（以及能源效率）的更多责任转移到程序员身上，而 Java 工作量的结果无疑会解决。

Fallacies and Pitfalls

> 谬论和陷阱

Given the lack of maturity in our understanding of parallel computing, there are many hidden pitfalls that will be uncovered either by careful designers or by unfor- tunate ones. Given the large amount of hype that has surrounded multiprocessors over the years, common fallacies abound. We have included a selection of them.

> 鉴于我们对平行计算的理解缺乏成熟度，因此有许多隐藏的陷阱会被仔细的设计师或不合理的陷阱所揭示。考虑到多年来围绕多处理器的大量炒作，普通谬论比比皆是。我们包括其中的选择。

Pitfall _Measuring performance of multiprocessors by linear speedup versus execution time_.

> 通过线性加速与执行时间来测量多处理器的性能。

Graphs like those in [Figures 5.32](#_bookmark254) and [5.33](#_bookmark256), which plot performance versus number of processors, showing linear speedup, a plateau, and then a falling off, have long been used to judge the success of parallel processors. Although speedup is one facet of a parallel program, it is not a direct measure of performance. The first issue is the power of the processors being scaled: a program that linearly improves per- formance to equal 100 Intel Atom processors (the low-end processor used for net- books) may be slower than the version run on an 8-core Xeon. Be especially careful of floating-point-intensive programs; processing elements without hardware assist may scale wonderfully but have poor collective performance.

> 像[图 5.32]（#_ bookmark254）和[5.33]（#_ bookmark256）之类的图形，这些图是绘图性能与处理器数量的图形，显示了线性加速，高原，然后掉落，长期以来一直用来判断成功的成功。并行处理器。尽管加速度是平行程序的一个方面，但它不是直接衡量性能的衡量标准。第一个问题是正在缩放的处理器的功能：一个线性提高相等 100 个 Intel Atom 处理器（用于网簿使用的低端处理器）的程序比在 8 核上运行的版本要慢。Xeon。特别谨慎浮动的计划；没有硬件辅助的处理元素可能会出色地扩展，但集体表现差。

Comparing execution times is fair only if you are comparing the best algo- rithms on each computer. Comparing the identical code on two computers may seem fair, but it is not; the parallel program may be slower on a uniprocessor than on a sequential version. Developing a parallel program will sometimes lead to algorithmic improvements, so comparing the previously best-known sequential program with the parallel code—which seems fair—will not compare equivalent algorithms. To reflect this issue, the terms _relative speedup_ (same program) and _true speedup_ (best program) are sometimes used.

> 仅当您比较每台计算机上的最佳算法时，比较执行时间才公平。比较两台计算机上的相同代码似乎很公平，但事实并非如此。在单层处理器上，并行程序可以比顺序版本慢。制定并行程序有时会导致算法改进，因此将先前最著名的顺序程序与并行代码（似乎很公平）进行比较，将不会比较等效算法。为了反映此问题，有时会使用术语_relative speedup_（同一程序）和_true speedup_（最佳程序）。

Results that suggest _superlinear_ performance, when a program on _n_ processors is more than _n_ times faster than the equivalent uniprocessor, may indicate that the comparison is unfair, although there are instances where “real” superlinear speedups have been encountered. For example, some scientific applications regu- larly achieve superlinear speedup for small increases in processor count (2 or 4 to 8 or 16). These results usually arise because critical data structures that do not fit into the aggregate caches of a multiprocessor with 2 or 4 processors fit into the aggregate cache of a multiprocessor with 8 or 16 processors. As we saw in the pre- vious section, other differences (such as high clock rate) may appear to yield super- linear speedups when comparing slightly different systems.

> 表明_superlineAr_性能的结果是，当_n_处理器上的程序比等效的单层处理器快的速度大于_n_倍时，可能表明比较不公平，尽管在某些情况下，“真实”超级线性加速已遇到。例如，一些科学应用程序正在规定实现超线性加速，以促进处理器计数（2 或 4 至 8 或 16）的少量增加。这些结果通常是因为不适合具有 2 或 4 个处理器的多处理器的聚合缓存的关键数据结构适合带有 8 或 16 个处理器的多处理器的聚合缓存。正如我们在预科部分中看到的那样，在比较略有不同的系统时，其他差异（例如高时钟速率）似乎会产生超线性加速。

In summary, comparing performance by comparing speedups is at best tricky and at worst misleading. Comparing the speedups for two different multiproces- sors does not necessarily tell us anything about the relative performance of the mul- tiprocessors, as we also saw in the previous section. Even comparing two different algorithms on the same multiprocessor is tricky because we must use true speedup, rather than relative speedup, to obtain a valid comparison.

> 总而言之，通过比较速度比较性能充其量是棘手的，而且最坏的误导。正如我们在上一节中所看到的那样，比较两个不同的多个多样性的加速器并不一定告诉我们有关 Mul-Tip 处理器的相对性能的任何信息。即使比较同一多处理器上的两种不同的算法也很棘手，因为我们必须使用真实的加速而不是相对加速来获得有效的比较。

Fallacy _Amdahl_’_s Law doesn_’_t apply to parallel computers_.

> 谬误_amdahl _’_ s Law do dos _’_ t 适用于并行计算机_。

In 1987 the head of a research organization claimed that Amdahl’s Law (see Section 1.9) had been broken by an MIMD multiprocessor. This statement hardly meant, however, that the law has been overturned for parallel computers; the neglected portion of the program will still limit performance. To understand the basis of the media reports, let’s see what [Amdahl (1967)](#_bookmark919) originally said:

> 1987 年，一个研究组织的负责人声称，Amdahl 的法律（请参阅第 1.9 节）被 MIMD 多处理器打破。但是，这种说法并不意味着该法律已被推翻用于平行计算机。该程序的被忽视部分仍将限制性能。要了解媒体报告的基础，让我们看看[Amdahl（1967）]（#_ bookmark919）最初说：

A fairly obvious conclusion which can be drawn at this point is that the effort expended on achieving high parallel processing rates is wasted unless it is accompanied by achievements in sequential processing rates of very nearly the same magnitude. \[p. 483\]

> 在这一点上可以得出一个相当明显的结论是，除非伴随着在顺序处理速率上的成就，否则浪费了实现高平行处理速率所花费的努力。\ [p。483 \]

One interpretation of the law was that, because portions of every program must be sequential, there is a limit to the useful economic number of processors—say, 100. By showing linear speedup with 1000 processors, this interpretation of Amdahl’s Law was disproved.

> 对法律的一种解释是，由于每个程序的某些部分必须是顺序的，因此有用的经济数量有限制，例如 100。通过用 1000 个处理器显示线性加速，对 Amdahl 定律的这种解释被驳回。

The basis for the statement that Amdahl’s Law had been “overcome” was the use of _scaled speedup_, also called _weak scaling_. The researchers scaled the bench- mark to have a dataset size that was 1000 times larger and compared the unipro- cessor and parallel execution times of the scaled benchmark. For this particular algorithm, the sequential portion of the program was constant independent of the size of the input, and the rest was fully parallel—thus, linear speedup with 1000 processors. Because the running time grew faster than linear, the program actually ran longer after scaling, even with 1000 processors.

> Amdahl 定律被“克服”的陈述的基础是使用_Scaled Speedup_，也称为_ Weak Scaling_。研究人员将基准标记缩放为具有大 1000 倍的数据集大小，并比较了缩放基准的单层和并行执行时间。对于该特定算法，程序的顺序部分是恒定的，独立于输入的大小，其余部分是完全平行的，因此，使用 1000 个处理器的线性加速。因为运行时间的增长速度比线性更快，所以该程序在缩放后实际运行时间更长，即使使用 1000 个处理器。

Speedup that assumes scaling of the input is not the same as true speedup, and reporting it as if it were is misleading. Because parallel benchmarks are often run on different-sized multiprocessors, it is important to specify what type of applica- tion scaling is permissible and how that scaling should be done. Although simply scaling the data size with processor count is rarely appropriate, assuming a fixed problem size for a much larger processor count (called _strong scaling_) is often inappropriate, as well, because it is likely that users given a much larger multipro- cessor would opt to run a larger or more detailed version of an application. See Appendix I for more discussion on this important topic.

> 假设输入缩放的加速度与真实加速并不相同，并将其报告好像是误导性的。由于平行基准通常在不同大小的多处理器上运行，因此指定允许使用哪种类型的应用程序缩放以及应如何进行缩放，这一点很重要。尽管简单地使用处理器计数将数据大小缩放很少是合适的，但是假设较大的处理器计数的固定问题大小（称为_strong 缩放_）通常是不合适的，因为用户可能会给用户提供更大的多个多核心者会给选择运行更大或更详细的应用程序。有关此重要主题的更多讨论，请参见附录 I。

Fallacy _Linear speedups are needed to make multiprocessors cost-effective_.

> 需要谬误_线程加速器来使多处理器成本效益。

It is widely recognized that one of the major benefits of parallel computing is to offer a “shorter time to solution” than the fastest uniprocessor. Many people, however, also hold the view that parallel processors cannot be as cost-effective as uniproces- sors unless they can achieve perfect linear speedup. This argument says that, because the cost of the multiprocessor is a linear function of the number of processors, any- thing less than linear speedup means that the performance/cost ratio decreases, mak- ing a parallel processor less cost-effective than using a uniprocessor.

> 人们普遍认识到，并行计算的主要好处之一是提供比最快的单层处理器提供的“解决方案更短”。但是，许多人还认为，平行处理器不能像单次旋转器那样具有成本效益，除非他们能够实现完美的线性加速。该论点说，由于多处理器的成本是处理器数量的线性函数，因此任何比线性加速少的东西都意味着性能/成本比下降，使并行处理器的成本效益低于使用 A 单层处理器。

The problem with this argument is that cost is not only a function of processor count but also depends on memory, I/O, and the overhead of the system (box, power supply, interconnect, etc.). It also makes less sense in the multicore era, when there are multiple processors per chip.

> 该论点的问题在于，成本不仅是处理器计数的函数，还取决于内存，I/O 和系统的开销（框，电源，互连等）。在多核时代，当每个芯片有多个处理器时，这也不太有意义。

The effect of including memory in the system cost was pointed out by [Wood](#_bookmark1018) [and Hill (1995)](#_bookmark1018). We use an example based on more recent data using TPC-C and SPECRate benchmarks, but the argument could also be made with a parallel sci- entific application workload, which would likely make the case even stronger.

> [wood]（#_ bookmark1018）[and Hill（1995）]（#_ bookmark1018）指出了在系统成本中包括内存的效果。我们使用 TPC-C 和 Specrate 基准测试的最新数据使用示例，但是该参数也可以使用并行的 Scipifific 应用程序工作负载进行，这可能会使情况更加强大。

![](./media/image306.png)<span id="_bookmark257" class="anchor"></span>72

> ！[]（./媒体/image306.png）<span ID =“ _ bookmark257” class =“锚”> </span> 72

Figure 5.34 Speedup for three benchmarks on an IBM eServer p5 multiprocessor when configured with 4, 8, 16, 32, and 64 processors. The _dashed line_ shows linear speedup.

> 图 5.34 使用 4、8、16、32 和 64 个处理器配置时，IBM Eserver P5 多处理器上的三个基准测试器的加速。_dashed Line_显示线性加速。

[Figure 5.34](#_bookmark257) shows the speedup for TPC-C, SPECintRate, and SPECfpRate on an IBM eServer p5 multiprocessor configured with 4–64 processors. The figure shows that only TPC-C achieves better than linear speedup. For SPECintRate and SPECfpRate, speedup is less than linear, but so is the cost, because unlike TPC-C, the amount of main memory and disk required both scale less than linearly. As [Figure 5.35](#_bookmark258) shows, larger processor counts can actually be more cost- effective than the 4-processor configuration. In comparing the cost-performance of two computers, we must be sure to include accurate assessments of both total system cost and what performance is achievable. For many applications with larger memory demands, such a comparison can dramatically increase the attractiveness of using a multiprocessor.

> [图 5.34]（#_ bookmark257）在 IBM Eserver P5 多处理器上配置了 4-64 个处理器上的 IBM Eserver P5 多处理器上显示了 TPC-C，Specintrate 和 SpecFprate 的速度。该图显示，只有 TPC-C 比线性加速更好。对于 Specintrate 和 SpecFprate，加速度小于线性，但是成本也是如此，因为与 TPC-C 不同，主内存和磁盘的量和磁盘的量所需的比例比线性少。如[图 5.35]（#_ bookmark258）所示，较大的处理器计数实际上比 4 处理器配置更具成本效益。在比较两台计算机的成本表现时，我们必须确保对系统总成本进行准确评估以及可以实现的性能。对于许多具有更大内存需求的应用程序，这种比较可以大大提高使用多处理器的吸引力。

Pitfall _Not developing the software to take advantage of, or optimize for, a multiproces- sor architecture_.

> 陷阱_不开发软件来利用或优化多处理器体系结构。

There is a long history of software lagging behind on multiprocessors, probably because the software problems are much harder. We give one example to show the subtlety of the issues, but there are many examples we could choose from.

> 在多处理器上，软件落后的历史悠久，这可能是因为软件问题要困难得多。我们举一个例子来显示问题的微妙之处，但是我们可以选择许多示例。

One frequently encountered problem occurs when software designed for a uni- processor is adapted to a multiprocessor environment. For example, the SGI operating system in 2000 originally protected the page table data structure with a single lock, assuming that page allocation was infrequent. In a uniprocessor, this does not represent a performance problem. In a multiprocessor, it can become a major performance bottleneck for some programs.

> 当专为大学设计器设计的软件适应多处理器环境时，就会发生一个经常遇到的问题。例如，假设页面分配很少见，2000 年的 SGI 操作系统最初用单个锁定了页面数据结构。在单层处理器中，这并不代表性能问题。在多处理器中，它可能成为某些程序的主要性能瓶颈。

Figure 5.35 The performance/cost for IBM eServer p5 multiprocessors with 4–64 processors is shown relative to the 4-processor configuration. Any measurement above 1.0 indicates thatthe configuration is more cost-effective than the 4-processor system. The 8-processor configurations show an advantage for all three benchmarks, whereas two of the three benchmarks show a cost-performance advantage in the 16- and 32-processor configurations. For TPC-C, the con- figurations are those used in the official runs, which means that disk and memory scale nearly linearly with processor count, and a 64-processor machine is approximately twice as expensive as a 32-processor version. In contrast, the disk and memory are scaled more slowly (although still faster than necessary to achieve the best SPECRate at 64 processors). In particular, the disk configurations go from one drive for the 4-processor version to four drives (140 GB) for the 64- processor version. Memory is scaled from 8 GiB for the 4-processor system to 20 GiB for the 64-processor system.

> 图 5.35 相对于 4 处理器配置，显示了具有 4–64 处理器的 IBM Eserver P5 多处理器的性能/成本。任何高于 1.0 的测量值都表明该配置比 4 处理器系统更具成本效益。8 处理器配置显示了所有三个基准的优势，而三个基准中的两个在 16 和 32 处理器配置中显示出成本效果的优势。对于 TPC-C，构造是在官方运行中使用的，这意味着与处理器计数几乎线性地线性磁盘和内存量表，而 64 个处理器机器的昂贵大约是 32 处理器版本的两倍。相比之下，磁盘和内存的缩放尺度较慢（尽管在 64 个处理器上达到最佳规格的速度仍然比必要的速度快）。特别是，磁盘配置从 4 个处理器版本的一个驱动器到 64 个处理器版本的四个驱动器（140 GB）。对于 4 个处理器系统，将存储器从 8 个 GIB 缩放到 64 处理器系统的 20 GIB。

Consider a program that uses a large number of pages that are initialized at startup, which UNIX does for statically allocated pages. Suppose the program is parallelized so that multiple processes allocate the pages. Because page allocation requires the use of the page table data structure, which is locked whenever it is in use, even an OS kernel that allows multiple threads in the OS will be serialized if the processes all try to allocate their pages at once (which is exactly what we might expect at initialization time).

> 考虑一个程序，该程序使用大量在启动时初始化的页面，UNIX 适用于静态分配的页面。假设程序是并行化的，因此多个过程分配页面。因为页面分配需要使用页表数据结构（每当使用时锁定），即使是一个允许在操作系统中的多个线程的 OS 内核，如果该过程都试图一次分配其页面（这是）正是我们在初始化时可能期望的）。

This page table serialization eliminates parallelism in initialization and has sig- nificant impact on overall parallel performance. This performance bottleneck per- sists even under multiprogramming. For example, suppose we split the parallel program apart into separate processes and run them, one process per processor, so that there is no sharing between the processes. (This is exactly what one user did, because he reasonably believed that the performance problem was due to unin- tended sharing or interference in his application.) Unfortunately, the lock still seri- alizes all the processes, so even the multiprogramming performance is poor. This pitfall indicates the kind of subtle but significant performance bugs that can arise when software runs on multiprocessors. Like many other key software compo- nents, the OS algorithms and data structures must be rethought in a multiprocessor context. Placing locks on smaller portions of the page table effectively eliminates the problem. Similar problems exist in memory structures, which increases the coherence traffic in cases where no sharing is actually occurring.

> 此页表序列化消除了初始化中的并行性，并对整体并行性能产生了显着影响。即使在多编程下，这种性能瓶颈也是如此。例如，假设我们将并行程序分开分为单独的流程并运行它们，每个处理器一个过程，因此过程之间没有共享。（这正是一个用户所做的，因为他合理地认为性能问题是由于对他的应用程序的分享或干扰。。这个陷阱指示了当软件在多处理器上运行时可能出现的微妙但重要的性能错误。像许多其他关键软件组合一样，必须在多处理器上下文中重新考虑 OS 算法和数据结构。将锁放在页面表的较小部分上可以有效消除问题。在内存结构中存在类似的问题，在不存在共享的情况下，这会增加连贯流量。

As multicore became the dominant theme in everything from desktops to servers, the lack of an adequate investment in parallel software became apparent. Given the lack of focus, it will likely be many years before the software systems we use adequately exploit the growing numbers of cores.

> 随着从台式机到服务器的所有事物中的主题成为主要主题，对并行软件缺乏足够的投资变得显而易见。鉴于缺乏重点，我们使用的软件系统可能会在充分利用越来越多的核心数量之前已经有很多年了。
