<img src="./media/image1.jpeg" style="width:7.5in;height:4.79in" />![](./media/image2.png)

> ooh n L. Hennessy David A. Patterson
>
> <img src="./media/image4.png" style="width:0.465in;height:0.475in" />COTPUTč

ARCH ITčCTURI

> **Computer Architecture Formulas**

1. _CPU time_ = Instruction count × Clock cycles per instruction ×
   Clock cycle time
2. X is _n_ times faster than Y: _n_ = Execution timeY / Execution
   timeX = PerformanceX / PerformanceY
3. _Amdahl’s Law:_ Speedupoverall =

> Execution timeold
>
> Execution timenew
>
> =
>
> (#1 – Fraction

1

enhanced

)+ Fractionenhanced

1. _Energy_ 1/ 2 × Capacitive load ×Voltage<sup>2</sup>
2. _Power_ 1/ 2 × Capacitive load × Voltage<sup>2</sup> × Frequency
   switched
3. *Power*static Currentstatic × Voltage

> Speedupenhanced

1. _Availability_ = Mean time to fail / (Mean time to fail + Mean time
   to repair)
2. _Die yield_ = Wafer yield ×1 / (1 + Defects per unit area ×Die
   area)_N_

> where Wafer yield accounts for wafers that are so bad they need not be
> tested and _N_ is a parameter called the process-complexity factor, a
> measure of manufacturing difficulty. _N_ ranges from 11.5 to 15.5 in 2011.

1. _Means—arithmetic (AM), weighted arithmetic (WAM), and geometric
   (GM):_

> _n n_
>
> 1
>
> <img src="./media/image5.png"
> style="width:0.12014in;height:0.14982in" /><img src="./media/image6.png"
> style="width:0.12015in;height:0.14982in" />AM = -- Time WAM =
>
> _n_
>
> Weight*i* ×Time*i*
>
> GM = _i_
>
> _i_ =1 _i_ =1
>
> where Time*i* is the execution time for the *i*th program of a total
> of _n_ in the workload, Weight*i* is the weighting of the *i*th
> program in the workload.

1. _Average memory-access time_ = Hit time + Miss rate × Miss penalty
2. _Misses per instruction_ = Miss rate × Memory access per instruction
3. _Cache index size:_ 2<sup>index</sup> = Cache size /(Block size ×
   Set associativity)
4. _Power Utilization Effectiveness_ (_PUE_

> **Rules of Thumb**

Total Facility Power

) of a Warehouse Scale Computer =
------------------**-**----------------------------**-**

IT Equipment Power

1. _Amdahl/Case Rule:_ A balanced computer system needs about 1 MB of
   main memory capacity and 1 megabit per second of I/O bandwidth per
   MIPS of CPU performance.
2. _90/10 Locality Rule:_ A program executes about 90% of its
   instructions in 10% of its code.
3. _Bandwidth Rule:_ Bandwidth grows by at least the square of the
   improvement in latency.
4. _2:1 Cache Rule:_ The miss rate of a direct-mapped cache of size _N_
   is about the same as a two-way set- associative cache of size _N_/2.
5. _Dependability Rule:_ Design with no single point of failure.
6. _Watt-Year Rule_: The fully burdened cost of a Watt per year in a
   Warehouse Scale Computer in North America in 2011, including the
   cost of amortizing the power and cooling infrastructure, is about
   $2.

> In Praise of _Computer Architecture: A Quantitative Approach_

### Sixth Edition

> “Although important concepts of architecture are timeless, this
> edition has been thoroughly updated with the latest technology
> developments, costs, examples, and references. Keeping pace with
> recent developments in open-sourced architec- ture, the instruction
> set architecture used in the book has been updated to use the RISC-V
> ISA.”
>
> —from the foreword by Norman P. Jouppi, Google
>
> “_Computer Architecture: A Quantitative Approach_ is a classic that,
> like fine wine, just keeps getting better. I bought my first copy as I
> finished up my undergraduate degree and it remains one of my most
> frequently referenced texts today.”
>
> —James Hamilton, Amazon Web Service
>
> “Hennessy and Patterson wrote the first edition of this book when
> graduate stu- dents built computers with 50,000 transistors. Today,
> warehouse-size computers contain that many servers, each consisting of
> dozens of independent processors and billions of transistors. The
> evolution of computer architecture has been rapid and relentless, but
> _Computer Architecture: A Quantitative Approach_ has kept pace, with
> each edition accurately explaining and analyzing the important
> emerging ideas that make this field so exciting.”
>
> —James Larus, Microsoft Research
>
> “Another timely and relevant update to a classic, once again also
> serving as a win- dow into the relentless and exciting evolution of
> computer architecture! The new discussions in this edition on the
> slowing of Moore's law and implications for future systems are
> must-reads for both computer architects and practitioners working on
> broader systems.”
>
> —Parthasarathy (Partha) Ranganathan, Google
>
> “I love the ‘Quantitative Approach’ books because they are written by
> engineers, for engineers. John Hennessy and Dave Patterson show the
> limits imposed by mathematics and the possibilities enabled by
> materials science. Then they teach through real-world examples how
> architects analyze, measure, and compromise to build working systems.
> This sixth edition comes at a critical time: Moore’s Law is fading
> just as deep learning demands unprecedented compute cycles. The new
> chapter on domain-specific architectures documents a number of prom-
> ising approaches and prophesies a rebirth in computer architecture.
> Like the scholars of the European Renaissance, computer architects
> must understand our own history, and then combine the lessons of that
> history with new techniques to remake the world.”
>
> —Cliff Young, Google
>
> This page intentionally left blank
>
> John L. Hennessy is a Professor of Electrical Engineering and Computer
> Science at Stanford University, where he has been a member of the
> faculty since 1977 and was, from 2000 to 2016, its 10th President. He
> currently serves as the Director of the Knight-Hennessy Fellow- ship,
> which provides graduate fellowships to potential future leaders.
> Hennessy is a Fellow of the IEEE and ACM, a member of the National
> Academy of Engineering, the National Acad- emy of Science, and the
> American Philosophical Society, and a Fellow of the American Acad- emy
> of Arts and Sciences. Among his many awards are the 2001
> Eckert-Mauchly Award for his contributions to RISC technology, the
> 2001 Seymour Cray Computer Engineering Award, and the 2000 John von
> Neumann Award, which he shared with David Patterson. He has also
> received 10 honorary doctorates.
>
> In 1981, he started the MIPS project at Stanford with a handful of
> graduate students. After completing the project in 1984, he took a
> leave from the university to cofound MIPS Com- puter Systems, which
> developed one of the first commercial RISC microprocessors. As of
> 2017, over 5 billion MIPS microprocessors have been shipped in devices
> ranging from video games and palmtop computers to laser printers and
> network switches. Hennessy subse- quently led the DASH (Director
> Architecture for Shared Memory) project, which prototyped the first
> scalable cache coherent multiprocessor; many of the key ideas have
> been adopted in modern multiprocessors. In addition to his technical
> activities and university responsibil- ities, he has continued to work
> with numerous start-ups, both as an early-stage advisor and an
> investor.
>
> David A. Patterson became a Distinguished Engineer at Google in 2016
> after 40 years as a UC Berkeley professor. He joined UC Berkeley
> immediately after graduating from UCLA. He still spends a day a week
> in Berkeley as an Emeritus Professor of Computer Science. His teaching
> has been honored by the Distinguished Teaching Award from the
> University of California, the Karlstrom Award from ACM, and the
> Mulligan Education Medal and Under- graduate Teaching Award from IEEE.
> Patterson received the IEEE Technical Achievement Award and the ACM
> Eckert-Mauchly Award for contributions to RISC, and he shared the IEEE
> Johnson Information Storage Award for contributions to RAID. He also
> shared the IEEE John von Neumann Medal and the C & C Prize with John
> Hennessy. Like his co-author, Patterson is a Fellow of the American
> Academy of Arts and Sciences, the Computer History Museum, ACM, and
> IEEE, and he was elected to the National Academy of Engineering, the
> National Academy of Sciences, and the Silicon Valley Engineering Hall
> of Fame. He served on the Information Technology Advisory Committee to
> the President of the United States, as chair of the CS division in the
> Berkeley EECS department, as chair of the Computing Research
> Association, and as President of ACM. This record led to Distinguished
> Service Awards from ACM, CRA, and SIGARCH. He is currently Vice-Chair
> of the Board of Directors of the RISC-V Foundation.
>
> At Berkeley, Patterson led the design and implementation of RISC I,
> likely the first VLSI reduced instruction set computer, and the
> foundation of the commercial SPARC architec- ture. He was a leader of
> the Redundant Arrays of Inexpensive Disks (RAID) project, which led to
> dependable storage systems from many companies. He was also involved
> in the Network of Workstations (NOW) project, which led to cluster
> technology used by Internet companies and later to cloud computing.
> His current interests are in designing domain-specific archi- tectures
> for machine learning, spreading the word on the open RISC-V
> instruction set archi- tecture, and in helping the UC Berkeley RISELab
> (Real-time Intelligent Secure Execution).

# John L. Hennessy

> _Stanford University_

# David A. Patterson

##### _University of California, Berkeley_

> With Contributions by
>
> Krste Asanovi´c
>
> _University of California, Berkeley_
>
> Jason D. Bakos
>
> _University of South Carolina_
>
> Robert P. Colwell
>
> _R&E Colwell & Assoc. Inc._ Abhishek Bhattacharjee _Rutgers
> University_ Thomas M. Conte _Georgia Tech_
>
> Jose´ Duato
>
> _Proemisa_
>
> Diana Franklin _University of Chicago_ David Goldberg _eBay_
>
> Norman P. Jouppi
>
> _Google_ Sheng Li _Intel Labs_
>
> Naveen Muralimanohar
>
> _HP Labs_
>
> Gregory D. Peterson _University of Tennessee_ Timothy M. Pinkston
> _University of Southern California_ Parthasarathy Ranganathan _Google_
>
> David A. Wood
>
> _University of Wisconsin_–_Madison_
>
> Cliff Young
>
> _Google_
>
> Amr Zaky
>
> ![](./media/image7.png)_University of Santa Clara_
>
> Morgan Kaufmann is an imprint of Elsevier
>
> 50 Hampshire Street, 5th Floor, Cambridge, MA 02139, United States
>
> © 2019 Elsevier Inc. All rights reserved.
>
> No part of this publication may be reproduced or transmitted in any
> form or by any means, electronic or mechanical, including
> photocopying, recording, or any information storage and retrieval
> system, without permission in writing from the publisher. Details on
> how to seek permission, further information about the Publisher’s
> permissions policies and our arrangements with organizations such as
> the Copyright Clearance Center and the Copyright Licensing Agency, can
> be found at our website:
> [www.elsevier.com/permissions](http://www.elsevier.com/permissions).
>
> This book and the individual contributions contained in it are
> protected under copyright by the Publisher (other than as may be noted
> herein).
>
> Notices
>
> Knowledge and best practice in this field are constantly changing. As
> new research and experience broaden our understanding, changes in
> research methods, professional practices, or medical treatment may
> become necessary.
>
> Practitioners and researchers must always rely on their own experience
> and knowledge in evaluating and using any information, methods,
> compounds, or experiments described herein. In using such information
> or methods they should be mindful of their own safety and the safety
> of others, including parties for whom they have a professional
> responsibility.
>
> To the fullest extent of the law, neither the Publisher nor the
> authors, contributors, or editors, assume any liability for any injury
> and/or damage to persons or property as a matter of products
> liability, negligence or otherwise, or from any use or operation of
> any methods, products, instructions, or ideas contained in the
> material herein.
>
> Library of Congress Cataloging-in-Publication Data
>
> A catalog record for this book is available from the Library of
> Congress
>
> British Library Cataloguing-in-Publication Data
>
> A catalogue record for this book is available from the British Library
> ISBN: 978-0-12-811905-1

![](./media/image8.png)

> _Publisher:_ Katey Birtcher
>
> _Acquisition Editor:_ Stephen Merken _Developmental Editor:_ Nate
> McFadden _Production Project Manager:_ Stalin Viswanathan _Cover
> Designer:_ Christian J. Bilbow
>
> Typeset by SPi Global, India
>
> _To Andrea, Linda, and our four sons_
>
> This page intentionally left blank
>
> Foreword
>
> _by Norman P. Jouppi, Google_
>
> Much of the improvement in computer performance over the last 40 years
> has been provided by computer architecture advancements that have
> leveraged Moore’s Law and Dennard scaling to build larger and more
> parallel systems. Moore’s Law is the observation that the maximum
> number of transistors in an integrated circuit doubles approximately
> every two years. Dennard scaling refers to the reduc- tion of MOS
> supply voltage in concert with the scaling of feature sizes, so that
> as transistors get smaller, their power density stays roughly
> constant. With the end of Dennard scaling a decade ago, and the recent
> slowdown of Moore’s Law due to a combination of physical limitations
> and economic factors, the sixth edition of the preeminent textbook for
> our field couldn’t be more timely. Here are some reasons. First,
> because domain-specific architectures can provide equivalent perfor-
> mance and power benefits of three or more historical generations of
> Moore’s Law and Dennard scaling, they now can provide better
> implementations than may ever be possible with future scaling of
> general-purpose architectures. And with the diverse application space
> of computers today, there are many potential areas for architectural
> innovation with domain-specific architectures. Second, high-quality
> implementations of open-source architectures now have a much lon- ger
> lifetime due to the slowdown in Moore’s Law. This gives them more
> oppor- tunities for continued optimization and refinement, and hence
> makes them more attractive. Third, with the slowing of Moore’s Law,
> different technology compo- nents have been scaling heterogeneously.
> Furthermore, new technologies such as
>
> 2.5D stacking, new nonvolatile memories, and optical interconnects
> have been developed to provide more than Moore’s Law can supply alone.
> To use these new technologies and nonhomogeneous scaling effectively,
> fundamental design decisions need to be reexamined from first
> principles. Hence it is important for students, professors, and
> practitioners in the industry to be skilled in a wide range of both
> old and new architectural techniques. All told, I believe this is the
> most exciting time in computer architecture since the industrial
> exploitation of instruction-level parallelism in microprocessors 25
> years ago.
>
> The largest change in this edition is the addition of a new chapter on
> domain- specific architectures. It’s long been known that customized
> domain-specific archi- tectures can have higher performance, lower
> power, and require less silicon area than general-purpose processor
> implementations. However when general-purpose

#### ix

> x ■ Foreword
>
> processors were increasing in single-threaded performance by 40% per
> year (see Fig. 1.11), the extra time to market required to develop a
> custom architecture vs. using a leading-edge standard microprocessor
> could cause the custom architecture to lose much of its advantage. In
> contrast, today single-core performance is improving very slowly,
> meaning that the benefits of custom architectures will not be made
> obsolete by general-purpose processors for a very long time, if ever.
> [Chapter 7](#_bookmark322) covers several domain-specific
> architectures. Deep neural networks have very high computation
> requirements but lower data precision requirements – this combination
> can benefit significantly from custom architectures. Two example
> architectures and implementations for deep neural networks are
> presented: one optimized for inference and a second optimized for
> training. Image processing is another example domain; it also has high
> computation demands and benefits from lower-precision data types.
> Furthermore, since it is often found in mobile devices, the power
> savings from custom architectures are also very valuable. Finally, by
> nature of their reprogrammability, FPGA-based accelerators can be used
> to implement a variety of different domain-specific architectures on a
> single device. They also can benefit more irregular applications that
> are frequently updated, like accelerating internet search.
>
> Although important concepts of architecture are timeless, this edition
> has been thoroughly updated with the latest technology developments,
> costs, examples, and references. Keeping pace with recent developments
> in open-sourced architecture, the instruction set architecture used in
> the book has been updated to use the RISC-V ISA.
>
> On a personal note, after enjoying the privilege of working with John
> as a grad- uate student, I am now enjoying the privilege of working
> with Dave at Google. What an amazing duo!

#### Foreword ix

#### Preface xvii

#### Acknowledgments xxv

#### Chapter 1 Fundamentals of Quantitative Design and Analysis

1. Introduction 2
2. Classes of Computers 6
3. Defining Computer Architecture 11
4. Trends in Technology 18
5. Trends in Power and Energy in Integrated Circuits 23
6. Trends in Cost 29
7. Dependability 36
8. Measuring, Reporting, and Summarizing Performance 39
9. Quantitative Principles of Computer Design 48
10. Putting It All Together: Performance, Price, and Power 55
11. Fallacies and Pitfalls 58
12. Concluding Remarks 64
13. Historical Perspectives and References 67

> Case Studies and Exercises by Diana Franklin 67

#### Chapter 2 Memory Hierarchy Design

<table>
<colgroup>
<col style="width: 92%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr class="header">
<th><blockquote>
<p>2.1 Introduction</p>
</blockquote></th>
<th>78</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><blockquote>
<p>2.2 Memory Technology and Optimizations</p>
</blockquote></td>
<td>84</td>
</tr>
<tr class="even">
<td><blockquote>
<p>2.3 Ten Advanced Optimizations of Cache Performance</p>
</blockquote></td>
<td>94</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>2.4 Virtual Memory and Virtual Machines</p>
</blockquote></td>
<td>118</td>
</tr>
<tr class="even">
<td><ol class="incremental" start="5" type="1">
<li><blockquote>
<p>Cross-Cutting Issues: The Design of Memory Hierarchies</p>
</blockquote></li>
<li><blockquote>
<p>Putting It All Together: Memory Hierarchies in the ARM Cortex-A53</p>
</blockquote></li>
</ol></td>
<td>126</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>and Intel Core i7 6700</p>
</blockquote></td>
<td>129</td>
</tr>
<tr class="even">
<td><blockquote>
<p>2.7 Fallacies and Pitfalls</p>
</blockquote></td>
<td>142</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>2.8 Concluding Remarks: Looking Ahead</p>
</blockquote></td>
<td>146</td>
</tr>
<tr class="even">
<td><blockquote>
<p>2.9 Historical Perspectives and References</p>
</blockquote></td>
<td>148</td>
</tr>
<tr class="odd">
<td></td>
<td>xi</td>
</tr>
</tbody>
</table>

> Case Studies and Exercises by Norman P. Jouppi, Rajeev

Balasubramonian, Naveen Muralimanohar, and Sheng Li 148

Chapter 3 Instruction-Level Parallelism and Its Exploitation

1. Instruction-Level Parallelism: Concepts and Challenges 168
2. Basic Compiler Techniques for Exposing ILP 176
3. Reducing Branch Costs With Advanced Branch Prediction 182
4. Overcoming Data Hazards With Dynamic Scheduling 191
5. Dynamic Scheduling: Examples and the Algorithm 201
6. Hardware-Based Speculation 208
7. Exploiting ILP Using Multiple Issue and Static Scheduling 218
8. Exploiting ILP Using Dynamic Scheduling, Multiple Issue, and

Speculation 222

1. Advanced Techniques for Instruction Delivery and Speculation 228
2. Cross-Cutting Issues 240
3. Multithreading: Exploiting Thread-Level Parallelism to Improve
   Uniprocessor Throughput 242
4. Putting It All Together: The Intel Core i7 6700 and ARM Cortex-A53
   247
5. Fallacies and Pitfalls 258
6. Concluding Remarks: What’s Ahead? 264
7. Historical Perspective and References 266

[Case Studies and Exercises by Jason D. Bakos and Robert P. Colwell
266](#case-studies-and-exercises-by-jason-d.-bakos-and-robert-p.-colwell)

Chapter 4 Data-Level Parallelism in Vector, SIMD, and GPU Architectures

1. Introduction 282
2. Vector Architecture 283
3. SIMD Instruction Set Extensions for Multimedia 304
4. Graphics Processing Units 310
5. Detecting and Enhancing Loop-Level Parallelism 336
6. Cross-Cutting Issues 345
7. Putting It All Together: Embedded Versus Server GPUs and

Tesla Versus Core i7 346

1. Fallacies and Pitfalls 353
2. Concluding Remarks 357
3. Historical Perspective and References 357

[Case Study and Exercises by Jason D. Bakos
357](#case-study-and-exercises-by-jason-d.-bakos)

Chapter 5 Thread-Level Parallelism

1. Introduction 368
2. Centralized Shared-Memory Architectures 377
3. Performance of Symmetric Shared-Memory Multiprocessors 393
4. Distributed Shared-Memory and Directory-Based Coherence 404
5. Synchronization: The Basics 412
6. Models of Memory Consistency: An Introduction 417
7. Cross-Cutting Issues 422
8. Putting It All Together: Multicore Processors and Their Performance
   426
9. Fallacies and Pitfalls 438
10. The Future of Multicore Scaling 442
11. Concluding Remarks 444
12. Historical Perspectives and References 445

[Case Studies and Exercises by Amr Zaky and David A. Wood
446](#case-studies-and-exercises-by-amr-zaky-and-david-a.-wood)

> Chapter 6 Warehouse-Scale Computers to Exploit Request-Level and
> Data-Level Parallelism

1. Introduction 466
2. Programming Models and Workloads for Warehouse-Scale

Computers 471

1. Computer Architecture of Warehouse-Scale Computers 477
2. The Efficiency and Cost of Warehouse-Scale Computers 482
3. Cloud Computing: The Return of Utility Computing 490
4. Cross-Cutting Issues 501
5. Putting It All Together: A Google Warehouse-Scale Computer 503
6. Fallacies and Pitfalls 514
7. Concluding Remarks 518
8. Historical Perspectives and References 519

[Case Studies and Exercises by Parthasarathy Ranganathan
519](#case-studies-and-exercises-by-parthasarathy-ranganathan)

Chapter 7 Domain-Specific Architectures

1. Introduction 540
2. Guidelines for DSAs 543
3. Example Domain: Deep Neural Networks 544
4. Google’s Tensor Processing Unit, an Inference Data

Center Accelerator 557

1. Microsoft Catapult, a Flexible Data Center Accelerator 567
2. Intel Crest, a Data Center Accelerator for Training 579
3. Pixel Visual Core, a Personal Mobile Device Image Processing Unit
   579
4. Cross-Cutting Issues 592
5. Putting It All Together: CPUs Versus GPUs Versus DNN Accelerators
   595
6. Fallacies and Pitfalls 602
7. Concluding Remarks 604
8. Historical Perspectives and References 606

[Case Studies and Exercises by Cliff Young
606](#case-studies-and-exercises-by-cliff-young)

<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 73%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="header">
<th colspan="2"><blockquote>
<p>Appendix A Instruction Set Principles</p>
</blockquote></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="2"><blockquote>
<p>A.1 Introduction</p>
</blockquote></td>
<td>A-2</td>
</tr>
<tr class="even">
<td colspan="2"><blockquote>
<p>A.2 Classifying Instruction Set Architectures</p>
</blockquote></td>
<td>A-3</td>
</tr>
<tr class="odd">
<td colspan="2"><blockquote>
<p>A.3 Memory Addressing</p>
</blockquote></td>
<td>A-7</td>
</tr>
<tr class="even">
<td colspan="2"><blockquote>
<p>A.4 Type and Size of Operands</p>
</blockquote></td>
<td>A-13</td>
</tr>
<tr class="odd">
<td colspan="2"><blockquote>
<p>A.5 Operations in the Instruction Set</p>
</blockquote></td>
<td>A-15</td>
</tr>
<tr class="even">
<td colspan="2"><blockquote>
<p>A.6 Instructions for Control Flow</p>
</blockquote></td>
<td>A-16</td>
</tr>
<tr class="odd">
<td colspan="2"><blockquote>
<p>A.7 Encoding an Instruction Set</p>
</blockquote></td>
<td>A-21</td>
</tr>
<tr class="even">
<td colspan="2"><blockquote>
<p>A.8 Cross-Cutting Issues: The Role of Compilers</p>
</blockquote></td>
<td>A-24</td>
</tr>
<tr class="odd">
<td colspan="2"><blockquote>
<p>A.9 Putting It All Together: The RISC-V Architecture</p>
</blockquote></td>
<td>A-33</td>
</tr>
<tr class="even">
<td colspan="2"><blockquote>
<p>A.10 Fallacies and Pitfalls</p>
</blockquote></td>
<td>A-42</td>
</tr>
<tr class="odd">
<td colspan="2"><blockquote>
<p>A.11 Concluding Remarks</p>
</blockquote></td>
<td>A-46</td>
</tr>
<tr class="even">
<td colspan="2"><blockquote>
<p>A.12 Historical Perspective and References</p>
</blockquote></td>
<td>A-47</td>
</tr>
<tr class="odd">
<td colspan="2"><blockquote>
<p>Exercises by Gregory D. Peterson</p>
</blockquote></td>
<td>A-47</td>
</tr>
<tr class="even">
<td><blockquote>
<p>Appendix B</p>
</blockquote></td>
<td><blockquote>
<p>Review of Memory Hierarchy</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>B.1 Introduction</p>
</blockquote></td>
<td>B-2</td>
</tr>
<tr class="even">
<td></td>
<td><ol class="incremental" start="2" type="1">
<li><blockquote>
<p>Cache Performance</p>
</blockquote></li>
<li><blockquote>
<p>Six Basic Cache Optimizations</p>
</blockquote></li>
</ol></td>
<td><blockquote>
<p>B-15</p>
<p>B-22</p>
</blockquote></td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>B.4 Virtual Memory</p>
</blockquote></td>
<td>B-40</td>
</tr>
<tr class="even">
<td></td>
<td><blockquote>
<p>B.5 Protection and Examples of Virtual Memory</p>
</blockquote></td>
<td>B-49</td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>B.6 Fallacies and Pitfalls</p>
</blockquote></td>
<td>B-57</td>
</tr>
<tr class="even">
<td></td>
<td><blockquote>
<p>B.7 Concluding Remarks</p>
</blockquote></td>
<td>B-59</td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>B.8 Historical Perspective and References</p>
</blockquote></td>
<td>B-59</td>
</tr>
<tr class="even">
<td></td>
<td><blockquote>
<p>Exercises by Amr Zaky</p>
</blockquote></td>
<td>B-60</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Appendix C</p>
</blockquote></td>
<td><blockquote>
<p>Pipelining: Basic and Intermediate Concepts</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td><blockquote>
<p>C.1 Introduction</p>
</blockquote></td>
<td>C-2</td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>C.2 The Major Hurdle of Pipelining—Pipeline Hazards</p>
</blockquote></td>
<td>C-10</td>
</tr>
<tr class="even">
<td></td>
<td><blockquote>
<p>C.3 How Is Pipelining Implemented?</p>
</blockquote></td>
<td>C-26</td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>C.4 What Makes Pipelining Hard to Implement?</p>
</blockquote></td>
<td>C-37</td>
</tr>
<tr class="even">
<td></td>
<td><blockquote>
<p>C.5 Extending the RISC V Integer Pipeline to Handle Multicycle</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>Operations</p>
<p>C.6 Putting It All Together: The MIPS R4000 Pipeline</p>
</blockquote></td>
<td><blockquote>
<p>C-45</p>
<p>C-55</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td><blockquote>
<p>C.7 Cross-Cutting Issues</p>
</blockquote></td>
<td>C-65</td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>C.8 Fallacies and Pitfalls</p>
</blockquote></td>
<td>C-70</td>
</tr>
<tr class="even">
<td></td>
<td><blockquote>
<p>C.9 Concluding Remarks</p>
</blockquote></td>
<td>C-71</td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>C.10 Historical Perspective and References</p>
</blockquote></td>
<td>C-71</td>
</tr>
<tr class="even">
<td></td>
<td><blockquote>
<p>Updated Exercises by Diana Franklin</p>
</blockquote></td>
<td>C-71</td>
</tr>
</tbody>
</table>

<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 74%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><blockquote>
<p><em>Online Appendices</em></p>
</blockquote></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Appendix D</p>
</blockquote></td>
<td><blockquote>
<p>Storage Systems</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td><blockquote>
<p>Appendix E</p>
</blockquote></td>
<td><blockquote>
<p>Embedded Systems</p>
<p><em>by Thomas M. Conte</em></p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Appendix F</p>
</blockquote></td>
<td><blockquote>
<p>Interconnection Networks</p>
<p><em>by Timothy M. Pinkston and Jos</em>´<em>e Duato</em></p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td><blockquote>
<p>Appendix G</p>
</blockquote></td>
<td><blockquote>
<p>Vector Processors in More Depth</p>
<p><em>by Krste Asanovic</em></p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Appendix H</p>
</blockquote></td>
<td><blockquote>
<p>Hardware and Software for VLIW and EPIC</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td><blockquote>
<p>Appendix I</p>
</blockquote></td>
<td><blockquote>
<p>Large-Scale Multiprocessors and Scientific Applications</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Appendix J</p>
</blockquote></td>
<td><blockquote>
<p>Computer Arithmetic</p>
<p><em>by David Goldberg</em></p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td><blockquote>
<p>Appendix K</p>
</blockquote></td>
<td><blockquote>
<p>Survey of Instruction Set Architectures</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Appendix L</p>
</blockquote></td>
<td><blockquote>
<p>Advanced Concepts on Address Translation</p>
<p><em>by Abhishek Bhattacharjee</em></p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td><blockquote>
<p>Appendix M</p>
</blockquote></td>
<td><blockquote>
<p>Historical Perspectives and References</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>References</p>
</blockquote></td>
<td>R-1</td>
</tr>
<tr class="even">
<td></td>
<td><blockquote>
<p>Index</p>
</blockquote></td>
<td>I-1</td>
</tr>
</tbody>
</table>

> This page intentionally left blank

### Why We Wrote This Book

> Through six editions of this book, our goal has been to describe the
> basic principles underlying what will be tomorrow’s technological
> developments. Our excitement about the opportunities in computer
> architecture has not abated, and we echo what we said about the field
> in the first edition: “It is not a dreary science of paper machines
> that will never work. No! It’s a discipline of keen intellectual
> interest, requiring the balance of marketplace forces to
> cost-performance-power, leading to glorious failures and some notable
> successes.”
>
> Our primary objective in writing our first book was to change the way
> people learn and think about computer architecture. We feel this goal
> is still valid and important. The field is changing daily and must be
> studied with real examples and measurements on real computers, rather
> than simply as a collection of defini- tions and designs that will
> never need to be realized. We offer an enthusiastic wel- come to
> anyone who came along with us in the past, as well as to those who are
> joining us now. Either way, we can promise the same quantitative
> approach to, and analysis of, real systems.
>
> As with earlier versions, we have strived to produce a new edition
> that will continue to be as relevant for professional engineers and
> architects as it is for those involved in advanced computer
> architecture and design courses. Like the first edi- tion, this
> edition has a sharp focus on new platforms—personal mobile devices and
> warehouse-scale computers—and new architectures—specifically, domain-
> specific architectures. As much as its predecessors, this edition aims
> to demystify computer architecture through an emphasis on
> cost-performance-energy trade-offs and good engineering design. We
> believe that the field has continued to mature and move toward the
> rigorous quantitative foundation of long-established scientific and
> engineering disciplines.
>
> xvii

### This Edition

> The ending of Moore’s Law and Dennard scaling is having as profound
> effect on computer architecture as did the switch to multicore. We
> retain the focus on the extremes in size of computing, with personal
> mobile devices (PMDs) such as cell phones and tablets as the clients
> and warehouse-scale computers offering cloud computing as the server.
> We also maintain the other theme of parallelism in all its forms:
> _data-level parallelism (DLP)_ in Chapters [1](#_bookmark2) and
> [4](#_bookmark165), _instruction-level par- allelism (ILP)_ in
> [Chapter 3](#_bookmark93), _thread-level parallelism_ in [Chapter
> 5](#_bookmark213), and _request- level parallelism_ (RLP) in [Chapter
> 6](#_bookmark268).
>
> The most pervasive change in this edition is switching from MIPS to
> the RISC- V instruction set. We suspect this modern, modular, open
> instruction set may become a significant force in the information
> technology industry. It may become as important in computer
> architecture as Linux is for operating systems.
>
> The newcomer in this edition is [Chapter 7](#_bookmark322), which
> introduces domain-specific architectures with several concrete
> examples from industry.
>
> As before, the first three appendices in the book give basics on the
> RISC-V instruction set, memory hierarchy, and pipelining for readers
> who have not read a book like _Computer Organization and Design_. To
> keep costs down but still sup- ply supplemental material that is of
> interest to some readers, available online at
> [https://www.elsevier.com/books-and-journals/book-companion/9780128119051](https://www.elsevier.com/books-and-journals/book-companion/9780128119051)
> are nine more appendices. There are more pages in these appendices
> than there are in this book!
>
> This edition continues the tradition of using real-world examples to
> demonstrate the ideas, and the “Putting It All Together” sections are
> brand new. The “Putting It All Together” sections of this edition
> include the pipeline organizations and memory hier- archies of the ARM
> Cortex A8 processor, the Intel core i7 processor, the NVIDIA GTX-280
> and GTX-480 GPUs, and one of the Google warehouse-scale computers.

### Topic Selection and Organization

> As before, we have taken a conservative approach to topic selection,
> for there are many more interesting ideas in the field than can
> reasonably be covered in a treat- ment of basic principles. We have
> steered away from a comprehensive survey of every architecture a
> reader might encounter. Instead, our presentation focuses on core
> concepts likely to be found in any new machine. The key criterion
> remains that of selecting ideas that have been examined and utilized
> successfully enough to permit their discussion in quantitative terms.
>
> Our intent has always been to focus on material that is not available
> in equiv- alent form from other sources, so we continue to emphasize
> advanced content wherever possible. Indeed, there are several systems
> here whose descriptions can- not be found in the literature. (Readers
> interested strictly in a more basic introduc- tion to computer
> architecture should read _Computer Organization and Design: The
> Hardware/Software Interface_.)

### An Overview of the Content

> [Chapter 1](#_bookmark2) includes formulas for energy, static power,
> dynamic power, integrated cir- cuit costs, reliability, and
> availability. (These formulas are also found on the front inside
> cover.) Our hope is that these topics can be used through the rest of
> the book. In addition to the classic quantitative principles of
> computer design and performance measurement, it shows the slowing of
> performance improvement of general-purpose microprocessors, which is
> one inspiration for domain-specific architectures.
>
> Our view is that the instruction set architecture is playing less of a
> role today than in 1990, so we moved this material to [Appendix
> A](#_bookmark391). It now uses the RISC-V architecture. (For quick
> review, a summary of the RISC-V ISA can be found on the back inside
> cover.) For fans of ISAs, Appendix K was revised for this edition and
> covers 8 RISC architectures (5 for desktop and server use and 3 for
> embedded use), the 80 86, the DEC VAX, and the IBM 360/370.
>
> We then move onto memory hierarchy in [Chapter 2](#_bookmark46), since
> it is easy to apply the cost-performance-energy principles to this
> material, and memory is a critical resource for the rest of the
> chapters. As in the past edition, [Appendix B](#_bookmark436) contains
> an introductory review of cache principles, which is available in case
> you need it. [Chapter 2](#_bookmark46) discusses 10 advanced
> optimizations of caches. The chapter includes virtual machines, which
> offer advantages in protection, software management, and hardware
> management, and play an important role in cloud computing. In addition
> to covering SRAM and DRAM technologies, the chapter includes new
> material both on Flash memory and on the use of stacked die packaging
> for extend- ing the memory hierarchy. The PIAT examples are the ARM
> Cortex A8, which is used in PMDs, and the Intel Core i7, which is used
> in servers.
>
> [Chapter 3](#_bookmark93) covers the exploitation of instruction-level
> parallelism in high- performance processors, including superscalar
> execution, branch prediction (including the new tagged hybrid
> predictors), speculation, dynamic scheduling, and simultaneous
> multithreading. As mentioned earlier, [Appendix C](#_bookmark481) is a
> review of pipelining in case you need it. [Chapter 3](#_bookmark93)
> also surveys the limits of ILP. Like [Chapter 2](#_bookmark46), the
> PIAT examples are again the ARM Cortex A8 and the Intel Core i7. While
> the third edition contained a great deal on Itanium and VLIW, this
> mate- rial is now in Appendix H, indicating our view that this
> architecture did not live up to the earlier claims.
>
> The increasing importance of multimedia applications such as games and
> video processing has also increased the importance of architectures
> that can exploit data level parallelism. In particular, there is a
> rising interest in computing using graph- ical processing units
> (GPUs), yet few architects understand how GPUs really work. We decided
> to write a new chapter in large part to unveil this new style of
> computer architecture. [Chapter 4](#_bookmark165) starts with an
> introduction to vector architectures, which acts as a foundation on
> which to build explanations of multimedia SIMD instruc- tion set
> extensions and GPUs. (Appendix G goes into even more depth on vector
> architectures.) This chapter introduces the Roofline performance model
> and then uses it to compare the Intel Core i7 and the NVIDIA GTX 280
> and GTX 480 GPUs. The chapter also describes the Tegra 2 GPU for PMDs.
>
> [Chapter 5](#_bookmark213) describes multicore processors. It explores
> symmetric and distributed-memory architectures, examining both
> organizational principles and performance. The primary additions to
> this chapter include more comparison of multicore organizations,
> including the organization of multicore-multilevel caches, multicore
> coherence schemes, and on-chip multicore interconnect. Topics in
> synchronization and memory consistency models are next. The example is
> the Intel Core i7. Readers interested in more depth on interconnection
> networks should read Appendix F, and those interested in larger scale
> multiprocessors and scientific applications should read Appendix I.
>
> [Chapter 6](#_bookmark268) describes warehouse-scale computers (WSCs).
> It was extensively revised based on help from engineers at Google and
> Amazon Web Services. This chapter integrates details on design, cost,
> and performance of WSCs that few archi- tects are aware of. It starts
> with the popular MapReduce programming model before describing the
> architecture and physical implementation of WSCs, includ- ing cost.
> The costs allow us to explain the emergence of cloud computing,
> whereby it can be cheaper to compute using WSCs in the cloud than in
> your local datacenter. The PIAT example is a description of a Google
> WSC that includes information published for the first time in this
> book.
>
> The new [Chapter 7](#_bookmark322) motivates the need for
> Domain-Specific Architectures (DSAs). It draws guiding principles for
> DSAs based on the four examples of DSAs. Each DSA corresponds to chips
> that have been deployed in commercial settings. We also explain why we
> expect a renaissance in computer architecture via DSAs given that
> single-thread performance of general-purpose microprocessors has
> stalled.
>
> This brings us to Appendices [A](#_bookmark391) through M. [Appendix
> A](#_bookmark391) covers principles of ISAs, including RISC-V, and
> Appendix K describes 64-bit versions of RISC V, ARM, MIPS, Power, and
> SPARC and their multimedia extensions. It also includes some classic
> architectures (80x86, VAX, and IBM 360/370) and popular embed- ded
> instruction sets (Thumb-2, microMIPS, and RISC V C). Appendix H is
> related, in that it covers architectures and compilers for VLIW ISAs.
>
> As mentioned earlier, [Appendix B](#_bookmark436) and [Appendix
> C](#_bookmark481) are tutorials on basic cach- ing and pipelining
> concepts. Readers relatively new to caching should read
> [Appen-](#_bookmark436) [dix B](#_bookmark436) before [Chapter
> 2](#_bookmark46), and those new to pipelining should read [Appendix
> C](#_bookmark481) before [Chapter 3](#_bookmark93).
>
> Appendix D, “Storage Systems,” has an expanded discussion of
> reliability and availability, a tutorial on RAID with a description of
> RAID 6 schemes, and rarely found failure statistics of real systems.
> It continues to provide an introduction to queuing theory and I/O
> performance benchmarks. We evaluate the cost, perfor- mance, and
> reliability of a real cluster: the Internet Archive. The “Putting It
> All Together” example is the NetApp FAS6000 filer.
>
> Appendix E, by Thomas M. Conte, consolidates the embedded material in
> one place.
>
> Appendix F, on interconnection networks, is revised by Timothy M.
> Pinkston and Jos´e Duato. Appendix G, written originally by Krste
> Asanovi´c, includes a description of vector processors. We think these
> two appendices are some of the best material we know of on each topic.
>
> Appendix H describes VLIW and EPIC, the architecture of Itanium.
>
> Appendix I describes parallel processing applications and coherence
> protocols for larger-scale, shared-memory multiprocessing. Appendix J,
> by David Goldberg, describes computer arithmetic.
>
> Appendix L, by Abhishek Bhattacharjee, is new and discusses advanced
> tech- niques for memory management, focusing on support for virtual
> machines and design of address translation for very large address
> spaces. With the growth in clouds processors, these architectural
> enhancements are becoming more important. Appendix M collects the
> “Historical Perspective and References” from each chapter into a
> single appendix. It attempts to give proper credit for the ideas in
> each chapter and a sense of the history surrounding the inventions. We
> like to think of this as presenting the human drama of computer
> design. It also supplies references that the student of architecture
> may want to pursue. If you have time, we recom- mend reading some of
> the classic papers in the field that are mentioned in these sections.
> It is both enjoyable and educational to hear the ideas directly from
> the creators. “Historical Perspective” was one of the most popular
> sections of prior
>
> editions.

### Navigating the Text

> There is no single best order in which to approach these chapters and
> appendices, except that all readers should start with [Chapter
> 1](#_bookmark2). If you don’t want to read every- thing, here are some
> suggested sequences:

- _Memory Hierarchy:_ [Appendix B](#_bookmark436), [Chapter

  > 2](#\_bookmark46), and Appendices D and M.
  >
- _Instruction-Level Parallelism:_ [Appendix C](#_bookmark481),

  > [Chapter 3](#_bookmark93), and Appendix H
  >
- _Data-Level Parallelism:_ Chapters [4](#_bookmark165),

  > [6](#_bookmark268), and [7](#_bookmark322), Appendix G
  >
- _Thread-Level Parallelism:_ [Chapter 5](#_bookmark213), Appendices F

  > and I
  >
- _Request-Level Parallelism:_ [Chapter 6](#_bookmark268)
- _ISA:_ Appendices A and K

> Appendix E can be read at any time, but it might work best if read
> after the ISA and cache sequences. Appendix J can be read whenever
> arithmetic moves you. You should read the corresponding portion of
> Appendix M after you complete each chapter.

### Chapter Structure

> The material we have selected has been stretched upon a consistent
> framework that is followed in each chapter. We start by explaining the
> ideas of a chapter. These ideas are followed by a “Crosscutting
> Issues” section, a feature that shows how the ideas covered in one
> chapter interact with those given in other chapters. This is
>
> xxii ■ Preface
>
> followed by a “Putting It All Together” section that ties these ideas
> together by showing how they are used in a real machine.
>
> Next in the sequence is “Fallacies and Pitfalls,” which lets readers
> learn from the mistakes of others. We show examples of common
> misunderstandings and architectural traps that are difficult to avoid
> even when you know they are lying in wait for you. The “Fallacies and
> Pitfalls” sections is one of the most popular sections of the book.
> Each chapter ends with a “Concluding Remarks” section.

### Case Studies With Exercises

> Each chapter ends with case studies and accompanying exercises.
> Authored by experts in industry and academia, the case studies explore
> key chapter concepts and verify understanding through increasingly
> challenging exercises. Instructors should find the case studies
> sufficiently detailed and robust to allow them to create their own
> additional exercises.
>
> Brackets for each exercise (&lt;chapter.section &gt;) indicate the
> text sections of
>
> primary relevance to completing the exercise. We hope this helps
> readers to avoid exercises for which they haven’t read the
> corresponding section, in addition to pro- viding the source for
> review. Exercises are rated, to give the reader a sense of the amount
> of time required to complete an exercise:
>
> \[10\] Less than 5 min (to read and understand)
>
> \[15\] 5–15 min for a full answer
>
> \[20\] 15–20 min for a full answer
>
> \[25\] 1 h for a full written answer
>
> \[30\] Short programming project: less than 1 full day of programming
>
> \[40\] Significant programming project: 2 weeks of elapsed time
> \[Discussion\] Topic for discussion with others
>
> Solutions to the case studies and exercises are available for
> instructors who register at _textbooks.elsevier.com_.

### Supplemental Materials

> A variety of resources are available online at
> [https://www.elsevier.com/books/](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1) > [computer-architecture/hennessy/978-0-12-811905-1](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1),
> including the following:

- Reference appendices, some guest authored by subject experts,
  covering a range of advanced topics
- Historical perspectives material that explores the development of
  the key ideas presented in each of the chapters in the text

  - Instructor slides in PowerPoint
  - Figures from the book in PDF, EPS, and PPT formats
  - Links to related material on the Web
  - List of errata

> New materials and links to other resources available on the Web will
> be added on a regular basis.

### Helping Improve This Book

> Finally, it is possible to make money while reading this book. (Talk
> about cost per- formance!) If you read the Acknowledgments that
> follow, you will see that we went to great lengths to correct
> mistakes. Since a book goes through many print- ings, we have the
> opportunity to make even more corrections. If you uncover any
> remaining resilient bugs, please contact the publisher by electronic
> mail
> ([ca6bugs](mailto:ca6bugs@mkp.com)@[mkp.com](mailto:ca6bugs@mkp.com)).
>
> We welcome general comments to the text and invite you to send them to
> a separate email address at
> [ca6comments](mailto:ca6comments@mkp.com)@[mkp.com](mailto:ca6comments@mkp.com).

### Concluding Remarks

> Once again, this book is a true co-authorship, with each of us writing
> half the chap- ters and an equal share of the appendices. We can’t
> imagine how long it would have taken without someone else doing half
> the work, offering inspiration when the task seemed hopeless,
> providing the key insight to explain a difficult concept, supply- ing
> over-the-weekend reviews of chapters, and commiserating when the
> weight of our other obligations made it hard to pick up the pen.
>
> Thus, once again, we share equally the blame for what you are about to
> read.
>
> _John Hennessy_ ■ _David Patterson_
>
> This page intentionally left blank
>
> Although this is only the sixth edition of this book, we have actually
> created ten different versions of the text: three versions of the
> first edition (alpha, beta, and final) and two versions of the second,
> third, and fourth editions (beta and final). Along the way, we have
> received help from hundreds of reviewers and users. Each of these
> people has helped make this book better. Thus, we have chosen to list
> all of the people who have made contributions to some version of this
> book.

### Contributors to the Sixth Edition

> Like prior editions, this is a community effort that involves scores
> of volunteers. Without their help, this edition would not be nearly as
> polished.

##### _Reviewers_

> Jason D. Bakos, University of South Carolina; Rajeev Balasubramonian,
> Univer- sity of Utah; Jose Delgado-Frias, Washington State University;
> Diana Franklin, The University of Chicago; Norman P. Jouppi, Google;
> Hugh C. Lauer, Worcester Polytechnic Institute; Gregory Peterson,
> University of Tennessee; Bill Pierce, Hood College; Parthasarathy
> Ranganathan, Google; William H. Robinson, Van- derbilt University; Pat
> Stakem, Johns Hopkins University; Cliff Young, Google; Amr Zaky,
> University of Santa Clara; Gerald Zarnett, Ryerson University; Huiyang
> Zhou, North Carolina State University.
>
> Members of the University of California-Berkeley Par Lab and RAD Lab
> who gave frequent reviews of Chapters [1](#_bookmark3),
> [4](#_bookmark166), and [6](#_bookmark269) and shaped the explanation
> of GPUs and WSCs: Krste Asanovi´c, Michael Armbrust, Scott Beamer,
> Sarah Bird, Bryan Catan- zaro, Jike Chong, Henry Cook, Derrick
> Coetzee, Randy Katz, Yunsup Lee, Leo Meyervich, Mark Murphy, Zhangxi
> Tan, Vasily Volkov, and Andrew Waterman.

##### _Appendices_

> Krste Asanovi´c, University of California, Berkeley
> [(Appendix](#_bookmark685) G); Abhishek Bhattacharjee, Rutgers
> University ([Appendix L](#_bookmark902)); Thomas M. Conte, North Caro-
> lina State University ([Appendix E](#_bookmark574)); Jos´e Duato,
> Universitat Politècnica de

#### xxv

> xxvi ■ Acknowledgments
>
> València and Simula ([Appendix F](#_bookmark594)); David Goldberg,
> Xerox PARC ([Appendix J](#_bookmark763)); Timothy M. Pinkston,
> University of Southern California ([Appendix F](#_bookmark594)).
>
> Jos´e Flich of the Universidad Polit´ecnica de Valencia provided
> significant contri- butions to the updating of [Appendix
> F](#_bookmark594).

##### _Case Studies With Exercises_

> Jason D. Bakos, University of South Carolina (Chapters
> [3](#_bookmark94) and [4](#_bookmark166)); Rajeev Balasu- bramonian,
> University of Utah ([Chapter 2](#_bookmark47)); Diana Franklin, The
> University of Chicago ([Chapter 1](#_bookmark3) and
> [Appendix](#_bookmark482) C); Norman P. Jouppi, Google, ([Chapter
> 2](#_bookmark47)); Naveen Muralimanohar, HP Labs ([Chapter
> 2](#_bookmark47)); Gregory Peterson, University of Tennessee
> ([Appendix A](#_bookmark392)); Parthasarathy Ranganathan, Google
> [(Chapter](#_bookmark269) 6); Cliff Young, Google ([Chapter
> 7](#_bookmark323)); Amr Zaky, University of Santa Clara [(Chapter
> 5](#_bookmark214) and [Appendix B](#_bookmark437)).
>
> Jichuan Chang, Junwhan Ahn, Rama Govindaraju, and Milad Hashemi
> assisted in the development and testing of the case studies and
> exercises for [Chapter 6](#_bookmark269).

##### _Additional Material_

> John Nickolls, Steve Keckler, and Michael Toksvig of NVIDIA (Chapter 4
> NVI- DIA GPUs); Victor Lee, Intel ([Chapter 4](#_bookmark166)
> comparison of Core i7 and GPU); John Shalf, LBNL ([Chapter
> 4](#_bookmark166) recent vector architectures); Sam Williams, LBNL
> (Roof- line model for computers in [Chapter 4](#_bookmark166)); Steve
> Blackburn of Australian National University and Kathryn McKinley of
> University of Texas at Austin (Intel perfor- mance and power
> measurements in [Chapter 5](#_bookmark214)); Luiz Barroso, Urs
> H€olzle, Jimmy Clidaris, Bob Felderman, and Chris Johnson of Google
> (the Google WSC in [Chapter 6](#_bookmark269)); James Hamilton of
> Amazon Web Services (power distribution and cost model in [Chapter
> 6](#_bookmark269)).
>
> Jason D. Bakos of the University of South Carolina updated the lecture
> slides for this edition.
>
> This book could not have been published without a publisher, of
> course. We wish to thank all the Morgan Kaufmann/Elsevier staff for
> their efforts and support. For this fifth edition, we particularly
> want to thank our editors Nate McFadden and Steve Merken, who
> coordinated surveys, development of the case studies and exer- cises,
> manuscript reviews, and the updating of the appendices.
>
> We must also thank our university staff, Margaret Rowland and Roxana
> Infante, for countless express mailings, as well as for holding down
> the fort at Stan- ford and Berkeley while we worked on the book.
>
> Our final thanks go to our wives for their suffering through
> increasingly early mornings of reading, thinking, and writing.

### Contributors to Previous Editions

##### _Reviewers_

> George Adams, Purdue University; Sarita Adve, University of Illinois
> at Urbana- Champaign; Jim Archibald, Brigham Young University; Krste
> Asanovi´c, Massa- chusetts Institute of Technology; Jean-Loup Baer,
> University of Washington; Paul Barr, Northeastern University; Rajendra
> V. Boppana, University of Texas, San Antonio; Mark Brehob, University
> of Michigan; Doug Burger, University of Texas, Austin; John Burger,
> SGI; Michael Butler; Thomas Casavant; Rohit Chan- dra; Peter Chen,
> University of Michigan; the classes at SUNY Stony Brook, Car- negie
> Mellon, Stanford, Clemson, and Wisconsin; Tim Coe, Vitesse
> Semiconductor; Robert P. Colwell; David Cummings; Bill Dally; David
> Douglas; Jos´e Duato, Universitat Politècnica de València and Simula;
> Anthony Duben, Southeast Missouri State University; Susan Eggers,
> University of Washington; Joel Emer; Barry Fagin, Dartmouth; Joel
> Ferguson, University of California, Santa Cruz; Carl Feynman; David
> Filo; Josh Fisher, Hewlett-Packard Laboratories; Rob Fowler, DIKU;
> Mark Franklin, Washington University (St. Louis); Kourosh Ghar-
> achorloo; Nikolas Gloy, Harvard University; David Goldberg, Xerox Palo
> Alto Research Center; Antonio González, Intel and Universitat
> Politècnica de Catalu- nya; James Goodman, University of
> Wisconsin-Madison; Sudhanva Gurumurthi, University of Virginia; David
> Harris, Harvey Mudd College; John Heinlein; Mark Heinrich, Stanford;
> Daniel Helman, University of California, Santa Cruz; Mark D. Hill,
> University of Wisconsin-Madison; Martin Hopkins, IBM; Jerry Huck,
> Hewlett-Packard Laboratories; Wen-mei Hwu, University of Illinois at
> Urbana- Champaign; Mary Jane Irwin, Pennsylvania State University;
> Truman Joe; Norm Jouppi; David Kaeli, Northeastern University; Roger
> Kieckhafer, University of Nebraska; Lev G. Kirischian, Ryerson
> University; Earl Killian; Allan Knies, Pur- due University; Don Knuth;
> Jeff Kuskin, Stanford; James R. Larus, Microsoft Research; Corinna
> Lee, University of Toronto; Hank Levy; Kai Li, Princeton Uni- versity;
> Lori Liebrock, University of Alaska, Fairbanks; Mikko Lipasti,
> University of Wisconsin-Madison; Gyula A. Mago, University of North
> Carolina, Chapel Hill; Bryan Martin; Norman Matloff; David Meyer;
> William Michalson, Worcester Polytechnic Institute; James Mooney;
> Trevor Mudge, University of Michigan; Ramadass Nagarajan, University
> of Texas at Austin; David Nagle, Carnegie Mel- lon University; Todd
> Narter; Victor Nelson; Vojin Oklobdzija, University of Cal- ifornia,
> Berkeley; Kunle Olukotun, Stanford University; Bob Owens, Pennsylvania
> State University; Greg Papadapoulous, Sun Microsystems; Joseph
> Pfeiffer; Keshav Pingali, Cornell University; Timothy M. Pinkston,
> University of Southern California; Bruno Preiss, University of
> Waterloo; Steven Przybylski; Jim Quinlan; Andras Radics; Kishore
> Ramachandran, Georgia Institute of Tech- nology; Joseph Rameh,
> University of Texas, Austin; Anthony Reeves, Cornell University;
> Richard Reid, Michigan State University; Steve Reinhardt, University
> of Michigan; David Rennels, University of California, Los Angeles;
> Arnold L. Rosenberg, University of Massachusetts, Amherst; Kaushik
> Roy, Purdue
>
> xxviii ■ Acknowledgments
>
> University; Emilio Salgueiro, Unysis; Karthikeyan Sankaralingam,
> University of Texas at Austin; Peter Schnorf; Margo Seltzer; Behrooz
> Shirazi, Southern Meth- odist University; Daniel Siewiorek, Carnegie
> Mellon University; J. P. Singh, Prin- ceton; Ashok Singhal; Jim Smith,
> University of Wisconsin-Madison; Mike Smith, Harvard University; Mark
> Smotherman, Clemson University; Gurindar Sohi, Uni- versity of
> Wisconsin-Madison; Arun Somani, University of Washington; Gene
> Tagliarin, Clemson University; Shyamkumar Thoziyoor, University of
> Notre Dame; Evan Tick, University of Oregon; Akhilesh Tyagi,
> University of North Car- olina, Chapel Hill; Dan Upton, University of
> Virginia; Mateo Valero, Universidad Polit´ecnica de Cataluña,
> Barcelona; Anujan Varma, University of California, Santa Cruz;
> Thorsten von Eicken, Cornell University; Hank Walker, Texas A&M; Roy
> Want, Xerox Palo Alto Research Center; David Weaver, Sun Microsystems;
> Shlomo Weiss, Tel Aviv University; David Wells; Mike Westall, Clemson
> Univer- sity; Maurice Wilkes; Eric Williams; Thomas Willis, Purdue
> University; Malcolm Wing; Larry Wittie, SUNY Stony Brook; Ellen Witte
> Zegura, Georgia Institute of Technology; Sotirios G. Ziavras, New
> Jersey Institute of Technology.

##### _Appendices_

> The vector appendix was revised by Krste Asanovi´c of the
> Massachusetts Institute of Technology. The floating-point appendix was
> written originally by David Gold- berg of Xerox PARC.

##### _Exercises_

> George Adams, Purdue University; Todd M. Bezenek, University of
> Wisconsin- Madison (in remembrance of his grandmother Ethel Eshom);
> Susan Eggers; Anoop Gupta; David Hayes; Mark Hill; Allan Knies; Ethan
> L. Miller, University of California, Santa Cruz; Parthasarathy
> Ranganathan, Compaq Western Research Laboratory; Brandon Schwartz,
> University of Wisconsin-Madison; Michael Scott; Dan Siewiorek; Mike
> Smith; Mark Smotherman; Evan Tick; Thomas Willis.

##### _Case Studies With Exercises_

> Andrea C. Arpaci-Dusseau, University of Wisconsin-Madison; Remzi H.
> Arpaci- Dusseau, University of Wisconsin-Madison; Robert P. Colwell,
> R&E Colwell & Assoc., Inc.; Diana Franklin, California Polytechnic
> State University, San Luis Obispo; Wen-mei W. Hwu, University of
> Illinois at Urbana-Champaign; Norman
>
> P. Jouppi, HP Labs; John W. Sias, University of Illinois at
> Urbana-Champaign; David A. Wood, University of Wisconsin-Madison.

##### _Special Thanks_

> Duane Adams, Defense Advanced Research Projects Agency; Tom Adams;
> Sarita Adve, University of Illinois at Urbana-Champaign; Anant
> Agarwal; Dave
>
> Albonesi, University of Rochester; Mitch Alsup; Howard Alt; Dave
> Anderson; Peter Ashenden; David Bailey; Bill Bandy, Defense Advanced
> Research Projects Agency; Luiz Barroso, Compaq’s Western Research Lab;
> Andy Bechtolsheim; _C. Gordon_ Bell; Fred Berkowitz; John Best, IBM;
> Dileep Bhandarkar; Jeff Bier, BDTI; Mark Birman; David Black; David
> Boggs; Jim Brady; Forrest Brewer; Aaron Brown, University of
> California, Berkeley; E. Bugnion, Compaq’s Western Research Lab; Alper
> Buyuktosunoglu, University of Rochester; Mark Callaghan; Jason F.
> Cantin; Paul Carrick; Chen-Chung Chang; Lei Chen, University of Roch-
> ester; Pete Chen; Nhan Chu; Doug Clark, Princeton University; Bob
> Cmelik; John Crawford; Zarka Cvetanovic; Mike Dahlin, University of
> Texas, Austin; Merrick Darley; the staff of the DEC Western Research
> Laboratory; John DeRosa; Lloyd Dickman; J. Ding; Susan Eggers,
> University of Washington; Wael El-Essawy, University of Rochester;
> Patty Enriquez, Mills; Milos Ercegovac; Robert Garner;
>
> K. Gharachorloo, Compaq’s Western Research Lab; Garth Gibson; Ronald
> Green- berg; Ben Hao; John Henning, Compaq; Mark Hill, University of
> Wisconsin- Madison; Danny Hillis; David Hodges; Urs H€olzle, Google;
> David Hough; Ed Hudson; Chris Hughes, University of Illinois at
> Urbana-Champaign; Mark John- son; Lewis Jordan; Norm Jouppi; William
> Kahan; Randy Katz; Ed Kelly; Richard Kessler; Les Kohn; John
> Kowaleski, Compaq Computer Corp; Dan Lambright; Gary Lauterbach, Sun
> Microsystems; Corinna Lee; Ruby Lee; Don Lewine; Chao-Huang Lin; Paul
> Losleben, Defense Advanced Research Projects Agency; Yung-Hsiang Lu;
> Bob Lucas, Defense Advanced Research Projects Agency; Ken Lutz; Alan
> Mainwaring, Intel Berkeley Research Labs; Al Marston; Rich Martin,
> Rutgers; John Mashey; Luke McDowell; Sebastian Mirolo, Trimedia Cor-
> poration; Ravi Murthy; Biswadeep Nag; Lisa Noordergraaf, Sun
> Microsystems; Bob Parker, Defense Advanced Research Projects Agency;
> Vern Paxson, Center for Internet Research; Lawrence Prince; Steven
> Przybylski; Mark Pullen, Defense Advanced Research Projects Agency;
> Chris Rowen; Margaret Rowland; Greg Semeraro, University of Rochester;
> Bill Shannon; Behrooz Shirazi; Robert Shom- ler; Jim Slager; Mark
> Smotherman, Clemson University; the SMT research group at the
> University of Washington; Steve Squires, Defense Advanced Research
> Pro- jects Agency; Ajay Sreekanth; Darren Staples; Charles Stapper;
> Jorge Stolfi; Peter Stoll; the students at Stanford and Berkeley who
> endured our first attempts at cre- ating this book; Bob Supnik; Steve
> Swanson; Paul Taysom; Shreekant Thakkar; Alexander Thomasian, New
> Jersey Institute of Technology; John Toole, Defense Advanced Research
> Projects Agency; Kees A. Vissers, Trimedia Corporation; Willa Walker;
> David Weaver; Ric Wheeler, EMC; Maurice Wilkes; Richard Zimmerman.
>
> _John Hennessy_ ■ _David Patterson_

## Index

1. [Introduction](#introduction) 2
2. [Classes of Computers](#classes-of-computers) 6
3. [Defining Computer Architecture](#defining-computer-architecture) 11
4. [Trends in Technology](#trends-in-technology) 18
5. [Trends in Power and Energy in Integrated Circuits](#trends-in-power-and-energy-in-integrated-circuits) 23
6. [Trends in Cost](#trends-in-cost) 29
7. [Dependability](#dependability) 36
8. [Measuring, Reporting, and Summarizing Performance](#measuring-reporting-and-summarizing-performance) 39
9. [Quantitative Principles of Computer Design](#quantitative-principles-of-computer-design) 48
10. [Putting It All Together: Performance, Price, and Power](#_bookmark30) 55
11. [Fallacies and Pitfalls](#_bookmark33) 58
12. [Concluding Remarks](#concluding-remarks-1) 64
13. [Historical Perspectives and References](#historical-perspectives-and-references) 67

> [Case Studies and Exercises by Diana
> Franklin](#case-studies-and-exercises-by-diana-franklin) 67
>
> An iPod, a phone, an Internet mobile communicator… these are NOT three
> separate devices! And we are calling it iPhone! Today Apple is going
> to reinvent the phone. And here it is.
>
> Steve Jobs, January 9, 2007
>
> New information and communications technologies, in particular
> high-speed Internet, are changing the way companies do business,
> transforming public service delivery and democratizing innovation.
> With 10 percent increase in high speed Internet connections, economic
> growth increases by 1.3 percent.
>
> The World Bank, July 28, 2009
>
> Computer Architecture.
> [https://doi.org/10.1016/B978-0-12-811905-1.00001-8](https://doi.org/10.1016/B978-0-12-811905-1.00001-8)
>
> © 2019 Elsevier Inc. All rights reserved.
