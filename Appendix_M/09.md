## The Development of Clusters ([Chapter 6](#_bookmark268))

> ##簇的开发([第 6 章](#_bookmark268))

In this section, we cover the development of clusters that were the foundation of warehouse-scale computers (WSCs) and of utility computing. (Readers interested in learning more should start with Barroso and H€olzle [2009] and the blog postings and talks of James Hamilton at [_http://perspectives.mvdirona.com_](http://perspectives.mvdirona.com/).)

> 在本节中，我们涵盖了仓库规模计算机(WSC)和实用计算的群集的开发。(有兴趣了解更多的读者应该从 Barroso 和 H€Olzle [2009]以及 James Hamilton 的博客帖子和谈话，网址为[_http：//perspectives.mvdirona.com_])

### Clusters, the Forerunner of WSCs

> ###簇，WSC 的先驱

Clusters were probably  "invented"  in the 1960s by customers who could not fit all their work on one computer or who needed a backup machine in case of failure of the primary machine [Pfister 1998]. Tandem introduced a 16-node cluster in 1975. Digital followed with VAX clusters, introduced in 1984. They were originally independent computers that shared I/O devices, requiring a distributed operating system to coordinate activity. Soon they had communication links between com- puters, in part so that the computers could be geographically distributed to increase availability in case of a disaster at a single site. Users log onto the cluster and are unaware of which machine they are running on. DEC (now HP) sold more than 25,000 clusters by 1993. Other early companies were Tandem (now HP) and IBM (still IBM). Today, virtually every company has cluster products. Most of these products are aimed at availability, with performance scaling as a secondary benefit. Scientific computing on clusters emerged as a competitor to MPPs. In 1993, the Beowulf project started with the goal of fulfilling NASA’s desire for a 1 GFLOPS computer for under $50,000. In 1994, a 16-node cluster built from off-the-shelf PCs using 80486s achieved that goal [Bell and Gray 2001]. This emphasis led to a variety of software interfaces to make it easier to submit, coordinate, and debug large programs or a large number of independent programs.

> 群集可能是在 1960 年代被 "发明" 的，这些客户无法将所有工作适合一台计算机或在主机失败时需要**备份**机器的所有工作[Pfister 1998]。Tandem 在 1975 年引入了 16 个节点群集。数字簇遵循 1984 年推出的 VAX 簇。它们最初是共享 I/O 设备的独立计算机，需要分布式操作系统以协调活动。很快，他们在计算机之间就具有通信链接，**部分原因是在单个网站上发生灾难的情况下，计算机可以在地理上分配以增加可用性**。用户登录群集，并且不知道他们正在运行的机器。到 1993 年，12 月(现已惠普)出售了 25,000 多个集群。其他早期公司是 Tandem(现为 HP)和 IBM(仍然是 IBM)。今天，几乎每个公司都有集群产品。这些产品中的大多数均针对可用性，并且性能缩放是次要的好处。集群的科学计算成为 MPP 的竞争对手。1993 年，Beowulf 项目的目标是满足 NASA 对 1 GFLOPS 计算机的渴望，价格低于 50,000 美元。1994 年，使用 80486S 建立的 16 个节点集群实现了该目标[Bell and Gray 2001]。这种强调导致了各种软件界面，使提交，协调和调试大型程序或大量独立程序变得更加容易。

Efforts were made to reduce latency of communication in clusters as well as to increase bandwidth, and several research projects worked on that problem. (One commercial result of the low-latency research was the VI interface standard, which has been embraced by Infiniband, discussed below.) Low latency then proved use- ful in other applications. For example, in 1997 a cluster of 100 UltraSPARC desk- top computers at the University of California–Berkeley, connected by 160 MB/sec per link Myrinet switches, was used to set world records in database sort—sorting 8.6 GB of data originally on disk in 1 minute—and in cracking an encrypted mes- sage—taking just 3.5 hours to decipher a 40-bit DES key.

> 努力减少群集中的沟通延迟以及增加带宽，几项研究项目在这一问题上发挥了作用。(低延迟研究的商业结果是 Infiniband 所接受的 VI 接口标准，下面讨论了。)当时在其他应用中证明了低潜伏期。例如，在 1997 年，加利福尼亚大学 - 伯克利分校的 100 台 Ultrasparc Desk-tep 计算机集团使用 160 mb/sec 的每个链接 Myrinet 交换机连接，用于在数据库排序中设置世界记录 - 以 8.6 GB 的数据为单位，最初是在 8.6 GB 的数据上。磁盘在 1 分钟内(并破解加密的 Mes-sage)，仅需 3.5 个小时即可破译 40 位 DES KEY。

This research project, called Network of Workstations [Anderson, Culler, and Patterson 1995], also developed the Inktomi search engine, which led to a start-up company with the same name. Eric Brewer led the Inktomi effort at Berkeley and then at the company to demonstrate the use of commodity hardware to build com- puting infrastructure for Internet services. Using standardized networks within a rack of PC servers gave Inktomi better scalability. In contrast, the strategy of the prior leading search engine Alta Vista was to build from large-scale SMPs. Compared to the high-performance computing work in clusters, the emphasis was on a relatively large number of low-cost nodes and a clear programming model. Hence, the NOW project and Inktomi are considered the foundation of WSCs and Cloud Computing. Google followed the example of Inktomi technology when it took the leading search engine mantle from Inktomi just as Inktomi had taken it from Alta Vista [Brin and Page 1998]. (Google’s initial innovation was search quality; the WSC innovations came much later.) For many years now, all Internet services have relied on cluster technology to serve their millions of customers.

> 该研究项目称为工作站网络[Anderson，Culler 和 Patterson 1995]，也开发了 Inktomi 搜索引擎，该引擎导致了一家具有相同名称的初创公司。埃里克·布鲁尔(Eric Brewer)领导了伯克利(Berkeley)的 Inktomi 努力，然后在公司展示了使用商品硬件来建立互联网服务的基础架构。在 PC 服务器机架中使用标准化网络可为 Inktomi 提供更好的可扩展性。相比之下，先前领先的搜索引擎 Alta Vista 的策略是从大型 SMP 中构建。与集群中的高性能计算工作相比，重点是相对较大的低成本节点和清晰的编程模型。因此，现在的项目和 Inktomi 被认为是 WSC 和云计算的基础。Google 遵循 Inktomi 技术的示例，就像 Inktomi 从 Alta Vista [Brin and Page 1998]中获取了 Inktomi 领先的搜索引擎地幔时。(Google 的最初创新是搜索质量；WSC 创新进行了很长时间。)多年来，所有互联网服务都依靠集群技术来为其数百万客户提供服务。

### Utility Computing, the Forerunner of Cloud Computing

> ###实用计算，云计算的先驱

As stated in the text, the earliest version of utility computing was timesharing. Although timesharing faded away over time with the creation of smaller and cheaper personal computers, in the last decade there have been many less than fully successful attempts to resuscitate utility computing. Sun began selling time on Sun Cloud at $1 per hour in 2000, HP offered a Utility Data Center in 2001, and Intel tried selling time on internal supercomputers in the early 2000s. Although they were commercially available, few customers used them.

> 如文本所述，最早的实用程序计算版本是分时的。尽管随着时间的流逝，随着时间的流逝，分时度保存逐渐消失，但在过去的十年中，较小，更便宜的个人计算机却逐渐消失，但在恢复实用程序计算的恢复时，已经进行了不完全成功的尝试。Sun 在 2000 年开始以每小时 1 美元的价格出售 Sun Cloud 的时间，HP 在 2001 年提供了一个公用事业数据中心，英特尔在 2000 年代初尝试在内部超级计算机上销售时间。尽管它们是商业上可用的，但很少有客户使用它们。

A related topic is _grid computing_, which was originally invented so that scien- tific programs could be run across geographically distributed computing facilities. At the time, some questioned the wisdom of this goal, setting aside how difficult it would be to achieve. Grid computing tended to require very large systems running very large programs, using multiple datacenters for the tasks. Single applications did not really run well when geographically distributed, given the long latencies inherent with long distance. This first step eventually led to some conventions for data access, but the grid computing community did not develop APIs that were useful beyond the high-performance computing community, so the cloud comput- ing effort shares little code or history with grid computing.

> 一个相关的主题是_grid Computing_，该主题最初是发明的，因此可以在地理分布式计算设施上运行 Scientific 程序。当时，有人质疑这个目标的智慧，搁置了实现目标的困难。网格计算往往需要非常大的系统运行非常大的程序，使用多个数据中心来进行任务。考虑到长距离固有的长潜伏期，单个应用程序在地理分布时的运行状态并不好。第一步最终导致了数据访问的一些惯例，但是网格计算社区并未开发出超出高性能计算社区以外的 API，因此云计算工作与网格计算共享很少的代码或历史记录。

Armbrust et al [2009] argued that, once the Internet service companies solved the operational problems to work at large scale, the significant economies of scale that they uncovered brought their costs down below those of smaller datacenters. Amazon recognized that if this cost advantage was true then Amazon should be able to make a profit selling this service. In 2006, Amazon announced Elastic Cloud Computing (EC2) at $0.10 per hour per instance. The subsequent popularity of EC2 led other Internet companies to offer cloud computing services, such as Google App Engine and Microsoft Azure, albeit at higher abstraction levels than the x86 virtual machines of Amazon Web Services. Hence, the current popularity of pay-as-you go computing isn’t because someone recently came up with the idea; it’s because the technology and business models have aligned so that companies can make money offering a service that many people want to use. Time will tell whether there will be many successful utility computing models or whether the industry will converge around a single standard. It will certainly be interesting to watch.

> Armbrust 等人[2009]认为，一旦互联网服务公司解决了大规模工作的运营问题，他们发现的重要规模经济将其成本降低到较小的数据中心的成本之下。亚马逊认识到，如果这种成本优势是正确的，那么亚马逊应该能够赚钱出售这项服务。2006 年，亚马逊以每小时 0.10 美元的价格宣布了弹性云计算(EC2)。EC2 的随后流行使其他 Internet 公司提供了云计算服务，例如 Google App Engine 和 Microsoft Azure，尽管比 Amazon Web Services 的 X86 虚拟机更高的抽象水平。因此，当前的付款方式计算的普及并不是因为最近有人提出了这个想法。这是因为技术和商业模式已经保持一致，以便公司可以赚钱，提供许多人想要使用的服务。时间会证明是否会有许多成功的公用事业计算模型，或者该行业是否会围绕单个标准汇合。观看肯定会很有趣。

### Containers

> ###容器

In the fall of 2003, many people were thinking about using containers to hold servers. Brewster Kahle, director and founder of the Internet Archive, gave talks about how he could fit the whole archive in a single 40-foot container. His interest was making copies of the Archive and distributing it around the world to ensure its survivability, thereby avoiding the fate of the Library of Alexandria that was destroyed by fire in 48 B.C.E. People working with Kahle wrote a white paper based on his talk in November 2003 to get more detail about what a container design would look like.

> 在 2003 年秋天，许多人正在考虑使用容器来容纳服务器。互联网档案馆的导演兼创始人布鲁斯特·卡尔(Brewster Kahle)发表了谈论，他如何在一个 40 英尺的容器中适应整个档案馆。他的兴趣是制作档案的副本，并在世界各地分发它以确保其生存能力，从而避免了亚历山大图书馆的命运，亚历山大图书馆在公元前 48 年被火灾摧毁。与卡尔(Kahle)一起工作的人在 2003 年 11 月根据他的演讲写了一份白皮书，以获取有关容器设计的外观的更多详细信息。

That same year, engineers at Google were also looking at building datacenters using containers and submitted a patent on aspects of it in December 2003. The first container for a datacenter was delivered in January 2005, and Google received the patent in October 2007. Google publicly revealed the use of containers in April 2009.

> 同年，Google 的工程师还在考虑使用集装箱建造数据中心，并在 2003 年 12 月就其方面提交了专利。数据中心的第一个容器于 2005 年 1 月交付，Google 于 2007 年 10 月获得了专利。揭示了 2009 年 4 月使用集装箱的使用。

Greg Papadopolous of Sun Microsystems and Danny Hillis of Applied Minds heard Kahle’s talk and designed a product called the Sun Modular Datacenter that debuted in October 2006. (The project code name was Black Box, a term many people still use.) This half-length (20-foot) container could hold 280 servers. This product release combined with Microsoft’s announcement that they were building a datacenter designed to hold 220 40-foot containers inspired many other compa- nies to offer containers and servers designed to be placed in them.

> Sun Microsystems 的 Greg Papadopolous 和 Applied Minds 的 Danny Hillis 听到了 Kahle 的演讲，并设计了一种名为 Sun Modular Datacenter 的产品，该产品于 2006 年 10 月首次亮相。(项目代码名称为 Black Box，许多人仍然使用。(20 英尺)容器可以容纳 280 台服务器。该产品发行版加上微软宣布他们正在建造一个数据中心，旨在容纳 220 英尺 40 英尺的容器，启发了许多其他组合，以提供旨在放置在其中的容器和服务器。

In a nice turn of events, in 2009 the Internet Archive migrated its data to a Sun Modular Datacenter. A copy of the Internet Archive is now at the New Library of Alexandria in Egypt, near the site of the original library.

> 在一个很好的事件中，2009 年，Internet 存档将其数据迁移到 Sun 模块化数据中心。互联网档案馆的副本现在在原始图书馆所在地附近的埃及亚历山大新图书馆。
