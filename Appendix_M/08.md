## The History of Multiprocessors and Parallel Processing ([Chapter 5](#_bookmark213) and [Appendices F](#_bookmark595), [G](#_bookmark686), and [I](#_bookmark736))

> ##多处理器和并行处理的历史记录（[[第 5 章]（#_ bookmark213）和[appendices f]（#_ bookmark595），[g]（#_ bookmark686）和[i]（#_ bookmark736））

There is a tremendous amount of history in multiprocessors; in this section, we divide our discussion by both time period and architecture. We start with the SIMD approach and the Illiac IV. We then turn to a short discussion of some other early experimental multiprocessors and progress to a discussion of some of the great debates in parallel processing. Next we discuss the historical roots of the present multiprocessors and conclude by discussing recent advances.

> 多处理器中有很多历史。在本节中，我们将讨论除以时期和架构。我们从 SIMD 方法和 Illiac IV 开始。然后，我们对其他一些早期的实验多处理器进行了简短的讨论，并进一步讨论了并行处理中的一些伟大辩论。接下来，我们讨论当前多处理器的历史根源，并通过讨论最近的进步来结束。

### SIMD Computers: Attractive Idea, Many Attempts, No Lasting Successes

> ### Simd 计算机：有吸引力的想法，许多尝试，没有持久的成功

_The cost of a general multiprocessor is, however, very high and further design options were considered which would decrease the cost without seriously degrading the power or efficiency of the system. The options consist of recen- tralizing one of the three major components._ … _Centralizing the [control unit] gives rise to the basic organization of [an]_ … _array processor such as the Illiac IV._

> _一般多处理器的成本非常高，并且考虑了进一步的设计选择，这将降低成本，而不会严重降低系统的功率或效率。选项包括重新构建三个主要组件之一。

Bouknight et al. [1972]

> Bouknight 等。[1972]

The SIMD model was one of the earliest models of parallel computing, dating back to the first large-scale multiprocessor, the Illiac IV. The key idea in that mul- tiprocessor, as in more recent SIMD multiprocessors, is to have a single instruction that operates on many data items at once, using many functional units.

> SIMD 模型是最早的并行计算模型之一，其历史可以追溯到第一个大规模多处理器 Illiac IV。与最新的 SIMD 多处理器一样，Mul-TipRocessor 的关键思想是，使用许多功能单元，具有单个指令，该指令一次在许多数据项上运行。

The earliest ideas on SIMD-style computers are from Unger [1958] and Slot- nick, Borck, and McReynolds [1962]. Slotnick’s Solomon design formed the basis of the Illiac IV, perhaps the most infamous of the supercomputer projects. Although successful in pushing several technologies that proved useful in later projects, it failed as a computer. Costs escalated from the $8 million estimate in 1966 to $31 million by 1972, despite construction of only a quarter of the planned multiprocessor. Actual performance was at best 15 MFLOPS versus initial predic- tions of 1000 MFLOPS for the full system [Hord 1982]. Delivered to NASA Ames Research in 1972, the computer took three more years of engineering before it was usable. These events slowed investigation of SIMD, but Danny Hillis [1985] resus- citated this style in the Connection Machine, which had 65,636 1-bit processors. Real SIMD computers need to have a mixture of SISD and SIMD instructions.

> SIMD 风格计算机的最早想法来自 Unger [1958]和 Slot-Nick，Borck 和 McReynolds [1962]。Slotnick 的所罗门设计构成了 Illiac IV 的基础，这也许是超级计算机项目中最臭名昭著的。尽管成功推动了几种在后来项目中有用的技术，但它是计算机的失败。尽管建造了计划的多处理器，但成本从 1966 年的 800 万美元估计升至 1972 年的 3100 万美元。对于完整系统，实际性能最多为 15 mflops，而 1000 mflops 的初始性能[Hord 1982]。该计算机于 1972 年交付给 NASA AMES Research，在可用的工程上又花费了三年的工程。这些事件减慢了对 SIMD 的调查，但 Danny Hillis [1985]在连接机中呼应了这种风格，该风格有 65,636 个 1 位处理器。真正的 SIMD 计算机需要具有 SISD 和 SIMD 指令的混合物。

There is an SISD host computer to perform operations such as branches and address calculations that do not need parallel operation. The SIMD instructions are broadcast to all the execution units, each of which has its own set of registers. For flexibility, individual execution units can be disabled during an SIMD instruc- tion. In addition, massively parallel SIMD multiprocessors rely on interconnection or communication networks to exchange data between processing elements.

> 有一台 SISD 主机计算机可以执行操作，例如分支和地址计算，这些计算不需要并行操作。SIMD 说明均广播到所有执行单元，每个执行单元都有自己的一组寄存器。为了灵活性，可以在模拟过程中禁用单个执行单元。此外，大规模并行的 SIMD 多处理器依靠互连或通信网络来交换处理元素之间的数据。

SIMD works best in dealing with arrays in for loops; hence, to have the opportu- nity for massive parallelism in SIMD there must be massive amounts of data, or _data parallelism_. SIMD is at its weakest in case statements, where each execution unit must perform a different operation on its data, depending on what data it has. The execution units with the wrong data are disabled so that the proper units can continue. Such sit- uations essentially run at 1/*n*th performance, where _n_ is the number of cases.

> SIMD 最适合处理阵列以进行循环；因此，要在 simd 中有大量并行性的机会，必须有大量的数据或_data parallealism_。在案件陈述的情况下，SIMD 是最弱的，每个执行单元必须在其数据上执行不同的操作，具体取决于其拥有的数据。具有错误数据的执行单元被禁用，以便可以继续进行适当的单位。这种情况基本上以 1/*n* 的性能运行，其中_n_是案例数。

The basic trade-off in SIMD multiprocessors is performance of a processor versus number of processors. Recent multiprocessors emphasize a large degree of parallelism over performance of the individual processors. The Connection Multiprocessor 2, for example, offered 65,536 single-bit-wide processors, while the Illiac IV had 64 64-bit processors.

> SIMD 多处理器中的基本权衡是处理器和处理器数量的性能。最近的多处理器强调了大量的平行性，而不是单个处理器的性能。例如，连接多处理器 2 提供了 65,536 个单位宽的处理器，而 ILLIAC IV 具有 64 位 64 位处理器。

After being resurrected in the 1980s, first by Thinking Machines and then by MasPar, the SIMD model has once again been put to bed as a general-purpose mul- tiprocessor architecture, for two main reasons. First, it is too inflexible. A number of important problems cannot use such a style of multiprocessor, and the architec- ture does not scale down in a competitive fashion; that is, small-scale SIMD multiprocessors often have worse cost-performance compared with that of the alternatives. Second, SIMD cannot take advantage of the tremendous performance and cost advantages of microprocessor technology. Instead of leveraging this low- cost technology, designers of SIMD multiprocessors must build custom processors for their multiprocessors.

> 在 1980 年代复活后，首先是通过思考机器，然后是 Maspar 复活，Simd 模型再次被作为通用的混音处理器建筑，出于两个主要原因。首先，它太僵化了。许多重要的问题不能使用这种多处理器风格，并且建筑不会以竞争方式缩小。也就是说，与替代方案相比，小规模的 SIMD 多处理器通常具有较差的成本效果。其次，SIMD 无法利用微处理器技术的巨大性能和成本优势。Simd 多处理器的设计师必须为其多处理器构建自定义处理器，而不是利用这种低成本的技术。

Although SIMD computers have departed from the scene as general-purpose alternatives, this style of architecture will continue to have a role in special-purpose

> 尽管 Simd 计算机已作为通用替代方案偏离了现场，但这种架构风格将继续在专业中发挥作用

designs. Many special-purpose tasks are highly data parallel and require a limited set of functional units. Thus, designers can build in support for certain operations, as well as hardwired interconnection paths among functional units. Such organi- zations are often called _array processors_, and they are useful for such tasks as image and signal processing.

> 设计。许多特殊用途的任务是高度数据平行的，需要有限的功能单元。因此，设计师可以为某些操作以及功能单元之间的硬连线互连路径提供支持。这样的组织通常称为_ array 处理器_，它们对于图像和信号处理等任务很有用。

### Other Early Experiments

> ###其他早期实验

It is difficult to distinguish the first MIMD multiprocessor. Surprisingly, the first computer from the Eckert-Mauchly Corporation, for example, had duplicate units to improve availability. Holland [1959] gave early arguments for multiple proces- sors. Two of the best-documented multiprocessor projects were undertaken in the 1970s at Carnegie Mellon University. The first of these was C.mmp [Wulf and Bell 1972; Wulf and Harbison 1978], which consisted of 16 PDP-11s connected by a crossbar switch to 16 memory units. It was among the first multiprocessors with more than a few processors, and it had a shared-memory programming model. Much of the focus of the research in the C.mmp project was on software, especially in the OS area. A later multiprocessor, Cm\* [Swan et al. 1977], was a cluster-based multiprocessor with a distributed memory and a nonuniform access time. The absence of caches and a long remote access latency made data placement critical. This multiprocessor and a number of application experiments are well described by Gehringer, Siewiorek, and Segall [1987]. Many of the ideas in these multiproces- sors would be reused in the 1980s when the microprocessor made it much cheaper to build multiprocessors.

> 很难区分第一个 MIMD 多处理器。令人惊讶的是，例如，Eckert-Mauchly Corporation 的第一台计算机具有重复的单位以提高可用性。Holland [1959]提出了多个过程的早期论点。1970 年代在卡内基·梅隆大学（Carnegie Mellon University）进行了两个最有据可查的多处理器项目。其中的第一个是 C.MMP [Wulf 和 Bell 1972；Wulf 和 Harbison 1978]，由 16 个 PDP-11 组成，该 PDP-11 通过横杆开关连接到 16 个存储器单元。它是最早拥有多个处理器的多处理器之一，并且具有共享的内存编程模型。C.MMP 项目中研究的重点大部分都放在软件上，尤其是在 OS 区域。后来的多处理器 CM \* [Swan 等。1977 年]，是一个基于群集的多处理器，具有分布式内存和不均匀的访问时间。没有缓存和较长的远程访问延迟使数据放置至关重要。Gehringer，Siewiorek 和 Segall [1987]很好地描述了这个多处理器和许多应用实验。在 1980 年代，当微处理器使构建多处理器的便宜得多时，这些多个媒介中的许多想法将被重复使用。

### Great Debates in Parallel Processing

> ###并行处理中的大辩论

_The turning away from the conventional organization came in the middle 1960s, when the law of diminishing returns began to take effect in the effort to increase the operational speed of a computer._ … _Electronic circuits are ultimately limited in their speed of operation by the speed of light_ … _and many of the circuits were already operating in the nanosecond range._

> _远离传统组织的转折是在 1960 年代中期，当时收益递减的定律开始生效，以提高计算机的运行速度。Light_…_ _和许多电路已经在纳秒范围内运行。

Bouknight et al. [1972]

> Bouknight 等。[1972]

… _sequential computers are approaching a fundamental physical limit on their potential computational power. Such a limit is the speed of light_ …

> …_序列计算机正在对其潜在计算能力的基本物理限制。这样的限制是光的速度...

Angel L. DeCegama

> Angel L. Degegama

_The Technology of Parallel Processing, Vol. I (1989)_

> _并行处理技术，第 1 卷。我（1989）_

… _today’s multiprocessors_ … _are nearing an impasse as technologies approach the speed of light. Even if the components of a sequential processor could be made to work this fast, the best that could be expected is no more than a few million instructions per second._

> …_ today 的多处理器_…_ _当技术接近光速时，即将接近僵局。即使可以使顺序处理器的组件能够快速运行，也可以预期的最好的方法不超过每秒几百万个说明。

David Mitchell

> 大卫·米切尔（David Mitchell）

_The Transputer: The Time Is Now (1989)_

> _ the Transputer：现在的时间是（1989）_

The quotes above give the classic arguments for abandoning the current form of computing, and Amdahl [1967] gave the classic reply in support of continued focus on the IBM 360 architecture. Arguments for the advantages of parallel exe- cution can be traced back to the 19th century [Menabrea 1842]! Yet, the effective- ness of the multiprocessor for reducing latency of individual important programs is still being explored. Aside from these debates about the advantages and limitations of parallelism, several hot debates have focused on how to build multiprocessors. It’s hard to predict the future, yet in 1989 Gordon Bell made two predictions for 1995. We included these predictions in the first edition of the book, when the out- come was completely unclear. We discuss them in this section, together with an assessment of the accuracy of the prediction.

> 上面的报价给出了放弃当前计算形式的经典论点，而 Amdahl [1967]给出了经典的答复，以支持继续关注 IBM 360 体系结构。关于平行探讨的优势的争论可以追溯到 19 世纪[Menabrea 1842]！然而，降低个人重要程序延迟的多处理器的有效性仍在探索。除了这些有关并行性的优势和局限性的辩论外，一些热门辩论还集中在如何建立多处理器上。目前很难预测未来，但在 1989 年，戈登·贝尔（Gordon Bell）对 1995 年进行了两个预测。我们在本书的第一版中包括了这些预测，而《淘汰》完全不清楚。我们在本节中讨论它们，并评估预测的准确性。

The first was that a computer capable of sustaining a teraFLOPS—one million MFLOPS—would be constructed by 1995, using either a multicomputer with 4K to 32K nodes or a Connection Multiprocessor with several million processing ele- ments [Bell 1989]. To put this prediction in perspective, each year the Gordon Bell Prize acknowledges advances in parallelism, including the fastest real program (highest MFLOPS). In 1989, the winner used an eight-processor Cray Y-MP to run at 1680 MFLOPS. On the basis of these numbers, multiprocessors and pro- grams would have to have improved by a factor of 3.6 each year for the fastest program to achieve 1 TFLOPS in 1995. In 1999, the first Gordon Bell prize winner crossed the 1 TFLOPS bar. Using a 5832-processor IBM RS/6000 SST system designed specially for Livermore Laboratories, they achieved 1.18 TFLOPS on a shock-wave simulation. This ratio represents a year-to-year improvement of 1.93, which is still quite impressive.

> 首先是，能够使用具有 4K 至 32K 节点的多机构或具有数百万个处理元素的连接多处理器来构建能够维持 Teraflops（一百万 mflops）的计算机[Bell 1989]。为了看这一预测，戈登·贝尔奖（Gordon Bell Prive）每年都承认并行的进步，包括最快的真实计划（最高 MFLOPS）。1989 年，获胜者使用八名处理器 Cray Y-MP 以 1680 Mflops 的身份运行。在这些数字的基础上，最快的计划每年必须每年提高 3.6 倍的多处理器和执行，才能在 1995 年获得 1 个 TFLOPS。1999 年，第一位戈登·贝尔奖得主越过 1 Tflops Bar。他们使用专门为 Livermore 实验室设计的 5832 处理器 IBM RS/6000 SST 系统，在冲击波模拟上实现了 1.18 Tflops。该比率逐年改善 1.93，这仍然令人印象深刻。

What has become recognized since the 1990s is that, although we may have the technology to build a TFLOPS multiprocessor, it is not clear that the machine is cost effective, except perhaps for a few very specialized and critically important applications related to national security. We estimated in 1990 that to achieve 1 TFLOPS would require a machine with about 5000 processors and would cost about $100 million. The 5832-processor IBM system at Livermore cost $110 mil- lion. As might be expected, improvements in the performance of individual micro- processors both in cost and performance directly affect the cost and performance of large-scale multiprocessors, but a 5000-processor system will cost more than 5000 times the price of a desktop system using the same processor. Since that time, much faster multiprocessors have been built, but the major improvements have increas- ingly come from the processors in the past five years, rather than fundamental breakthroughs in parallel architecture.

> 自 1990 年代以来已得到认可的是，尽管我们可能拥有建造 TFLOPS 多处理器的技术，但尚不清楚机器具有成本效益，除非可能与一些与国家安全有关的非常专业且至关重要的应用。我们估计在 1990 年，要实现 1 个 TFLOPS 将需要一台拥有约 5000 个处理器的机器，并且耗资约 1 亿美元。Livermore 的 5832 Processor IBM 系统的价格为 110 万美元。可以预料的是，成本和性能方面的单个微处理者的性能的改善直接影响大型多处理器的成本和性能，但是 5000 个处理器系统的成本将是使用台式机系统价格的 5000 倍以上同一处理器。自那时以来，已经建立了更快的多处理器，但是在过去五年中，处理器的主要改进量增加了，而不是在平行体系结构中的基本突破。

The second Bell prediction concerned the number of data streams in supercom- puters shipped in 1995. Danny Hillis believed that, although supercomputers with a small number of data streams may be the best sellers, the biggest multiprocessors would be multiprocessors with many data streams, and these would perform the bulk of the computations. Bell bet Hillis that in the last quarter of calendar year 1995 more sustained MFLOPS would be shipped in multiprocessors using few data streams (≤100) rather than many data streams (≥1000). This bet concerned only supercomputers, defined as multiprocessors costing more than $1 million and used for scientific applications. Sustained MFLOPS was defined for this bet as the number of floating-point operations per _month_, so availability of multiprocessors affects their rating.

> 第二个钟声预测涉及 1995 年运送的超级计算机中数据流的数量。丹尼·希尔斯（Danny Hillis）认为，尽管具有少量数据流的超级计算机可能是最畅销的人，但最大的多处理器将是具有许多数据流的多处理器，并且这些将执行大部分计算。Bell Bet Hillis 认为，在 1995 年日历年的最后一个季度，将使用很少的数据流（≤100）而不是许多数据流（≥1000）在多处理器中发货。这个赌注仅涉及超级计算机，该超级计算机被定义为耗资超过 100 万美元的多处理器，用于科学应用。该赌注定义为持续的 mflops 是每个_ month_的浮点操作的数量，因此多处理器的可用性会影响其评分。

In 1989, when this bet was made, it was totally unclear who would win. In 1995, a survey of the current publicly known supercomputers showed only six multiprocessors in existence in the world with more than 1000 data streams, so Bell’s prediction was a clear winner. In fact, in 1995, much smaller microprocessor-based multiprocessors ( 20 processors) were becoming domi- nant. In 1995, a survey of the 500 highest-performance multiprocessors in use (based on Linpack ratings), called the TOP500, showed that the largest number of multiprocessors were bus-based shared-memory multiprocessors! By 2005, various clusters or multicomputers played a large role. For example, in the top 25 systems, 11 were custom clusters, such as the IBM Blue Gene system or the Cray XT3; 10 were clusters of shared-memory multiprocessors (both using distrib- uted and centralized memory); and the remaining 4 were clusters built using PCs with an off-the-shelf interconnect.

> 1989 年，当下注时，完全不清楚谁会获胜。1995 年，对当前公开的超级计算机的一项调查显示，世界上只有六个具有 1000 多个数据流的多处理器，因此贝尔的预测是一个明显的赢家。实际上，在 1995 年，基于微处理器的多处理器（20 个处理器）变得越来越多。在 1995 年，对使用中的 500 个最高性能多处理器（基于 Linpack 评分）的调查称为 Top500，表明最多的多处理器是基于总线的共享 - 内存多处理器！到 2005 年，各种群集或多机构发挥了重要作用。例如，在前 25 个系统中，有 11 个是自定义簇，例如 IBM 蓝色基因系统或 Cray XT3；10 是共享内存多处理器的簇（均使用分布式和集中存储器）；其余 4 个是使用带有现成互连的 PC 构建的簇。

### More Recent Advances and Developments

> ###最新进展和发展

With the primary exception of the parallel vector multiprocessors (see Appendix G) and more recently of the IBM Blue Gene design, all other recent MIMD computers have been built from off-the-shelf microprocessors using a bus and log- ically central memory or an interconnection network and a distributed memory. A number of experimental multiprocessors built in the 1980s further refined and enhanced the concepts that form the basis for many of today’s multiprocessors.

> 除了平行矢量多处理器（请参阅附录 G）和最近的 IBM 蓝色基因设计以外，所有其他最近的 MIMD 计算机都是使用公共汽车和日志中心存储器或一个中央存储器或一个近期的所有其他 MIMD 计算机构建的互连网络和分布式内存。1980 年代建造的许多实验多处理器进一步完善并增强了构成当今许多多处理器的基础的概念。

##### _The Development of Bus-Based Coherent Multiprocessors_

> ##### _基于公共汽车的连贯多处理器的开发

Although very large mainframes were built with multiple processors in the 1960s and 1970s, multiprocessors did not become highly successful until the 1980s. Bell [1985] suggested that the key was that the smaller size of the microprocessor allowed the memory bus to replace the interconnection network hardware and that portable operating systems meant that multiprocessor projects no longer required the invention of a new operating system. In his paper, Bell defined the terms _mul- tiprocessor_ and _multicomputer_ and set the stage for two different approaches to building larger scale multiprocessors.

> 尽管在 1960 年代和 1970 年代使用多个处理器建造了非常大的大型机，但直到 1980 年代，多处理器才变得非常成功。Bell [1985]建议关键是微处理器的尺寸较小，使内存总线可以替换互连网络硬件，并且便携式操作系统意味着多处理器项目不再需要新操作系统的发明。贝尔在他的论文中定义了术语_mul-tipRocessor_和_multicomputer_，并为构建较大尺度多处理器的两种不同方法设定了舞台。

The first bus-based multiprocessor with snooping caches was the Synapse N+ 1 described by Frank [1984]. Goodman [1983] wrote one of the first papers to describe snooping caches. The late 1980s saw the introduction of many commer- cial bus-based, snooping cache architectures, including the Silicon Graphics 4D/240 [Baskett, Jermoluk, and Solomon 1988], the Encore Multimax [Wilson 1987], and the Sequent Symmetry [Lovett and Thakkar 1988]. The mid-1980s

> Frank [1984]描述的第一个带有窥探缓存的总线多处理器是 Synapse N+ 1。Goodman [1983]写了第一篇描述窥探缓存的论文。1980 年代后期看到了许多商业巴士基于窥探的缓存体系结构，包括硅图形 4D/240 [Baskett，Jermoluk 和 Solomon 1988]，Encore Multimax [Wilson 1987]，以及 sequent Syonstry [Lovett [Lovett [和 Thakkar 1988]。1980 年代中期

Figure M.2 Five snooping protocols summarized. Archibald and Baer [1986] use these names to describe the five protocols, and Eggers [1989] summarizes the similarities and differences as shown in this figure. The Firefly protocol was named for the experimental DEC Firefly multiprocessor, in which it appeared. The alternative names for protocols are based on the states they support: M Modified, E Exclusive (private clean), S Shared, I Invalid, O Owner (shared dirty).

> 图 M.2 总结了五个窥探协议。Archibald and Baer [1986]使用这些名称来描述五个方案，Eggers [1989]总结了该图所示的相似性和差异。萤火虫协议以实验性 DEC 萤火虫多处理器的形式命名。协议的替代名称基于它们支持的状态：M 修改，E 独家（私有清洁），共享，我无效，O 所有者（共享脏）。

saw an explosion in the development of alternative coherence protocols, and Archibald and Baer [1986] provided a good survey and analysis, as well as refer- ences to the original papers. [Figure M.2](#_bookmark912) summarizes several snooping cache coher- ence protocols and shows some multiprocessors that have used or are using that protocol.

> 在开发替代连贯协议的开发中爆炸了，Archibald and Baer [1986]提供了良好的调查和分析，并参考了原始论文。[图 M.2]（#_ bookmark912）总结了几个窥探缓存协调协议，并显示了一些使用或正在使用该协议的多处理器。

The early 1990s saw the beginning of an expansion of such systems with the use of very wide, high-speed buses (the SGI Challenge system used a 256-bit, packet-oriented bus supporting up to 8 processor boards and 32 processors) and later the use of multiple buses and crossbar interconnects—for example, in the Sun SPARCCenter and Enterprise systems (Charlesworth [1998] discussed the interconnect architecture of these multiprocessors). In 2001, the Sun Enterprise servers represented the primary example of large-scale (&gt;16 processors), symmet- ric multiprocessors in active use. Today, most bus-based machines offer only four or so processors and switches, or alternative designs are used for eight or more.

> 1990 年代初期，使用非常宽的高速公共汽车开始扩展此类系统（SGI 挑战系统使用了 256 位，面向数据包的公共汽车，最多支持 8 个处理器板和 32 个处理器），然后使用多个总线和横梁互连，例如，在 Sun Sparccenter 和 Enterprise Systems 中（Charlesworth [1998]讨论了这些多处理器的互连体系结构）。在 2001 年，Sun Enterprise 服务器代表了大规模（＆GT; 16 处理器）的主要示例，即在主动使用中对称的多处理器。如今，大多数基于巴士的机器仅提供四个左右的处理器和开关，或者使用八个或更多的替代设计。

### Toward Large-Scale Multiprocessors

> ###朝向大型多处理器

In the effort to build large-scale multiprocessors, two different directions were explored: message-passing multicomputers and scalable shared-memory multipro- cessors. Although there had been many attempts to build mesh and hypercube- connected multiprocessors, one of the first multiprocessors to successfully bring together all the pieces was the Cosmic Cube built at Caltech [Seitz 1985]. It intro- duced important advances in routing and interconnect technology and substantially reduced the cost of the interconnect, which helped make the multicomputer viable. The Intel iPSC 860, a hypercube-connected collection of i860s, was based on these ideas. More recent multiprocessors, such as the Intel Paragon, have used networks with lower dimensionality and higher individual links. The Paragon also employed a separate i860 as a communications controller in each node, although a number of users have found it better to use both i860 processors for computation as well as communication. The Thinking Multiprocessors CM-5 made use of off-the-shelf microprocessors and a fat tree interconnect (see Appendix F). It provided user- level access to the communication channel, thus significantly improving commu- nication latency. In 1995, these two multiprocessors represented the state of the art in message-passing multicomputers.

> 为了构建大规模多处理器，探索了两个不同的方向：通过消息传播的多机计算机和可扩展的共享内存多处理器。尽管有许多尝试建立网格和超立方体连接的多处理器，但最早成功地将所有碎片汇集在一起的多处理器之一是加州理工学院建造的宇宙立方体[Seitz 1985]。它介绍了路由和互连技术方面的重要进展，并大大降低了互连的成本，这有助于使多机构可行。Intel IPSC 860 是 I860 的 HyperCube 连接系列，是基于这些想法的。最近的多处理器，例如英特尔·帕拉贡（Intel Paragon），使用了具有较低维度和较高单个链接的网络。Paragon 在每个节点中还使用单独的 i860 作为通信控制器，尽管许多用户发现使用两个 i860 处理器进行计算和通信更好。思维多处理器 CM-5 使用了现成的微处理器和胖树互连（请参阅附录 F）。它提供了对通信渠道的用户级别访问，从而显着改善了通信延迟。1995 年，这两位多处理器以消息传播的多机构代表了最新技术。

Early attempts at building a scalable shared-memory multiprocessor include the IBM RP3 [Pfister et al. 1985], the NYU Ultracomputer [Elder et al. 1985; Schwartz 1980], the University of Illinois Cedar project [Gajksi et al. 1983], and the BBN Butterfly and Monarch [BBN Laboratories 1986; Rettberg et al. 1990]. These multiprocessors all provided variations on a nonuniform distributed-memory model and, hence, are distributed shared-memory (DSM) multiprocessors, but they did not support cache coherence, which substantially complicated programming. The RP3 and Ultracomputer projects both explored new ideas in synchronization (fetch-and-operate) as well as the idea of combining references in the network. In all four multiprocessors, the interconnect networks turned out to be more costly than the processing nodes, raising problems for smaller versions of the multiprocessor. The Cray T3D/E (see Arpaci et al. [1995] for an evaluation of the T3D and Scott [1996] for a description of the T3E enhancements) builds on these ideas, using a noncoherent shared address space but building on the advances in interconnect technology developed in the multicomputer domain (see Scott and Thorson [1996]).

> 早期尝试构建可扩展的共享内存多处理器包括 IBM RP3 [Pfister 等。1985]，纽约大学超计算机[Elder 等。1985;Schwartz 1980]，伊利诺伊大学雪松项目[Gajksi 等。1983]和 BBN 蝴蝶和君主[BBN 实验室 1986;Rettberg 等。1990]。这些多处理器都提供了在非均匀分布式内存模型上的变体，因此是分布式共享记忆（DSM）多处理器的变体，但它们不支持 CACHE 相干性，而 Cache 相干性基本上是复杂的。RP3 和 Ultracomputer 项目既探索了同步（获取和操作）的新想法，也探讨了在网络中结合参考的想法。在所有四个多处理器中，互连网络都比处理节点更为昂贵，这为多处理器的较小版本引起了问题。Cray T3D/E（请参阅 Arpaci 等[1995]有关 T3D 和 Scott [1996]的评估，以进行 T3E 增强功能的描述）建立在这些思想的基础上，使用非合并的共享地址空间，但建立在基础上的进步在多机构域中开发的互连技术（参见 Scott 和 Thorson [1996]）。

Extending the shared-memory model with scalable cache coherence was done by combining a number of ideas. Directory-based techniques for cache coherence were actually known before snooping cache techniques. In fact, the first cache coherence protocols actually used directories, as described by Tang [1976] and implemented in the IBM 3081. Censier and Feautrier [1978] described a directory coherence scheme with tags in memory. The idea of distributing directories with the memories to obtain a scalable implementation of cache coherence was first described by Agarwal et al. [1988] and served as the basis for the Stanford DASH multiprocessor (see Lenoski et al. [1990, 1992]), which was the first operational cache-coherent DSM multiprocessor. DASH was a “plump” node cc-NUMA machine that used four-processor SMPs as its nodes, interconnecting them in a style similar to that of Wildfire but using a more scalable two-dimensional grid rather than a crossbar for the interconnect.

> 通过组合许多想法，可以将共享内存模型扩展使用可扩展的缓存连贯性。在窥探缓存技术之前，基于目录的基于直接的高速缓存连贯技术。实际上，如 Tang [1976]所述，第一个缓存相干协议实际使用了目录，并在 IBM 3081 中实现。Censierand Feautrier [1978]描述了具有内存中标签的目录相干方案。Agarwal 等人首先描述了用记忆分配目录以获得可扩展的缓存相干实现的想法。[1988]并作为斯坦福破折号多处理器的基础（参见 Lenoski 等人[1990，1992]），这是第一个操作缓存 DSM 多处理器。DASH 是一种“丰满”节点 CC-NUMA 机器，它使用四处理器 SMP 作为其节点，以类似于野火的样式互连，但使用更可扩展的二维网格，而不是用于互连的横杆。

The Kendall Square Research KSR-1 [Burkhardt et al. 1992] was the first com- mercial implementation of scalable coherent shared memory. It extended the basic DSM approach to implement a concept called _cache-only memory architecture_ (COMA), which makes the main memory a cache. In the KSR-1, memory blocks could be replicated in the main memories of each node with hardware support to handle the additional coherence requirements for these replicated blocks. (The KSR-1 was not strictly a pure COMA because it did not migrate the home location of a data item but always kept a copy at home. Essentially, it implemented only replication.) Many other research proposals [Falsafi and Wood 1997; Hagersten, Landin, and Har- idi 1992; Saulsbury et al. 1995; Stenstr€om, Joe, and Gupta 1992] for COMA-style architectures and similar approaches that reduce the burden of nonuniform memory access through migration [Chandra et al. 1994; Soundararajan et al. 1998] were devel- oped, but there have been no further commercial implementations.

> 肯德尔广场研究 KSR-1 [Burkhardt 等。[1992]是可扩展相干共享内存的第一个商业实施。它扩展了基本的 DSM 方法，以实现一个称为_ Cache-仅 CACHE-MEMORY ARTECTERTER_（COMA）的概念，该概念使主内存成为缓存。在 KSR-1 中，可以在每个节点的主要记忆中复制内存块，并具有硬件支持，以处理这些重复的块的其他连贯要求。（KSR-1 并不是严格的昏迷，因为它没有迁移数据项的家用位置，而是始终保存在家中。本质上，它仅实施复制。）许多其他研究建议[Falsafi and Wood 1997;Hagersten，Landin 和 Haridi 1992;Saulsbury 等。1995;Stenstr€Om，Joe 和 Gupta 1992]用于昏迷风格的体系结构和类似的方法，从而减轻了通过移民的不一致记忆的负担[Chandra 等。1994;Soundararajan 等。[1998]是开发的，但没有进一步的商业实施。

The Convex Exemplar implemented scalable coherent shared memory using a two-level architecture: At the lowest level, eight-processor modules are built using a crossbar. A ring can then connect up to 32 of these modules, for a total of 256 processors (see Thekkath et al. [1997] for an evaluation). Laudon and Lenoski [1997] described the SGI Origin, which was first delivered in 1996 and is closely based on the original Stanford DASH machine, although including a number of innovations for scalability and ease of programming. Origin uses a bit vector for the directory structure, which is either 16 or 32 bits long. Each bit represents a node, which consists of two processors; a coarse bit vector representation allows each bit to represent up to 8 nodes for a total of 1024 processors. As Galles [1996] described, a high-performance fat hypercube is used for the global interconnect. Hristea, Lenoski, and Keen [1997] have provided a thorough evaluation of the performance of the Origin memory system.

> 凸范例使用两级体系结构实现了可扩展的共享内存：在最低级别，使用横梁构建了八处理器模块。然后，一个环可以连接到其中 32 个模块，总共 256 个处理器（有关评估，请参见 Thekkath 等人[1997]）。Laudon 和 Lenoski [1997]描述了 SGI Origin，该 SGI 起源于 1996 年首次交付，并基于原始的 Stanford Dash Machine，尽管包括许多创新，以进行可扩展性和易于编程。Origin 使用一个位矢量来实现目录结构，该目录结构长 16 或 32 位。每个位代表一个节点，该节点由两个处理器组成。粗糙的位矢量表示允许每个位最多表示 8 个节点，总计 1024 个处理器。正如 Galles [1996]所述，全球互连使用高性能脂肪超立方体。Hristea，Lenoski 和 Keen [1997]对原始记忆系统的性能进行了彻底的评估。

Several research prototypes were undertaken to explore scalable coherence with and without multithreading. These include the MIT Alewife machine [Agarwal et al. 1995] and the Stanford FLASH multiprocessor [Gibson et al. 2000; Kuskin et al. 1994].

> 进行了几种研究原型，以探索有或没有多线程的可扩展相干性。其中包括 MIT Alewife 机器[Agarwal 等。[1995]和斯坦福闪光多处理器[Gibson 等。2000;Kuskin 等。1994]。

### Clusters

> ###集群

Clusters were probably “invented” in the 1960s by customers who could not fit all their work on one computer or who needed a backup machine in case of failure of the primary machine [Pfister 1998]. Tandem introduced a 16-node cluster in 1975. Digital followed with VAX clusters, introduced in 1984. They were originally independent computers that shared I/O devices, requiring a distributed operating system to coordinate activity. Soon they had communication links between com- puters, in part so that the computers could be geographically distributed to increase availability in case of a disaster at a single site. Users log onto the cluster and are unaware of which machine they are running on. DEC (now HP) sold more than 25,000 clusters by 1993. Other early companies were Tandem (now HP) and IBM (still IBM). Today, virtually every company has cluster products. Most of these products are aimed at availability, with performance scaling as a secondary benefit.

> 群集可能是在 1960 年代被“发明”的，这些客户无法将所有工作适合一台计算机或在主机失败时需要备份机器的所有工作[Pfister 1998]。Tandem 在 1975 年引入了 16 个节点群集。数字簇遵循 1984 年推出的 VAX 簇。它们最初是共享 I/O 设备的独立计算机，需要分布式操作系统以协调活动。很快，他们在计算机之间就具有通信链接，部分原因是在单个网站上发生灾难的情况下，计算机可以在地理上分配以增加可用性。用户登录群集，并且不知道他们正在运行的机器。到 1993 年，12 月（现已惠普）出售了 25,000 多个集群。其他早期公司是 Tandem（现为 HP）和 IBM（仍然是 IBM）。今天，几乎每个公司都有集群产品。这些产品中的大多数均针对可用性，并且性能缩放是次要的好处。

Scientific computing on clusters emerged as a competitor to MPPs. In 1993, the Beowulf project started with the goal of fulfilling NASA’s desire for a 1 GFLOPS computer for under $50,000. In 1994, a 16-node cluster built from off-the-shelf

> 集群的科学计算成为 MPP 的竞争对手。1993 年，Beowulf 项目的目标是满足 NASA 对 1 GFLOPS 计算机的渴望，价格低于 50,000 美元。1994 年，一个由现成的 16 节点集群

PCs using 80486s achieved that goal [Bell and Gray 2001]. This emphasis led to a variety of software interfaces to make it easier to submit, coordinate, and debug large programs or a large number of independent programs.

> 使用 80486 的 PC 实现了该目标[Bell and Gray 2001]。这种强调导致了各种软件界面，使提交，协调和调试大型程序或大量独立程序变得更加容易。

Efforts were made to reduce latency of communication in clusters as well as to increase bandwidth, and several research projects worked on that problem. (One commercial result of the low-latency research was the VI interface standard, which has been embraced by Infiniband, discussed below.) Low latency then proved useful in other applications. For example, in 1997 a cluster of 100 Ultra- SPARC desktop computers at the University of California–Berkeley, connected by 160 MB/sec per link Myrinet switches, was used to set world records in database sort—sorting 8.6 GB of data originally on disk in 1 minute—and in cracking an encrypted message—taking just 3.5 hours to decipher a 40-bit DES key.

> 努力减少群集中的沟通延迟以及增加带宽，几项研究项目在这一问题上发挥了作用。（低延迟研究的一个商业结果是 Infiniband 所接受的 VI 接口标准，下面讨论了。）当时，低潜伏期在其他应用中被证明是有用的。例如，在 1997 年，加利福尼亚大学 - 伯克利分校的 100 个超级台式计算机集群，通过链接 myrinet 交换机连接 160 mb/sec，用于在数据库中设置世界记录，以 8.6 GB 的数据为单位，最初是在 8.6 GB 的数据上在 1 分钟内磁盘（并在打开加密消息时）仅传到 3.5 个小时的时间才能破译 40 位 DES KEY。

This research project, called Network of Workstations [Anderson, Culler, and Patterson 1995], also developed the Inktomi search engine, which led to a startup company with the same name. Google followed the example of Inktomi to build search engines from clusters of desktop computers rather large-scale SMPs, which was the strategy of the leading search engine Alta Vista that Google overtook [Brin and Page 1998]. In 2011, nearly all Internet services rely on clusters to serve their millions of customers.

> 该研究项目称为工作站网络[Anderson，Culler 和 Patterson 1995]，也开发了 Inktomi 搜索引擎，该引擎导致了一家具有相同名称的初创公司。Google 遵循 Inktomi 的示例，从台式计算机的群集中构建搜索引擎，这是 Google 越过领先的搜索引擎 Alta Vista 的策略[Brin and Page 1998]。在 2011 年，几乎所有的互联网服务都依靠群集为数百万客户提供服务。

Clusters are also very popular with scientists. One reason is their low cost, so individual scientists or small groups can own a cluster dedicated to their programs. Such clusters can get results faster than waiting in the long job queues of the shared MPPs at supercomputer centers, which can stretch to weeks. For those interested in learning more, Pfister [1998] wrote an entertaining book on clusters.

> 集群在科学家中也非常受欢迎。原因之一是他们的低成本，因此个人科学家或小组可以拥有专门针对其计划的集群。与等待超级计算机中心共享 MPP 的长期工作队列相比，这种群集的结果可以更快，后者可以延长数周。对于那些有兴趣了解更多信息的人，Pfister [1998]写了一本关于群集的娱乐性书。

##### _Recent Trends in Large-Scale Multiprocessors_

> ##### _大规模多处理器中的趋势

In the mid- to late 1990s, it became clear that the hoped for growth in the market for ultralarge-scale parallel computing was unlikely to occur. Without this market growth, it became increasingly clear that the high-end parallel computing market could not support the costs of highly customized hardware and software designed for a small market. Perhaps the most important trend to come out of this observa- tion was that clustering would be used to reach the highest levels of performance. There are now four general classes of large-scale multiprocessors:

> 在 1990 年代中期至后期，很明显，超级尺度平行计算市场上对市场增长的希望不太可能发生。没有这种市场增长，越来越清楚的是，高端平行计算市场无法支持为小市场设计的高度定制硬件和软件的成本。从这种观察中出现的最重要的趋势也许是将聚类用于达到最高水平的性能。现在有四个通用类的大规模多处理器：

- Clusters that integrate standard desktop motherboards using interconnection technology such as Myrinet or Infiniband.

> - 使用 Myrinet 或 Infiniband 等互连技术集成标准桌面主板的群集。

- Multicomputers built from standard microprocessors configured into proces- sing elements and connected with a custom interconnect. These include the Cray XT3 (which used an earlier version of Cray interconnect with a simple cluster architecture) and IBM Blue Gene (more on this unique machine momentarily).

> - 由标准微处理器构建的多机计算机，该标准微处理器配置为 proces-new 元素，并与自定义互连连接。其中包括 Cray XT3（使用了较早版本的 Cray 互连与简单的群集体系结构）和 IBM 蓝色基因（暂时使用这款独特的机器）。

- Clusters of small-scale shared-memory computers, possibly with vector support, which includes the Earth Simulator (which has its own journal available online).

> - 小型共享内存计算机群，可能具有矢量支持，其中包括“地球模拟器”（在线上有其自己的期刊）。

- Large-scale shared-memory multiprocessors, such as the Cray X1 [Dunigan et al. 2005] and SGI Origin and Altix systems. The SGI systems have also been configured into clusters to provide more than 512 processors, although only message passing is supported across the clusters.

> - 大规模共享记忆多处理器，例如 Cray X1 [Dunigan 等。2005]和 SGI Origin 和 Altix 系统。SGI 系统还已将其配置为簇，以提供超过 512 个处理器，尽管在整个群集中仅支持消息传递。

The IBM Blue Gene is the most interesting of these designs since its rationale par- allels the underlying causes of the recent trend toward multicore in uniprocessor architectures. Blue Gene started as a research project within IBM aimed at the pro- tein sequencing and folding problem. The Blue Gene designers observed that power was becoming an increasing concern in large-scale multiprocessors and that the performance/watt of processors from the embedded space was much better that those in the high-end uniprocessor space. If parallelism was the route to high performance, why not start with the most efficient building block and simply have more of them?

> IBM Blue 基因是这些设计中最有趣的基因，因为它的理由构成了近期趋势的基本原因，该趋势是 Uniprocessor 体系结构中多核的趋势。Blue Gene 最初是 IBM 中的研究项目，针对蛋白测序和折叠问题。蓝色基因设计师观察到，在大规模多处理器中，功率正变得越来越关注，并且来自嵌入式空间的处理器的性能/瓦特要比高端单局部空间中的那些空间的性能要好得多。如果并行性是高性能的途径，为什么不从最有效的构建块开始，而只是拥有更多呢？

Thus, Blue Gene is constructed using a custom chip that includes an embedded PowerPC microprocessor offering half the performance of a high-end PowerPC, but at a much smaller fraction of the area of power. This allows more system func- tions, including the global interconnect, to be integrated onto the same die. The result is a highly replicable and efficient building block, allowing Blue Gene to reach much larger processor counts more efficiently. Instead of using stand-alone microprocessors or standard desktop boards as building blocks, Blue Gene uses processor cores. There is no doubt that such an approach provides much greater efficiency. Whether the market can support the cost of a customized design and special software remains an open question.

> 因此，蓝色基因是使用定制芯片构建的，该芯片包括嵌入式 PowerPC 微处理器，提供高端 PowerPC 的性能的一半，但在动力区域的一部分小得多。这允许将更多的系统函数（包括全局互连）集成到同一模具上。结果是一个高度可复制和有效的构建块，使蓝色基因可以更有效地达到更大的处理器计数。蓝色基因不使用独立的微处理器或标准桌面板作为构建块，而是使用处理器内核。毫无疑问，这种方法可提供更大的效率。市场是否可以支持定制设计和特殊软件的成本仍然是一个悬而未决的问题。

In 2006, a Blue Gene processor at Lawrence Livermore with 32K processors (and scheduled to go to 65K in late 2005) holds a factor of 2.6 lead in Linpack performance over the third-place system consisting of 20 SGI Altix 512-processor systems interconnected with Infiniband as a cluster.

> 2006 年，劳伦斯·利弗莫尔（Lawrence Livermore）拥有 32K 处理器的蓝色基因处理器（计划在 2005 年末达到 65K），在 Linpack 性能中的第三名系统中，Linpack 性能为 2.6 倍，由 20 个 SGI Altix 512 512 512 Processor Systems 组成 Infiniband 作为群集。

Blue Gene’s predecessor was an experimental machine, QCDOD, which pio- neered the concept of a machine using a lower-power embedded microprocessor and tightly integrated interconnect to drive down the cost and power consumption of a node.

> Blue Gene 的前身是一台实验机 QCDOD，它使用低功率嵌入式微处理器和紧密整合的互连来降低节点的成本和功耗。

##### _Developments in Synchronization and Consistency Models_

> ##### _在同步和一致性模型中开发_

A wide variety of synchronization primitives have been proposed for shared- memory multiprocessors. Mellor-Crummey and Scott [1991] provided an over- view of the issues as well as efficient implementations of important primitives, such as locks and barriers. An extensive bibliography supplies references to other important contributions, including developments in spin locks, queuing locks, and barriers. Lamport [1979] introduced the concept of sequential consistency and what correct execution of parallel programs means. Dubois, Scheurich, and Briggs [1988] introduced the idea of weak ordering (originally in 1986). In 1990, Adve and Hill provided a better definition of weak ordering and also defined the concept of data-race-free; at the same conference, Gharachorloo and his colleagues [1990] introduced release consistency and provided the first data on the performance of relaxed consistency models. More relaxed consistency models have been widely adopted in microprocessor architectures, including the Sun SPARC, Alpha, and IA-64. Adve and Gharachorloo [1996] have provided an excellent tutorial on mem- ory consistency and the differences among these models.

> 已经为共享存储器多处理器提出了多种同步原始图。Mellor-Crummey 和 Scott [1991]对这些问题以及重要原始物（例如锁和障碍物）的有效实施提供了详尽的看法。广泛的书目向其他重要贡献提供了参考，包括旋转锁，排队锁和障碍的发展。Lamport [1979]介绍了顺序一致性的概念以及平行程序的正确执行含义。Dubois，Scheurich 和 Briggs [1988]介绍了弱排序的想法（最初于 1986 年）。在 1990 年，Adve 和 Hill 提供了更好的定义弱序定义，还定义了无数据竞赛的概念。在同一会议上，Gharachorloo 和他的同事[1990]引入了发布一致性，并提供了有关放松一致性模型的第一个数据。在微处理器架构中，包括 Sun Sparc，Alpha 和 IA-64 在内的微处理器结构中广泛采用了更轻松的一致性模型。Adve 和 Gharachorloo [1996]提供了有关 Mem-Ory 一致性以及这些模型之间差异的出色教程。

### Other References

> ###其他参考

The concept of using virtual memory to implement a shared address space among distinct machines was pioneered in Kai Li’s Ivy system in 1988. There have been sub- sequent papers exploring hardware support issues, software mechanisms, and pro- gramming issues. Amza et al. [1996] described a system built on workstations using a new consistency model, Kontothanassis et al. [1997] described a software shared- memory scheme using remote writes, and Erlichson et al. [1996] described the use of shared virtual memory to build large-scale multiprocessors using SMPs as nodes. Thereisanalmostunbounded amountofinformation onmultiprocessorsandmul- ticomputers: Conferences, journal papers, and even books seem to appear faster than any single person can absorb the ideas. No doubt many of these papers will go unno- ticed—not unlike the past. Most of the major architecture conferences contain papers on multiprocessors. An annual conference, Supercomputing _XY_ (where _X_ and _Y_ are the last two digits of the year), brings together users, architects, software developers, and vendors, and the proceedings are published in book, CD-ROM, and online (see [_www.scXY.org_](http://www.scXY.org/)) form. Two major journals, _Journal of Parallel and Distributed Com- puting_ and the _IEEE Transactions on Parallel and Distributed Systems_, contain papers on all aspects of parallel processing. Several books focusing on parallel pro- cessing are included in the following references, with Culler, Singh, and Gupta [1999] being the most recent, large-scale effort. For years, Eugene Miya of NASA Ames Research Center has collected an online bibliography of parallel-processing papers.

> 使用虚拟内存在不同的机器之间实现共享地址空间的概念是在 1988 年的 Kai Li 的 Ivy 系统中开创的。有一篇文章探讨了硬件支持问题，软件机制和程序性问题。Amza 等。[1996]描述了一种使用新的一致性模型建立在工作站的系统，Kontothanassis 等人。[1997]描述了使用远程写入的软件共享 - 内存方案，以及 Erlichson 等。[1996]描述了使用共享虚拟内存来构建使用 SMP 作为节点的大规模多处理器。最大程度地说明了最大的信息，即涉及大型计算机：会议，期刊论文，甚至书籍似乎比任何一个人都能吸收这些想法的速度更快。毫无疑问，这些论文中的许多论文都会毫无意义 - 与过去不同。大多数主要的体系结构会议都包含有关多处理器的论文。年度会议，超级计算_xy_（其中_x_和_y_是一年的最后两位数字），将用户，建筑师，软件开发人员和供应商汇集在一起，以及程序以书籍，CD-ROM 和在线发布（请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参阅[请参见_www.scxy.org_]（[www.scxy.org/](http://www.scxy.org/)））表格。两个主要期刊，分布式和分布式 compoting_和并行和分布式系统上的_ieee 交易，在并行处理的各个方面都包含论文。以下参考文献中包括了几本关注并行处理的书籍，Culler，Singh 和 Gupta [1999]是最近的大规模努力。多年来，NASA AMES 研究中心的 Eugene Miya 已收集了并行处理论文的在线书目。

The bibliography, which now contains more than 35,000 entries, is available online at [_liinwww.ira.uka.de/bibliography/Parallel/Eugene/index.html_](http://liinwww.ira.uka.de/bibliography/Parallel/Eugene/index.html).

> 该参考书目现在包含 35,000 多个条目，可在线提供，网址为[_liinwwww.ira.uka.de/bibliography/parallel/parallel/eugene/reugene/index.html_]/EEUGENE/index.html）。

In addition to documenting the discovery of concepts now used in practice, these references also provide descriptions of many ideas that have been explored and found wanting, as well as ideas whose time has just not yet come. Given the move toward multicore and multiprocessors as the future of high-performance computer architecture, we expect that many new approaches will be explored in the years ahead. A few of them will manage to solve the hardware and software problems that have been the key to using multiprocessing for the past 40 years!

> 除了记录现在在实践中使用的概念的发现外，这些参考文献还提供了许多已经探索和发现的想法的描述，以及时间尚未到来的想法。鉴于朝着多进取和多处理器作为高性能计算机体系结构的未来，我们希望在未来几年中探索许多新方法。他们中的一些人将设法解决过去 40 年来使用多处理的关键的硬件和软件问题！
